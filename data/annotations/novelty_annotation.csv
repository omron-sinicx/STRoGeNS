,text,label
0,"In this section, we first review CD formulae in a broader context, and then discuss normalizing flow-based models, which are core to the proposed CD metric.

**CD Formulae**. The CD assessment is necessary for day-to-day color control, and is indispensable for color matching in color industries. Admittedly, CD formulae have accelerated the instrumental pass/fail devices for color judgments, but much still needs to be done for complete satisfaction. The scientific investigation of perceptual CDs can be dated at least back to Young and Helmholtz, who proposed and developed the trichromatic theory of color, which is the foundation of the metameric color matching experiment. CIELAB [1] is one of the most successful CD metrics recommended by CIE in 1976, and has been widely adopted in industry for a long time. However, the CIELAB color space is not perceptually uniform [2], which motivates the development of CIE94 [3] and CIEDE2000  through the introduction of application-specific parameters. Other CIELAB-based CD metrics include JPC79 , BFD(\(l\):\(c\)) [4], and CMC(\(l\):\(c\)) [5]. The introduced parameters are primarily calibrated using uniformly colored patches, digital or printed, which are statistically and semantically different from photographic images. Thus, the generalization of these metrics to photographic images is somewhat limited, especially when misalignment due to geometric distortions is present.

To incorporate spatial context into CD assessment, Zhang and Wandell [6] presented S-CIELAB, which extends CIELAB by adding spatial low-pass filtering as preprocessing. Similarly, Ouni _et al_. [7] provided a spatial extension of CIEDE2000. Lee _et al_. [8] re-examined histogram intersection, which is widely used in color image index, for the purpose of color image similarity assessment. Hong and Luo [9] chose to give larger weights to areas with spatially homogeneous colors and pixels with larger CDs. This method was later augmented by spatial filtering . Lee and Plataniois [10] built upon the philosophy of color structural similarity, and gave the hue component careful treatment with circular statistics. Jaramillo _et al_.  grouped the same texture areas for human-like CDassessment, using local binary patterns as texture descriptors.

General-purpose image quality models, including full-reference ones - SSIM [11], VSI [12], LPIPS [13] and DISTS [14], reduced-reference ones - Wang05 [15] and Yu09 [16], and no-reference ones - BRISQUE [17], NIQE [18] and Gao13 [19] can be directly adopted for CD assessment, regarding CDs as a particular form of ""visual degradations."" Meanwhile, just-noticeable difference (JND) methods, _e.g._, Butteraugli [20] and 'HLIP , also attempt to characterize visually indistinguishable color changes between two images. In the era of deep learning, due to the lack of sufficient human-labeled training data, DNN-based CD formulae are rarely proposed. Wang _et al._[21] created the first largest image dataset, SPCL, for perceptual CD assessment, and made one of the first attempts to train a DNN-based CD measure for photographic images. However, the underlying feature transform is not mathematically bijective.

**Normalizing Flow-based Models**. Normalizing flow-based generative models are constructed by bijective functions \(f:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\), with typically easy-to-compute analytical inverse \(f^{-1}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\). The primary goal of \(f\) is to map raw data \(\mathbf{x}\) to samples \(\mathbf{z}=f(\mathbf{x})\) from a simple probability distribution \(p_{\mathcal{Z}}(\mathbf{z})\). Many classic machine learning algorithms can be cast in this framework, such as principal component analysis (PCA, where \(f\) is a linear transform and \(p_{\mathcal{Z}}(\mathbf{z})\) is standard Gaussian) and independent component analysis (ICA, where \(f\) is again linear and \(p_{\mathcal{Z}}\) is factorized and heavy-tailed).

In 2014, Dinh _et al._[22] proposed non-linear independent component estimation (NICE), as a generalization of ICA. NICE is considered the first normalizing flow with the introduction of the _additive coupling_ to ease the calculation of the Jacobian determinant. To make flow-based models more suitable for image-related tasks, Dinh _et al._[23] extended NICE to RealNVP, which admits a _multi-scale autoregressive_ architecture, implemented by _squeezing_ and _affine coupling_. Kingma and Dhariwal [24] introduced the _invertible_\(1\times 1\)_convolution_ (_i.e._, the linear transform in PCA) to replace the fixed random permutation for splitting the channel dimension during multi-scale processing. The batch normalization in RealNVP is also replaced with activation normalization (_i.e._, _actnorm_). To allow unconstrained architectural design, Grathwohl _et al._[25] leveraged the Hutchinson's trace estimator for scalable and unbiased estimation of the log-density. Similarly, Behrmann _et al._[26] proposed invertible residual networks (i-ResNet), introducing a tractable estimation to the Jacobian log-determinant of a residual block. Other representative normalizing flow work includes hierarchical recursive coupling [27] for increasing flow expressiveness, Wavelet Flow [28] for scaling flow to ultra-high dimensional data, and Discrete Flow [29] for discrete data modeling. In this paper, we do not use normalizing flow for generative modeling, but for invertible feature transform.",1
1,"In this section, we first review CD formulae in a broader context, and then discuss normalizing flow-based models, which are core to the proposed CD metric.

**CD Formulae**. The CD assessment is necessary for day-to-day color control, and is indispensable for color matching in color industries. Admittedly, CD formulae have accelerated the instrumental pass/fail devices for color judgments, but much still needs to be done for complete satisfaction. The scientific investigation of perceptual CDs can be dated at least back to Young and Helmholtz, who proposed and developed the trichromatic theory of color, which is the foundation of the metameric color matching experiment. CIELAB [1] is one of the most successful CD metrics recommended by CIE in 1976, and has been widely adopted in industry for a long time. However, the CIELAB color space is not perceptually uniform [2], which motivates the development of CIE94 [3] and CIEDE2000  through the introduction of application-specific parameters. Other CIELAB-based CD metrics include JPC79 , BFD(\(l\):\(c\)) [4], and CMC(\(l\):\(c\)) [5]. The introduced parameters are primarily calibrated using uniformly colored patches, digital or printed, which are statistically and semantically different from photographic images. Thus, the generalization of these metrics to photographic images is somewhat limited, especially when misalignment due to geometric distortions is present.

To incorporate spatial context into CD assessment, Zhang and Wandell [6] presented S-CIELAB, which extends CIELAB by adding spatial low-pass filtering as preprocessing. Similarly, Ouni _et al_. [7] provided a spatial extension of CIEDE2000. Lee _et al_. [8] re-examined histogram intersection, which is widely used in color image index, for the purpose of color image similarity assessment. Hong and Luo [9] chose to give larger weights to areas with spatially homogeneous colors and pixels with larger CDs. This method was later augmented by spatial filtering . Lee and Plataniois [10] built upon the philosophy of color structural similarity, and gave the hue component careful treatment with circular statistics. Jaramillo _et al_.  grouped the same texture areas for human-like CDassessment, using local binary patterns as texture descriptors.

General-purpose image quality models, including full-reference ones - SSIM [11], VSI [12], LPIPS [13] and DISTS [14], reduced-reference ones - Wang05 [15] and Yu09 [16], and no-reference ones - BRISQUE [17], NIQE [18] and Gao13 [19] can be directly adopted for CD assessment, regarding CDs as a particular form of ""visual degradations."" Meanwhile, just-noticeable difference (JND) methods, _e.g._, Butteraugli [20] and 'HLIP , also attempt to characterize visually indistinguishable color changes between two images. In the era of deep learning, due to the lack of sufficient human-labeled training data, DNN-based CD formulae are rarely proposed. Wang _et al._[21] created the first largest image dataset, SPCL, for perceptual CD assessment, and made one of the first attempts to train a DNN-based CD measure for photographic images. However, the underlying feature transform is not mathematically bijective.

**Normalizing Flow-based Models**. Normalizing flow-based generative models are constructed by bijective functions \(f:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\), with typically easy-to-compute analytical inverse \(f^{-1}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\). The primary goal of \(f\) is to map raw data \(\mathbf{x}\) to samples \(\mathbf{z}=f(\mathbf{x})\) from a simple probability distribution \(p_{\mathcal{Z}}(\mathbf{z})\). Many classic machine learning algorithms can be cast in this framework, such as principal component analysis (PCA, where \(f\) is a linear transform and \(p_{\mathcal{Z}}(\mathbf{z})\) is standard Gaussian) and independent component analysis (ICA, where \(f\) is again linear and \(p_{\mathcal{Z}}\) is factorized and heavy-tailed).

In 2014, Dinh _et al._[22] proposed non-linear independent component estimation (NICE), as a generalization of ICA. NICE is considered the first normalizing flow with the introduction of the _additive coupling_ to ease the calculation of the Jacobian determinant. To make flow-based models more suitable for image-related tasks, Dinh _et al._[23] extended NICE to RealNVP, which admits a _multi-scale autoregressive_ architecture, implemented by _squeezing_ and _affine coupling_. Kingma and Dhariwal [24] introduced the _invertible_\(1\times 1\)_convolution_ (_i.e._, the linear transform in PCA) to replace the fixed random permutation for splitting the channel dimension during multi-scale processing. The batch normalization in RealNVP is also replaced with activation normalization (_i.e._, _actnorm_). To allow unconstrained architectural design, Grathwohl _et al._[25] leveraged the Hutchinson's trace estimator for scalable and unbiased estimation of the log-density. Similarly, Behrmann _et al._[26] proposed invertible residual networks (i-ResNet), introducing a tractable estimation to the Jacobian log-determinant of a residual block. Other representative normalizing flow work includes hierarchical recursive coupling [27] for increasing flow expressiveness, Wavelet Flow [28] for scaling flow to ultra-high dimensional data, and Discrete Flow [29] for discrete data modeling. ",0
2," Despite the surprising zero-shot performance of PLMs, recent works show that ICL can bring the performance to the next level. Augmenting PLMs with ICL achieves SOTA results on a wide range of NLP tasks, ranging from question answering , information retrieval [1], math word problem , commonsense reasoning , and fact checking [2] etc. The instability of ICL, however, has encouraged researchers to explore methods that search for robust and high-performing prompts. These methods can be categorized as follows based on the target of searching/optimization:

Template searchfocuses on searching for the template that can guide PLM's behavior and steer its best performance. Great advances have been made in template searching using various methods: PLMs , heuristic rules ; [7]; [8]; [3], reinforcement learning , genetic algorithms [6], or by hands [4]; [5]. Nonetheless, all these methods require a high-quality validation set to do prompt selection or optimization. Unlike them, our framework does not require a validation set.

When the validation set is not available, researchers propose to search prompts using entropy [9] or mutual information [4]. It's worth mentioning that these two works and all aforementioned methods search at the _corpus-level_: they pick the best-performing template with or without a validation set and then equally apply this template to all test examples during inference. However, corpus-level methods might be sub-optimal. If we consider the _No Free Lunch Theorem_, finding one single template that works well for all testing examples is nearly impossible.

In-context example search,unlike template search, is rarely explored in the literature despite that they also have a huge impact on ICL performance ([5]; [9]). [9] first propose a learning-free corpus-level method for in-context example search. However, they only consider an impractical setting with only 4 examples and their 24 permutations (\({}^{4}P_{4}=4!=24\)). [10] find examples that are semantically similar to a test sample can serve as a good choice for its in-context examples. However, the reason why such a simple heuristic works is unclear. [12] extend this nearest neighbor search and further take the diversity of examples into consideration. Inspired by these methods, recent studies propose to learn to retrieve in-context examples ([11]).

",1
3," Despite the surprising zero-shot performance of PLMs, recent works show that ICL can bring the performance to the next level. Augmenting PLMs with ICL achieves SOTA results on a wide range of NLP tasks, ranging from question answering , information retrieval [1], math word problem , commonsense reasoning , and fact checking [2] etc. The instability of ICL, however, has encouraged researchers to explore methods that search for robust and high-performing prompts. These methods can be categorized as follows based on the target of searching/optimization:

Template searchfocuses on searching for the template that can guide PLM's behavior and steer its best performance. Great advances have been made in template searching using various methods: PLMs , heuristic rules ; [7]; [8]; [3], reinforcement learning , genetic algorithms [6], or by hands [4]; [5]. Nonetheless, all these methods require a high-quality validation set to do prompt selection or optimization. 

When the validation set is not available, researchers propose to search prompts using entropy [9] or mutual information [4]. It's worth mentioning that these two works and all aforementioned methods search at the _corpus-level_: they pick the best-performing template with or without a validation set and then equally apply this template to all test examples during inference. However, corpus-level methods might be sub-optimal. 

In-context example search,unlike template search, is rarely explored in the literature despite that they also have a huge impact on ICL performance ([5]; [9]). [9] first propose a learning-free corpus-level method for in-context example search. However, they only consider an impractical setting with only 4 examples and their 24 permutations (\({}^{4}P_{4}=4!=24\)). [10] find examples that are semantically similar to a test sample can serve as a good choice for its in-context examples. However, the reason why such a simple heuristic works is unclear. [12] extend this nearest neighbor search and further take the diversity of examples into consideration. Inspired by these methods, recent studies propose to learn to retrieve in-context examples ([11]).

",0
4," **Entropy based method.** Nunes and Demiris [1] proposed the entropy minimization framework (EMin) to obviate the need for an explicit intermediate image-based representation. They estimate the motion transformations by minimizing the dispersion of events, measured via a family of entropy functions. Since the entropy function has quadratic complexity, they extended EMin with an approximated version (AEMin) that searches events within a certain distance and an online version (IncEMin [2]) that incrementally estimates the motion parameters, achieving real-time rotational motion estimation. In contrast, our TS loss obviates the pair-wise calculation of events, showing a much lower computational complexity than the entropy loss. In addition, our approach is capable of conducting real-time estimations with sampled events and exhibiting superior accuracy compared to IncEMin.

**3D Points based method.** Liu _et al._[3] considered events as 3D points in the spatio-temporal space. They proposed a spatio-temporal registration algorithm (STR) that estimates rotational motion transformations by splitting events into early and late parts and registering events one-by-one based on trimmed iterative closest points [4]. STR requires costly computation with a nearest-neighbor search strategy. In addition, it is prone to incorrect registrations with noise events. In our work, we handle the data association problem implicitly and we apply a denoising operation on the TS map to reduce the influence of the noise.

**Image based method.** To combine events with well-established frame-based vision tools, some works [5][6][7][8] chose to transform events into an image-based representation. Gallego _et al._[7][9] proposed the Contrast Maximization (CMax) framework, which accumulates events to produce an image of warped events and maximizes the image contrast with respect to the motion parameters. Cheng _et al._[8] developed a spatio-temporal Poisson point process (ST-PPP) that aligns events through a maximum likelihood approach. The hyper-parameter \(\lambda\) of the Poisson pro cess is environment-specific and requires re-measurement when switching scenes or event cameras. Mitrokhin [6] proposed a method to estimate similarity transformations with metrics in the time-image, where they minimize a time-image loss (, the sum of gradients of the time-image) with respect to the motion parameters. In [10][11], the CMax framework was evaluated with twenty more image-based loss metrics. The contrast loss [7] achieves the best performance in terms of accuracy and efficiency, while the time-image loss [6] reports the lowest accuracy. A distinct difference between our approach and [6] is that our approach constructs the TS map with the minimum timestamps of the events warped at the same pixel. Thus not all the event timestamps are involved in optimization.

",1
5," **Entropy based method.** Nunes and Demiris [1] proposed the entropy minimization framework (EMin) to obviate the need for an explicit intermediate image-based representation. They estimate the motion transformations by minimizing the dispersion of events, measured via a family of entropy functions. Since the entropy function has quadratic complexity, they extended EMin with an approximated version (AEMin) that searches events within a certain distance and an online version (IncEMin [2]) that incrementally estimates the motion parameters, achieving real-time rotational motion estimation. 

**3D Points based method.** Liu _et al._[3] considered events as 3D points in the spatio-temporal space. They proposed a spatio-temporal registration algorithm (STR) that estimates rotational motion transformations by splitting events into early and late parts and registering events one-by-one based on trimmed iterative closest points [4]. STR requires costly computation with a nearest-neighbor search strategy. In addition, it is prone to incorrect registrations with noise events. 

**Image based method.** To combine events with well-established frame-based vision tools, some works [5][6][7][8] chose to transform events into an image-based representation. Gallego _et al._[7][9] proposed the Contrast Maximization (CMax) framework, which accumulates events to produce an image of warped events and maximizes the image contrast with respect to the motion parameters. Cheng _et al._[8] developed a spatio-temporal Poisson point process (ST-PPP) that aligns events through a maximum likelihood approach. The hyper-parameter \(\lambda\) of the Poisson pro cess is environment-specific and requires re-measurement when switching scenes or event cameras. Mitrokhin [6] proposed a method to estimate similarity transformations with metrics in the time-image, where they minimize a time-image loss (, the sum of gradients of the time-image) with respect to the motion parameters. In [10][11], the CMax framework was evaluated with twenty more image-based loss metrics. The contrast loss [7] achieves the best performance in terms of accuracy and efficiency, while the time-image loss [6] reports the lowest accuracy. A distinct difference between our approach and [6] is that our approach constructs the TS map with the minimum timestamps of the events warped at the same pixel. Thus not all the event timestamps are involved in optimization.

",0
6," **Prompt Tuning.** Prompt tuning is first introduced in the NLP area [1] to close the gap between pre-training and downstream tasks. Petroni et al. [2] manually create cloze-style prompts to elicit knowledge from pre-trained language models in a ""fill-in-the-blank"" way. Further, prompt tuning is introduced in vision-language understanding , which can enhance the generalizability of large vision-language models [3][4][5] on a wide range of vision-language understanding tasks [6][7][8][9][10][11][12][13][14]. As manually designing suitable prompts for different tasks is time-consuming and usually sub-optimal, recent works [15][16] propose to optimize a set of continuous learnable prompt embeddings. Concretely, CoOp  optimizes continuous prompt embeddings to improve the few-shot generalizability of CLIP. CoCoOp [17] proposes to learn image-conditioned prompts to further improve the generalizability of CoOp. ProDA [18] learns a distribution of diverse prompts via Gaussian distribution to handle the varying visual representations. To further enhance CLIP's adaption capability, Tip-Adapter builds a key-value cache model from the few-shot training samples to perform feature retrieval. Instead of designing a specific prompt tuning method, we propose a model-agnostic meta-prompt learning framework to improve the adaptation ability and cross-domain generalizability of the prompt tuning methods, which can be incorporated into existing methods in a plug-and-play fashion.

**Meta-Learning.** Meta-learning aims to enable efficient adaptation ability of models by leveraging the experience from learning across a set of tasks. Meta-learning approaches can be categorized as: metric-based [19][20][21], memory-based [22][23][24], and optimization-based [25][26][27]. Our framework is based on the optimization-based method (_i.e._, MAML [26]). Rather than relying on human-annotated meta-training tasks, our method can automatically generate a diverse set of meta-training tasks by cross-modal hierarchical clustering. Li et al. [28] propose to synthesize domain shift during meta-training to learn a domain-generalizable initialization. Differently, we present a novel gradient regulating function that actively transforms the updated gradient into a domain-generalizable direction.

",1
7," **Prompt Tuning.** Prompt tuning is first introduced in the NLP area [1] to close the gap between pre-training and downstream tasks. Petroni et al. [2] manually create cloze-style prompts to elicit knowledge from pre-trained language models in a ""fill-in-the-blank"" way. Further, prompt tuning is introduced in vision-language understanding , which can enhance the generalizability of large vision-language models [3][4][5] on a wide range of vision-language understanding tasks [6][7][8][9][10][11][12][13][14]. As manually designing suitable prompts for different tasks is time-consuming and usually sub-optimal, recent works [15][16] propose to optimize a set of continuous learnable prompt embeddings. Concretely, CoOp  optimizes continuous prompt embeddings to improve the few-shot generalizability of CLIP. CoCoOp [17] proposes to learn image-conditioned prompts to further improve the generalizability of CoOp. ProDA [18] learns a distribution of diverse prompts via Gaussian distribution to handle the varying visual representations. 

**Meta-Learning.** Meta-learning aims to enable efficient adaptation ability of models by leveraging the experience from learning across a set of tasks. Meta-learning approaches can be categorized as: metric-based [19][20][21], memory-based [22][23][24], and optimization-based [25][26][27]. Our framework is based on the optimization-based method (_i.e._, MAML [26]). Rather than relying on human-annotated meta-training tasks, our method can automatically generate a diverse set of meta-training tasks by cross-modal hierarchical clustering. Li et al. [28] propose to synthesize domain shift during meta-training to learn a domain-generalizable initialization. 

",0
8," **Domain Generalization (DG).** DG typically involves training models on single or multiple labeled data sources to generalize well to novel test time data sources (unseen during training). Several approaches have been proposed to tackle domain generalization [1][2], such as decomposing a model into domain invariant and specific components and utilizing the former to make predictions [3], learning domain specific masks for generalization , using meta-learning to train a robust model [4][5][6][7], manipulating feature statistics to augment training data [8][9][10], and using models crafted based on risk minimization formalisms [11]. More recently, properly tuned ERMs (Empirical Risk Minimization) have proven to be a competitive DG approach [12], with follow-up work adopting various optimization and regularization techniques [13][14] on top.

**Single Domain Generalization (SDG).** Unlike DG which leverages diversity across multiple sources for better generalization, SDG considers generalizing from a single source. Notable approaches for SDG use meta-learning [15] by considering strongly augmented source images as meta-target data (by exposing the model to increasingly distinct augmented views of the source data [16][17]) and learning feature normalization schemes with auxiliary objectives [18].

**Synthetic-to-Real Generalization (Syn-to-Real).** Prior work on syn-to-real generalization has mostly focused on some specific methods, including learning feature normalization / whitening schemes [19], using external data for style injection [20][21], explicitly optimizing for robustness [22], leveraging strong augmentations / domain randomization [23][21], consistency objectives [24] and using contrastive techniques to aid generalization [25]. Some approaches have also considered adapting from synthetic to real images, using techniques such as adversarial training [26], adversarial alignment losses [27], balancing transferability and discriminability [28] and feature alignment [29]. Pasta is more similar to the kind of methods adopting augmentations for improving out-of-the-box generalization We consider \(3\) of the most commonly studied syn-to-real generalization settings - (1) Semantic Segmentation - GTAV \(\rightarrow\)Real, (2) Object Detection - Sim10K [30]\(\rightarrow\)Real and (3) Object Recognition - VisDA-C [31] Syn\(\rightarrow\)Real. [32] recently proposed tailoring synthetic data for better generalization.

**Fourier Generalization & Adaptation Methods.** Prior work that explored augmenting images in the Fourier domain (as opposed to the pixel space) rely on a key empirical observation [33][34][35][36] that the phase component of the Fourier spectrum tends to preserve high-level semantics, and therefore, they focused mostly on perturbing the amplitude. Pasta is in line with this style of approach. Amplitude Jitter (AJ) [37] and Amplitude Mixup (AM) [37] are methods similar to Pasta that augment images by perturbing their amplitude spectra. While AM mixes the amplitude spectra of different images, AJ applies uniform perturbation with a single jitter value \(\epsilon\). FSDR [38], on the other hand, isolates domain variant and invariant frequency components using extra data and sets up a learning paradigm. Building on top of [37][39] adds a significance mask when linearly in terpolating amplitudes. [40] only perturbs image-frequency components that capture little semantic information. [41] uses an encoder-decoder to obtain high/low frequency features and augments images by adding noise to high frequency phase and low frequency amplitude. [42][43] study how amplitude and phase perturbations impact robustness to natural corruptions [44]. In contrast to these works, Pasta's simple strategy of perturbing amplitude spectra in a structured way and leads to strong out-of-the-box generalization without the need for specialized components, extra data, task-specific design, or changes to learning rules.

",1
9," **Domain Generalization (DG).** DG typically involves training models on single or multiple labeled data sources to generalize well to novel test time data sources (unseen during training). Several approaches have been proposed to tackle domain generalization [1][2], such as decomposing a model into domain invariant and specific components and utilizing the former to make predictions [3], learning domain specific masks for generalization , using meta-learning to train a robust model [4][5][6][7], manipulating feature statistics to augment training data [8][9][10], and using models crafted based on risk minimization formalisms [11]. More recently, properly tuned ERMs (Empirical Risk Minimization) have proven to be a competitive DG approach [12], with follow-up work adopting various optimization and regularization techniques [13][14] on top.

**Single Domain Generalization (SDG).** Unlike DG which leverages diversity across multiple sources for better generalization, SDG considers generalizing from a single source. Notable approaches for SDG use meta-learning [15] by considering strongly augmented source images as meta-target data (by exposing the model to increasingly distinct augmented views of the source data [16][17]) and learning feature normalization schemes with auxiliary objectives [18].

**Synthetic-to-Real Generalization (Syn-to-Real).** Prior work on syn-to-real generalization has mostly focused on some specific methods, including learning feature normalization / whitening schemes [19], using external data for style injection [20][21], explicitly optimizing for robustness [22], leveraging strong augmentations / domain randomization [23][21], consistency objectives [24] and using contrastive techniques to aid generalization [25]. Some approaches have also considered adapting from synthetic to real images, using techniques such as adversarial training [26], adversarial alignment losses [27], balancing transferability and discriminability [28] and feature alignment [29]. Pasta is more similar to the kind of methods adopting augmentations for improving out-of-the-box generalization 

**Fourier Generalization & Adaptation Methods.** Prior work that explored augmenting images in the Fourier domain (as opposed to the pixel space) rely on a key empirical observation [33][34][35][36] that the phase component of the Fourier spectrum tends to preserve high-level semantics, and therefore, they focused mostly on perturbing the amplitude. Pasta is in line with this style of approach. Amplitude Jitter (AJ) [37] and Amplitude Mixup (AM) [37] are methods similar to Pasta that augment images by perturbing their amplitude spectra. While AM mixes the amplitude spectra of different images, AJ applies uniform perturbation with a single jitter value \(\epsilon\). FSDR [38], on the other hand, isolates domain variant and invariant frequency components using extra data and sets up a learning paradigm. Building on top of [37][39] adds a significance mask when linearly in terpolating amplitudes. [40] only perturbs image-frequency components that capture little semantic information. [41] uses an encoder-decoder to obtain high/low frequency features and augments images by adding noise to high frequency phase and low frequency amplitude. [42][43] study how amplitude and phase perturbations impact robustness to natural corruptions [44]. 

",0
10," **Object Attribute** depicts the physical properties like color, size, shape, _etc_. It usually plays the role of intermedia between pixels and higher-level concepts, _e.g_., prompting object recognition [1], affordance learning [2], zero-shot learning [3], and object detection . Recently, several large-scale datasets [1][4][5][6][7] are released. For attribute recognition, besides direct attribute classification [3][4] and leveraging the correlation between attribute-attribute and attribute-object [8][9][10], intrinsic properties (compositionality, contextuality [11], symmetry [12][13]) of attribute-object are also proven useful. [11] uses the model weight space to encode the attributes to model the compositionality and contextuality.  uses the attributes as linear operators to transform object embeddings. [12] leverages the symmetry property to model the attribute changes within attribute-object coupling and decoupling.

**Object Affordance.** is introduced by [14]. Affordance learning has two canonical paradigms: direct mapping [15] or indirect method [16][17] with intermediates like object category, attribute, and 3D contents. Some works learned affordance from human-object interactions (HOI) to encode the relation between object and action [18][19]. Visual Genome  provides relations between objects, including actions instead of affordances. However, these relations cover limited and sparse affordances. Differently, we use easily accessible object images as the knowledge source and densely annotate all attributes/affordances for all objects. Besides the vision community, the robot community pays much attention to affordance [20][21] for grasping and manipulation. For instance, [20] utilized the robot to discover the object affordance via self-supervised learning. Recently, several datasets [22][23][24] have been proposed. IIT-AFF [22] collected ten daily indoor objects and provided nine common affordances to construct a dataset for robot applications. Zhu _et al_.  built a dataset containing attribute, affordance, human pose, and HOI spatial configuration. But labeling pose and HOI are costly. Chao _et al_. [23] proposed a _semantic_ category-level affordance dataset including 91 objects  and 957 affordances.

**Causal Inference.** There is increasing literature on exploiting causal inference [25] in machine learning, especially with causal graphical models [26][25], including feature selection [27] and learning [28], video analysis [29][30], reinforcement learning [31][32], _etc_. Recently, Wang _et al_. [33] studied the causal relation between objects in images and used intervention [25] to alleviate the observation bias. Atzmon _et al_. [34] analyze the causal generative model of compositional zero-shot learning and disentangle the representations of attributes and objects. Here, we explore the causal relations between three object levels and apply backdoor adjustment [25] to alleviate the existing bias.

",1
11," **Object Attribute** depicts the physical properties like color, size, shape, _etc_. It usually plays the role of intermedia between pixels and higher-level concepts, _e.g_., prompting object recognition [1], affordance learning [2], zero-shot learning [3], and object detection . Recently, several large-scale datasets [1][4][5][6][7] are released. For attribute recognition, besides direct attribute classification [3][4] and leveraging the correlation between attribute-attribute and attribute-object [8][9][10], intrinsic properties (compositionality, contextuality [11], symmetry [12][13]) of attribute-object are also proven useful. [11] uses the model weight space to encode the attributes to model the compositionality and contextuality.  uses the attributes as linear operators to transform object embeddings. [12] leverages the symmetry property to model the attribute changes within attribute-object coupling and decoupling.

**Object Affordance.** is introduced by [14]. Affordance learning has two canonical paradigms: direct mapping [15] or indirect method [16][17] with intermediates like object category, attribute, and 3D contents. Some works learned affordance from human-object interactions (HOI) to encode the relation between object and action [18][19]. Visual Genome  provides relations between objects, including actions instead of affordances. However, these relations cover limited and sparse affordances. Differently, we use easily accessible object images as the knowledge source and densely annotate all attributes/affordances for all objects. Besides the vision community, the robot community pays much attention to affordance [20][21] for grasping and manipulation. For instance, [20] utilized the robot to discover the object affordance via self-supervised learning. Recently, several datasets [22][23][24] have been proposed. IIT-AFF [22] collected ten daily indoor objects and provided nine common affordances to construct a dataset for robot applications. Zhu _et al_.  built a dataset containing attribute, affordance, human pose, and HOI spatial configuration. But labeling pose and HOI are costly. Chao _et al_. [23] proposed a _semantic_ category-level affordance dataset including 91 objects  and 957 affordances.

**Causal Inference.** There is increasing literature on exploiting causal inference [25] in machine learning, especially with causal graphical models [26][25], including feature selection [27] and learning [28], video analysis [29][30], reinforcement learning [31][32], _etc_. Recently, Wang _et al_. [33] studied the causal relation between objects in images and used intervention [25] to alleviate the observation bias. Atzmon _et al_. [34] analyze the causal generative model of compositional zero-shot learning and disentangle the representations of attributes and objects. 

",0
12," Dynamic human modeling has shown promising results in utilizing various representations such as point clouds [1], meshes , voxels [2], and neural implicit functions [3], with models like SMPL [4][5] being commonly used for parameterizing the human body. Since the introduction of NeRF , neural human representation [6][7][8][9] has achieved remarkable progress on representing dynamic human bodies from sparse-view videos. Among them, Neural Actor  and Neural Body [6] pioneer in combining NeRF  with SMPL deformable meshes to represent human bodies with complex motions. Subsequent works have further improved on the generalizability  and animatability [10] of human bodies. To support multi-person modeling, recent works [11][12] have proposed to segment each human into 3D bounding boxes and learn a separate layered dynamic NeRF for each person. Other works [13][14][15] are specifically designed to reconstruct the dynamic human and object with RGB-D or multi-view videos as inputs. They track the human and object pose, and separately reconstruct them with volumetric fusion [14], neural texture blending [15], or neural rendering [16].

Despite achieving promising results, these approaches require multi-view videos or RGB-D as input, limiting their real-world applications. To solve this problem, Human-NeRF [17] is proposed to represent moving humans from a monocular video by the human pose-driven deformation module and canonical space. NeuMan  is the first successful attempt at reconstructing both the dynamic human and static background from a single video. However, Neuman  does not support human-environment interactions

Most prior approaches on dynamic scene modeling require synchronized multi-view videos [18][19][20][11][21] or depth [22][23] as input. Recent studies have built upon NeRF  to reconstruct dynamic neural radiance fields from monocular videos by either learning a deformation field that maps dynamic observation to canonical field [24][25][26] or building 4D spatio-temporal radiance fields [27]. Among them, Nerfies [25] associates latent codes with the deformation field and HyperNeRF  represents motion in a high-dimensional space. D\({}^{2}\)NeRF [28] builds upon HyperNeRF  and further decouples the dynamic components from the static background, and represents them separately with a HyperNeRF  and NeRF . DynIBaR  proposes a motion-adjusted multi-view feature aggregation module to synthesize new viewpoints by aggregating features from nearby views. Other studies have introduced voxel grids [29] or planar representations  for fast dynamic radiance fields reconstruction. While these approaches have achieved high-fidelity dynamic view synthesis results, they are restricted to simple scene deformations. In contrast, our HOSNeRF is capable of representing significant human-object motions and interactions in complex environments.

",1
13," Dynamic human modeling has shown promising results in utilizing various representations such as point clouds [1], meshes , voxels [2], and neural implicit functions [3], with models like SMPL [4][5] being commonly used for parameterizing the human body. Since the introduction of NeRF , neural human representation [6][7][8][9] has achieved remarkable progress on representing dynamic human bodies from sparse-view videos. Among them, Neural Actor  and Neural Body [6] pioneer in combining NeRF  with SMPL deformable meshes to represent human bodies with complex motions. Subsequent works have further improved on the generalizability  and animatability [10] of human bodies. To support multi-person modeling, recent works [11][12] have proposed to segment each human into 3D bounding boxes and learn a separate layered dynamic NeRF for each person. Other works [13][14][15] are specifically designed to reconstruct the dynamic human and object with RGB-D or multi-view videos as inputs. They track the human and object pose, and separately reconstruct them with volumetric fusion [14], neural texture blending [15], or neural rendering [16].

Despite achieving promising results, these approaches require multi-view videos or RGB-D as input, limiting their real-world applications. To solve this problem, Human-NeRF [17] is proposed to represent moving humans from a monocular video by the human pose-driven deformation module and canonical space. NeuMan  is the first successful attempt at reconstructing both the dynamic human and static background from a single video. However, Neuman  does not support human-environment interactions

Most prior approaches on dynamic scene modeling require synchronized multi-view videos [18][19][20][11][21] or depth [22][23] as input. Recent studies have built upon NeRF  to reconstruct dynamic neural radiance fields from monocular videos by either learning a deformation field that maps dynamic observation to canonical field [24][25][26] or building 4D spatio-temporal radiance fields [27]. Among them, Nerfies [25] associates latent codes with the deformation field and HyperNeRF  represents motion in a high-dimensional space. D\({}^{2}\)NeRF [28] builds upon HyperNeRF  and further decouples the dynamic components from the static background, and represents them separately with a HyperNeRF  and NeRF . DynIBaR  proposes a motion-adjusted multi-view feature aggregation module to synthesize new viewpoints by aggregating features from nearby views. Other studies have introduced voxel grids [29] or planar representations  for fast dynamic radiance fields reconstruction. While these approaches have achieved high-fidelity dynamic view synthesis results, they are restricted to simple scene deformations. 

",0
14," **Uncertainty estimation.** Typical neural networks can not detect their own failure. However, this ability can be important in several real-world applications, like rejecting unseen samples, and providing prediction confidence, to name a few. Bayesian NN [1][2][3][4] predicts epistemic uncertainty as the mutual information between model parameters and samples. By assuming a probabilistic prior on the network, it approximates prediction variance by sampling weight during inference. Several works [5] choose to model epistemic and aleatoric uncertainties separately. The Subjective Logic on which our method is established falls within the realm of epistemology instead of a frequentist (aleatoric) view. In other words, we focus on further separating epistemic uncertainty into confusion and ignorance.

Evidential deep learning [6][7][8][9], in contrast, proposes to learn the prior of the predictions directly. The prior, known as evidential prior, is interpreted as beliefs in Dempster-Shafer Threory . In [9], they model the first- and second-order uncertainties by introducing an auxiliary uncertainty network to approximate the difference between Dirichlet distributions.

While uncertainty is provided to describe the variance of model prediction, it is not clear if uncertainty comes from different sources when dealing with in-distribution or out-of-distribution data. Orthogonal to previous approaches, we separate the uncertainty into confusion and ignorance in this work. Confusion depicts the uncertainty between different known classes, while ignorance decides whether the sample is unknown. With this separation, we can make dynamic predictions on known classes and reject unknown classes at the same time.

**Open Set Recognition.** Machine learning models are usually designed with the closed-set assumption, where testing data shares the same distribution as the training. Open-set recognition (OSR) [10] introduces semantic shifts to the problem. Samples are from the classes that are not in the training set. Out-of-distribution (OOD) [11] detection introduces domain shifts to the testing set. In both of the settings, models should have the ability to reject unknown samples.

In general, OSR and OOD methods reject unknown samples depending on reweighting outputs [10][12][13][11][14][15], getting better feature embedding metrics [16][17], and exploring reconstruction errors [18][19][20][21]. These metrics are all related to the quality of classifier prediction. However, the recognition can fail on closed-set samples because of the existence of confusion. In this work, we show that when the confusion between known classes is adequately modeled, unknown samples can be identified more accurately.

**Conformal Prediction.** Parallel to our task, conformal prediction is a paradigm that could provide single or multiple predictions by empirically constructing confidence regions [22][23]. However, conformal prediction is confined to closed scenarios without open samples, as the empirical quantile is established on a labeled validation set sampled from the same testing distribution.

",1
15," **Uncertainty estimation.** Typical neural networks can not detect their own failure. However, this ability can be important in several real-world applications, like rejecting unseen samples, and providing prediction confidence, to name a few. Bayesian NN [1][2][3][4] predicts epistemic uncertainty as the mutual information between model parameters and samples. By assuming a probabilistic prior on the network, it approximates prediction variance by sampling weight during inference. Several works [5] choose to model epistemic and aleatoric uncertainties separately. The Subjective Logic on which our method is established falls within the realm of epistemology instead of a frequentist (aleatoric) view. 

Evidential deep learning [6][7][8][9], in contrast, proposes to learn the prior of the predictions directly. The prior, known as evidential prior, is interpreted as beliefs in Dempster-Shafer Threory . In [9], they model the first- and second-order uncertainties by introducing an auxiliary uncertainty network to approximate the difference between Dirichlet distributions.

While uncertainty is provided to describe the variance of model prediction, it is not clear if uncertainty comes from different sources when dealing with in-distribution or out-of-distribution data.  

**Open Set Recognition.** Machine learning models are usually designed with the closed-set assumption, where testing data shares the same distribution as the training. Open-set recognition (OSR) [10] introduces semantic shifts to the problem. Samples are from the classes that are not in the training set. Out-of-distribution (OOD) [11] detection introduces domain shifts to the testing set. In both of the settings, models should have the ability to reject unknown samples.

In general, OSR and OOD methods reject unknown samples depending on reweighting outputs [10][12][13][11][14][15], getting better feature embedding metrics [16][17], and exploring reconstruction errors [18][19][20][21]. These metrics are all related to the quality of classifier prediction. However, the recognition can fail on closed-set samples because of the existence of confusion. 

**Conformal Prediction.** Parallel to our task, conformal prediction is a paradigm that could provide single or multiple predictions by empirically constructing confidence regions [22][23]. However, conformal prediction is confined to closed scenarios without open samples, as the empirical quantile is established on a labeled validation set sampled from the same testing distribution.

",0
16," Since SRCNN  first introduces deep convolution neural networks (CNNs) to the image SR task and obtains superior performance over conventional SR methods, numerous deep networks [1][2][3][4][5][6][7][8][9] have been proposed for SR to further improve the reconstruction quality. For instance, many methods apply more elaborate convolution module designs, such as residual block [6][10] and dense block [9], to enhance the model representation ability. Several works explore more different frameworks like recursive neural network [11][12] and graph neural network [13]. To improve perceptual quality, [10][14][15] introduce adversarial learning to generate more realistic results. By using attention mechanism, [1][16][7][17] achieve further improvement in terms of reconstruction fidelity. Recently, a series of Transformer-based networks [18][4][5] are proposed and constantly refresh the state-of-the-art of SR task, showing the powerful representation ability of Transformer.

To better understand the working mechanisms of SR networks, several works are proposed to analyze and interpret the SR networks. LAM [19] adopts the integral gradient method to explore which input pixels contribute most to the final performance. DDR [20] reveals the deep semantic representations in SR networks based on deep feature dimensionality reduction and visualization. FAIG [21] aims to find discriminative filters for specific degradations in blind SR. RDSR [22] introduces channel saliency map to demonstrate that Dropout can help prevent co-adapting for real-SR networks. SRGA [23] aims to evaluate the generalization ability of SR methods. In this work, we exploit LAM [19] to analyse and understand the behavior of SR networks.

Recently, Transformer [24] has attracted the attention of computer vision community due to its success in the field of natural language processing. A series of Transformer-based methods [25][26][27][28][29][30][31][32][33][34][35] have been developed for high-level vision tasks, including image classification [27][30][31][36], object detection [37][25], segmentation [38], _etc_. Although vision Transformer has shown its superiority on modeling long-range dependency [27][39], there are still many works demonstrating that the convolution can help Transformer achieve better visual representation [29][33][40][35]. Due to the impressive performance, Transformer has also been introduced for low-level vision tasks [41][18][4][42][5][43][44][45]. Specifically, IPT [18] develops a ViT-style network and introduces multi-task pre-training for image processing. SwinIR [5] proposes an image restoration Transformer based on [31]. VRT [42] introduces Transformer-based networks to video restoration. EDT [4] adopts self-attention mechanism and multi-related-task pre-training strategy to further refresh the state-of-the-art of SR. However, existing works still cannot fully exploit the potential of Transformer, while our method can activate more input pixels for better reconstruction.

",1
17," Since SRCNN  first introduces deep convolution neural networks (CNNs) to the image SR task and obtains superior performance over conventional SR methods, numerous deep networks [1][2][3][4][5][6][7][8][9] have been proposed for SR to further improve the reconstruction quality. For instance, many methods apply more elaborate convolution module designs, such as residual block [6][10] and dense block [9], to enhance the model representation ability. Several works explore more different frameworks like recursive neural network [11][12] and graph neural network [13]. To improve perceptual quality, [10][14][15] introduce adversarial learning to generate more realistic results. By using attention mechanism, [1][16][7][17] achieve further improvement in terms of reconstruction fidelity. Recently, a series of Transformer-based networks [18][4][5] are proposed and constantly refresh the state-of-the-art of SR task, showing the powerful representation ability of Transformer.

To better understand the working mechanisms of SR networks, several works are proposed to analyze and interpret the SR networks. LAM [19] adopts the integral gradient method to explore which input pixels contribute most to the final performance. DDR [20] reveals the deep semantic representations in SR networks based on deep feature dimensionality reduction and visualization. FAIG [21] aims to find discriminative filters for specific degradations in blind SR. RDSR [22] introduces channel saliency map to demonstrate that Dropout can help prevent co-adapting for real-SR networks. SRGA [23] aims to evaluate the generalization ability of SR methods. 

Recently, Transformer [24] has attracted the attention of computer vision community due to its success in the field of natural language processing. A series of Transformer-based methods [25][26][27][28][29][30][31][32][33][34][35] have been developed for high-level vision tasks, including image classification [27][30][31][36], object detection [37][25], segmentation [38], _etc_. Although vision Transformer has shown its superiority on modeling long-range dependency [27][39], there are still many works demonstrating that the convolution can help Transformer achieve better visual representation [29][33][40][35]. Due to the impressive performance, Transformer has also been introduced for low-level vision tasks [41][18][4][42][5][43][44][45]. Specifically, IPT [18] develops a ViT-style network and introduces multi-task pre-training for image processing. SwinIR [5] proposes an image restoration Transformer based on [31]. VRT [42] introduces Transformer-based networks to video restoration. EDT [4] adopts self-attention mechanism and multi-related-task pre-training strategy to further refresh the state-of-the-art of SR. 

",0
18," NCD techniques have been proposed to classify data with various constraints on unlabeled data. One category of the methods presented pre-training the model on the labeled set and fine-tuning it on the unlabeled set using unsupervised clustering losses [1][2][3][4][5]. Another category assumed the availability of both the labeled and unlabeled data, and trained networks jointly with a labeled novel class loss within the semi-supervised scheme [6][7][8][9][10][11]. Han _et al._[12] proposed transferring knowledge from labeled to unlabeled data using ranking statistics in the joint learning stage. Recently, GCD [13] and XCon [14] tackled the more realistic scenario of joint datasets and distinguished known and unknown classes using prior knowledge. However, these approaches did not consider the continual learning scheme. To address the limitation, FRoST [15] and NCDwF [16] froze feature extractors and added the second head for each novel class, as much as the given number of novel categories. However, the methods employed disjoint sets. GM [17] proposed to consider novel category discovery on the joint datasets, but still require prior knowledge, such as the proportion of novel samples.

Most of the image retrieval methods have utilized metric learning and can be categorized into two approaches. Pair-based methods exploited contrastive loss [18][19][20] and triplet loss [21][22], that pull together data pairs in the same class and push apart those in different classes. Multiple data-based [23][24] methods proposed considering the relations between multiple data. Entire data-based approaches [25][26] presented considering all data in a batch, leveraging fine-grained semantic relations between them while requiring high computation costs and slow convergence. In contrast, proxy-based methods [27][28][29] employed fewer proxies than the training set, reducing training complexity. While these methods improved training convergence, they did not consider data-to-data relations, as each data was associated with its proxy. PA [30] inherited the strength of pair- and proxy-based methods, achieving fast and reliable convergence, robustness opposing noisy data, and leveraging rich data-to-data relations.

Recently proposed methods for learning with noisy labels have highlighted the importance of discriminating between clean and noise-labeled data to improve performance. DivideMix [31] used GMM to distinguish between clean and noisy labeled data and treated the latter as unlabeled for semi-supervised learning. AugDesc [32] employed data augmentation to enhance the differentiation between clean and noisy labeled data, while INCV [33] introduced cross-validation to separate clean data from noisy training data. SplitNet [34] leveraged a compact network to perceive the difference between clean and noisy labels, improving model performance by more accurately differentiating noise.

",0
19," **Tabular SSP.** Stochastic Shortest Path (SSP) is a popular variant of Markov Decision Process, which can be dated back to ; ; . The regret minimization problem of SSP was first studied by [2], which proposed the first algorithm with a regret of \(\widetilde{\mathcal{O}}(D^{3/2}S\sqrt{AK/c_{\text{min}}})\), and a parameter-free algorithm with an \(\mathcal{O}(K^{3/2})\) regret bound. Here \(D\) is the smallest expected hitting time from any starting state to the goal state and \(c_{\text{min}}\) is the assumed positive lower bound of the cost function. It was improved by [3] to \(\mathcal{O}(B_{*}S\sqrt{AK})\) when \(B_{*}\) is known and \(\mathcal{O}(B_{*}^{3/2}S\sqrt{AK})\) in the parameter-free case. There is still a \(\sqrt{S}\) gap from the lower bound of \(\Omega(B_{*}\sqrt{SAK})\) proved in the same paper. Later,  proposed an algorithm using the technique of reducing SSP to a finite-horizon MDP with a large terminal cost. This algorithm achieves the lower bound, but it requires some prior knowledge of \(T_{*}\), which can be bypassed by using the trivial upper bound \(T_{*}\leq B_{*}/c_{\text{min}}\), and \(B_{*}\) to properly tune the horizon and terminal cost in the reduction. As mentioned in Remark 2 of , this large dependence on \(1/c_{\text{min}}\) will not work well without the assumption \(c_{\text{min}}>0\). Concurrently,  avoided this requirement. They first developed an algorithm that knows \(T_{\pm}\) without assuming \(c_{\text{min}}>0\). This algorithm achieves an \(\widetilde{\mathcal{O}}(B_{*}\sqrt{SAK})\) regret upper bound, matching the lower bound. They also introduced a parameter-free algorithm that does not require knowing \(T_{*}\) in advance. For the case where \(B_{*}\) is unknown,  proposed an algorithm with a 'doubling trick' to guess the unknown \(B_{*}\) from scratch. Using the analysis framework called implicit finite horizon approximation,  proposed the first model-free algorithm which is minimax optimal under strictly positive costs. They also introduced a model-based minimax optimal algorithm without this assumption that is computationally more efficient. In other aspects of the literature, [1] introduced the first posterior sampling algorithm for SSP.  studied the problem of SSP with access to a generative model. Moreover, ; ;  studied the problem with adversarial costs.

**RL with Linear Function Approximation.** There exists a large number of works studying RL with linear function approximation (; [7]; [8]; [13]; ; [11];; [12]; [6]). The counterpart of the SSP we study in episodic MDPs is called linear mixture MDPs, where the transition probability of the MDP is based on a linear mixture model ([4]; [5]; ; ; b). [9] proposed an algorithm to achieve a nearly minimax optimal regret bound in episodic MDP. Recently, a new work can achieve horizon-free regret bound for linear mixture MDPs ([6]). In the SSP setting,  is the first to study a linear SSP model, which assumes there exist some feature maps and that both the cost function and the transition probability are linear in the feature maps. They proposed a computationally inefficient algorithm with a regret of \(\widetilde{\mathcal{O}}(\sqrt{d^{3}B_{*}^{3}K/c_{\text{min}}})\). [10] improved this result by a computationally efficient algorithm with an \(\widetilde{\mathcal{O}}(\sqrt{d^{3}B_{*}^{2}T_{*}K})\) regret. To avoid the undesirable dependency on \(T_{*}\), [10] also proposed a computationally inefficient algorithm with a regret bound of \(\widetilde{\mathcal{O}}(d^{3.5}B_{*}\sqrt{K})\) by constructing some confidence sets.

**Linear Mixture SSP.** Linear Mixture SSP is a different type of linear function approximation from linear SSP, which was first studied by [15]. In their work, they proposed an algorithm (LEVIS) with a regret of \(\widetilde{\mathcal{O}}(dB_{*}^{1.5}\sqrt{K/c_{\text{min}}})\) and an improved version (LEVIS\({}^{+}\)) with a regret of \(\widetilde{\mathcal{O}}(dB_{*}\sqrt{K/c_{\text{min}}})\). [10] proposed another algorithm (UCRL-VTR-SSP) with an \(\widetilde{\mathcal{O}}(B_{*}\sqrt{dT_{*}K}+dB_{*}\sqrt{K})\) regret. When \(d\geq T_{*}\), the result is nearly optimal, but in other cases, the dependency on \(T_{*}\) is undesirable. Similar to the tabular setting, this dependency can be bypassed by replacing \(T_{*}\) with the upper bound \(T_{*}\leq B_{*}/c_{\text{min}}\).

",0
20," **Pre-training in computer vision.** Large-scale pre-training has become the new fuel empowering computer vision. Contrastive learning and related methods ([12]; [11]; [1]; [10]) learn visual representations by modeling image similarity ([4]) and dissimilarity ([5]) between two or more views. Masked Image Modeling (MIM) ([2]) pursues a different direction by learning to predict removed pixels ([6]), discrete visual tokens ([7]), or pre-computed features ([3]). Language-supervised pre-training, e.g., CLIP ([8]) and related works (; [9]), has been established as a powerful paradigm for learning visual representations. While pre-trained models attract increasing attention in the vision field, no large-scale evaluation has compared the various models available for motor control. This work aims to benchmark the plethora of pre-trained vision models to explore which ones are the most effective for visuomotor control.

**Pre-trained vision models for motor control.** The application of pre-trained vision models to problems in motor control is a rapidly growing field ([18]; [14]), with studies such as RRL (Shah & ), PIE-G ([13]), and MVP ([19]) demonstrating the effectiveness of supervised or self-supervised pre-trained vision models as visual representations for RL agents. PVR ([20]) and R3M ([15]) find that vision models pre-trained on real-world data enable data-efficient behavior cloning on diverse control tasks. VIP ([16]) proposes a self-supervised pre-trained vision model capable of producing dense reward signals. Concurrently, [17] show that a carefully designed Learning-from-Scratch (LfS) baseline is competitive with methods that leverage pre-trained vision models. However, most approaches train the agent with only BC or only RL, with limited or no discussion on how policy learning choices are made. Thus, it remains unclear whether the effectiveness of pre-trained vision models is consistent across different policy learning methods.

**Policy learning.** Reinforcement learning (RL) (Sutton & ) and imitation learning (IL) () are two mainstream approaches for policy learning. The gap between image-based RL and state-based RL has been significantly bridged, largely due to ideas like autoencoder-based architectures ([26]; ), self-supervised objectives (; [24]), and data augmentation ([25]; ). IL can be broadly categorized into Behavior Cloning (BC) ([27]) and Inverse Reinforcement Learning (IRL) ([23]). BC is extremely sample-efficient but may suffer on out-of-distributions samples () or copycat problems ([21]). IRL focuses on learning a robust reward function ([22]). In this work, we aim to contrast the merits of three different policy learning methods (i.e., RL, BC, IRL) and their properties with respect to re-appropriating pre-trained general vision models for downstream control-specific problems.

",1
21," **Pre-training in computer vision.** Large-scale pre-training has become the new fuel empowering computer vision. Contrastive learning and related methods ([12]; [11]; [1]; [10]) learn visual representations by modeling image similarity ([4]) and dissimilarity ([5]) between two or more views. Masked Image Modeling (MIM) ([2]) pursues a different direction by learning to predict removed pixels ([6]), discrete visual tokens ([7]), or pre-computed features ([3]). Language-supervised pre-training, e.g., CLIP ([8]) and related works (; [9]), has been established as a powerful paradigm for learning visual representations. While pre-trained models attract increasing attention in the vision field, no large-scale evaluation has compared the various models available for motor control. 

**Pre-trained vision models for motor control.** The application of pre-trained vision models to problems in motor control is a rapidly growing field ([18]; [14]), with studies such as RRL (Shah & ), PIE-G ([13]), and MVP ([19]) demonstrating the effectiveness of supervised or self-supervised pre-trained vision models as visual representations for RL agents. PVR ([20]) and R3M ([15]) find that vision models pre-trained on real-world data enable data-efficient behavior cloning on diverse control tasks. VIP ([16]) proposes a self-supervised pre-trained vision model capable of producing dense reward signals. Concurrently, [17] show that a carefully designed Learning-from-Scratch (LfS) baseline is competitive with methods that leverage pre-trained vision models. However, most approaches train the agent with only BC or only RL, with limited or no discussion on how policy learning choices are made. 

**Policy learning.** Reinforcement learning (RL) (Sutton & ) and imitation learning (IL) () are two mainstream approaches for policy learning. The gap between image-based RL and state-based RL has been significantly bridged, largely due to ideas like autoencoder-based architectures ([26]; ), self-supervised objectives (; [24]), and data augmentation ([25]; ). IL can be broadly categorized into Behavior Cloning (BC) ([27]) and Inverse Reinforcement Learning (IRL) ([23]). BC is extremely sample-efficient but may suffer on out-of-distributions samples () or copycat problems ([21]). IRL focuses on learning a robust reward function ([22]). 

",0
22," Transformers have exhibited great success in NLP [1][2][3] and computer vision [4][5][6][7][8], which has motivated researchers to explore their application in point cloud registration. The deep closest point (DCP) [9] utilizes a dynamic graph CNN (DGCNN)  to separately extract features and introduces a transformer [10] to capture the correlations between point clouds. Predator [11] leverages attention mechanisms to conduct information aggregation across a pair of point clouds and predicts overlapping regions for feature sampling purposes, achieving significantly enhanced performance in low-overlap scenarios. CoFiNet [12] extracts features via attention mechanisms in a coarse-to-fine manner and achieves promising performance. The registration transformer (RegTR) [8] utilizes attention layers to directly generate correspondences.

In general, these methods introduce transformers to enable information exchange and encode contextual information, which facilitates the prediction of putative correspondences. However, the limited shared characteristics of the low-overlap point cloud pairs produce obstacles when identifying common structures. To address this issue, several methods have attempted to enhance the ability of networks to capture common structures with dedicated designed encoding modules. Lepard [13] disentangles point cloud representation and utilizes rotary positional encoding  to explicitly reveal 3D relative distance information. The geometric transformer [14] calculates pair-wise distances and triplet-wise angles and combines them with self-attention to capture geometric features. Nonetheless, these methods sacrifice versatility and introduce additional computational complexity during the inference process.

Auxiliary training has been extensively studied in the field of point cloud research, as it can provide a multifold regularization effect during the optimization process while remaining heterogeneous to the main task. SA-SSD [15] introduces an auxiliary network to convert the extracted features back to pointwise representations and then performs foreground segmentation and pointwise center estimation. LabelEnc  employs a novel label encoding function to learn latent embeddings from the given ground truth labels, thus providing auxiliary supervision for the training process. LG3D [16] serves as an auxiliary network to achieve enhanced feature learning by obtaining critical representations and fusing object point clouds into the original input point clouds. However, these works focus on single point cloud and are less suitable for registration tasks. In the domain of point cloud registration, DVD [17] additionally utilizes self-reconstruction and normal estimation tasks to consider the intrinsic structural consistency of point clouds. Although DVD also introduces reconstruction tasks, it reconstructs the input point cloud itself, ignoring the relations across point clouds. DeepMapping [18] utilizes deep neural networks (DNNs) as an auxiliary function to model the structure of a scene by estimating the occupancy status of the global coordinates. Different from these methods, we guide the backbone by reformulating the point cloud registration problem as a masking and reconstruction task.

As a promising self-supervised learning scheme, MDM has achieved promising performance in NLP [1][19] and computer vision [20][21]. Data2vec [22] extracts information based on a masked view of the input and predicts contextualized latent representations that contain information from the entire input. The masked autoencoder (MAE) [20] randomly masks patches of the input image. Then, the MAE learns the latent representations from the unmasked patches and reconstructs the missing pixels. Unlike the MAE, SimMIM [21] utilizes a linear layer as its decoder to directly generate the predicted pixels. Following the recent advances in MDM, adaptation has been investigated for use with point clouds. Point-MAE [23] utilizes an asymmetric transformer autoencoder with a shifting mask token operation and learns latent features from the unmasked points to reconstruct the masked points. Voxel-MAE [24] first voxelizes the input point cloud and then predicts the occupancy values of masked voxels instead of the coordinates of the points. However, fundamental differences between point cloud registration and other point cloud processing tasks pose barriers to applying MDM to point cloud registration.

",1
23," Transformers have exhibited great success in NLP [1][2][3] and computer vision [4][5][6][7][8], which has motivated researchers to explore their application in point cloud registration. The deep closest point (DCP) [9] utilizes a dynamic graph CNN (DGCNN)  to separately extract features and introduces a transformer [10] to capture the correlations between point clouds. Predator [11] leverages attention mechanisms to conduct information aggregation across a pair of point clouds and predicts overlapping regions for feature sampling purposes, achieving significantly enhanced performance in low-overlap scenarios. CoFiNet [12] extracts features via attention mechanisms in a coarse-to-fine manner and achieves promising performance. The registration transformer (RegTR) [8] utilizes attention layers to directly generate correspondences.

In general, these methods introduce transformers to enable information exchange and encode contextual information, which facilitates the prediction of putative correspondences. However, the limited shared characteristics of the low-overlap point cloud pairs produce obstacles when identifying common structures. To address this issue, several methods have attempted to enhance the ability of networks to capture common structures with dedicated designed encoding modules. Lepard [13] disentangles point cloud representation and utilizes rotary positional encoding  to explicitly reveal 3D relative distance information. The geometric transformer [14] calculates pair-wise distances and triplet-wise angles and combines them with self-attention to capture geometric features. Nonetheless, these methods sacrifice versatility and introduce additional computational complexity during the inference process.

Auxiliary training has been extensively studied in the field of point cloud research, as it can provide a multifold regularization effect during the optimization process while remaining heterogeneous to the main task. SA-SSD [15] introduces an auxiliary network to convert the extracted features back to pointwise representations and then performs foreground segmentation and pointwise center estimation. LabelEnc  employs a novel label encoding function to learn latent embeddings from the given ground truth labels, thus providing auxiliary supervision for the training process. LG3D [16] serves as an auxiliary network to achieve enhanced feature learning by obtaining critical representations and fusing object point clouds into the original input point clouds. However, these works focus on single point cloud and are less suitable for registration tasks. In the domain of point cloud registration, DVD [17] additionally utilizes self-reconstruction and normal estimation tasks to consider the intrinsic structural consistency of point clouds. Although DVD also introduces reconstruction tasks, it reconstructs the input point cloud itself, ignoring the relations across point clouds. DeepMapping [18] utilizes deep neural networks (DNNs) as an auxiliary function to model the structure of a scene by estimating the occupancy status of the global coordinates. 

As a promising self-supervised learning scheme, MDM has achieved promising performance in NLP [1][19] and computer vision [20][21]. Data2vec [22] extracts information based on a masked view of the input and predicts contextualized latent representations that contain information from the entire input. The masked autoencoder (MAE) [20] randomly masks patches of the input image. Then, the MAE learns the latent representations from the unmasked patches and reconstructs the missing pixels. Unlike the MAE, SimMIM [21] utilizes a linear layer as its decoder to directly generate the predicted pixels. Following the recent advances in MDM, adaptation has been investigated for use with point clouds. Point-MAE [23] utilizes an asymmetric transformer autoencoder with a shifting mask token operation and learns latent features from the unmasked points to reconstruct the masked points. Voxel-MAE [24] first voxelizes the input point cloud and then predicts the occupancy values of masked voxels instead of the coordinates of the points. 

",0
24," Traditional research about SGG is also called visual relationship detection. VRD  first proposes the SGG task based on visual object proposals from RCNN [1][2]. Researchers have gradually realized the importance of SGG in image understanding, and many subsequent works including IMP [3], Motifs [4], VCTree [5] follow this task. These works respectively introduce message passing structures, such as IMP [3], MSDN [6], GPS-Net [7], GB-Net , CISC [8], tree structures including VCTree [5] and CogTree [9], graph structures including G-RCNN , KERN [10] and GCN-SGG [11]. Pixels2Graphs [12] and FCSGG [13] directly predict object pairs and relationships from images, without relying on RCNN results. Seq2Seq-RL [14] introduces using the global context and the seq2seq transformer to estimate the scene graph. SSC-RCNN [15] achieves one-stage SGG through triple query based on Sparse R-CNN. OpenPSG [16] combines the panoptic segmentation and the SGG, and uses the transformer structure to simultaneously predict panoptic masks and relationships. However, in these methods, visual features always play a dominant role in SGG and context features are often used as auxiliary information. For example, RelDN [17] predicts the predicate in the spatial, semantics and visual three channels respectively, and designs the contrastive losses. GPS-Net [7] concats visual features, class scores and spatial features as node features and predicts predicates between nodes based on node features. Motifs [4] has proposed to use the global bounding boxes and labels for edge prediction, but global context information cannot effectively deal with long-tail bias, and the inference speed of bidirectional LSTM is slow. In our methods, we only use local context and visual features are discarded. Our methods extract object pairs for contextual augmentation training, and uses the results of the contextual scene graph results to guide visual SGG.

In recent years, due to the extreme imbalance of predicate categories in the SGG dataset, some works have focused on the long-tail bias to improve the performance of tail predicate predictions. These works can be divided according to whether the training is biased or not. For biased SGG training, extra information is often learned to help remove bias during inference. TDE [18] proposes the causal graph and tries to make the model recognize the deep mean of object features. Cogtree [9] proposes a coarse-to-fine method and debris from biased predictions, while BPL-SA [19] introduces the confusion matrix. DLFE [20] proposes the label frequency estimation and learns the label frequencies in biased training to remove reporting bias.

For unbiased SGG training, additional data processing steps help the model to train unbiased. PCPL [21] proposes the predicate correlation and enables the model to distinguish similar predicates, such as '_on_' and '_parked on_'. GFAL [22] introduces the graph density-aware losses for unbiased training. DT2-ACBS [23] introduces rebalanced sampling strategy and discusses the impact of different sampling strategy on the SGG task. NICE [24] analyzes the samples in the dataset to optimize more accurate labels and generate pseudo-labels that are not labeled. IETrans [25] proposes internal transfer and external transfer to enhance SGG dataset. BGNN [26] introduces a bipartite graph network with bi-level data sampling that can account for the overall recall and the mean recall of predicates. We believe small changes in objects for producing fake images do not change predicates between objects, so we project fake images to the context level to increase the number of context samples. Then we can obtain diverse context samples for unbiased training of the contextual SGG.

",1
25," Traditional research about SGG is also called visual relationship detection. VRD  first proposes the SGG task based on visual object proposals from RCNN [1][2]. Researchers have gradually realized the importance of SGG in image understanding, and many subsequent works including IMP [3], Motifs [4], VCTree [5] follow this task. These works respectively introduce message passing structures, such as IMP [3], MSDN [6], GPS-Net [7], GB-Net , CISC [8], tree structures including VCTree [5] and CogTree [9], graph structures including G-RCNN , KERN [10] and GCN-SGG [11]. Pixels2Graphs [12] and FCSGG [13] directly predict object pairs and relationships from images, without relying on RCNN results. Seq2Seq-RL [14] introduces using the global context and the seq2seq transformer to estimate the scene graph. SSC-RCNN [15] achieves one-stage SGG through triple query based on Sparse R-CNN. OpenPSG [16] combines the panoptic segmentation and the SGG, and uses the transformer structure to simultaneously predict panoptic masks and relationships. However, in these methods, visual features always play a dominant role in SGG and context features are often used as auxiliary information. For example, RelDN [17] predicts the predicate in the spatial, semantics and visual three channels respectively, and designs the contrastive losses. GPS-Net [7] concats visual features, class scores and spatial features as node features and predicts predicates between nodes based on node features. Motifs [4] has proposed to use the global bounding boxes and labels for edge prediction, but global context information cannot effectively deal with long-tail bias, and the inference speed of bidirectional LSTM is slow. 

In recent years, due to the extreme imbalance of predicate categories in the SGG dataset, some works have focused on the long-tail bias to improve the performance of tail predicate predictions. These works can be divided according to whether the training is biased or not. For biased SGG training, extra information is often learned to help remove bias during inference. TDE [18] proposes the causal graph and tries to make the model recognize the deep mean of object features. Cogtree [9] proposes a coarse-to-fine method and debris from biased predictions, while BPL-SA [19] introduces the confusion matrix. DLFE [20] proposes the label frequency estimation and learns the label frequencies in biased training to remove reporting bias.

For unbiased SGG training, additional data processing steps help the model to train unbiased. PCPL [21] proposes the predicate correlation and enables the model to distinguish similar predicates, such as '_on_' and '_parked on_'. GFAL [22] introduces the graph density-aware losses for unbiased training. DT2-ACBS [23] introduces rebalanced sampling strategy and discusses the impact of different sampling strategy on the SGG task. NICE [24] analyzes the samples in the dataset to optimize more accurate labels and generate pseudo-labels that are not labeled. IETrans [25] proposes internal transfer and external transfer to enhance SGG dataset. BGNN [26] introduces a bipartite graph network with bi-level data sampling that can account for the overall recall and the mean recall of predicates. 

",0
26," In this section, we provide a succinct overview of photometric stereo literature focusing on the single orthographic camera assumption. Alternative setups (_e.g._, perspective, multi-view cameras) are beyond the scope of this work.

**Optimization-based Approach:** The majority of photometric stereo methods assume calibrated, directional lighting following Woodham [1] and optimize parameters by inversely solving a physics-based image formation model. This approach can be further categorized into robust methods, where non-Lambertian components are treated as outliers [2][3]; model-based methods, which explicitly account for non-Lambertian reflectance [4][5][6]; and example-based methods [7][8][9] that leverage the observations of known objects captured under identical conditions as the target scene. The uncalibrated task is akin to the calibrated one, but with unknown lighting parameters. Until recently, most uncalibrated photometric stereo algorithms assumed Lambertian integrable surfaces and aimed to resolve the General Bas-Relief ambiguity [10][11][12][13][14][15][16]. In contrast to these works, photometric stereo under natural lights has also been explored, wherein natural illumination is approximated using spherical harmonics [17], dominant sun lighting [18][19], or equivalent directional lighting [20][21]. Although most optimization-based methods do not require external training data, they are fundamentally limited in handling global illumination phenomena (_e.g._, inter-reflections) that cannot be described by the predefined point-wise image formation model.

**Learning-based Approach:** Learning-based methods are effective in addressing complex phenomena that are challenging to represent within simple image formation models. However, the first photometric stereo network [22] necessitated consistent lighting conditions during both training and testing. To address this limitation, various strategies have been investigated, such as observation maps [23], set-pooling [24], graph-convolution [25], and self-attention [26][27]. Furthermore, researchers have explored uncalibrated deep photometric stereo networks [28][29][30], where lighting parameters and surface normals are recovered sequentially. Self-supervised neural inverse rendering methods have been developed without the need for exter nal data supervision. Taniai and Maehara [31] used neural networks instead of parametric physical models, with images and lighting as input. This work was expanded by Li and Li [32][33], who incorporated recent neural coordinate-based representations . However, despite their tremendous efforts, these methods are designed to work with only single directional light source and have limited ability to generalize to more complex lighting environments.

**Universal Photometric Stereo Network:** The universal photometric stereo network (UniPS) [34] was the first to eliminate the prior lighting model assumption by leveraging a non-physical lighting representation called global lighting contexts. These global lighting contexts are recovered for each lighting condition through pixel-wise communication of hierarchical feature maps along the light-axis using Transformers [35]. During surface normal prediction, a single location is individually selected, and the network aggregates all the global lighting contexts (bilinearly interpolated from the canonical resolution) and raw observations at the location under different lighting conditions to pixel-wise predict the surface normal. This method introduced two strategies to handle high-resolution images: down-sampling images to the canonical resolution for recovering global lighting contexts, and employing pixel-wise surface normal prediction. Although these two concepts contributed to the scalability of image size, they resulted in performance degradation due to the loss of input information and the absence of a non-local perspective, as previously discussed.

Our work draws inspiration from [34] and shares some fundamental ideas, particularly the use of Transformers [35] for communicating and aggregating features along the light-axis. However, our method diverges from [34] by fully utilizing input information in a non-local manner, which leads to a significant enhancement in reconstruction quality.

",1
27," In this section, we provide a succinct overview of photometric stereo literature focusing on the single orthographic camera assumption. Alternative setups (_e.g._, perspective, multi-view cameras) are beyond the scope of this work.

**Optimization-based Approach:** The majority of photometric stereo methods assume calibrated, directional lighting following Woodham [1] and optimize parameters by inversely solving a physics-based image formation model. This approach can be further categorized into robust methods, where non-Lambertian components are treated as outliers [2][3]; model-based methods, which explicitly account for non-Lambertian reflectance [4][5][6]; and example-based methods [7][8][9] that leverage the observations of known objects captured under identical conditions as the target scene. The uncalibrated task is akin to the calibrated one, but with unknown lighting parameters. Until recently, most uncalibrated photometric stereo algorithms assumed Lambertian integrable surfaces and aimed to resolve the General Bas-Relief ambiguity [10][11][12][13][14][15][16]. In contrast to these works, photometric stereo under natural lights has also been explored, wherein natural illumination is approximated using spherical harmonics [17], dominant sun lighting [18][19], or equivalent directional lighting [20][21]. Although most optimization-based methods do not require external training data, they are fundamentally limited in handling global illumination phenomena (_e.g._, inter-reflections) that cannot be described by the predefined point-wise image formation model.

**Learning-based Approach:** Learning-based methods are effective in addressing complex phenomena that are challenging to represent within simple image formation models. However, the first photometric stereo network [22] necessitated consistent lighting conditions during both training and testing. To address this limitation, various strategies have been investigated, such as observation maps [23], set-pooling [24], graph-convolution [25], and self-attention [26][27]. Furthermore, researchers have explored uncalibrated deep photometric stereo networks [28][29][30], where lighting parameters and surface normals are recovered sequentially. Self-supervised neural inverse rendering methods have been developed without the need for exter nal data supervision. Taniai and Maehara [31] used neural networks instead of parametric physical models, with images and lighting as input. This work was expanded by Li and Li [32][33], who incorporated recent neural coordinate-based representations . However, despite their tremendous efforts, these methods are designed to work with only single directional light source and have limited ability to generalize to more complex lighting environments.

**Universal Photometric Stereo Network:** The universal photometric stereo network (UniPS) [34] was the first to eliminate the prior lighting model assumption by leveraging a non-physical lighting representation called global lighting contexts. These global lighting contexts are recovered for each lighting condition through pixel-wise communication of hierarchical feature maps along the light-axis using Transformers [35]. During surface normal prediction, a single location is individually selected, and the network aggregates all the global lighting contexts (bilinearly interpolated from the canonical resolution) and raw observations at the location under different lighting conditions to pixel-wise predict the surface normal. 



",0
28," Existing camera calibration methods.Many 3D computer vision methods assume that lens distortion is radially symmetric around the center of the image. Various camera models such as the radial [1] (bicubic ), division [2], FOV models , and rational model [3] are used to simulate such radially symmetric distortion. Numerous calibration toolboxes and pipelines [4][5] have been developed and integrated to OpenCV . Recently, BabelCalib  proposed a robust optimization strategy for parametric models. However, parametric models are only approximate models of real lenses; in practice, the real distortion includes effects caused by complex lens systems (which lead to combinations of different types of distortions) determined by the camera geometry and by the (not perfectly planar) shape of the lens [6].

When calibrating a camera system with an unknown lens it is difficult to decide in advance which particular model fits the real type of camera projection best. To avoid having to choose, one can instead use a single generic model to approximate most common types of projection. A generic camera model [7][8][9][10][11] associates each pixel with a 3D ray. These methods are designed for generality and flexibility and introduce an extreme number of parameters. In practice, classical sparse calibration patterns do not provide enough measurements for such generic models. [12] uses these models to obtain dense matches using displays that can encode their pixel positions or interpolate between sparse features. However, interpolation leads to inaccurate and sub-optimal performance. Therefore, models with lower calibration data requirements have been proposed [13]. Recently, Schops _et al._[14] extends [13] with a new calibration patterns and detectors to improve the calibration accuracy for generic cameras. [15] replaces the explicit parametric model with a regularization term that forces the underlying distortion map to be smooth.

Neural network-based camera calibration.Several prior works treat the optical components of displays and cam eras as differentiable layers (neural network layers) that can be trained jointly with the computational blocks of an imaging/display system [16][17][18]. Other works estimate camera parameters from single image observations using CNNs [19][20]. For multi-view, joint optimization of camera parameters and neural scene representations, representative works include BARF [21], NeRF\(--\)[22], Self-Calibrating Neural Radiance Fields [23] and the point-based neural rendering pipeline of Ruckert et al. [24]

**Learned markers and keypoint detectors.** Lens models can either be optimized during 3D reconstruction or in a separate calibration stage that uses keypoint positions corresponding to a known 3D structure. Many calibration packages use a checkerboard pattern  due to its simplicity and to be able to utilize line fitting to increase corner detection accuracy. Schops et al. [14] propose a star-based pattern similar to Siemens stars  to increase the amount of gradient information available. They use AprilTags [25] to initialize their point search, while we use ArUco tags  in a similar way on our proposed marker board.

However, all these boards are manually designed. In contrast, [26] uses a random pattern optimized to produce strong feature responses for keypoint detectors. This leads to significantly more points (on the order of thousands), albeit with lower detection accuracy. Hu et al. [27] propose to use a deep-learning based detector. Grinchuk et al. [28] propose to use a learning-based approach for creating markers by generating binary codes and rendering them on distorted and transformed image patches. Peace et al. [29] use end-to-end trainable systems for marker detection, but focus on fiducial-like markers with a unique marker identification. These systems usually require larger markers with a unique identifier to enable direct estimation of camera pose relative to a single marker. In contrast, we base our board on a marker detector with very high accuracy keypoint detection, as we only care about point detection accuracy and identify points on the board using a few low-accuracy ArUco tags. This leads to a higher number of extracted keypoints and high center point extraction accuracy.

**Invertible Neural Networks.** Our paper models lens distortion using an invertible mapping enforced through the neural network architecture. Invertible neural networks have been studied extensively in the context of normalizing flows, where network inverses are required for computing log-likelihoods for generative models [30][31][32][33]. Since our application does not require the estimation of the Jacobian for generative tasks, we opt to use an invertible residual network due to its expressive power and convergence speed. Invertible residual networks have been applied to many tasks, such as shape deformation [34][35], image denoising [36], and tone mapping [37]. In this paper, we explore their applicability to the problem of lens distortion.

",1
29," Existing camera calibration methods.Many 3D computer vision methods assume that lens distortion is radially symmetric around the center of the image. Various camera models such as the radial [1] (bicubic ), division [2], FOV models , and rational model [3] are used to simulate such radially symmetric distortion. Numerous calibration toolboxes and pipelines [4][5] have been developed and integrated to OpenCV . Recently, BabelCalib  proposed a robust optimization strategy for parametric models. However, parametric models are only approximate models of real lenses; in practice, the real distortion includes effects caused by complex lens systems (which lead to combinations of different types of distortions) determined by the camera geometry and by the (not perfectly planar) shape of the lens [6].

When calibrating a camera system with an unknown lens it is difficult to decide in advance which particular model fits the real type of camera projection best. To avoid having to choose, one can instead use a single generic model to approximate most common types of projection. A generic camera model [7][8][9][10][11] associates each pixel with a 3D ray. These methods are designed for generality and flexibility and introduce an extreme number of parameters. In practice, classical sparse calibration patterns do not provide enough measurements for such generic models. [12] uses these models to obtain dense matches using displays that can encode their pixel positions or interpolate between sparse features. However, interpolation leads to inaccurate and sub-optimal performance. Therefore, models with lower calibration data requirements have been proposed [13]. Recently, Schops _et al._[14] extends [13] with a new calibration patterns and detectors to improve the calibration accuracy for generic cameras. [15] replaces the explicit parametric model with a regularization term that forces the underlying distortion map to be smooth.

Neural network-based camera calibration.Several prior works treat the optical components of displays and cam eras as differentiable layers (neural network layers) that can be trained jointly with the computational blocks of an imaging/display system [16][17][18]. Other works estimate camera parameters from single image observations using CNNs [19][20]. For multi-view, joint optimization of camera parameters and neural scene representations, representative works include BARF [21], NeRF\(--\)[22], Self-Calibrating Neural Radiance Fields [23] and the point-based neural rendering pipeline of Ruckert et al. [24]

**Learned markers and keypoint detectors.** Lens models can either be optimized during 3D reconstruction or in a separate calibration stage that uses keypoint positions corresponding to a known 3D structure. Many calibration packages use a checkerboard pattern  due to its simplicity and to be able to utilize line fitting to increase corner detection accuracy. Schops et al. [14] propose a star-based pattern similar to Siemens stars  to increase the amount of gradient information available. They use AprilTags [25] to initialize their point search, while we use ArUco tags  in a similar way on our proposed marker board.

However, all these boards are manually designed. In contrast, [26] uses a random pattern optimized to produce strong feature responses for keypoint detectors. This leads to significantly more points (on the order of thousands), albeit with lower detection accuracy. Hu et al. [27] propose to use a deep-learning based detector. Grinchuk et al. [28] propose to use a learning-based approach for creating markers by generating binary codes and rendering them on distorted and transformed image patches. Peace et al. [29] use end-to-end trainable systems for marker detection, but focus on fiducial-like markers with a unique marker identification. These systems usually require larger markers with a unique identifier to enable direct estimation of camera pose relative to a single marker. 

**Invertible Neural Networks.** Our paper models lens distortion using an invertible mapping enforced through the neural network architecture. Invertible neural networks have been studied extensively in the context of normalizing flows, where network inverses are required for computing log-likelihoods for generative models [30][31][32][33]. Since our application does not require the estimation of the Jacobian for generative tasks, we opt to use an invertible residual network due to its expressive power and convergence speed. Invertible residual networks have been applied to many tasks, such as shape deformation [34][35], image denoising [36], and tone mapping [37]. 

",0
30," Incomplete multi-view learning can be generally divided into two main lines in terms of how to handle missing views. Specifically, existing works mainly focus on neglecting or complementing the missing views based on deep-learning methods. **Methods without imputation.** The methods only use present views and directly learn the common latent subspace or representation for all views in clustering [1][2] and classification [3][4]. **Generative Methods.** The methods impute missing views with the present views and then utilize the reconstructed complete data to conduct downstream tasks [5][6][7][8][9][10][11]. Specifically, one of the most popular ways is applying the structure of variational auto-encoder on partial multi-view data to reconstruct missing views [5][6][7]. Generative adversarial network is also used to generate missing views [10][11]. Besides, there are some methods to obtain imputations based on kernel CCA [8], spectral graph , and information theory [9]. Compared with the above algorithms, our method obtains multiple imputations instead of single imputation and then dynamically evaluates the imputation quality. Thus the more reliable downstream classification tasks can be performed.

One of the key points of our method is to explore and exploit the uncertainty in missing data. To achieve high-quality uncertainty estimation, many approaches have been proposed [12][13][14]. The uncertainty in deep learning can be generally divided into aleatoric uncertainty and epistemic uncertainty [15][16]. Aleatoric uncertainty refers to the uncertainty caused by data and it measures the inherent noise of data. Aleatoric uncertainty can be further divided into homoscedastic uncertainty and heteroscedastic uncertainty, while the first one which varies with different tasks is usually used to estimate the uncertainty in multi-task learning [17][18], and the latter one which varies with input is useful when the input space includes variable noise [19][20]. On the other hand, epistemic uncertainty refers to uncertainty caused by insufficient model training and can be eliminated in theory. It can be estimated by predicting an uncertain observation using models with different parameters, the instability of predicting results just reflects the epistemic uncertainty [15]. In this work, we estimate the aleatoric uncertainty of imputations by adopting subjective logic [21] and Dempster-Shafer theory  to construct a trustworthy and reliable multi-view classification network.

",1
31," Incomplete multi-view learning can be generally divided into two main lines in terms of how to handle missing views. Specifically, existing works mainly focus on neglecting or complementing the missing views based on deep-learning methods. **Methods without imputation.** The methods only use present views and directly learn the common latent subspace or representation for all views in clustering [1][2] and classification [3][4]. **Generative Methods.** The methods impute missing views with the present views and then utilize the reconstructed complete data to conduct downstream tasks [5][6][7][8][9][10][11]. Specifically, one of the most popular ways is applying the structure of variational auto-encoder on partial multi-view data to reconstruct missing views [5][6][7]. Generative adversarial network is also used to generate missing views [10][11]. Besides, there are some methods to obtain imputations based on kernel CCA [8], spectral graph , and information theory [9]. 

One of the key points of our method is to explore and exploit the uncertainty in missing data. To achieve high-quality uncertainty estimation, many approaches have been proposed [12][13][14]. The uncertainty in deep learning can be generally divided into aleatoric uncertainty and epistemic uncertainty [15][16]. Aleatoric uncertainty refers to the uncertainty caused by data and it measures the inherent noise of data. Aleatoric uncertainty can be further divided into homoscedastic uncertainty and heteroscedastic uncertainty, while the first one which varies with different tasks is usually used to estimate the uncertainty in multi-task learning [17][18], and the latter one which varies with input is useful when the input space includes variable noise [19][20]. On the other hand, epistemic uncertainty refers to uncertainty caused by insufficient model training and can be eliminated in theory. It can be estimated by predicting an uncertain observation using models with different parameters, the instability of predicting results just reflects the epistemic uncertainty [15]. 

",0
32," In this section, we review related work in the field of SLP. We cover avatar approaches using HamNoSys as input and gloss approaches. Moreover, we cover HamNoSys generation work, including translating spoken language text or videos into HamNoSys, as these tasks allow for a full translation pipeline together with our work. Furthermore, we mention diffusion models as a source of inspiration for our method, and text-to-motion works, explaining how our problem and data are different from them.

Since the early 2000s, there have been several research projects exploring avatars animated from HamNoSys, such as VisiCast [1], eSign , dicta-sign , and JASigning . While these avatars produce sign sequences, they are not popular among the deaf community due to under-articulated and unnatural movements, making the avatars difficult to understand . Furthermore, the robotic movements of these avatars can make viewers uncomfortable due to the uncanny valley phenomenon [2]. In addition, as illustrated in Fig. 2, these avatars do not perform all hand motions correctly. A later work , uses motion capture data to create more stable and realistic avatars. However, this method is limited to a small set of phrases due to the high data collection and annotation cost.

To cope with these challenges, a recent work  suggests combining generative models with a motion graph (MG)  and Neural Machine Translation. They translate spoken language sentences into gloss sequences that condition an MG to find a pose sequence representing the input from a dictionary of poses. The sequence is then converted to a video using a GAN. A similar work  suggests progressive transformers for generating signed pose sequences from spoken language through glosses. Like Stoll _et al._, they use a closed set of dictionary signs as the signs in their output sequence, which makes these solutions language and data-specific. A later work [3] uses learned ""cheremes1"" to generate signs. Similarly to glosses and phonemes, cheremes are language specific. In contrast, since our method uses a universal notation, it works for languages it was trained on and for unseen languages, as long as the individual glyphs exist in our dataset.

Recently, different HamNoSys generation tasks have been researched. For example, Skobov _et al._ suggested a method for automatically annotating videos into HamNoSys [4]. Further research on this task could enhance the capabilities of our model, by creating more labeled data. Translating spoken language text into HamNoSys has also been researched , and if improved, can allow a complete translation pipeline from spoken language text into Sign languages using our model.

Diffusion models [5][6] recently showed impressive results on image and video generation tasks [7][8][9]. Generation is done using a learned gradual process, with equal input and output sizes. The model gradually changes the input to get the desired output. In this work, we take inspiration from diffusion models in the sense that our model learns to gradually convert the input (a sequence of a duplicated reference frame) into the desired pose sequence.

In recent years, works on motion generation from English text [10][11][12][13][14] showed impressive results. While these works may seem related to our task, they use 3D motion capture data, which is not available for our task. As detailed in Sec. 4, our data is collected using a pose estimation model over sign videos; thus, it is both 2D and imperfect, with many missing and incorrect keypoints. Moreover, since the text in these works is written in English, recent works [11][14][12] take advantage of large pre-trained language models such as BERT [15], CLIP [16], etc. As HamNoSys is not a common language, with limited available resources, we cannot use pre-trained models as they do.

",1
33," In this section, we review related work in the field of SLP. 

Since the early 2000s, there have been several research projects exploring avatars animated from HamNoSys, such as VisiCast [1], eSign , dicta-sign , and JASigning . While these avatars produce sign sequences, they are not popular among the deaf community due to under-articulated and unnatural movements, making the avatars difficult to understand . Furthermore, the robotic movements of these avatars can make viewers uncomfortable due to the uncanny valley phenomenon [2]. In addition, as illustrated in Fig. 2, these avatars do not perform all hand motions correctly. A later work , uses motion capture data to create more stable and realistic avatars. However, this method is limited to a small set of phrases due to the high data collection and annotation cost.

To cope with these challenges, a recent work  suggests combining generative models with a motion graph (MG)  and Neural Machine Translation. They translate spoken language sentences into gloss sequences that condition an MG to find a pose sequence representing the input from a dictionary of poses. The sequence is then converted to a video using a GAN. A similar work  suggests progressive transformers for generating signed pose sequences from spoken language through glosses. Like Stoll _et al._, they use a closed set of dictionary signs as the signs in their output sequence, which makes these solutions language and data-specific. A later work [3] uses learned ""cheremes1"" to generate signs. Similarly to glosses and phonemes, cheremes are language specific. 

Recently, different HamNoSys generation tasks have been researched. For example, Skobov _et al._ suggested a method for automatically annotating videos into HamNoSys [4]. Further research on this task could enhance the capabilities of our model, by creating more labeled data. Translating spoken language text into HamNoSys has also been researched , and if improved, can allow a complete translation pipeline from spoken language text into Sign languages using our model.

Diffusion models [5][6] recently showed impressive results on image and video generation tasks [7][8][9]. Generation is done using a learned gradual process, with equal input and output sizes. The model gradually changes the input to get the desired output. 

In recent years, works on motion generation from English text [10][11][12][13][14] showed impressive results. While these works may seem related to our task, they use 3D motion capture data, which is not available for our task. As detailed in Sec. 4, our data is collected using a pose estimation model over sign videos; thus, it is both 2D and imperfect, with many missing and incorrect keypoints. Moreover, since the text in these works is written in English, recent works [11][14][12] take advantage of large pre-trained language models such as BERT [15], CLIP [16], etc. As HamNoSys is not a common language, with limited available resources, we cannot use pre-trained models as they do.

",0
34," Visual Prompting Tuning.Prompting is initially proposed in NLP [1][2]. [1] demonstrates strong generalization to downstream transfer learning tasks even in the few-shot or zero-shot settings with manually chosen prompts in GPT-3. Recently, prompting [3][4] has been adapted to vision tasks. [3] proposes memory tokens which is a set of learnable embedding vectors for each transformer layer. VPT [4] proposes similar ideas and investigates the generality and feasibility of visual prompting via extensive experiments spanning multiple kinds of recognition tasks across multiple domains and backbone architectures. Unlike VPT, whose main focus is on recognition tasks, our work aims at exploring optimal visual content for low-level structure segmentation.

Forgery Detection.The goal of forgery detection is to detect pixels that are manually manipulated, such as pixels that are removed, replaced, or edited. Early approaches [5] detect region splicing through inconsistencies in local noise levels, based on the fact that images of different origins might contain different noise characteristics introduced by the sensors or post-processing steps. Other clues are found to be helpful, such as SIFT [6], JPEG compression artifacts [7] and re-sampling artifacts [8][9]. Recently, approaches have moved towards end-to-end deep learning methods for solving specific forensics tasks using labeled training data [10][11][12]. Salloun  learn to detect splicing by training a fully convolutional network on labeled training data. [13][11][14][12] propose improved architectures. Islam [10] incorporate Generative Adversarial Network (GAN) to detect copy-move forgeries. Huh  propose to take photographic metadata as a free and plentiful supervisory signal for learning self-consistency and apply the trained model to detect splices. Recently, TransForensic [15] leverages vision transformers [16] to tackle the problem. High-frequency components still served as useful prior in this field. RGB-N [17] designs an additional noise stream. ObjectFormer [18] extracts high-frequency features as complementary signals to visual content. But unlike ObjectFormer, our main focus is to leverage high-frequency components as a prompting design to efficiently and effectively adapt to different low-level segmentation tasks.

Defocus Blur Detection.Given an image, defocus blur detection aims at separating in-focus and out-of-focus regions, which could be potentially useful for auto-refocus [19], salient object detection [20] and image retargeting [21]. Traditional approaches mainly focus on designing hand-crafted features based on gradient [22][23][24] or edge [25][26]. In the deep era, most methods delve into CNN architectures [27][28][29][30]. [27] proposes the first CNN-based method using both hand-crafted and deep features. BTBNet [29] develops a fully convolutional network to integrate low-level clues and high-level semantic information. DeFusionNet [28] recurrently fuses and refines multi-scale deep features for defocus blur detection. CENet [30] learns multiple smaller defocus blur detectors and ensembles them to enhance diversity.  further employs the depth information as additional supervision and proposes a joint learning framework inspired by knowledge distillation. [31] explores deep ensemble networks for defocus blur detection. [32] proposes to learn generator to generate mask in an adversarial manner.

Shadow Detection.Shadows occur frequently in natural scenes, and have hints for scene geometry [33], light conditions [33] and camera location  and lead to challenging cases in many vision tasks including image segmentation [34] and object tracking [35][36]. Early attempts explore illumination [37] and hand-crafted features [38][39]. In the deep era, some methods mainly focus on the design of CNN architectures [40] or involving the attention modules (_e.g._, the direction-aware attention [41], distraction-aware module [42]). Recent works [43] utilize the lighting as additional prior, for example, ADNet  generates the adversarial training samples for better detection and FDRNet [43] arguments the training samples by additionally adjusted brightness. MTMT [44] leverages the mean teacher model to explore unlabeled data for semi-supervised shadow detection.

Camouflaged Object Detection.Detecting camouflaged objects is a challenging task as foreground objects are often with visual similar patterns to the background. Early works distinguish the foreground and background through low-level clues such as texture [45], brightness [46], and color . Recently, deep learning-based methods [47][48][49][50][51] show their strong ability in detecting complex camouflage objects. Le  propose the first end-to-end network for camouflaged object detection, which is composed of a classification branch and a segmentation branch. Fan [47] develops a search-identification network and the largest camouflaged object detection dataset. PFNet [48] is a bio-inspired framework that mimics the process of positioning and identification in predation. FBNet [51] suggests disentangling frequency modeling and enhancing the important frequency component.

",1
35," Visual Prompting Tuning.Prompting is initially proposed in NLP [1][2]. [1] demonstrates strong generalization to downstream transfer learning tasks even in the few-shot or zero-shot settings with manually chosen prompts in GPT-3. Recently, prompting [3][4] has been adapted to vision tasks. [3] proposes memory tokens which is a set of learnable embedding vectors for each transformer layer. VPT [4] proposes similar ideas and investigates the generality and feasibility of visual prompting via extensive experiments spanning multiple kinds of recognition tasks across multiple domains and backbone architectures. 

Forgery Detection.The goal of forgery detection is to detect pixels that are manually manipulated, such as pixels that are removed, replaced, or edited. Early approaches [5] detect region splicing through inconsistencies in local noise levels, based on the fact that images of different origins might contain different noise characteristics introduced by the sensors or post-processing steps. Other clues are found to be helpful, such as SIFT [6], JPEG compression artifacts [7] and re-sampling artifacts [8][9]. Recently, approaches have moved towards end-to-end deep learning methods for solving specific forensics tasks using labeled training data [10][11][12]. Salloun  learn to detect splicing by training a fully convolutional network on labeled training data. [13][11][14][12] propose improved architectures. Islam [10] incorporate Generative Adversarial Network (GAN) to detect copy-move forgeries. Huh  propose to take photographic metadata as a free and plentiful supervisory signal for learning self-consistency and apply the trained model to detect splices. Recently, TransForensic [15] leverages vision transformers [16] to tackle the problem. High-frequency components still served as useful prior in this field. RGB-N [17] designs an additional noise stream. ObjectFormer [18] extracts high-frequency features as complementary signals to visual content. 

Defocus Blur Detection.Given an image, defocus blur detection aims at separating in-focus and out-of-focus regions, which could be potentially useful for auto-refocus [19], salient object detection [20] and image retargeting [21]. Traditional approaches mainly focus on designing hand-crafted features based on gradient [22][23][24] or edge [25][26]. In the deep era, most methods delve into CNN architectures [27][28][29][30]. [27] proposes the first CNN-based method using both hand-crafted and deep features. BTBNet [29] develops a fully convolutional network to integrate low-level clues and high-level semantic information. DeFusionNet [28] recurrently fuses and refines multi-scale deep features for defocus blur detection. CENet [30] learns multiple smaller defocus blur detectors and ensembles them to enhance diversity.  further employs the depth information as additional supervision and proposes a joint learning framework inspired by knowledge distillation. [31] explores deep ensemble networks for defocus blur detection. [32] proposes to learn generator to generate mask in an adversarial manner.

Shadow Detection.Shadows occur frequently in natural scenes, and have hints for scene geometry [33], light conditions [33] and camera location  and lead to challenging cases in many vision tasks including image segmentation [34] and object tracking [35][36]. Early attempts explore illumination [37] and hand-crafted features [38][39]. In the deep era, some methods mainly focus on the design of CNN architectures [40] or involving the attention modules (_e.g._, the direction-aware attention [41], distraction-aware module [42]). Recent works [43] utilize the lighting as additional prior, for example, ADNet  generates the adversarial training samples for better detection and FDRNet [43] arguments the training samples by additionally adjusted brightness. MTMT [44] leverages the mean teacher model to explore unlabeled data for semi-supervised shadow detection.

Camouflaged Object Detection.Detecting camouflaged objects is a challenging task as foreground objects are often with visual similar patterns to the background. Early works distinguish the foreground and background through low-level clues such as texture [45], brightness [46], and color . Recently, deep learning-based methods [47][48][49][50][51] show their strong ability in detecting complex camouflage objects. Le  propose the first end-to-end network for camouflaged object detection, which is composed of a classification branch and a segmentation branch. Fan [47] develops a search-identification network and the largest camouflaged object detection dataset. PFNet [48] is a bio-inspired framework that mimics the process of positioning and identification in predation. FBNet [51] suggests disentangling frequency modeling and enhancing the important frequency component.

",0
36," **Pretrained Vision-Language Models for Cross Modal Matching.** Owing to the success of Transformer ([1]) architecture equipped with pretrain-finetuning () learning method, pretrained VLMs have made a remarkable performance in cross-modal matching or reasoning tasks (), especially image-text retrieval. Early pretrained VLMs utilize BERT ([4])-like single encoder architecture to encode and fuse the image-text information, then perform image-text reasoning such as ViLBERT ([3]), VisualBERT ([5]), and Oscar (). In addition, dual-encoder architecture such as CLIP ([2]), and ALBERT ([6]), performs better than single-encoder architecture on image-text matching tasks and is widely used in industry because of its efficiency.

**Divide-and-Conquer for Question Answering.** The divide-and-conquer algorithm ()aims to divide the complex problem into multiple simple problems and then combine the subproblem results to achieve the final solution. This idea has been used in complex question-answering tasks in the natural language processing area. [9] proposed to utilize the decomposition of complex questions for semantic parsing. [8] adopt the question decomposition and rescoring method to perform multi-hop reading comprehension, which makes the reasoning path interpretable and robust. [7] utilized the QDMR structures of complex questions to conduct the decompose-synthesize text-to-SQL transformation. Previous pipeline approaches may lead to error cascades in the upper inference process due to the incompleteness or error of decomposed text. The image-text retrieval task has strict requirements on the correctness of text semantic understanding, thus we propose an end-to-end divide-and-conquer method for alleviating the error cascade issue via the whole learning process.

**Dual-Process Theory.** The dual-process theory shows that human brains have two different thinking Systems. System 1 performs analogical reasoning, and System 2 performs conscious logical reasoning. Combining this theory with practical tasks, some researchers designed various approaches. [13] believed that combining vector space models with external knowledge graphs could be regarded as thinking 'fast' in vector space along with thinking'slow' and 'deeply' by reasoning over the knowledge graph. [10] also proposed to use a deep learning network with a tree search engine as System 1 and System 2, respectively, for sequential decision-making problems. [12];  advocated the design of a conscious network to achieve the leap from System 1 to System 2. [11] designed a neural-symbolic system for natural language understanding tasks, which combines the explicit symbolic calculation-based System 2 and fast deep learning network-based System 1. For complex multi-modal reasoning problem, e.g., image retrieval from linguistically complex text, humans usually combine System 1 and System 2 to obtain the final solution. However, current methods relying mainly on deep learning networks resemble System 1 and lack the logical reasoning capability, thus suffering from image-text reasoning with the complex description. In this light, we make the first attempt to combine System 1 and System 2 to tackle this issue by designing a neural divide-and-conquer reasoning framework. We introduce a neural-symbolic reasoner in System 2 to conduct the logical operation. The overall framework contains analogical and logical reasoning as humans think, making appreciable gains.

",1
37," **Pretrained Vision-Language Models for Cross Modal Matching.** Owing to the success of Transformer ([1]) architecture equipped with pretrain-finetuning () learning method, pretrained VLMs have made a remarkable performance in cross-modal matching or reasoning tasks (), especially image-text retrieval. Early pretrained VLMs utilize BERT ([4])-like single encoder architecture to encode and fuse the image-text information, then perform image-text reasoning such as ViLBERT ([3]), VisualBERT ([5]), and Oscar (). In addition, dual-encoder architecture such as CLIP ([2]), and ALBERT ([6]), performs better than single-encoder architecture on image-text matching tasks and is widely used in industry because of its efficiency.

**Divide-and-Conquer for Question Answering.** The divide-and-conquer algorithm ()aims to divide the complex problem into multiple simple problems and then combine the subproblem results to achieve the final solution. This idea has been used in complex question-answering tasks in the natural language processing area. [9] proposed to utilize the decomposition of complex questions for semantic parsing. [8] adopt the question decomposition and rescoring method to perform multi-hop reading comprehension, which makes the reasoning path interpretable and robust. [7] utilized the QDMR structures of complex questions to conduct the decompose-synthesize text-to-SQL transformation. Previous pipeline approaches may lead to error cascades in the upper inference process due to the incompleteness or error of decomposed text. 

**Dual-Process Theory.** The dual-process theory shows that human brains have two different thinking Systems. System 1 performs analogical reasoning, and System 2 performs conscious logical reasoning. Combining this theory with practical tasks, some researchers designed various approaches. [13] believed that combining vector space models with external knowledge graphs could be regarded as thinking 'fast' in vector space along with thinking'slow' and 'deeply' by reasoning over the knowledge graph. [10] also proposed to use a deep learning network with a tree search engine as System 1 and System 2, respectively, for sequential decision-making problems. [12];  advocated the design of a conscious network to achieve the leap from System 1 to System 2. [11] designed a neural-symbolic system for natural language understanding tasks, which combines the explicit symbolic calculation-based System 2 and fast deep learning network-based System 1. For complex multi-modal reasoning problem, e.g., image retrieval from linguistically complex text, humans usually combine System 1 and System 2 to obtain the final solution. However, current methods relying mainly on deep learning networks resemble System 1 and lack the logical reasoning capability, thus suffering from image-text reasoning with the complex description. ",0
38," Vision-Language Models (VLMs).Foundation models (e.g., GPT-3 [1], PaLM [2], and Florence [3]) trained on massive data show a surprising ability on many applications. In computer vision, milestone works, i.e., CLIP [4] and ALIGN [5], which learn the aligned embedding space of text and images via contrastive learning, demonstrate surprising transferability on downstream tasks. They inspire many researchers to explore better vision-language pre-training [6][7][8][9][10][3]. To this day, CLIP, trained on 400 million image-text pairs, is still one of the best VLM released publicly. VLMs also show great potential to address various visual tasks with the language prior, including detection [11][12][13], segmentation [14][15][16], and recognition [17][18][19].

Prompt Learning.Prompt learning is initially proposed for adapting the large pre-trained language models in natural language processing (NLP) [1][20]. Since various NLP tasks can be unified as the ""_text-to-text_"" problem [21], the specialized prompt is applied to guide the language model to answer the corresponding question [1][22][23]. However, manual crafting of prompts is difficult and often sub-optimal. Recently, automatic prompt generation [24][25][26][27][28] has emerged as a promising way to adapt language models effectively.

In computer vision, the pioneering work, Context Optimization (CoOp) , employs prompt learning to generate an appropriate prompt closer to the task context for improving the recognition of VLMs. Due to its simplicity and effectiveness, many works extend CoOp and apply prompt learning to board vision tasks [29][30][17][31][32][33][34]. Despite various progressions of existing works, adapting VLMs to multi-task learning with prompting is still an under-explored problem. In addition, although Conditional CoOp [33] also discusses the poor generalization of the task-individual prompt on _unseen classes_, it does not obtain better _in-distribution_ generalization, even worse than CoOp. Our HiPro demonstrates that training prompts with data from multiple tasks can effectively improve the in-distribution generalization of prompt learning. The most related work [35] is leveraging prompt learning for multiple perception tasks in autonomous driving scenarios. However, it has many specialized designing for autonomous driving, which is difficult to extend to other multi-task learning settings.

Multi-Task Learning.Multi-task learning (MTL) aims to improve the average performance of multiple target tasks from training together. Common methods design strategies or structures to share information across tasks, including hard sharing [36], soft sharing [37][38][39], and learnable sharing [40][41][42][43]. However, training different tasks on a shared model raises the difficulty of optimization and could lead to a negative transfer. Several works attempt to identify the suitable combination of tasks that can benefit from training together, also referred to as _task grouping_[44][45][46]. Other popular methods [47][48][49][50][51] aim to improve the optimization dynamics of MTL, e.g., modifying the gradient direction for mitigating conflict [51]. Despite significant progress, the exploration of MTL based on the modern large-scale VLM is still limited, which is an important step for developing the in-the-wild vision system. In addition, our method, guiding the frozen VLM to address various tasks with the lightweight prompt, is an efficient multi-task learner. We also compare HiPro with advanced MTL methods based on their variants of prompt learning. HiPro demonstrates clear improvements compared with MTL baselines.

",1
39," Vision-Language Models (VLMs).Foundation models (e.g., GPT-3 [1], PaLM [2], and Florence [3]) trained on massive data show a surprising ability on many applications. In computer vision, milestone works, i.e., CLIP [4] and ALIGN [5], which learn the aligned embedding space of text and images via contrastive learning, demonstrate surprising transferability on downstream tasks. They inspire many researchers to explore better vision-language pre-training [6][7][8][9][10][3]. To this day, CLIP, trained on 400 million image-text pairs, is still one of the best VLM released publicly. VLMs also show great potential to address various visual tasks with the language prior, including detection [11][12][13], segmentation [14][15][16], and recognition [17][18][19].

Prompt Learning.Prompt learning is initially proposed for adapting the large pre-trained language models in natural language processing (NLP) [1][20]. Since various NLP tasks can be unified as the ""_text-to-text_"" problem [21], the specialized prompt is applied to guide the language model to answer the corresponding question [1][22][23]. However, manual crafting of prompts is difficult and often sub-optimal. Recently, automatic prompt generation [24][25][26][27][28] has emerged as a promising way to adapt language models effectively.

In computer vision, the pioneering work, Context Optimization (CoOp) , employs prompt learning to generate an appropriate prompt closer to the task context for improving the recognition of VLMs. Due to its simplicity and effectiveness, many works extend CoOp and apply prompt learning to board vision tasks [29][30][17][31][32][33][34]. Despite various progressions of existing works, adapting VLMs to multi-task learning with prompting is still an under-explored problem. In addition, although Conditional CoOp [33] also discusses the poor generalization of the task-individual prompt on _unseen classes_, it does not obtain better _in-distribution_ generalization, even worse than CoOp. The most related work [35] is leveraging prompt learning for multiple perception tasks in autonomous driving scenarios. However, it has many specialized designing for autonomous driving, which is difficult to extend to other multi-task learning settings.

Multi-Task Learning.Multi-task learning (MTL) aims to improve the average performance of multiple target tasks from training together. Common methods design strategies or structures to share information across tasks, including hard sharing [36], soft sharing [37][38][39], and learnable sharing [40][41][42][43]. However, training different tasks on a shared model raises the difficulty of optimization and could lead to a negative transfer. Several works attempt to identify the suitable combination of tasks that can benefit from training together, also referred to as _task grouping_[44][45][46]. Other popular methods [47][48][49][50][51] aim to improve the optimization dynamics of MTL, e.g., modifying the gradient direction for mitigating conflict [51]. Despite significant progress, the exploration of MTL based on the modern large-scale VLM is still limited, which is an important step for developing the in-the-wild vision system. 

",0
40," Multiple Instance Learning (MIL)  has been widely used in WSI analysis with its unique learning paradigm in recent years [1][2][3]. MIL is a weakly supervised learning framework that utilizes coarse-grained bag labels for training instead of fine-grained instance annotations. Previous algorithms can be broadly categorized into two groups: instance-level [4][5][3] and embedding-level [6][7][8][9]. The former obtain instance labels and aggregate them to obtain the bag label, whereas the latter aggregate all instance features into a high-level bag embedding for bag prediction. Most embedding-level methods share the basic idea of AB-MIL [10], which employs learnable weights to aggregate salient instance features into bag embedding. Furthermore, some MIL frameworks [1][3][9] mine more salient instances making classification easier and facilitating classification. For example, Lu _et al_. selected the most salient instances based on their attention scores (e.g., maximum and minimum scores) to compute instance-level loss and improve performance . Zhang _et al_. proposed a class activation map (CAM) based on the AB-MIL paradigm to better mine salient instances and used AB-MIL to aggregate them into bag embedding [9]. In addition, feature clustering methods [7][11] computed cluster centroids of all feature embeddings and used representative embeddings for the final prediction. However, all these methods focused excessively on salient instances in training, which are easy instances with high confidence scores and can be easily classified. As a result, they overlook the importance of hard instances for training. In this paper, we intend to mine hard instances for improving WSI classification performance.

Hard sample mining is a popular technique to speed up convergence and enhance the discriminative power of the model in many deep learning areas, such as face recognition [12], object detection [13][14], person re-identification [15][16][17][18], and deep metric learning [19][20]. The main idea behind this technique is to select the samples which are hard to classify correctly (i.e., hard negatives and hard positives) for alleviating the imbalance between positive and negative samples and facilitating model training. There are generally three groups of approachesfor evaluating sample difficulty: loss-based [21], similarity-based [22], and learnable weight-based [23]. Typically, these strategies require complete sample supervision information. Drawing on the ideas of the above works, we propose a hard instance mining approach in MIL, mining hard examples at the instance level. In this, there are no complete instance labels, only the bag label is available. Similar to our approach, Li _et al_. utilized attention scores to identify salient instances from false negative bags to serve as hard negative instances and used them to compose the hard bags for improving classification performance . A key difference is that we indirectly mine hard instances by masking out the most salient instances rather than directly locating hard negative instances.

",1
41," Multiple Instance Learning (MIL)  has been widely used in WSI analysis with its unique learning paradigm in recent years [1][2][3]. MIL is a weakly supervised learning framework that utilizes coarse-grained bag labels for training instead of fine-grained instance annotations. Previous algorithms can be broadly categorized into two groups: instance-level [4][5][3] and embedding-level [6][7][8][9]. The former obtain instance labels and aggregate them to obtain the bag label, whereas the latter aggregate all instance features into a high-level bag embedding for bag prediction. Most embedding-level methods share the basic idea of AB-MIL [10], which employs learnable weights to aggregate salient instance features into bag embedding. Furthermore, some MIL frameworks [1][3][9] mine more salient instances making classification easier and facilitating classification. For example, Lu _et al_. selected the most salient instances based on their attention scores (e.g., maximum and minimum scores) to compute instance-level loss and improve performance . Zhang _et al_. proposed a class activation map (CAM) based on the AB-MIL paradigm to better mine salient instances and used AB-MIL to aggregate them into bag embedding [9]. In addition, feature clustering methods [7][11] computed cluster centroids of all feature embeddings and used representative embeddings for the final prediction. However, all these methods focused excessively on salient instances in training, which are easy instances with high confidence scores and can be easily classified. As a result, they overlook the importance of hard instances for training. 

Hard sample mining is a popular technique to speed up convergence and enhance the discriminative power of the model in many deep learning areas, such as face recognition [12], object detection [13][14], person re-identification [15][16][17][18], and deep metric learning [19][20]. The main idea behind this technique is to select the samples which are hard to classify correctly (i.e., hard negatives and hard positives) for alleviating the imbalance between positive and negative samples and facilitating model training. There are generally three groups of approachesfor evaluating sample difficulty: loss-based [21], similarity-based [22], and learnable weight-based [23]. Typically, these strategies require complete sample supervision information. 

",0
42," Methods that use part-of-speech , entropy , latent semantic analysis  and temporal semantic indexing  have been proposed for detecting changes in word meanings. In SemEval-2020 Task 1  two subtasks were proposed for detecting lexical semantic change: a binary classification task (for a given set of target words, decide which words had their meaning altered, and which ones not) and a ranking task (rank a set of target words according to their degree of lexical semantic change between the two corpora). Giulianelli et al. giulianelli2020supervised showed that contextualised embeddings obtained from an MLM can be used to measure the change of word meaning. Rosin and Radinsky rosin2022supervised proposed a temporal attention mechanism by extending the self-attention mechanism in transformers, where time stamps of the documents are considered when computing the attention scores. Aida and Bollegala aida2023temporal proposed a method to predict semantic change of words by comparing the distributions of contextualised embeddings of the word between two given corpora, sampled at different points in time. Our goal in this paper extends beyond the detection of a subset of words with a change in lexical semantics, and to adapt MLMs over time.

DWEs [15, 14, 16] incorporate extralinguistic information such as time, demographic or social aspects of words with linguistic information. Welch et al. welch2020context learnt demographic word embeddings, covering attributes such as age, gender, location and religion. Zeng et al. zeng2017socialized learnt _socialised_ word embeddings considering both a social media user's personal characteristics of language use and that user's social relationships. However, Hofmann et al. hofmann2021supervised showed that temporal factors have a stronger impact than socio-cultural factors when determining the semantic variations of words. Consequently, in this paper we focus on the temporal adaptation of DCWEs.

Diachronic Language Models that capture the meanings of words at a particular point in time have been trained using historical corpora [14, 15]. These prior work learn independent word embedding models from different corpora. This is problematic because information related to a word is not shared across different models resulting in inefficient learning, especially when word occurrences within a single snapshot of a corpus are too sparse to learn accurate embeddings.

Rudolph and Blei rudolph2018supervised proposed a dynamic Bernoulli embedding method based on exponential family embeddings, where each word is represented by a one-hot vector with dimensionality set to the vocabulary size. This model is extended to the temporal case by considering different time-slices where only the word embedding vector is time-specific and the context vectors are shared across the corpus and over time-slices. Because the joint distribution over time and context is intractable, they maximise the pseudo log-likelihoodof the conditional distribution for learning the parameters of their DWE model. [1] proposed a domain adaptation method based on automatically learnt prompts. Given a test example, they generate a unique prompt and conditioned on it, then predict labels for test examples. Although their method uses prompts to adapt a model, they do _not_ consider temporal adaptation of MLMs, which is our focus. Moreover, we do not require any labelled examples in our proposal.

Amba [2] proposed a model updating method using vocabulary composition and data sampling to adapt language models to continuously evolving web content. However, their work is specific to one dataset and two classification tasks, and focuses on incremental training.  introduced a benchmark for ever-evolving language models, utilising the difference between consecutive snapshots of datasets, to track language models' ability to retain existing knowledge while incorporating new knowledge at each time point.  studied the lifelong language model pretraining problem, where the goal is to continually update pretrained language models using emerging data. [3] introduced a diagnostic dataset to investigate language models for factual knowledge that changes over time and proposed an approach to jointly model texts with their timestamps. They also demonstrated that models trained with temporal context can be adapted to new data without retraining from scratch. [4] proposed TempoBERT, where they insert a special time-related token to each sentence and fine-tune BERT using a customised time masking. TempoBERT reports superior results in  Task 1 semantic variation detection benchmark. As shown later in SS4.1, our proposed method outperforms TempoBERT.

[5] proposed DCWEs, which are computed in two stages. First, words are mapped to dynamic type-level representations considering temporal and social information. The type-level representation of a word is formed by combining a non-dynamic embedding of a word and a dynamic offset that is specific to the social and temporal aspects of the word. Second, these dynamic embeddings are converted to context-dependent token-level representations. To the best of our knowledge, this is the only word embedding method that produces both dynamic as well as contextualised representations, thus mostly relates to us. As shown in SS4, our proposed method outperforms their DCWEs on four datasets.

",1
43," Methods that use part-of-speech , entropy , latent semantic analysis  and temporal semantic indexing  have been proposed for detecting changes in word meanings. In SemEval-2020 Task 1  two subtasks were proposed for detecting lexical semantic change: a binary classification task (for a given set of target words, decide which words had their meaning altered, and which ones not) and a ranking task (rank a set of target words according to their degree of lexical semantic change between the two corpora). Giulianelli et al. giulianelli2020supervised showed that contextualised embeddings obtained from an MLM can be used to measure the change of word meaning. Rosin and Radinsky rosin2022supervised proposed a temporal attention mechanism by extending the self-attention mechanism in transformers, where time stamps of the documents are considered when computing the attention scores. Aida and Bollegala aida2023temporal proposed a method to predict semantic change of words by comparing the distributions of contextualised embeddings of the word between two given corpora, sampled at different points in time. 

DWEs [15, 14, 16] incorporate extralinguistic information such as time, demographic or social aspects of words with linguistic information. Welch et al. welch2020context learnt demographic word embeddings, covering attributes such as age, gender, location and religion. Zeng et al. zeng2017socialized learnt _socialised_ word embeddings considering both a social media user's personal characteristics of language use and that user's social relationships. However, Hofmann et al. hofmann2021supervised showed that temporal factors have a stronger impact than socio-cultural factors when determining the semantic variations of words. 

Diachronic Language Models that capture the meanings of words at a particular point in time have been trained using historical corpora [14, 15]. These prior work learn independent word embedding models from different corpora. This is problematic because information related to a word is not shared across different models resulting in inefficient learning, especially when word occurrences within a single snapshot of a corpus are too sparse to learn accurate embeddings.

Rudolph and Blei rudolph2018supervised proposed a dynamic Bernoulli embedding method based on exponential family embeddings, where each word is represented by a one-hot vector with dimensionality set to the vocabulary size. This model is extended to the temporal case by considering different time-slices where only the word embedding vector is time-specific and the context vectors are shared across the corpus and over time-slices. Because the joint distribution over time and context is intractable, they maximise the pseudo log-likelihoodof the conditional distribution for learning the parameters of their DWE model. [1] proposed a domain adaptation method based on automatically learnt prompts. Given a test example, they generate a unique prompt and conditioned on it, then predict labels for test examples. 

Amba [2] proposed a model updating method using vocabulary composition and data sampling to adapt language models to continuously evolving web content. However, their work is specific to one dataset and two classification tasks, and focuses on incremental training.  introduced a benchmark for ever-evolving language models, utilising the difference between consecutive snapshots of datasets, to track language models' ability to retain existing knowledge while incorporating new knowledge at each time point.  studied the lifelong language model pretraining problem, where the goal is to continually update pretrained language models using emerging data. [3] introduced a diagnostic dataset to investigate language models for factual knowledge that changes over time and proposed an approach to jointly model texts with their timestamps. They also demonstrated that models trained with temporal context can be adapted to new data without retraining from scratch. [4] proposed TempoBERT, where they insert a special time-related token to each sentence and fine-tune BERT using a customised time masking. TempoBERT reports superior results in  Task 1 semantic variation detection benchmark. 

[5] proposed DCWEs, which are computed in two stages. First, words are mapped to dynamic type-level representations considering temporal and social information. The type-level representation of a word is formed by combining a non-dynamic embedding of a word and a dynamic offset that is specific to the social and temporal aspects of the word. Second, these dynamic embeddings are converted to context-dependent token-level representations. 

",0
44," Some works [1][2] have studied the connection between current optimization approaches, such as SGD , Adam [3], AdamW [4] and others [5][6] and in-distribution generalization. Some literature shows that Adam is more vulnerable to sharp minima than SGD [7], which may result in worse generalization [8][9][10]. Some follow-ups [11][12][1][13] propose generalizable optimizers to address this problem. However, the generalization ability and convergence speed is often a trade-off [14][1][6][13][5]. Different optimizers may favor different tasks and network architectures(e.g., SGD is often chosen for ResNet [15] while AdamW [4] for ViTs [16]). Selecting the right optimizer is critical for performance and convergence while the understanding of its relationship to model generalization remains nascent [2]. [17] finds that fine-tuned SGD outperforms Adam on OOD tasks. Yet all the previous works do not consider the effect of optimizers in current DG benchmarks and evaluation protocols. In this paper, we discuss the selection of the default optimizer in DG benchmarks.

Domain generalization (DG) aims to improve the generalization ability to novel domains [18][19]. A common approach is to learn domain-invariant or causal features over multiple source domains [20][21][22][23][24][25][26][27][28][29][30] or aggregating domain-specific modules [31][32]. Some works propose to enlarge the input space by augmentation of training data [33][34][35][36][37][38]. Exploit regularization with meta-learning [20][39] and Invariant Risk Minimization (IRM) framework [40] are also proposed for DG. Recently, several works propose to ensemble model weights for better generalization [41][42][43][44][45][46][47] and achieve outstanding performance.

Recent works study the relationship between flatter minima lead and better generalization on in-distribution data [14][48][49][50]. [51] reviews the literature related to generalization and the sharpness of minima. It highlights the role of maximum Hessian eigenvalue in deciding the sharpness of minima [14][52]. Some simple strategies are proposed to optimize maximum Hessian eigenvalue, such as choosing a large learning rate [53][54][55] and smaller batch size [53][56]. Sharpness-Aware Minimization (SAM) [2] and its variants [48][57][58][59][60][61][62][63], which are representative training algorithm to seek zeroth-order flatness, show outstanding performance on in-distribution generalization. Most recently, several works discuss the relationship between generalization and gradient norm [64][65]. GAM[66] proposes to optimize first-order flatness for better generalization. [41][67] show that flatness minima also lead to superior OOD generalization. However, none of the previous works consider the optimization in DG nor provide theoretical assurance of their approach.

",1
45," Some works [1][2] have studied the connection between current optimization approaches, such as SGD , Adam [3], AdamW [4] and others [5][6] and in-distribution generalization. Some literature shows that Adam is more vulnerable to sharp minima than SGD [7], which may result in worse generalization [8][9][10]. Some follow-ups [11][12][1][13] propose generalizable optimizers to address this problem. However, the generalization ability and convergence speed is often a trade-off [14][1][6][13][5]. Different optimizers may favor different tasks and network architectures(e.g., SGD is often chosen for ResNet [15] while AdamW [4] for ViTs [16]). Selecting the right optimizer is critical for performance and convergence while the understanding of its relationship to model generalization remains nascent [2]. [17] finds that fine-tuned SGD outperforms Adam on OOD tasks. Yet all the previous works do not consider the effect of optimizers in current DG benchmarks and evaluation protocols. 

Domain generalization (DG) aims to improve the generalization ability to novel domains [18][19]. A common approach is to learn domain-invariant or causal features over multiple source domains [20][21][22][23][24][25][26][27][28][29][30] or aggregating domain-specific modules [31][32]. Some works propose to enlarge the input space by augmentation of training data [33][34][35][36][37][38]. Exploit regularization with meta-learning [20][39] and Invariant Risk Minimization (IRM) framework [40] are also proposed for DG. Recently, several works propose to ensemble model weights for better generalization [41][42][43][44][45][46][47] and achieve outstanding performance.

Recent works study the relationship between flatter minima lead and better generalization on in-distribution data [14][48][49][50]. [51] reviews the literature related to generalization and the sharpness of minima. It highlights the role of maximum Hessian eigenvalue in deciding the sharpness of minima [14][52]. Some simple strategies are proposed to optimize maximum Hessian eigenvalue, such as choosing a large learning rate [53][54][55] and smaller batch size [53][56]. Sharpness-Aware Minimization (SAM) [2] and its variants [48][57][58][59][60][61][62][63], which are representative training algorithm to seek zeroth-order flatness, show outstanding performance on in-distribution generalization. Most recently, several works discuss the relationship between generalization and gradient norm [64][65]. GAM[66] proposes to optimize first-order flatness for better generalization. [41][67] show that flatness minima also lead to superior OOD generalization. However, none of the previous works consider the optimization in DG nor provide theoretical assurance of their approach.

",0
46," **Deep learning based SR methods**. [1][2][3][4] have achieved impressive performances, in multi-scale scenarios one has to train and store several models for each scale factor, which is unfeasible when considering time and memory budgets. In recent years, several methods [5][6][7] are proposed to achieve arbitrary-scale SR with a single model, but their performances are limited when dealing with out-of-distribution scaling factors. Inspired by INF, LIIF [8] takes continuous coordinates and latent variables as inputs, and employs an MLP to achieve outstanding performances for both in-distribution and out-of-distribution factors. In contrast, LTE [9] transforms input coordinates into the Fourier domain and uses the dominant frequencies extracted from latent variables to address the spectral bias problem [10][11]. In a nutshell, treating images as RGB-valued functions and sharing the implicit function space are the keys to the success of LIIF-like works [8][9]. Nevertheless, a purely local decoder, like MLP, is not able to accurately approximate arbitrary images, although it is rather sensitive to the input coordinates.

**Neural Operators**. Recently, a novel neural network architecture, Neural Operator (NO), was proposed for discretization invariant solutions of PDEs via infinite-dimensional operator learning [12][13][14]. Neural operators only need to be trained once and are capable of transferring solutions between differently discretized meshes while keeping a fixed approximation error. A valuable merit of NO is that it does not require knowledge of the underlying PDE, which allows us to introduce it by the following abstract form,where \(u:D\rightarrow\mathbb{R}^{d_{a}}\) is the solution function residing in the Banach space \(\mathcal{U}\), and \(\mathbf{L}:\mathcal{A}\rightarrow\mathbf{L}(\mathcal{U};\mathcal{U}^{*})\) is an operator-valued functional that maps the coefficient function \(a\in\mathcal{A}\) of the PDE to \(f\in\mathcal{U}^{*}\), the dual space of \(\mathcal{U}\). As in many cases the inverse operator of \(\mathbf{L}\) even does not exist, NO seeks a feasible operator \(\mathcal{G}:\mathcal{A}\rightarrow\mathcal{U},a\mapsto u\), directly mapping the coefficient to the solution within an acceptable tolerance.

The operator \(\mathcal{G}\) is numerically approximated by training a neural network \(\mathcal{G}_{\theta}:\mathcal{A}\rightarrow\mathcal{U}\), where \(\theta\) are the trainable parameters. Suppose we have \(N\) pairs of observations \(\{a_{j},u_{j}\}_{j=1}^{N}\) where the input functions \(a_{j}\) are sampled from probability measure \(\mu\) compactly supported on \(\mathcal{A}\), and \(u_{j}=\mathcal{G}(a_{j})\) are used as the supervisory output functions. The infinitely dimensional operator learning problem \(\mathcal{G}\leftarrow\mathcal{G}_{\theta}\) thus is associated with the empirical-risk minimization problem . In practice, we actually measure the approximation loss using the sampled observationsand \(a_{o}^{(j)}\), which are the direct results of discretization:

\[\min_{\theta}\mathbb{E}_{a\sim\mu}\|\mathcal{G}(a)-\mathcal{G}_{ \theta}(a)\|_{\mathcal{U}} \tag{2}\] \[\approx\min_{\theta}\frac{1}{N}\sum_{j=1}^{N}\|u_{o}^{(j)}- \mathcal{G}_{\theta}(a_{o}^{(j)}\|_{\mathcal{U}}.\]

Similar to classical feedforward neural networks (FFNs), the NO is of an iterative architecture. For ease of exposition, suppose \(\mathcal{A}\) is defined on the bounded domain \(D\subset\mathbb{R}^{d}\), and the inputs and outputs of the intermediate layers are all vector-valued functions, with dimension \(d_{z}\). Then \(\mathcal{G}_{\theta}:\mathcal{A}\rightarrow\mathcal{U}\) can be formulated as follows:

\[z_{0}(x) =\mathcal{L}(x,a(x)), \tag{3}\] \[z_{t+1}(x) =\sigma(W_{t}z_{t}(x)+(\mathcal{K}_{t}(z_{t};\Phi))(x)),\] (4) \[u(x) =\mathcal{P}(z_{T}(x)), \tag{5}\]where \(\mathcal{L}:\mathbb{R}^{d_{a}+d}\rightarrow\mathbb{R}^{d_{z}}\), and \(\mathcal{P}:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{u}}\) are the local lifting and projection functions respectively, mapping the input \(a\) to its first layer hidden representation \(z_{0}\) and the last layer hidden representation \(z_{T}\) back to the output function \(u\). \(W:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{z}}\) is a point-wise linear transformation, and \(\sigma:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{z}}\) is the nonlinear activation function.

Although the PDE in (1) point-wisely defines the behavior of the solution function \(u(x)\), the solution operator \(\mathcal{G}\) we are seeking should exhibit the non-local property such that \(\mathcal{G}(a)\) can approximate \(u\) everywhere rather than locally. For this purpose, NO may employ the kernel integral operators \(\mathcal{K}_{t}:\{z_{t}:D_{t}\rightarrow\mathbb{R}^{d_{z}}\}\mapsto\{z_{t+1}:D_ {t+1}\rightarrow\mathbb{R}^{d_{z}}\}\) to maintain the continuum in the spatial domain, and one of the most adopted forms [13] is defined as

\[(\mathcal{K}_{t}(z_{t};\Phi))(x)=\int_{D}K_{t}(x,y;\Phi)z_{t}(y)\mathrm{d}y, \quad\forall x\in D, \tag{6}\]where the kernel matrix \(K_{t}:\mathbb{R}^{d+d}\rightarrow\mathbb{R}^{d_{z}\times d_{z}}\) is parameterized by \(\Phi\).

",1
47," **Deep learning based SR methods**. [1][2][3][4] have achieved impressive performances, in multi-scale scenarios one has to train and store several models for each scale factor, which is unfeasible when considering time and memory budgets. In recent years, several methods [5][6][7] are proposed to achieve arbitrary-scale SR with a single model, but their performances are limited when dealing with out-of-distribution scaling factors. Inspired by INF, LIIF [8] takes continuous coordinates and latent variables as inputs, and employs an MLP to achieve outstanding performances for both in-distribution and out-of-distribution factors. In contrast, LTE [9] transforms input coordinates into the Fourier domain and uses the dominant frequencies extracted from latent variables to address the spectral bias problem [10][11]. In a nutshell, treating images as RGB-valued functions and sharing the implicit function space are the keys to the success of LIIF-like works [8][9]. Nevertheless, a purely local decoder, like MLP, is not able to accurately approximate arbitrary images, although it is rather sensitive to the input coordinates.

**Neural Operators**. Recently, a novel neural network architecture, Neural Operator (NO), was proposed for discretization invariant solutions of PDEs via infinite-dimensional operator learning [12][13][14]. Neural operators only need to be trained once and are capable of transferring solutions between differently discretized meshes while keeping a fixed approximation error. A valuable merit of NO is that it does not require knowledge of the underlying PDE, which allows us to introduce it by the following abstract form,where \(u:D\rightarrow\mathbb{R}^{d_{a}}\) is the solution function residing in the Banach space \(\mathcal{U}\), and \(\mathbf{L}:\mathcal{A}\rightarrow\mathbf{L}(\mathcal{U};\mathcal{U}^{*})\) is an operator-valued functional that maps the coefficient function \(a\in\mathcal{A}\) of the PDE to \(f\in\mathcal{U}^{*}\), the dual space of \(\mathcal{U}\). As in many cases the inverse operator of \(\mathbf{L}\) even does not exist, NO seeks a feasible operator \(\mathcal{G}:\mathcal{A}\rightarrow\mathcal{U},a\mapsto u\), directly mapping the coefficient to the solution within an acceptable tolerance.

The operator \(\mathcal{G}\) is numerically approximated by training a neural network \(\mathcal{G}_{\theta}:\mathcal{A}\rightarrow\mathcal{U}\), where \(\theta\) are the trainable parameters. Suppose we have \(N\) pairs of observations \(\{a_{j},u_{j}\}_{j=1}^{N}\) where the input functions \(a_{j}\) are sampled from probability measure \(\mu\) compactly supported on \(\mathcal{A}\), and \(u_{j}=\mathcal{G}(a_{j})\) are used as the supervisory output functions. The infinitely dimensional operator learning problem \(\mathcal{G}\leftarrow\mathcal{G}_{\theta}\) thus is associated with the empirical-risk minimization problem . 

\[\min_{\theta}\mathbb{E}_{a\sim\mu}\|\mathcal{G}(a)-\mathcal{G}_{ \theta}(a)\|_{\mathcal{U}} \tag{2}\] \[\approx\min_{\theta}\frac{1}{N}\sum_{j=1}^{N}\|u_{o}^{(j)}- \mathcal{G}_{\theta}(a_{o}^{(j)}\|_{\mathcal{U}}.\]

Similar to classical feedforward neural networks (FFNs), the NO is of an iterative architecture. For ease of exposition, suppose \(\mathcal{A}\) is defined on the bounded domain \(D\subset\mathbb{R}^{d}\), and the inputs and outputs of the intermediate layers are all vector-valued functions, with dimension \(d_{z}\). Then \(\mathcal{G}_{\theta}:\mathcal{A}\rightarrow\mathcal{U}\) can be formulated as follows:

\[z_{0}(x) =\mathcal{L}(x,a(x)), \tag{3}\] \[z_{t+1}(x) =\sigma(W_{t}z_{t}(x)+(\mathcal{K}_{t}(z_{t};\Phi))(x)),\] (4) \[u(x) =\mathcal{P}(z_{T}(x)), \tag{5}\]where \(\mathcal{L}:\mathbb{R}^{d_{a}+d}\rightarrow\mathbb{R}^{d_{z}}\), and \(\mathcal{P}:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{u}}\) are the local lifting and projection functions respectively, mapping the input \(a\) to its first layer hidden representation \(z_{0}\) and the last layer hidden representation \(z_{T}\) back to the output function \(u\). \(W:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{z}}\) is a point-wise linear transformation, and \(\sigma:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{z}}\) is the nonlinear activation function.

Although the PDE in (1) point-wisely defines the behavior of the solution function \(u(x)\), the solution operator \(\mathcal{G}\) we are seeking should exhibit the non-local property such that \(\mathcal{G}(a)\) can approximate \(u\) everywhere rather than locally. For this purpose, NO may employ the kernel integral operators \(\mathcal{K}_{t}:\{z_{t}:D_{t}\rightarrow\mathbb{R}^{d_{z}}\}\mapsto\{z_{t+1}:D_ {t+1}\rightarrow\mathbb{R}^{d_{z}}\}\) to maintain the continuum in the spatial domain, and one of the most adopted forms [13] is defined as

\[(\mathcal{K}_{t}(z_{t};\Phi))(x)=\int_{D}K_{t}(x,y;\Phi)z_{t}(y)\mathrm{d}y, \quad\forall x\in D, \tag{6}\]where the kernel matrix \(K_{t}:\mathbb{R}^{d+d}\rightarrow\mathbb{R}^{d_{z}\times d_{z}}\) is parameterized by \(\Phi\).

",0
48," **Group convolutions:** In 2016, Cohen and Welling proposed G-CNN [1], enabling equivariance beyond translations on 2D images with generalized G-convolutions (group convolutions) over the group of 90-degree rotations, which is one of the earliest efforts in equivariant deep learning. Group convolution is similar to conventional convolutions but has an extended domain for feature maps and kernels. The idea was then applied to different networks to enable equivariance for \(\mathrm{SE}(2)\)[1][2], \(\mathrm{SO}(3)\)[3], \(\mathrm{SE}(3)\)[4], and \(\mathrm{E}(3)\)[5] groups up to some discretization. The idea mainly works with finite (discretized) groups, as it is convenient to parameterize feature maps and kernels on the discretized group elements just as on pixel grids. Group convolutions have a relatively simple structure, making them straightforward to apply, but a major downside is that the lifted domain of features and kernels causes higher computational and memory costs, and the problem is more prominent when the group is large. Groups convs can also work with continuous groups, for example, with the help of Monte Carlo (MC) estimation as in [6], but they suffer from a large memory burden in MC sampling when the number of layers grows [7].

**Steerable CNNs:** Another line of work is steerable CNNs. Instead of augmenting the domain of feature maps, steerable CNNs generalize the space of feature values to be _steerable_, i.e., the feature values transform predictably as the input transforms. The way the feature transforms is called the group representation in the feature space, governed by the feature _type_. Features of _scalar_ type are kept unchanged under group actions, and we call the group representation trivial. For _vector_ or _tensor_ feature types, the representation is not trivial, and the features will change with group actions, for example, through rotation matrix multiplication. Steerable CNNs work for both discretized and continuous groups. One may freely design the feature types based on pre-determined basic types and corresponding representations (irreducible representations, i.e., _irreps_) as building blocks for a given group. Group convolutions can be viewed as a special case of steerable CNNs with _regular_ representations, i.e., channels of a feature vector undergo permutations when transformed. Examples of steerable CNNs include [8][9][10][11]. While the framework of steerable CNNs generalizes group convolutions with more flexibility, it requires a good understanding of representation theory and involves generalized Fourier analysis when working with a continuous group, which makes the structure complicated and challenging to apply in real-world applications broadly. The work of [12][13] also found empirically that steerable CNNs with irreps underperform regular representations in certain tasks.

**Theoretical progress:** There has been a lot of progress in the theoretical development of equivariant networks that are not group-specific. [14] developed a general formulation for steerable CNNs with scalar type features, and [15][16] generalized it to arbitrary feature types. It is proven that all equivariant linear maps can be written as convolutions. The formulation from the perspective of Fourier analysis was presented in [17]. [13] found the general form of solution for the equivariant kernels for \(E(2)\) group, later generalized to any compact group [18]. [19] proposed an algorithm to solve for the equivariance constraint for arbitrary matrix group. The formulation of group convolution based on MC estimation was proposed for any Lie group with [6] or without [7] surjective exponential maps. Equivariant non-linear layers like transformers [12][20] and equivariant set and graph networks [21][22][19] are also proposed, but they are not the focus of this paper.

**Applications of equivariant learning in perception tasks:** We want to highlight a few equivariant networks that gain attention in perception applications due to their simplicity and practicality. Vector Neurons [23] is a PointNet-like \(\mathrm{SO}(3)\)-equivariant network for 3D point cloud learning, later applied to point cloud registration [24] and manipulation [25]. It can be viewed as a special case of TFN [9] with type-1 features and self-interactions only. E2CNN [13] as an \(\mathrm{SE}(2)\)-equivariant framework was applied in several image processing tasks [26][27], given its generality and user-friendly library. DEVIANT [28] applied scale-equivariant convolutions in monocular 3D object detection. EPN [4] is a group convolution network with \(\mathrm{SE}(3)\)-equivariance for 3D point cloud learning based on KPConv [29] and was used in practice, for example in place recognition task . We also position this proposed work in this category, aiming to promote the application of equivariant learning with our efficient and easy-to-use design. Our work is developed based on EPN, which also serves as a major baseline in this paper.

",1
49," **Group convolutions:** In 2016, Cohen and Welling proposed G-CNN [1], enabling equivariance beyond translations on 2D images with generalized G-convolutions (group convolutions) over the group of 90-degree rotations, which is one of the earliest efforts in equivariant deep learning. Group convolution is similar to conventional convolutions but has an extended domain for feature maps and kernels. The idea was then applied to different networks to enable equivariance for \(\mathrm{SE}(2)\)[1][2], \(\mathrm{SO}(3)\)[3], \(\mathrm{SE}(3)\)[4], and \(\mathrm{E}(3)\)[5] groups up to some discretization. The idea mainly works with finite (discretized) groups, as it is convenient to parameterize feature maps and kernels on the discretized group elements just as on pixel grids. Group convolutions have a relatively simple structure, making them straightforward to apply, but a major downside is that the lifted domain of features and kernels causes higher computational and memory costs, and the problem is more prominent when the group is large. Groups convs can also work with continuous groups, for example, with the help of Monte Carlo (MC) estimation as in [6], but they suffer from a large memory burden in MC sampling when the number of layers grows [7].

**Steerable CNNs:** Another line of work is steerable CNNs. Instead of augmenting the domain of feature maps, steerable CNNs generalize the space of feature values to be _steerable_, i.e., the feature values transform predictably as the input transforms. The way the feature transforms is called the group representation in the feature space, governed by the feature _type_. Features of _scalar_ type are kept unchanged under group actions, and we call the group representation trivial. For _vector_ or _tensor_ feature types, the representation is not trivial, and the features will change with group actions, for example, through rotation matrix multiplication. Steerable CNNs work for both discretized and continuous groups. One may freely design the feature types based on pre-determined basic types and corresponding representations (irreducible representations, i.e., _irreps_) as building blocks for a given group. Group convolutions can be viewed as a special case of steerable CNNs with _regular_ representations, i.e., channels of a feature vector undergo permutations when transformed. Examples of steerable CNNs include [8][9][10][11]. While the framework of steerable CNNs generalizes group convolutions with more flexibility, it requires a good understanding of representation theory and involves generalized Fourier analysis when working with a continuous group, which makes the structure complicated and challenging to apply in real-world applications broadly. The work of [12][13] also found empirically that steerable CNNs with irreps underperform regular representations in certain tasks.

**Theoretical progress:** There has been a lot of progress in the theoretical development of equivariant networks that are not group-specific. [14] developed a general formulation for steerable CNNs with scalar type features, and [15][16] generalized it to arbitrary feature types. It is proven that all equivariant linear maps can be written as convolutions. The formulation from the perspective of Fourier analysis was presented in [17]. [13] found the general form of solution for the equivariant kernels for \(E(2)\) group, later generalized to any compact group [18]. [19] proposed an algorithm to solve for the equivariance constraint for arbitrary matrix group. The formulation of group convolution based on MC estimation was proposed for any Lie group with [6] or without [7] surjective exponential maps. 

**Applications of equivariant learning in perception tasks:** We want to highlight a few equivariant networks that gain attention in perception applications due to their simplicity and practicality. Vector Neurons [23] is a PointNet-like \(\mathrm{SO}(3)\)-equivariant network for 3D point cloud learning, later applied to point cloud registration [24] and manipulation [25]. It can be viewed as a special case of TFN [9] with type-1 features and self-interactions only. E2CNN [13] as an \(\mathrm{SE}(2)\)-equivariant framework was applied in several image processing tasks [26][27], given its generality and user-friendly library. DEVIANT [28] applied scale-equivariant convolutions in monocular 3D object detection. EPN [4] is a group convolution network with \(\mathrm{SE}(3)\)-equivariance for 3D point cloud learning based on KPConv [29] and was used in practice, for example in place recognition task . ",0
50," SISR aims to learn mapping functions between LR and HR image pairs. It has improved dramatically with the advent of learning-based methods using large-scale datasets. SRCNN  first proposes a learning-based SR framework using CNN, and after that, EDSR [1] and RCAN  suggested a deeper network structure using residual blocks and an attention mechanism respectively. Also, with the advent of transformer-based architecture [2] together, researches started to solve vision problems using the corresponding architecture, and SwinIR  improved the performance of SISR by using swin transformer [3]. However, SISR is an inherently ill-posed problem, and there is no unique solution, which causes the SR results to produce blurry images. To address this, some studies have improved the perceptual quality of SISR using discriminative networks [4][5] and adaptive targets [6]. Still, reconstruction accuracy and perceptual quality of SISR are a trade-off. To solve this problem, our method proposes an SR update module that receives guidance from radiance fields and refines the results from SISR features.

Unlike SISR, there are studies that perform SR from multiple images. Video super-resolution (VSR) has the additional problem of exploiting the information from multiple frames of video with deep correlation. Some studies propose a sliding window framework to predict the optical flow of LR frames or a framework using a recurrent model architecture [7][8][9][10]. Reference-based super-resolution (RSR) is an approach to improve the details of LR images through HR images given as reference images. Some studies propose a deformable convolution or the cosine similarity between the reference and LR images [11][12][13][14]. Recently, a study proposes a method to perform MVSR, which generates HR reference images using given LR inputs and paired depths [15]. However, it requires depth maps as inputs, and since each image is processed independently, it is difficult to maintain multi-view consistency. Our method improves the performance of MVSR by updating the SR outputs during the process of optimizing the radiance fields from the given LR inputs. In addition, we demonstrate that our method is superior through quantitative comparison with existing methods for performing VSR and MVSR.

Through the development of implicit neural representation [16] (INR), various studies have been actively conducted to represent 2D images and 3D spaces as multi-scale representations. LIIF [17] and SphereSR [18] propose continuous image representations that enable SISR with an arbitrary resolution on planar and spherical images. In radiance fields, mip-NeRF [19] proposes a scale-dependent positional encoding, allowing for multiple-scale supervision. BACON [20] enables multi-scale decomposition without multi-scale supervision through bandwidth constraints. Both studies [19][20] improve down-scaling performance in radiance fields. In contrast, NeRF-SR [21] proposes a super-sampling strategy that improves up-scaling performance by learning in an unsupervised manner. We present a new methodology that can improve HRNVS performance through the interaction of 2D SISR and 5D radiance fields, breaking away from the current methodology that depends on the characteristics of INR.

Some studies improve NVS performance through radiance fields using physics-based multi-view geometry techniques for train-view images requiring image enhancement. NeRF-W [22] solves the problem of inputs with variable illumination and transient occluders by relaxing strict consistency assumptions. Deblur-NeRF [23] solves the problem of blurry input by developing a module that models spatially-varying blur kernels. RawNeRF [24] enables high-dynamic range (HDR) novel view synthesis by learning NeRF from raw data inputs and synthesizing raw output images. HDR-NeRF [25] makes exposure control and HDR image rendering possible by learning two implicit functions, radiance field and tone mapper. We propose a new method that finally enables HRNVS by allowing SR input images to be appropriately super-resolved during radiance fields optimization simultaneously. Unlike the image enhancement method using the existing radiance fields, we solve the problem by repeatably updating the train-view images which is the source of the radiance fields optimization.

",1
51," SISR aims to learn mapping functions between LR and HR image pairs. It has improved dramatically with the advent of learning-based methods using large-scale datasets. SRCNN  first proposes a learning-based SR framework using CNN, and after that, EDSR [1] and RCAN  suggested a deeper network structure using residual blocks and an attention mechanism respectively. Also, with the advent of transformer-based architecture [2] together, researches started to solve vision problems using the corresponding architecture, and SwinIR  improved the performance of SISR by using swin transformer [3]. However, SISR is an inherently ill-posed problem, and there is no unique solution, which causes the SR results to produce blurry images. To address this, some studies have improved the perceptual quality of SISR using discriminative networks [4][5] and adaptive targets [6]. Still, reconstruction accuracy and perceptual quality of SISR are a trade-off. 

Unlike SISR, there are studies that perform SR from multiple images. Video super-resolution (VSR) has the additional problem of exploiting the information from multiple frames of video with deep correlation. Some studies propose a sliding window framework to predict the optical flow of LR frames or a framework using a recurrent model architecture [7][8][9][10]. Reference-based super-resolution (RSR) is an approach to improve the details of LR images through HR images given as reference images. Some studies propose a deformable convolution or the cosine similarity between the reference and LR images [11][12][13][14]. Recently, a study proposes a method to perform MVSR, which generates HR reference images using given LR inputs and paired depths [15]. However, it requires depth maps as inputs, and since each image is processed independently, it is difficult to maintain multi-view consistency. 

Through the development of implicit neural representation [16] (INR), various studies have been actively conducted to represent 2D images and 3D spaces as multi-scale representations. LIIF [17] and SphereSR [18] propose continuous image representations that enable SISR with an arbitrary resolution on planar and spherical images. In radiance fields, mip-NeRF [19] proposes a scale-dependent positional encoding, allowing for multiple-scale supervision. BACON [20] enables multi-scale decomposition without multi-scale supervision through bandwidth constraints. Both studies [19][20] improve down-scaling performance in radiance fields. In contrast, NeRF-SR [21] proposes a super-sampling strategy that improves up-scaling performance by learning in an unsupervised manner. 

Some studies improve NVS performance through radiance fields using physics-based multi-view geometry techniques for train-view images requiring image enhancement. NeRF-W [22] solves the problem of inputs with variable illumination and transient occluders by relaxing strict consistency assumptions. Deblur-NeRF [23] solves the problem of blurry input by developing a module that models spatially-varying blur kernels. RawNeRF [24] enables high-dynamic range (HDR) novel view synthesis by learning NeRF from raw data inputs and synthesizing raw output images. HDR-NeRF [25] makes exposure control and HDR image rendering possible by learning two implicit functions, radiance field and tone mapper. 

",0
52," **Parameter perturbation attacks.** Parameter perturbation attacks were given under different names such as fault injection attack, fault sneaking attack, stealth attack, and weight corruption. The fault injection attack ([4]) was first proposed by Liu et al., and was further studied in ([1]; [6]). In ([1]), the first physical fault injection attack on DNNs was given, by using the laser injection technique on embedded systems. In ([5]; ; [3]), some security boundaries for parameter perturbations were given. In ([2]; 2021), the stealth attack was proposed to make the attacked DNN output a desired label for a given input image.

The adversarial parameter attack has the following advantages compared to previous works. First, by keeping the accuracy and reducing the robustness, the adversarial parameter attack is more difficult to be recognized. Second, we prove the existence of adversarial parameters under reasonable assumptions, while most previous works rely on experimental results.

**Algorithms to train robust DNNs.** Many methods were proposed to train more robust DNNs to defend adversarial samples (). The adversarial training proposed by ([7]) is considered to be one of the most effective methods to train robust networks, and is used to compute adversarial parameters in this paper. Methods to train DNNs that are more robust against parameter perturbation attacks were also proposed ([4]; [6]; ). In (;a;b), the adversarial weight perturbation was proposed which was a generalization of the adversarial training by considering both the adversarial examples and the adversarial parameters, and hence led to more robust networks.

**Theory of adversarial examples.** The existence of adversarial examples was usually demonstrated with numerical experiments, and mathematical guaranteed results were desirable. Along this line of research, it was proved that a well-learned DNN always has adversarial examples for certain classification functions and data distributions ([8]). In ([2]; 2021), it was proved that there exist attacked DNNs that give a desired label for any sample.

Theories for certified robustness of DNNs were given in several aspects. In (; ; [11]), some security boundaries of adversaries were given. In ([9]), the randomized smoothing method was proposed and security boundaries of adversaries were given. Lower bounds on stability in terms of the classification function were also given in ([10]; [2]). However, these safety bounds are usually very small when the depth of the DNN is large. In (), the information-theoretically safe bias classifier was introduced by making the gradient of the DNN random. In this paper, we show that by making small perturbations to the parameters, the DNN will have adversarial examples with a high probability for any input.

",1
53," **Parameter perturbation attacks.** Parameter perturbation attacks were given under different names such as fault injection attack, fault sneaking attack, stealth attack, and weight corruption. The fault injection attack ([4]) was first proposed by Liu et al., and was further studied in ([1]; [6]). In ([1]), the first physical fault injection attack on DNNs was given, by using the laser injection technique on embedded systems. In ([5]; ; [3]), some security boundaries for parameter perturbations were given. In ([2]; 2021), the stealth attack was proposed to make the attacked DNN output a desired label for a given input image.



**Algorithms to train robust DNNs.** Many methods were proposed to train more robust DNNs to defend adversarial samples (). The adversarial training proposed by ([7]) is considered to be one of the most effective methods to train robust networks, and is used to compute adversarial parameters in this paper. Methods to train DNNs that are more robust against parameter perturbation attacks were also proposed ([4]; [6]; ). In (;a;b), the adversarial weight perturbation was proposed which was a generalization of the adversarial training by considering both the adversarial examples and the adversarial parameters, and hence led to more robust networks.

**Theory of adversarial examples.** The existence of adversarial examples was usually demonstrated with numerical experiments, and mathematical guaranteed results were desirable. Along this line of research, it was proved that a well-learned DNN always has adversarial examples for certain classification functions and data distributions ([8]). In ([2]; 2021), it was proved that there exist attacked DNNs that give a desired label for any sample.

Theories for certified robustness of DNNs were given in several aspects. In (; ; [11]), some security boundaries of adversaries were given. In ([9]), the randomized smoothing method was proposed and security boundaries of adversaries were given. Lower bounds on stability in terms of the classification function were also given in ([10]; [2]). However, these safety bounds are usually very small when the depth of the DNN is large. In (), the information-theoretically safe bias classifier was introduced by making the gradient of the DNN random. 

",0
54," A variety of modern layout analysis datasets have been created in recent years. In 2009, Antonacopoulos et al. [1] presented the PRImA dataset, which was the first commonly used real-world dataset with 305 images of magazines and scientific articles. In 2019, Zhong et al. [2] published the PubLayNet dataset, which contains over 360,000 page samples annotated with typical document layout elements such as text, heading, list, graphic, and table. Annotations were automatically generated by matching PDFs and XML formats of articles from the PubMed Central Open Access subset. In 2020, researchers at Microsoft Research Asia built the DocBank dataset [3], which contains 500,000 document pages and fine-grained token-level annotations for document layout analysis. It was developedbased on a large number of PDF files of papers compiled by the LaTeX tool. Unlike the conventional manual annotating process, they approach obtaining high-quality annotations using a weakly supervised approach in a simple and efficient manner. In 2022, IBM researchers presented the DocLayNet dataset [4], which contains 80,863 manually annotated pages. It contains six document types (technical manuals, annual company reports, legal text, and government tenders), 11 categories of annotations, and four languages (English documents close to 95%). A few pages in the DocLayNet dataset have multiple manual annotations, which allows for experiments in annotation uncertainty and quality control analysis.

However, the predominant document format for large datasets is PDF, not scanned and photographed images as in real-world scenarios. Only a few public datasets include real-world data. The variety of layouts in current public datasets is still very limited and is not conducive to the development of logical layout analysis. Currently, 95% of the publicly available datasets are English documents, which are largely unsuitable for Asian language documents. To this end, we propose the \(M^{6}Doc\) dataset to facilitate the development of layout analysis.

Earlier layout analysis methods [5][6][7][8] used rule-based and heuristic algorithms, so they were limited to applications on certain simple types of documents, and the generalization performance of such methods was poor. However, with the development of deep learning, DLA methods based on deep learning have been developed to tackle challenging tasks. Mainstream approaches include object detection-based models [9][10][11], segmentation-based models [12][13], and multi-modal methods [14][15]. For example, Li et al. [10] considered DLA as an object detection task and added a domain adaptation module to study cross-domain document object detection tasks. Lee et al. [12] used segmentation methods to solve DLA problems and introduced trainable multiplication layer techniques for improving the accuracy of object boundary detection to improve the performance of pixel-level segmentation networks. Zhang et al.  proposed a unified framework for multi-modal layout analysis by introducing semantic information in a new semantic branch of Mask R-CNN [16] and a module for modeling element relationships. Behind their success, large datasets are required for training and evaluating the models.

However, the lack of a multi-format, multi-type, multi-language, and multi-label categorized logical layout analysis dataset makes it difficult for current methods to obtain good results in real-world and other language scenarios. Moreover, a data format that links visual and textual features has not yet been established for multi-modal tasks.

",1
55," A variety of modern layout analysis datasets have been created in recent years. In 2009, Antonacopoulos et al. [1] presented the PRImA dataset, which was the first commonly used real-world dataset with 305 images of magazines and scientific articles. In 2019, Zhong et al. [2] published the PubLayNet dataset, which contains over 360,000 page samples annotated with typical document layout elements such as text, heading, list, graphic, and table. Annotations were automatically generated by matching PDFs and XML formats of articles from the PubMed Central Open Access subset. In 2020, researchers at Microsoft Research Asia built the DocBank dataset [3], which contains 500,000 document pages and fine-grained token-level annotations for document layout analysis. It was developedbased on a large number of PDF files of papers compiled by the LaTeX tool. Unlike the conventional manual annotating process, they approach obtaining high-quality annotations using a weakly supervised approach in a simple and efficient manner. In 2022, IBM researchers presented the DocLayNet dataset [4], which contains 80,863 manually annotated pages. It contains six document types (technical manuals, annual company reports, legal text, and government tenders), 11 categories of annotations, and four languages (English documents close to 95%). A few pages in the DocLayNet dataset have multiple manual annotations, which allows for experiments in annotation uncertainty and quality control analysis.

However, the predominant document format for large datasets is PDF, not scanned and photographed images as in real-world scenarios. Only a few public datasets include real-world data. The variety of layouts in current public datasets is still very limited and is not conducive to the development of logical layout analysis. Currently, 95% of the publicly available datasets are English documents, which are largely unsuitable for Asian language documents. 

Earlier layout analysis methods [5][6][7][8] used rule-based and heuristic algorithms, so they were limited to applications on certain simple types of documents, and the generalization performance of such methods was poor. However, with the development of deep learning, DLA methods based on deep learning have been developed to tackle challenging tasks. Mainstream approaches include object detection-based models [9][10][11], segmentation-based models [12][13], and multi-modal methods [14][15]. For example, Li et al. [10] considered DLA as an object detection task and added a domain adaptation module to study cross-domain document object detection tasks. Lee et al. [12] used segmentation methods to solve DLA problems and introduced trainable multiplication layer techniques for improving the accuracy of object boundary detection to improve the performance of pixel-level segmentation networks. Zhang et al.  proposed a unified framework for multi-modal layout analysis by introducing semantic information in a new semantic branch of Mask R-CNN [16] and a module for modeling element relationships. Behind their success, large datasets are required for training and evaluating the models.



",0
56," **Camouflaged object detection.** Unlike existing object detection tasks, camouflaged object detection (COD) poses new challenges for mining subtle discriminative features under complex camouflage strategies [1]. Early techniques utilized the hand-crafted operators for COD [2], which were only applicable to camouflaged scenarios with simple backgrounds. Recent research has leveraged the huge capacity of deep learning to detect camouflaged objects in a learning manner [1][3][4]. Inspired by the hunting process of predators, SINet [1] designed a bio-inspired network to gradually search and locate the camouflaged object. PFNet [4] proposed the position module and focus module to imitate human identification with the distraction mining strategy. By simulating human behaviors in understanding complex scenarios, SegMaR [3] integrated segment, magnify and reiterate in a coarse-to-fine manner using the multi-stage strategy. However, these COD solutions mainly focus on mimicking biovision systems, which can be easily confused by complex camouflaged strategies and struggle to excavate the subtle discriminative features, thus failing to handle the IS and ED challenges (see Fig. 1). Unlike these human perception-oriented techniques, we first propose to address the COD task from a decomposition perspective by decomposing the extracted features into different frequency bands with learnable wavelets and filtering out the most informative bands to excavate those inconspicuous discriminative features, thus remedying the human visual deficiency and solving the IS challenge. To handle the ED challenge, we propose learning an auxiliary edge reconstruction task along with the COD task to facilitate the generation of precise segmentation results with clear object boundaries.

**Deep wavelet decomposition.** Deep wavelet decomposition is an effective tool to decompose image/feature into various frequency components and has gained immense popularity in many domains, such as image restoration [5]and style transfer [6]. To handle the IS challenge, we introduce deep wavelet decomposition into the COD task. Furthermore, to better accommodate the COD data, we employ the learnable wavelets for deep adaptive feature decomposition, whose coefficients are updated following AWD [7].

**ODE-inspired network.** Researchers have established a relationship between ODE and neural networks.  first analyzed ResNet from the perspective of discrete ODE and [8] further extended ResNet to an ODE-inspired network architecture with a more accurate transmission. Since then, ODE-inspired networks are widely utilized in many fields, such as image dehazing [9] and machine translation [10]. In this paper, to accommodate the fine-grained property of the edge, we propose an ODE-inspired edge reconstruction module with the second-order Runge-Kutta and a weighted gate mechanism, aiming to generate more accurate boundaries. Furthermore, we apply the Hamiltonian system to our OER module to ensure the stability of edge reconstruction.

",1
57," **Camouflaged object detection.** Unlike existing object detection tasks, camouflaged object detection (COD) poses new challenges for mining subtle discriminative features under complex camouflage strategies [1]. Early techniques utilized the hand-crafted operators for COD [2], which were only applicable to camouflaged scenarios with simple backgrounds. Recent research has leveraged the huge capacity of deep learning to detect camouflaged objects in a learning manner [1][3][4]. Inspired by the hunting process of predators, SINet [1] designed a bio-inspired network to gradually search and locate the camouflaged object. PFNet [4] proposed the position module and focus module to imitate human identification with the distraction mining strategy. By simulating human behaviors in understanding complex scenarios, SegMaR [3] integrated segment, magnify and reiterate in a coarse-to-fine manner using the multi-stage strategy. However, these COD solutions mainly focus on mimicking biovision systems, which can be easily confused by complex camouflaged strategies and struggle to excavate the subtle discriminative features, thus failing to handle the IS and ED challenges (see Fig. 1). 

**Deep wavelet decomposition.** Deep wavelet decomposition is an effective tool to decompose image/feature into various frequency components and has gained immense popularity in many domains, such as image restoration [5]and style transfer [6]. 

**ODE-inspired network.** Researchers have established a relationship between ODE and neural networks.  first analyzed ResNet from the perspective of discrete ODE and [8] further extended ResNet to an ODE-inspired network architecture with a more accurate transmission. Since then, ODE-inspired networks are widely utilized in many fields, such as image dehazing [9] and machine translation [10]. 

",0
58," While the literature on minimal adversarial attacks for classification is vast, the research on attacks for segmentation is much less developed. The main work on adversarial attacks for segmentation is done by Xie _et al._[1]. It proposes a simple algorithm to generate adversarial perturbations for dense prediction tasks, including object detection and segmentation, called the Dense Adversary Generation (DAG) attack. In this attack, the rescaled gradient of the loss w.r.t. the input is added to the current perturbation, until the stopping criterion is reached, _i.e_. a given percentage of pixels is adversarial. In each iteration, the total loss is the sum of the losses over pixels that are not adversarial. This can be seen as a form of greedy algorithm. See Appendix A for the complete algorithm of the DAG attack and a discussion on the stopping criterion used. In practice, this attack is quite efficient, however, it simply accumulates gradients until the stopping criterion is reached. Therefore, it does not minimize the norm considered. Cisse _et al._ propose the Houdini attack for several tasks [2], including segmentation. The goal of this approach is to maximize a surrogate loss for a given perturbation budget (_i.e_. constraint on the \(\ell_{\infty}\)-norm), hence not producing minimal perturbations. More recently, Ozbulak _et al._ studied adversarial examples on a medical image segmentation task . They propose a targeted attack to minimize the \(\ell_{2}\)-norm, which is a regular penalty method. The weight of the penalty terms is fixed to 1, however, leading to large perturbations.

Other works study the robustness of segmentation models against adversarial attacks [3][4][5]. In these works, the authors use FGSM [6] or an iterative version of FGSM. However, FGSM is not a minimization attack and is known to provide rough robustness evaluations. This leads to largely overestimated robustness results on both Pascal VOC 2012 and Cityscapes in [3].

Even though most adversarial attacks were designed for classification, some may be adapted for segmentation tasks. In particular, \(\ell_{\infty}\) attacks that do not rely on projections onto an estimated decision boundary can be used for segmentation (as opposed to DeepFool [7] or FAB [8]). These attacks are PGD [9], FMN [10] and PDPGD [11]. Note that PDPGD [11] relies on a proximal splitting method, but uses the AdaProx algorithm [12]; the latter is appealing but, unlike the prox-Newton algorithm it is inspired from [13], introduces a mismatch between the scaling in the computation of the proximity operator and the step-size of the gradient step. The convergence study of such an algorithm would be quite challenging in the non-convex case. It is also known that, even in the convex case, when such a mismatched algorithm converges, the asymptotic point differs from the solution to the original optimization problem [14].

",0
59," Dataset Distillation.First introduced by [1], dataset distillation is the task of synthesizing a smaller dataset from a large-scale dataset such as CIFAR100 [2], so that the network trained on the distilled data has a performance comparable to that of the network trained on the source large-scale data. Recent work has significantly improved the performance of networks trained on distilled data and reduced the computational and time overhead of the distillation process while compressing the dataset size to one image per class [3][4][5][6][7][8][9][10][11]. Dataset distillation problem is treated as a gradient-based hyperparameter optimization [1]. DC performs distillation by matching the gradients generated from distilled data and full data [8]. DSA further improves the results by differentiable Siamese augmentations [9]. Other SOTA methods include matching trajectories of each parameter between the training on distilled data and full data [3], optimizing soft labels [12], minimizing reconstruction errors [13], and using neural networks to regress features from synthetic samples to real ones [14]. The current focus of DD is on computational expense and training performance, and to the best of our knowledge, the difficulties in calibrating over-confident DDNNs remain untouched.

**Neural Network Calibration.** The importance of neural network calibration has been emphasized and received increasing attention [15], with the aim of matching the output probability of a neural network (also known as the network output confidence) with the actual accuracy. [15] also introduces the concept of Expected Calibration Error (ECE), which has now become a standard metric for quantitatively measuring calibration quality. A higher ECE implies a poorer calibration of the neural network, while a 0 implies a perfect calibration. Recent calibration methods that have been proposed for networks trained on large-scale datasets include Label Smoothing (LS) [16], which smooths a one-hot class label with uniform noise during training, forcing the model to learn loose predictions. Mixup is similar to label smoothing, where different data-label pairs are mixed to form new data points [17][18]. Focal loss (FL), originally designed to address the class imbalance, modifies the traditional cross-entropy loss in classification problems by adding a moderation term, thus allowing the model to focus more on difficult examples that are easily misclassified but difficult to learn [19][20]. Temperature scaling (TS) is an after-training calibration method applied to fully trained and fixed-weight networks [15]. As an extension of Platt scaling [21], the temperature scaling method scales the output, denoted by \(z\), of the last layer of the network with a scaler T before converting it into a probability:

Other work has discussed the necessity [22] and hardness of network calibration [23][24][25], as well as the degradation of calibration with distribution shift or model size [26][27].

",1
60," Dataset Distillation.First introduced by [1], dataset distillation is the task of synthesizing a smaller dataset from a large-scale dataset such as CIFAR100 [2], so that the network trained on the distilled data has a performance comparable to that of the network trained on the source large-scale data. Recent work has significantly improved the performance of networks trained on distilled data and reduced the computational and time overhead of the distillation process while compressing the dataset size to one image per class [3][4][5][6][7][8][9][10][11]. Dataset distillation problem is treated as a gradient-based hyperparameter optimization [1]. DC performs distillation by matching the gradients generated from distilled data and full data [8]. DSA further improves the results by differentiable Siamese augmentations [9]. Other SOTA methods include matching trajectories of each parameter between the training on distilled data and full data [3], optimizing soft labels [12], minimizing reconstruction errors [13], and using neural networks to regress features from synthetic samples to real ones [14]. 

**Neural Network Calibration.** The importance of neural network calibration has been emphasized and received increasing attention [15], with the aim of matching the output probability of a neural network (also known as the network output confidence) with the actual accuracy. [15] also introduces the concept of Expected Calibration Error (ECE), which has now become a standard metric for quantitatively measuring calibration quality. A higher ECE implies a poorer calibration of the neural network, while a 0 implies a perfect calibration. Recent calibration methods that have been proposed for networks trained on large-scale datasets include Label Smoothing (LS) [16], which smooths a one-hot class label with uniform noise during training, forcing the model to learn loose predictions. Mixup is similar to label smoothing, where different data-label pairs are mixed to form new data points [17][18]. Focal loss (FL), originally designed to address the class imbalance, modifies the traditional cross-entropy loss in classification problems by adding a moderation term, thus allowing the model to focus more on difficult examples that are easily misclassified but difficult to learn [19][20]. Temperature scaling (TS) is an after-training calibration method applied to fully trained and fixed-weight networks [15]. As an extension of Platt scaling [21], the temperature scaling method scales the output, denoted by \(z\), of the last layer of the network with a scaler T before converting it into a probability:



",0
61," Contrastive SSL.Contrastive SSL uses positive and negative pairs of augmented samples [1][2][3][4][5][6]. The commonly used InfoNCE loss [5] consists of an alignment term, which maximizes the similarity between positive pairs, and a uniformity term, which minimizes the similarity between negative pairs [6]. SimCLR [1] is a state-of-the-art contrastive SSL method. However, to obtain effective representations, SimCLR requires a large number of negative pairs [1], or, in other words, a large batch (or memory bank) size \(n\). This can be a computational bottleneck, as the loss computation of SimCLR takes \(O(n^{2}d)\) time where \(d\) is the dimensionality of the projected embeddings.

Non-contrastive SSL Using Asymmetric Architecture.Recently, researchers have started exploring non-contrastive approaches to SSL, i.e., those that do not use negative pairs for training. To overcome collapsed embeddings, models such as BYOL [7] and SimSiam [8] introduce asymmetry in the architecture, e.g., by suppressing gradient updates and/or using the moving average of network parameters for one branch of the Siamese network. These methods are heuristically motivated, as collapsed embeddings are not explicitly penalized; however, they are effective in practice.

Non-contrastive SSL by Decorrelating Regularization.A line of work exists that maintains a standard (symmetric) Siamese network but introduces loss functions to suppress collapsed embeddings. These loss functions also have regularization terms to promote feature decorrelation. Barlow Twins [9] is the first method in this line. It uses a decorrelating regularizer based on cross-correlation across two views. VICReg [10] uses regularizers that are defined in terms of covariance matrices of individual views. We review these methods in detail in Sec. 3.

Non-contrastive SSL by Whitening.Some authors [11][12] have used whitening to explicitly decorrelate features during training, as opposed to performing regularization. Subsequent work  whitened both features and instances. Because the whitening procedures used in these approaches require the computation of all the eigenvalues of the covariance matrices, a training epoch takes \(O(\min(dn^{2},nd^{2}))\) time, which is inefficient with large \(d\) or \(n\).

Use of Convolution in Machine Learning.Convolution is the fundamental building block of convolution neural networks (CNNs). CNNs take (linear) convolution of input vectors with small learnable kernels to extract local features. Although FFT reduces the asymptotic complexity of convolution computation, it is seldom used with CNNs, because the size of kernels is typically too small to benefit from speed-up by FFT. In contrast to CNNs, we use (circular) convolution to compute summary statistics of covariance and cross-correlation matrices.

In other areas of machine learning, circular convolution and its non-commutative analogue, circular correlation, have been used for implementing associative memory [13]. The idea has recently been applied to knowledge graph embeddings (KGEs) [14], with the resulting model later shown [15] to be isomorphic to complex-valued KGEs [16] by means of the convolution theorem.

",1
62," Contrastive SSL.Contrastive SSL uses positive and negative pairs of augmented samples [1][2][3][4][5][6]. The commonly used InfoNCE loss [5] consists of an alignment term, which maximizes the similarity between positive pairs, and a uniformity term, which minimizes the similarity between negative pairs [6]. SimCLR [1] is a state-of-the-art contrastive SSL method. However, to obtain effective representations, SimCLR requires a large number of negative pairs [1], or, in other words, a large batch (or memory bank) size \(n\). This can be a computational bottleneck, as the loss computation of SimCLR takes \(O(n^{2}d)\) time where \(d\) is the dimensionality of the projected embeddings.

Non-contrastive SSL Using Asymmetric Architecture.Recently, researchers have started exploring non-contrastive approaches to SSL, i.e., those that do not use negative pairs for training. To overcome collapsed embeddings, models such as BYOL [7] and SimSiam [8] introduce asymmetry in the architecture, e.g., by suppressing gradient updates and/or using the moving average of network parameters for one branch of the Siamese network. These methods are heuristically motivated, as collapsed embeddings are not explicitly penalized; however, they are effective in practice.

Non-contrastive SSL by Decorrelating Regularization.A line of work exists that maintains a standard (symmetric) Siamese network but introduces loss functions to suppress collapsed embeddings. These loss functions also have regularization terms to promote feature decorrelation. Barlow Twins [9] is the first method in this line. It uses a decorrelating regularizer based on cross-correlation across two views. VICReg [10] uses regularizers that are defined in terms of covariance matrices of individual views. 

Non-contrastive SSL by Whitening.Some authors [11][12] have used whitening to explicitly decorrelate features during training, as opposed to performing regularization. Subsequent work  whitened both features and instances. Because the whitening procedures used in these approaches require the computation of all the eigenvalues of the covariance matrices, a training epoch takes \(O(\min(dn^{2},nd^{2}))\) time, which is inefficient with large \(d\) or \(n\).

Use of Convolution in Machine Learning.Convolution is the fundamental building block of convolution neural networks (CNNs). CNNs take (linear) convolution of input vectors with small learnable kernels to extract local features. Although FFT reduces the asymptotic complexity of convolution computation, it is seldom used with CNNs, because the size of kernels is typically too small to benefit from speed-up by FFT. 

In other areas of machine learning, circular convolution and its non-commutative analogue, circular correlation, have been used for implementing associative memory [13]. The idea has recently been applied to knowledge graph embeddings (KGEs) [14], with the resulting model later shown [15] to be isomorphic to complex-valued KGEs [16] by means of the convolution theorem.

",0
63," For both UFET and FET, due to the use of large entity type sets, it is labor-intensive to manually annotate training examples. Thus, different approaches ([2]; [1]; [3]) of automatically generating weakly labeled training examples are proposed. Among them, the most commonly used method is to link entity mentions to a knowledge base, and then use the types of the corresponding entities as labels ([2]; [4]; [1]). Additionally, [1] propose to use the head word of the mention phrase as its type label. [3] generate entity type labels for mentions with a prompt-based method.

With different ways to create large amounts of training data automatically, the incorrectness of the generated labels become a problem. Many entity typing studies ([7]; [6]; [8]) seek to obtain better models when using weak training data. For example, [5] learn a neural model to correct noisy entity type labels and filter unuseful examples. [8] learn a backbone model as a feature extractor and a noise estimator, and perform feature cluster based loss correction afterwards.

Recently, there are more entity typing studies that do not follow the commonly adopted approach of training with distantly labeled data created by using a knowledge base. Some of them also do not require a designated training set for each entity type schema. For example, [10] exploit indirect supervision from natural language inference.  employ self-supervision instead of explicit type labels. [9] use automatic label interpretation and instance generation to achieve few-shot FET.

",0
64," **Image Deblurring.** With the advances in vision benchmarks, CNN models have excelled in various image enhancement tasks [1][2][3][4][5][6][7][8], by innovative architectures and specialized modules. Image deblurring seeks to produce sharp images from blurred ones. Traditional efforts to refine deblurring performance hinge on various priors for natural images and kernels, such as the sparse kernel prior [9], \(l_{0}\) gradient prior [10], normalized sparsity prior [11], and dark channels [12]. Yet, these approaches often fall short when addressing spatially variant blur. The advent of deep learning shifts focus towards advanced non-uniform deblurring techniques [13][14][15][16][17][18]. For instance, Nah _et al._[19] employ a multi-scale loss function for a fine-tuned approach. DeepRFT [13], on the other hand, leverages the spectral difference between a sharp image and its blurry one, addressing limitations in the spatial domain.

**Video Deblurring.** While single image deblurring focuses solely on one frame, video deblurring leverages temporal information to yield visually compelling outcomes. Many existing approaches employ CNN-based structures. In this domain, temporal alignment is designed to harness sharp patches from adjacent frames. Several methods use optical flow [20] and deformable convolution [21] to estimate the motions and align them with adjacent frames explicitly or implicitly. In addition to alignment, the rational use of multiple frames is also significant. Li _et al._[22] effectively exploited the depth map as guidance through Spatial Feature Transform (SFT) [23] to better extract the blurred frames' features. The authors in [24] developed a temporal sharpness prior to achieve the decent latent frame restoration. Lai _et al._[25] crafted a correlation-based aggregation module to efficiently process neighboring sharp patches, while RTA [26] brought an iterative alignment process, allowing for incremental motion compensation enhancements. However, the above methods less consider the frequency spectrum, which may limit the exploration of temporal information in video deblurring.

",0
65," **2D Image Synthesis**. GANs  have been extensively utilized to generate photorealistic images [1][2][3][4][5], perform image-to-image translation [6][7][8], and image editing [9][10][11]. Recently, compositional approaches [12][13] have also been explored in the context of image generation. Similar to our work, GANformer2 [13] also divides the generation process into two steps: planning and execution. In our work, we guide the _3D_ generation process using semantic layouts and demonstrate that CC3D can render multi-view consistent images of multi-object scenes.

**3D Object Generation**. To scale 2D GANs to 3D domain, many recent works explored combining image generators with 3D representations. These models are supervised only with unstructured image collections along with a pre-defined camera distribution. While earlier works [14][15][16][17][18] provided limited visual fidelity and geometric accuracy, recently, several works tried to address these limitations. The majority of these approaches [19][20][21][22][23][24][25][26] use a style-based generator [2] to synthesize a neural field which can be used for volume rendering [27]. Although these approaches can produce high quality images for single-object scenes, they fail to scale to complex scenes with multiple objects. In this work, we also employ a style-based generator in combination with volume rendering but as our model explicitly models the compositional nature of 3D scenes, it can successfully generate plausible indoor and outdoor 3D scenes.

**Multi-Object Generation**. Our work is closely related to recent approaches that model scenes using 3D-aware image generators [28][29]. Among the first, GIRAFFE [28] proposed to represent scenes using multiple locally defined NeRFs. However, while [28] can be efficiently applied on scenes containing only a few objects with limited texture variation, such as the CLEVR [30] dataset, it fails to generalize to more complex scenes. To improve the visual quality of [28], GIRAFFE-HD [29] employed a style-based generator. Even though this allows their model to composit multiple objects of the same class, e.g., cars, into a single scene at inference time, learning compositional scene generation from multi-object scenes of different classes remains an open problem.

**Large-Scale Scene Generation**. Plan2Scene [31] focuses on the task of converting a floorplan accompanied by a sparse set of images into a textured mesh for the entire scene. Although their representation is compositional by construction, [31] is not generative and requires multi-view supervision. Closely related to our work, another line of research [32][33] aims at generating large-scale scenes using locally conditioned neural fields. Unlike previous approaches that sample camera poses from a sphere targeted towards the origin, constraining them to \(SO(3)\), GSN [32] considers scene generation conditioned on a freely moving camera defined in \(SE(3)\). Although this setup permits generating scenes from arbitrary viewpoints, it makes training significantly harder, as datasets are not aligned and the range of possible camera poses drastically increases. GAUDI [33] further improves the quality by disentangling camera poses from geometry and appearance. Unlike GAUDI [33] that assumes multi-view input images with known camera poses, our model can be trained using unstructured set of images.

**Indoor Scene Generation**. Recently, several works [34][35][36][37][38] proposed to pose the scene generation task as an object layout prediction problem. For example, ATISS

[37] uses an autoregressive transformer to generate synthetic indoor environments as an unordered set of objects. LEGONet [38] learns to iteratively refine random object placements to generate realistic furniture arrangements. These works represent a scene layout as a set of 3D labeled bounding boxes, which can be replaced with textured meshes from a dataset of assets. In contrast, we rely on a GAN to learn a mapping between a 2D compositional scene layout to a 3D scene, without having to rely on object retrieval to produce 3D objects. We see our work as an orthogonal work to [34][35][36][37] as they can be used to generate scene layouts, which in turn can be used as our conditioning.

**Concurrent Works**. Several concurrent works explored extending 3D GANs to more complex scenarios. 3DGP [39] tackles non-aligned datasets by incorporating depth estimation and a novel camera parameterization, but their model focuses only on single objects. SceneDreamer [40] generates unbounded landscapes from 2D image collections and semantic labels. However, their model is supervised with a ground truth height field, whereas we learn the density field only from 2D image collections. InfiniCity [41] synthesizes large-scale 3D city environments but requires expensive annotations such as CAD models. Similar to ours, pix2pix3D [42] generates 3D objects given a 2D semantic map, but it only focuses on single-object scenes. In concurrent work, DisCoScene [43] investigates compositional scene generation with layout priors using single-view image collections. Their approach follows [28] and generates each object and the background independently. Unlike our work, DisCoScene conditions the scene generation on 3D layout priors, as opposed to 2D layouts, and assumes that the per-object attributes (size, pose) are sampled from a pre-defined prior distribution. Instead, we do not assume this type of supervision. Moreover, unlike [43], we explore rendering from freely moving cameras as opposed to cameras on a sphere.

",1
66," **2D Image Synthesis**. GANs  have been extensively utilized to generate photorealistic images [1][2][3][4][5], perform image-to-image translation [6][7][8], and image editing [9][10][11]. Recently, compositional approaches [12][13] have also been explored in the context of image generation. Similar to our work, GANformer2 [13] also divides the generation process into two steps: planning and execution. 

**3D Object Generation**. To scale 2D GANs to 3D domain, many recent works explored combining image generators with 3D representations. These models are supervised only with unstructured image collections along with a pre-defined camera distribution. While earlier works [14][15][16][17][18] provided limited visual fidelity and geometric accuracy, recently, several works tried to address these limitations. The majority of these approaches [19][20][21][22][23][24][25][26] use a style-based generator [2] to synthesize a neural field which can be used for volume rendering [27]. Although these approaches can produce high quality images for single-object scenes, they fail to scale to complex scenes with multiple objects. 

**Multi-Object Generation**. Our work is closely related to recent approaches that model scenes using 3D-aware image generators [28][29]. Among the first, GIRAFFE [28] proposed to represent scenes using multiple locally defined NeRFs. However, while [28] can be efficiently applied on scenes containing only a few objects with limited texture variation, such as the CLEVR [30] dataset, it fails to generalize to more complex scenes. To improve the visual quality of [28], GIRAFFE-HD [29] employed a style-based generator. Even though this allows their model to composit multiple objects of the same class, e.g., cars, into a single scene at inference time, learning compositional scene generation from multi-object scenes of different classes remains an open problem.

**Large-Scale Scene Generation**. Plan2Scene [31] focuses on the task of converting a floorplan accompanied by a sparse set of images into a textured mesh for the entire scene. Although their representation is compositional by construction, [31] is not generative and requires multi-view supervision. Closely related to our work, another line of research [32][33] aims at generating large-scale scenes using locally conditioned neural fields. Unlike previous approaches that sample camera poses from a sphere targeted towards the origin, constraining them to \(SO(3)\), GSN [32] considers scene generation conditioned on a freely moving camera defined in \(SE(3)\). Although this setup permits generating scenes from arbitrary viewpoints, it makes training significantly harder, as datasets are not aligned and the range of possible camera poses drastically increases. GAUDI [33] further improves the quality by disentangling camera poses from geometry and appearance. 

**Indoor Scene Generation**. Recently, several works [34][35][36][37][38] proposed to pose the scene generation task as an object layout prediction problem. For example, ATISS

[37] uses an autoregressive transformer to generate synthetic indoor environments as an unordered set of objects. LEGONet [38] learns to iteratively refine random object placements to generate realistic furniture arrangements. These works represent a scene layout as a set of 3D labeled bounding boxes, which can be replaced with textured meshes from a dataset of assets. In contrast, we rely on a GAN to learn a mapping between a 2D compositional scene layout to a 3D scene, without having to rely on object retrieval to produce 3D objects. 

**Concurrent Works**. Several concurrent works explored extending 3D GANs to more complex scenarios. 3DGP [39] tackles non-aligned datasets by incorporating depth estimation and a novel camera parameterization, but their model focuses only on single objects. SceneDreamer [40] generates unbounded landscapes from 2D image collections and semantic labels. However, their model is supervised with a ground truth height field, whereas we learn the density field only from 2D image collections. InfiniCity [41] synthesizes large-scale 3D city environments but requires expensive annotations such as CAD models. Similar to ours, pix2pix3D [42] generates 3D objects given a 2D semantic map, but it only focuses on single-object scenes. In concurrent work, DisCoScene [43] investigates compositional scene generation with layout priors using single-view image collections. Their approach follows [28] and generates each object and the background independently. 

",0
67," Federated Learning.FL [1] is a learning framework to train a global model on distributed data of multiple clients with privacy protection. One of the most important challenges is data heterogeneity. Many previous studies focused on this problem [2][3][4][5][6] with the assumption of a perfectly balanced global dataset (all local data). Recent works [7][8][9] proposed to handle class imbalance issue in FL. For example, CReFF [10] deal with federated long-tailed data inspired by [11]. However, these methods usually required additional private information except for model parameters of the clients with the privacy concerns, _e.g._, CReFF [10] requires feature gradients of clients' data. Moreover, they only focused on datasets with a few classes, and their effectiveness may diminish on the large-scale imbalanced datasets with a larger amount of classes [12].

Long-tailed Learning.Real-world data often exhibits a long-tailed distribution, where the majority classes have massive samples and the minority classes only have a few samples [13]. Many re-balance strategies are proposed to address such imbalance issues. Data re-sampling [14][15][11] is a common type, such as over-sampling the minority samples [11] or under-sampling [15] the majority samples. Another scheme to learn a balanced model is loss re-weighting [16]. Generally, these methods tend to give a large training loss for the minority samples. Recent studies mainly focused on a good representation space to improve the generalization ability. PaCo [17] introduced a contrastive learning method over the long-tailed dataset. Ensemble learning is also effective in long-tailed learning [18][19][20][21]. Although these re-balance strategies worked well on the centralized imbalance datasets, it remains a question whether they are useful in federated long-tailed learning. In this work, we theoretically and experimentally explore this issue and propose a novel algorithm to achieve a global balance training with existing re-balance strategies for federated long-tailed learning.

",1
68," Federated Learning.FL [1] is a learning framework to train a global model on distributed data of multiple clients with privacy protection. One of the most important challenges is data heterogeneity. Many previous studies focused on this problem [2][3][4][5][6] with the assumption of a perfectly balanced global dataset (all local data). Recent works [7][8][9] proposed to handle class imbalance issue in FL. For example, CReFF [10] deal with federated long-tailed data inspired by [11]. However, these methods usually required additional private information except for model parameters of the clients with the privacy concerns, _e.g._, CReFF [10] requires feature gradients of clients' data. Moreover, they only focused on datasets with a few classes, and their effectiveness may diminish on the large-scale imbalanced datasets with a larger amount of classes [12].

Long-tailed Learning.Real-world data often exhibits a long-tailed distribution, where the majority classes have massive samples and the minority classes only have a few samples [13]. Many re-balance strategies are proposed to address such imbalance issues. Data re-sampling [14][15][11] is a common type, such as over-sampling the minority samples [11] or under-sampling [15] the majority samples. Another scheme to learn a balanced model is loss re-weighting [16]. Generally, these methods tend to give a large training loss for the minority samples. Recent studies mainly focused on a good representation space to improve the generalization ability. PaCo [17] introduced a contrastive learning method over the long-tailed dataset. Ensemble learning is also effective in long-tailed learning [18][19][20][21]. Although these re-balance strategies worked well on the centralized imbalance datasets, it remains a question whether they are useful in federated long-tailed learning. 

",0
69," The correctness of AD has been extensively studied, especially in the past few years. When a program uses only differentiable functions, AD is shown to compute its standard derivative at all real-valued inputs ([5]; ; [4]; [6]; ; [3]; [1]; ; ). In contrast, when a program uses non-differentiable functions, the program itself can be non-differentiable, and AD can return a value different from its standard derivative, at some real-valued inputs. Nevertheless, for a large class of programs, such inputs are shown to be in a Lebesgue measure-zero subset of the real-valued input domain (; ; [2]; ). All these works consider the case when inputs to AD are real-valued, while our work focuses on the case when the inputs are machine-representable.

The Clarke subdifferential and its connection to AD have been studied for decades. Some classes of functions (e.g., subdifferentially regular or strictly differentiable) are shown to admit exact chain rules for the Clarke subdifferential (e.g., Theorems 2.3.9, 2.3.10, and 2.6.6 of [7] and Theorem 10.6 of ), and this implies that AD always computes a Clarke subderivative for a certain class of programs. However, this class of programs is restrictive, excluding even simple neural networks (e.g., \((1-\mathrm{ReLU}(x))^{2}\)) (). In contrast, our Theorem 3.6 shows that AD always computes a Clarke subderivative of neural networks with bias parameters. For piecewise differentiable functions, the Clarke subdifferential can be expressed in terms of the standard derivatives of underlying differentiable functions (e.g., Proposition 4.3.1 of ), but this result is not directly related to AD.

A variety of algorithms (other than AD) have been proposed to compute a Clarke subgradient of a scalar program, correctly and efficiently. For a large class of programs \(f:\mathbb{R}^{n}\to\mathbb{R}\) and an input \(x\in\mathbb{R}^{n}\), the algorithm by  computes a Clarke subgradient of \(f\) at \(x\) in time \(\mathcal{O}(T)\) almost surely, while the algorithms by [8];  compute the quantity in time \(\mathcal{O}(nT)\) deterministically, where \(T\) denotes time to evaluate \(f(x)\). Our Theorem 3.6 provides a relevant result as described above, but we point out that our work is about analyzing the correctness of vanilla (forward/reverse-mode) AD, not about proposing a new algorithm.

Recently, [9] empirically studied how the choice of \(D^{\mathsf{Ab}}\mathrm{ReLU}(0)\) changes the output of AD and the training of neural networks. In contrast, our work theoretically studies the correctness of AD. Further connections between this and our work are discussed in SS6.

",1
70," The correctness of AD has been extensively studied, especially in the past few years. When a program uses only differentiable functions, AD is shown to compute its standard derivative at all real-valued inputs ([5]; ; [4]; [6]; ; [3]; [1]; ; ). In contrast, when a program uses non-differentiable functions, the program itself can be non-differentiable, and AD can return a value different from its standard derivative, at some real-valued inputs. Nevertheless, for a large class of programs, such inputs are shown to be in a Lebesgue measure-zero subset of the real-valued input domain (; ; [2]; ). 

The Clarke subdifferential and its connection to AD have been studied for decades. Some classes of functions (e.g., subdifferentially regular or strictly differentiable) are shown to admit exact chain rules for the Clarke subdifferential (e.g., Theorems 2.3.9, 2.3.10, and 2.6.6 of [7] and Theorem 10.6 of ), and this implies that AD always computes a Clarke subderivative for a certain class of programs. However, this class of programs is restrictive, excluding even simple neural networks (e.g., \((1-\mathrm{ReLU}(x))^{2}\)) (). 

A variety of algorithms (other than AD) have been proposed to compute a Clarke subgradient of a scalar program, correctly and efficiently. For a large class of programs \(f:\mathbb{R}^{n}\to\mathbb{R}\) and an input \(x\in\mathbb{R}^{n}\), the algorithm by  computes a Clarke subgradient of \(f\) at \(x\) in time \(\mathcal{O}(T)\) almost surely, while the algorithms by [8];  compute the quantity in time \(\mathcal{O}(nT)\) deterministically, where \(T\) denotes time to evaluate \(f(x)\). 

Recently, [9] empirically studied how the choice of \(D^{\mathsf{Ab}}\mathrm{ReLU}(0)\) changes the output of AD and the training of neural networks. 

",0
71," **Visual Navigation --** is a rich problem that involves perception, mapping and decision making, with required capacities being highly dependent on the specific task. A summary of reasoning in navigation has been given in [1], differentiating, for instance, between waypoint navigation (_Pointgoal_) [1] or finding objects of semantic categories (_ObjectGoal_) [1]. More recent tasks have been explicitely designed to evaluate and encourage mapping objects of interest during navigation itself [2][3]. They are of sequential nature and use external objects, which are not part of the scanned 3D scenes but randomly placed. In this work we address _Multi-Object Navigation (MultiON)_[3].

**Mapping and Representations --** Classical methods often rely on SLAM [4][5] which has been proposed in different variants (2D or 3D metric, topological, eventually with semantics) and observations (LIDAR, visual etc.). The objective is to integrate observations and odometry estimates over a trajectory and reconstruct the scene. Differentiable variants have been proposed recently [6][7]. Mapping can also be discovered through interactions by a blind agent [8]. Visual Navigation can also be framed as an end-to-end learning problem, where representations are learned automatically from different signals, in particular RL. Memory can take the form of vectorial representations in recurrent units [9][10][11], with hybrid variants including mapping [12][13][14]. Recent work tends to augment agents with more structured memories. Examples are spatial metric tensors, which can contain occupancy [15], semantics [16] or be fully latent, effectively corresponding to inductive biases of the neural agents [17][18]. Other alternatives are topological maps [12] or self-attention and transformers [19] adapted to navigation [20][21][22][23].

**Implicit representations --** were initially targeting 3D reconstruction [24][25][26]. The core idea is to replace the need for discretizing 3D space into voxels [27], 3D points [28] or meshes [29], by an implicit representation of the 3D structure of the scene through the parameters of a learned neural network. Recent work [30] achieved state-of-the-art performance on novel view synthesis with neural implicit representations. The NeRF paper introduced a differentiable volume rendering loss allowing to supervise 3D scene reconstruction from only 2D supervision . For a more detailed overview of recent advances in the rapidly growing field, we refer the reader to [31].

**Implicit representations in robotics --** are a recent phenomenon, used to represent density [32] or to perform visuomotor control [33]. Related to goal-oriented navigation, some work targets SLAM with neural implicit representations [34], follow-up adding semantics [35], learned from sparse semantic annotations of the scene. [36] is also built on top of [34] and allows a user to interactively provide semantic annotation for the implicit representation to be trained on in real time. [37] proposes a hierarchical implicit representation of a scene to scale to larger environments and obtain a more detailed reconstruction. [38] combines feature-based SLAM and NeRF. Our work goes beyond implicit SLAM and does not stop at reconstructing a scene. We not only build implicit representations dynamically during the episode, we also use them in a down-stream navigation task _without_ requiring any initial rollout for pre-training or building a representation. We also combine two different implicit representations targeting semantics vs. scene structure.

**Analyzing the neural network function space --** implicit representations are instances of function spaces, which are represented through their trainable parameters. Previous work performed analyses by predicting accuracy from network weights [39][40] or the generality gap between train and test performance from hidden activations [41][42]. A direction pioneered by Hypernetworks  directly predict the network weights. Recently, [43] generate the weights of a CNN from support samples in the context of few-shot learning. More related to our work, [44] learns to predict the weights of an implicit representation based on external factors in the context of spatio-temporal dynamics encoding. In this work, we learn a direct mapping between an implicit representation, represented by its weights, to an actionable embedding summarizing the scene globally.

",1
72," **Visual Navigation --** is a rich problem that involves perception, mapping and decision making, with required capacities being highly dependent on the specific task. A summary of reasoning in navigation has been given in [1], differentiating, for instance, between waypoint navigation (_Pointgoal_) [1] or finding objects of semantic categories (_ObjectGoal_) [1]. More recent tasks have been explicitely designed to evaluate and encourage mapping objects of interest during navigation itself [2][3]. They are of sequential nature and use external objects, which are not part of the scanned 3D scenes but randomly placed. I

**Mapping and Representations --** Classical methods often rely on SLAM [4][5] which has been proposed in different variants (2D or 3D metric, topological, eventually with semantics) and observations (LIDAR, visual etc.). The objective is to integrate observations and odometry estimates over a trajectory and reconstruct the scene. Differentiable variants have been proposed recently [6][7]. Mapping can also be discovered through interactions by a blind agent [8]. Visual Navigation can also be framed as an end-to-end learning problem, where representations are learned automatically from different signals, in particular RL. Memory can take the form of vectorial representations in recurrent units [9][10][11], with hybrid variants including mapping [12][13][14]. Recent work tends to augment agents with more structured memories. Examples are spatial metric tensors, which can contain occupancy [15], semantics [16] or be fully latent, effectively corresponding to inductive biases of the neural agents [17][18]. Other alternatives are topological maps [12] or self-attention and transformers [19] adapted to navigation [20][21][22][23].

**Implicit representations --** were initially targeting 3D reconstruction [24][25][26]. The core idea is to replace the need for discretizing 3D space into voxels [27], 3D points [28] or meshes [29], by an implicit representation of the 3D structure of the scene through the parameters of a learned neural network. Recent work [30] achieved state-of-the-art performance on novel view synthesis with neural implicit representations. The NeRF paper introduced a differentiable volume rendering loss allowing to supervise 3D scene reconstruction from only 2D supervision . For a more detailed overview of recent advances in the rapidly growing field, we refer the reader to [31].

**Implicit representations in robotics --** are a recent phenomenon, used to represent density [32] or to perform visuomotor control [33]. Related to goal-oriented navigation, some work targets SLAM with neural implicit representations [34], follow-up adding semantics [35], learned from sparse semantic annotations of the scene. [36] is also built on top of [34] and allows a user to interactively provide semantic annotation for the implicit representation to be trained on in real time. [37] proposes a hierarchical implicit representation of a scene to scale to larger environments and obtain a more detailed reconstruction. [38] combines feature-based SLAM and NeRF. 

**Analyzing the neural network function space --** implicit representations are instances of function spaces, which are represented through their trainable parameters. Previous work performed analyses by predicting accuracy from network weights [39][40] or the generality gap between train and test performance from hidden activations [41][42]. A direction pioneered by Hypernetworks  directly predict the network weights. Recently, [43] generate the weights of a CNN from support samples in the context of few-shot learning. More related to our work, [44] learns to predict the weights of an implicit representation based on external factors in the context of spatio-temporal dynamics encoding. 

",0
73," Invariant Self-supervised learningTwo main families of methods can be distinguished: contrastive and non-contrastive. Contrastive methods (; [3]; ; ; ) mostly rely on the InfoNCE criterion ([1]) except for ([2]) which uses squared similarities between the embedding. A clustering variant of contrastive learning has also emerged (; ; ) and can be thought of as contrastive methods, but between cluster centroids instead of samples. Non-contrastive methods ([4]; ; ; [8]; [7]; [6]; ) aim at bringing together embeddings of positive samples, similar to contrastive learning. However, a key difference with contrastive methods lies in how those methods prevent a representational collapse. In the former, the criterion explicitly pushes away negative samples, i.e., all samples that are not positive, from each other. In the latter, the criterion considers the embeddings as a whole and encourages information content maximization to avoid collapse, e.g., by regularizing the empirical covariance matrix of the embeddings. While we study methods from both families in our experiments, they have been shown to lead to very similar representations ([5]).

Introducing equivariance in invariant self-supervised learningWhile most of the aforementioned works focus on learning representations that are invariant to augmentations, some works have instead tried to learn representations where information about certain transformations is preserved. This can be done by predicting the augmentation parameters ([9]; ; [11]), or by introducing other transformations such as image rotations (). Preserving the augmentations' strength in the representations can also be used to learn less invariant representations ([10]). These methods offer no guarantees on the existence of a mapping between transformed representations in latent space, nor ways to prove its existence or lack thereof. As such these methods cannot be considered to truly be equivariant.

Equivariant representation learningPrevious works have explored equivariant representation learning using autoencoders, such as transforming autoencoders (), Homeomorphic VAEs ([12]) or ([14]). Recent works such as EquiMod () or SEN ([15]) have also included a predictor that enables the steering of representations in latent space, without requiring reconstruction. These methods form the basis for our comparisons. In ([13]), representations are split in class and pose, i.e. invariant and equivariant, and assumes a simple equivariant latent space where the group action is the same as in the underlying data, e.g. 3 dimensions to represent pose. This assumes prior knowledge on the group of transformations, and can prove limited when the transformations cause a loss of information. Transformations are also assumed to be small, similarly as for SEN. We aim at deriving a more general predictor architecture with no such priors. In (), equivariant representations are learned with no knowledge of the group element associated with the transformation, but by having pairs of samples where the same transformation was applied.

",1
74," Invariant Self-supervised learningTwo main families of methods can be distinguished: contrastive and non-contrastive. Contrastive methods (; [3]; ; ; ) mostly rely on the InfoNCE criterion ([1]) except for ([2]) which uses squared similarities between the embedding. A clustering variant of contrastive learning has also emerged (; ; ) and can be thought of as contrastive methods, but between cluster centroids instead of samples. Non-contrastive methods ([4]; ; ; [8]; [7]; [6]; ) aim at bringing together embeddings of positive samples, similar to contrastive learning. However, a key difference with contrastive methods lies in how those methods prevent a representational collapse. In the former, the criterion explicitly pushes away negative samples, i.e., all samples that are not positive, from each other. In the latter, the criterion considers the embeddings as a whole and encourages information content maximization to avoid collapse, e.g., by regularizing the empirical covariance matrix of the embeddings. 

Introducing equivariance in invariant self-supervised learningWhile most of the aforementioned works focus on learning representations that are invariant to augmentations, some works have instead tried to learn representations where information about certain transformations is preserved. This can be done by predicting the augmentation parameters ([9]; ; [11]), or by introducing other transformations such as image rotations (). Preserving the augmentations' strength in the representations can also be used to learn less invariant representations ([10]). These methods offer no guarantees on the existence of a mapping between transformed representations in latent space, nor ways to prove its existence or lack thereof. As such these methods cannot be considered to truly be equivariant.

Equivariant representation learningPrevious works have explored equivariant representation learning using autoencoders, such as transforming autoencoders (), Homeomorphic VAEs ([12]) or ([14]). Recent works such as EquiMod () or SEN ([15]) have also included a predictor that enables the steering of representations in latent space, without requiring reconstruction. These methods form the basis for our comparisons. In ([13]), representations are split in class and pose, i.e. invariant and equivariant, and assumes a simple equivariant latent space where the group action is the same as in the underlying data, e.g. 3 dimensions to represent pose. This assumes prior knowledge on the group of transformations, and can prove limited when the transformations cause a loss of information. Transformations are also assumed to be small, similarly as for SEN.  In (), equivariant representations are learned with no knowledge of the group element associated with the transformation, but by having pairs of samples where the same transformation was applied.

",0
75," **Vision-and-Language Navigation (VLN).** Since its introduction in [1], many variants of the Vision-and-Language Navigation (VLN) task have been proposed including those in continuous simulators . We refer the reader to [2] for a comprehensive survey. In this work, we examine agents in the Room-Across-Room (RxR) dataset [3] which extends the original VLN task to a multilingual setting with longer, more complex trajectories and pose traces which provide temporal alignment between instruction words and visual observations. There has also been significant modelling work to develop instruction-following agents [4][5][6][7][8][4][9] and we examine three recent models in our analysis [6][7][9].

The RxR task is situated in the Matterport3D [10] environments which provide an interface for agents to move through the environment along a graph of panoramic viewpoints taken in real environments. Matterport3D also provides region annotations for room type which we utilize in our experiments. We also leverage annotations from the REVERIE [11] dataset which extends VLN settings with an additional goal of identifying an object described by a referring expression. Specifically, using the annotations from REVERIEv1 to identify visible objects at each viewpoint.

**Evaluating VLN Agents.** In standard settings, VLN agents are evaluated by metrics that focus on either the agent reaching the goal efficiently (Success weighted by inverse Path Length [12]) or by their trajectory's alignment with the ground truth path (Normalized Dynamic Time Warping [1]). These metrics focus on the agent's performance in aggregate and do not examine agent performance on the level of sub-instruction or skills.

Some works have examined VLN agent behavior more closely by masking or replacing portions of the instructions [13][14] and observing the resulting change to overall task metrics like those described above. Zhu _et al_. [14] find that VLN agents still achieve relatively high success rates even when all references to visual objects are masked from instructions. These findings cast doubt on the vision-language alignment ability of these agents. [13] also perform masking experiments but come to different conclusions, with some models relying more heavily on nouns than directional words. In both works, agent performance is measured on an episodic level that relies on a sequence of agent decisions; however, single errors in a trajectory may compound and result in misestimating the impact of masked terms. In contrast to these works, we present a skill-based analysis of VLN agents by constructing specific intervention episodes wherein the appropriate next action is known.

**Behavioral Analysis of AI Models.** Recent work in natural language processes has applied behavioral analysis to examine specific skill capabilities. Like us, Riberio _et al_. [15] develop an intervention paradigm wherein dataset examples are modified in ways such that the desired change in model behavior is knowable. These examples and their associated skills are collected into a ""checklist"" that can be used to evaluate models. Likewise, our work can be construed as generating a checklist of skills for VLN. Yang _et al_. [16] follow a similar paradigm and develop a method to automatically generate test cases using a large language models [17].

",1
76," **Vision-and-Language Navigation (VLN).** Since its introduction in [1], many variants of the Vision-and-Language Navigation (VLN) task have been proposed including those in continuous simulators . We refer the reader to [2] for a comprehensive survey. In this work, we examine agents in the Room-Across-Room (RxR) dataset [3] which extends the original VLN task to a multilingual setting with longer, more complex trajectories and pose traces which provide temporal alignment between instruction words and visual observations. 

The RxR task is situated in the Matterport3D [10] environments which provide an interface for agents to move through the environment along a graph of panoramic viewpoints taken in real environments. Matterport3D also provides region annotations for room type which we utilize in our experiments. 

**Evaluating VLN Agents.** In standard settings, VLN agents are evaluated by metrics that focus on either the agent reaching the goal efficiently (Success weighted by inverse Path Length [12]) or by their trajectory's alignment with the ground truth path (Normalized Dynamic Time Warping [1]). These metrics focus on the agent's performance in aggregate and do not examine agent performance on the level of sub-instruction or skills.

Some works have examined VLN agent behavior more closely by masking or replacing portions of the instructions [13][14] and observing the resulting change to overall task metrics like those described above. Zhu _et al_. [14] find that VLN agents still achieve relatively high success rates even when all references to visual objects are masked from instructions. These findings cast doubt on the vision-language alignment ability of these agents. [13] also perform masking experiments but come to different conclusions, with some models relying more heavily on nouns than directional words. In both works, agent performance is measured on an episodic level that relies on a sequence of agent decisions; however, single errors in a trajectory may compound and result in misestimating the impact of masked terms. 

**Behavioral Analysis of AI Models.** Recent work in natural language processes has applied behavioral analysis to examine specific skill capabilities. Like us, Riberio _et al_. [15] develop an intervention paradigm wherein dataset examples are modified in ways such that the desired change in model behavior is knowable. These examples and their associated skills are collected into a ""checklist"" that can be used to evaluate models. Likewise, our work can be construed as generating a checklist of skills for VLN. Yang _et al_. [16] follow a similar paradigm and develop a method to automatically generate test cases using a large language models [17].

",0
77," **Audio-Visual Joint Learning.** Audio-visual joint learning has been addressed in many previous works [1][2][3][4][5][6][7][8][9][10] to learn the audio-visual correlation between two distinct modalities from videos. Such cross-modal alignments are beneficial for many audio-visual tasks, such as audio-event localization [11], audio-visual spatialization [12][6][13], audio-visual navigation [14][15] and audio-visual parsing [16][17]. In this work, our main focus is to learn compact audio-visual representations for localizing individual sources on images from sound mixtures, which is more demanding than the tasks aforementioned above.

**Audio-Visual Source Separation.** Audio-visual source separation aims to separate individual sound sources from the audio mixture given the image with sources on it. In recent years, researchers [4][18][19][20] have tried to explore diverse pipelines to learn discriminative visual representations from images for source separation. Zhao _et al._ first proposed a ""Mix-and-Separate"" network to capture the alignment between pixels and the spectral components of audio for the reconstruction of each input source spectrogram. With the benefit of visual cues, MP-Net [20] utilized a recursive MinusPlus Net to separate all salient sounds from the mixture. Tian _et al._[18] used a cyclic co-learning framework with sounding object visual grounding to separate visual sound sources. More recently, more types of modalities have been explored to boost the performance of audio-visual source separation, such as motion in SoM [10], gesture composed of pose and keypoints in MG [21], and spatio-temporal visual scene graphs in AVSGS [22]. Different from them, we do not need to recover the audio spectrogram of individual sources from the mixture. Instead, we leverage the category-aware representations of individual sources to localize the corresponding regions for each source, where learnable audio-visual class tokens are applied as the desirable guidance.

**Visual Sound Source Localization.** Visual sound source localization is a typical and challenging problem that predicts the location of individual sound sources in a video. Early works [23][24][25] applied traditional machine learning approaches, such as statistical models [25] and canonical correlation analysis [24] to learn low-level alignment between audio and visual representations. With the success of deep neural nets, recent researchers [26][5][27][28][9][29] explored many architectures to learn the audio-visual correspondence for localizing single-source sounds. Attention10k [9] localized a sound source in the image using a two-stream architecture with an attention mechanism. Hard sample mining was introduced in LVS [26] to optimize a differentiable threshold-based contrastive loss for predicting discriminative audio-visual correspondence maps. More recently, a multiple-instance contrastive learning framework was proposed in EZVSL [28] to align regions with the most corresponding audio without negative regions involved.

Due to the natural mixed property of sounds in our environment, recent works [30][31] also have explored different frameworks to localize multiple sources on frames from a sound mixture simultaneously. DSOL [31] utilized a two-stage training framework to deal with silence for category-aware sound source localization. More recently, a contrastive random walk model was trained in Mix-and-Localize [30] to link each audio node with an image node using a transition probability of audio-visual similarity. While those single-source and multi-source approaches achieve promising performance in sound localization, they can only handle a fixed number of sources and they cannot learn discriminative class-aware representations for individual sources. In contrast, we develop a fully novel framework to aggregate compact category-wise audio and visual source representations with explicit learnable source class tokens. To the best of our knowledge, we are the first to leverage an explicit grouping mechanism for sound source localization. Our experiments in Section 4.2 also demonstrate the effectiveness of AVGN in both single-source and multi-source localization.

",1
78," **Audio-Visual Joint Learning.** Audio-visual joint learning has been addressed in many previous works [1][2][3][4][5][6][7][8][9][10] to learn the audio-visual correlation between two distinct modalities from videos. Such cross-modal alignments are beneficial for many audio-visual tasks, such as audio-event localization [11], audio-visual spatialization [12][6][13], audio-visual navigation [14][15] and audio-visual parsing [16][17]. 

**Audio-Visual Source Separation.** Audio-visual source separation aims to separate individual sound sources from the audio mixture given the image with sources on it. In recent years, researchers [4][18][19][20] have tried to explore diverse pipelines to learn discriminative visual representations from images for source separation. Zhao _et al._ first proposed a ""Mix-and-Separate"" network to capture the alignment between pixels and the spectral components of audio for the reconstruction of each input source spectrogram. With the benefit of visual cues, MP-Net [20] utilized a recursive MinusPlus Net to separate all salient sounds from the mixture. Tian _et al._[18] used a cyclic co-learning framework with sounding object visual grounding to separate visual sound sources. More recently, more types of modalities have been explored to boost the performance of audio-visual source separation, such as motion in SoM [10], gesture composed of pose and keypoints in MG [21], and spatio-temporal visual scene graphs in AVSGS [22]. 

**Visual Sound Source Localization.** Visual sound source localization is a typical and challenging problem that predicts the location of individual sound sources in a video. Early works [23][24][25] applied traditional machine learning approaches, such as statistical models [25] and canonical correlation analysis [24] to learn low-level alignment between audio and visual representations. With the success of deep neural nets, recent researchers [26][5][27][28][9][29] explored many architectures to learn the audio-visual correspondence for localizing single-source sounds. Attention10k [9] localized a sound source in the image using a two-stream architecture with an attention mechanism. Hard sample mining was introduced in LVS [26] to optimize a differentiable threshold-based contrastive loss for predicting discriminative audio-visual correspondence maps. More recently, a multiple-instance contrastive learning framework was proposed in EZVSL [28] to align regions with the most corresponding audio without negative regions involved.

Due to the natural mixed property of sounds in our environment, recent works [30][31] also have explored different frameworks to localize multiple sources on frames from a sound mixture simultaneously. DSOL [31] utilized a two-stage training framework to deal with silence for category-aware sound source localization. More recently, a contrastive random walk model was trained in Mix-and-Localize [30] to link each audio node with an image node using a transition probability of audio-visual similarity. While those single-source and multi-source approaches achieve promising performance in sound localization, they can only handle a fixed number of sources and they cannot learn discriminative class-aware representations for individual sources. 

",0
79," **Semantic segmentation.** Network architecture for semantic segmentation has evolved for years, from CNNs [1][2][3] to Transformers [4][5][6][7][1]. Another line of research works focuses on enhancing the extracted representations like integrating attention mechanisms [8][9][10][11] or context representations [12][13][14][15] into segmentation models. BLV is complementary to these various frameworks and improves several state-of-the-art methods consistently.

**Semi-supervised semantic segmentation.** To alleviate the heavy need for large-scale annotated data, semi-supervised semantic segmentation has become a research hotspot. There are two typical frameworks for this task: consistency regularization [16][17][18] and self-training [19][20][21][22]. Consistency regularization applies various perturbations [17] on training data and forces consistent predictions between the perturbed and the unperturbed input [18]. Self-training [23][24][20][23][25][13][26] uses the predictions from the pre-trained model as the ""ground-truth"" of the unlabeled data and then trains a semantic segmentation model in a fully-supervised manner. These two frameworks have no specialized operations for long-tail data. To this end, we provide a concise and generic approach that can be integrated into any framework.

**Unsupervised domain adaptive semantic segmentation.** UDA semantic segmentation aims at learning segmentation model that transfer knowledge from labeled source domain to unlabeled target domain. Early methods for UDA segmentation focus on enabling the model to extract to domain-invariant features. They align the cross-domain feature distribution at image level [27][28][28], feature level [29][30][31][32] and output level [33][32][34] via image style transfer [35][36][37][38][39], image feature domain discriminator [40][41][42][40] or well-designed metrics [43][44][45]. Follow-up study [46][47] suggests that the self-training-based pipeline leads to more consistent improvement. Recently, DAFormer [48] and HRDA [49] provide a self-training-based Transformer architecture together with many efficient training strategies, which can achieve consistent improvement over other competitors. BLV can be simply integrated into existing pipelines, and consistently improve their performance.

**Long-tail learning.** Since the long-tail phenomenon is common  in deep learning, the performance of the model tends to be dominated by the head category, while the learning of the tail category is severely underdeveloped. One intuitive solution to alleviate unbalanced data distribution is data processing, which typically consists of three ways: over-sampling [50][51][52], under-sampling [53][54] and data augmentation [14]. Various methods have been proposed to alleviate the long-tail phenomenon in semantic segmentation, which can be mainly divided into three settings: fully supervised [55][56], semi-supervised [57][58][59], and UDA [60][61][62]. It is noteworthy that existing methods are usually limited to a specific setting and lack generalizability.

**Noise-based augmentation.** To improve model robustness and avoid over-fitting, augmenting data with noise [63][64][65] at image level or feature level is widely applied to model training. Techniques [66][67] like Dropout [68], color jittering [69], gaussian noise, are the most common methods and proved to be simple yet efficient, but they might also introducing task-agnostic bias . Another line of research aims to optimize the noise added in extracted features to ""fool"" the model [70]. The optimized noise is defined as adversarial examples, which are commonly recognized as the hard sample for the model. Methods like _M2m_[71] and _AdvProp_[72] utilize adversarial examples to augment the training data and significantly improve model robustness. Prior arts focus on improving the robustness yet ignoring the prevalence of long-tail data, whereas our BLV can alleviate the feature squeeze caused by long-tail data effectively.

",1
80," **Semantic segmentation.** Network architecture for semantic segmentation has evolved for years, from CNNs [1][2][3] to Transformers [4][5][6][7][1]. Another line of research works focuses on enhancing the extracted representations like integrating attention mechanisms [8][9][10][11] or context representations [12][13][14][15] into segmentation models. 

**Semi-supervised semantic segmentation.** To alleviate the heavy need for large-scale annotated data, semi-supervised semantic segmentation has become a research hotspot. There are two typical frameworks for this task: consistency regularization [16][17][18] and self-training [19][20][21][22]. Consistency regularization applies various perturbations [17] on training data and forces consistent predictions between the perturbed and the unperturbed input [18]. Self-training [23][24][20][23][25][13][26] uses the predictions from the pre-trained model as the ""ground-truth"" of the unlabeled data and then trains a semantic segmentation model in a fully-supervised manner. These two frameworks have no specialized operations for long-tail data. 

**Unsupervised domain adaptive semantic segmentation.** UDA semantic segmentation aims at learning segmentation model that transfer knowledge from labeled source domain to unlabeled target domain. Early methods for UDA segmentation focus on enabling the model to extract to domain-invariant features. They align the cross-domain feature distribution at image level [27][28][28], feature level [29][30][31][32] and output level [33][32][34] via image style transfer [35][36][37][38][39], image feature domain discriminator [40][41][42][40] or well-designed metrics [43][44][45]. Follow-up study [46][47] suggests that the self-training-based pipeline leads to more consistent improvement. Recently, DAFormer [48] and HRDA [49] provide a self-training-based Transformer architecture together with many efficient training strategies, which can achieve consistent improvement over other competitors. 

**Long-tail learning.** Since the long-tail phenomenon is common  in deep learning, the performance of the model tends to be dominated by the head category, while the learning of the tail category is severely underdeveloped. One intuitive solution to alleviate unbalanced data distribution is data processing, which typically consists of three ways: over-sampling [50][51][52], under-sampling [53][54] and data augmentation [14]. Various methods have been proposed to alleviate the long-tail phenomenon in semantic segmentation, which can be mainly divided into three settings: fully supervised [55][56], semi-supervised [57][58][59], and UDA [60][61][62]. It is noteworthy that existing methods are usually limited to a specific setting and lack generalizability.

**Noise-based augmentation.** To improve model robustness and avoid over-fitting, augmenting data with noise [63][64][65] at image level or feature level is widely applied to model training. Techniques [66][67] like Dropout [68], color jittering [69], gaussian noise, are the most common methods and proved to be simple yet efficient, but they might also introducing task-agnostic bias . Another line of research aims to optimize the noise added in extracted features to ""fool"" the model [70]. The optimized noise is defined as adversarial examples, which are commonly recognized as the hard sample for the model. Methods like _M2m_[71] and _AdvProp_[72] utilize adversarial examples to augment the training data and significantly improve model robustness. 

",0
81," Dual-process theory (; ) argues there are two cognitive systems underpinning human reasoning: System 1 and System 2. The purpose of clarifying these systems is that they have the potential to help us construct artificial intelligence systems that benefit from human flexibility and methodical generalization.

Dual process system model guidance is not new. [3] simulated Systems 1 and 2 to improve consistency and coherence of neural networks. Similar to several studies ; [2]; [1], in addition to System 1 for the generation, we develop a distinct model as System 2, called Verifier. The Verifier checks the feasibility and correctness of the generator's content and collaboratively solves the reasoning task together.

Many works exploit the multi-step reasoning ability of language models.  showed that training a verifier to score the solutions generated by a fine-tuned GPT-3 could improve the performance compared to solely fine-tuning a GPT-3. [4] discovered that asking the language model to write the intermediate process could achieve better results on various NLP tasks. Likewise, Chain-of-Thought (CoT) prompts () prepended exemplars with intermediate reasoning steps as prompts and achieved SoTA on several reasoning benchmarks by using large-scale PLMs. [5] further boosted CoT's performance by sampling a bunch of possible solutions and then obtained the final answer by majority voting. DIVERSE ([2]) proved diverse CoT prompts and an extra verifier were both helpful for PLMs to solve reasoning problems. [6] found that by simply adding ""Let's think step by step"" after the question. PLMs could successfully step by step solve the problems, called Zero-shot-CoT.

These above methods rely on extremely large language models, resulting in high computational cost and time-consuming. Moreover, several works (; [6]) point out that neither CoT nor Zero-shot-CoT is helpful to smaller models. While our method does not necessarily require extremely large PLMs and can work with models with different size scales, thus reducing computational cost and inference time. Our approach has competitive zero-shot performance thanks to the efficient and collaborative application of a dual-process system.

",1
82," Dual-process theory (; ) argues there are two cognitive systems underpinning human reasoning: System 1 and System 2. The purpose of clarifying these systems is that they have the potential to help us construct artificial intelligence systems that benefit from human flexibility and methodical generalization.

Dual process system model guidance is not new. [3] simulated Systems 1 and 2 to improve consistency and coherence of neural networks. Similar to several studies ; [2]; [1], in addition to System 1 for the generation, we develop a distinct model as System 2, called Verifier. The Verifier checks the feasibility and correctness of the generator's content and collaboratively solves the reasoning task together.

Many works exploit the multi-step reasoning ability of language models.  showed that training a verifier to score the solutions generated by a fine-tuned GPT-3 could improve the performance compared to solely fine-tuning a GPT-3. [4] discovered that asking the language model to write the intermediate process could achieve better results on various NLP tasks. Likewise, Chain-of-Thought (CoT) prompts () prepended exemplars with intermediate reasoning steps as prompts and achieved SoTA on several reasoning benchmarks by using large-scale PLMs. [5] further boosted CoT's performance by sampling a bunch of possible solutions and then obtained the final answer by majority voting. DIVERSE ([2]) proved diverse CoT prompts and an extra verifier were both helpful for PLMs to solve reasoning problems. [6] found that by simply adding ""Let's think step by step"" after the question. PLMs could successfully step by step solve the problems, called Zero-shot-CoT.

These above methods rely on extremely large language models, resulting in high computational cost and time-consuming. Moreover, several works (; [6]) point out that neither CoT nor Zero-shot-CoT is helpful to smaller models. 

",0
83," The techniques proposed to combat catastrophic forgetting can be broadly categorized into three [1]. (1) The regularisation-based methods that add an extra regularisation loss term either to penalize changes to the network parameters that are important for previous tasks [2][3] or to distill knowledge from previous tasks to the current task [4][5]. (2) The parameter isolation-based methods that assign each task with an isolated set of parameters to prevent task interference either by dynamically increasing the network capacity [6][7][8] or by masking previous task parameters in a fixed size network [9][10][11]. Although parameter isolation methods are effective in overcoming catastrophic forgetting, they experience either a linear increase in network parameters or a decrease in capacity per task as the number of tasks grows [12]. (3) The rehearsal-based methods that store a small subset of previous task data to either retrain [13] or constrain the optimisation [14][15][16] during the learning of new tasks in order to retain the discriminability between old and new classes. However, these methods also encounter pitfalls due to memory limitations, and other pragmatic concerns such as privacy or consent issues when storing samples. An alternative to rehearsal-based methods is ""pseudo-rehearsal"", which involves training a generative model to mimic past task distributions [17][18]. Despite the encouraging results, generative models are computationally expensive to train [19] and are also prone to catastrophic forgetting [20]. This motivated the development of NECIL strategies that neither depends on real nor fake past samples [21][22][23].

NECIL methods benefit from powerful feature extractors learning transferable features across tasks, as demonstrated by SDC [23], which showed that embedding networks suffer significantly less from catastrophic forgetting. PASS [21] also showed that self-supervised learning alleviates task-level overfitting. Furthermore, to maintain the decision boundaries of previously learned classes, PASS introduced a class-mean prototype augmentation technique based on Gaussian noise. While this technique aids in the retention of old information, it can be further improved by leveraging the knowledge of the distribution of classes in the feature space. Accordingly, IL2A [24] proposed storing covariance matrices to retain class variations, but this approach can be memory intensive. SSRE [22] proposed a dynamic structure reorganization strategy to retain and transfer knowledge between tasks along with a prototype selection mechanism that utilizes an up-sampling technique of non-augmented class-mean prototypes. Similar to these approaches, we also store the mean prototype, while proposing a new method to augment them. To this end, we use the topological connections derived from an NG-like vector quantization to generate prototypes that lie within the shared feature regions of the confusing classes which aid in establishing better class discrimination.

Vector Quantization (VQ), a technique used to discretize a continuous data space into a finite set of ""coding vectors"" (CVs) was popularised with the advents of Self-organizing Maps (SOMs) . In addition to quantizing the data manifold, a SOM captures a topological mapping from data to the CVs. Neural Gas (NG) networks [25][26], on the other hand, were introduced to address a shortcoming of the original SOM by allowing a generic graph structure rather than a fixed lattice structure. In NG, the CVs are adjusted to capture the data-dense regions, and the edges between these CVs are formed based on their proximity. These edges and the CVs form a graph that approximates the topology of the data manifold.

Coding vector-based learning can be traced back to the K-nearest neighbor (K-NN) algorithm [27]. For instance, Learning Vector Quantization (LVQ) was proposed to derive the CVs used in a 1-NN classifier [28][29]. Despite common roots, LVQ algorithms and unsupervised VQ algorithms such as SOM differ in their primary usages of CVs; the unsupervised algorithms attempt to obtain a set of CVs to best represent the data while LVQ algorithms attempt to reduce the misclassification rate by focusing on the decision boundaries between classes. These complementary properties allow us to combine unsupervised and supervised VQ methods  to obtain CVs to both reduce the misclassification rate and represent the data distribution .

Multiple studies explored the integration of the hierarchical feature-extracting capability of deep feature extractors with VQ  which were also later adapted to Continual Learning. TPCIL [30] proposed to retain the topology of the feature space to preserve old knowledge over the increments. IDLVQ [31] proposed to adapt a margin-based loss for the task of few-shot class incremental learning (FSCIL) - a special case of CIL, therefore not directly transferrable to CIL/NECIL - to create a large margin between classes to mitigate overlap. TOPIC [32] was also proposed for the FSCIL setting with the aim of preserving old knowledge by stabilizing a NG network. We highlight that changes to the topology are possible due to the inevitable feature drift occurring over incremental steps thus a method that uses both augmented prototypes and new data to update the topological graph between CVs is warrented.

",1
84," The techniques proposed to combat catastrophic forgetting can be broadly categorized into three [1]. (1) The regularisation-based methods that add an extra regularisation loss term either to penalize changes to the network parameters that are important for previous tasks [2][3] or to distill knowledge from previous tasks to the current task [4][5]. (2) The parameter isolation-based methods that assign each task with an isolated set of parameters to prevent task interference either by dynamically increasing the network capacity [6][7][8] or by masking previous task parameters in a fixed size network [9][10][11]. Although parameter isolation methods are effective in overcoming catastrophic forgetting, they experience either a linear increase in network parameters or a decrease in capacity per task as the number of tasks grows [12]. (3) The rehearsal-based methods that store a small subset of previous task data to either retrain [13] or constrain the optimisation [14][15][16] during the learning of new tasks in order to retain the discriminability between old and new classes. However, these methods also encounter pitfalls due to memory limitations, and other pragmatic concerns such as privacy or consent issues when storing samples. An alternative to rehearsal-based methods is ""pseudo-rehearsal"", which involves training a generative model to mimic past task distributions [17][18]. Despite the encouraging results, generative models are computationally expensive to train [19] and are also prone to catastrophic forgetting [20]. 

NECIL methods benefit from powerful feature extractors learning transferable features across tasks, as demonstrated by SDC [23], which showed that embedding networks suffer significantly less from catastrophic forgetting. PASS [21] also showed that self-supervised learning alleviates task-level overfitting. Furthermore, to maintain the decision boundaries of previously learned classes, PASS introduced a class-mean prototype augmentation technique based on Gaussian noise. While this technique aids in the retention of old information, it can be further improved by leveraging the knowledge of the distribution of classes in the feature space. Accordingly, IL2A [24] proposed storing covariance matrices to retain class variations, but this approach can be memory intensive. SSRE [22] proposed a dynamic structure reorganization strategy to retain and transfer knowledge between tasks along with a prototype selection mechanism that utilizes an up-sampling technique of non-augmented class-mean prototypes. 

Vector Quantization (VQ), a technique used to discretize a continuous data space into a finite set of ""coding vectors"" (CVs) was popularised with the advents of Self-organizing Maps (SOMs) . In addition to quantizing the data manifold, a SOM captures a topological mapping from data to the CVs. Neural Gas (NG) networks [25][26], on the other hand, were introduced to address a shortcoming of the original SOM by allowing a generic graph structure rather than a fixed lattice structure. In NG, the CVs are adjusted to capture the data-dense regions, and the edges between these CVs are formed based on their proximity. These edges and the CVs form a graph that approximates the topology of the data manifold.

Coding vector-based learning can be traced back to the K-nearest neighbor (K-NN) algorithm [27]. For instance, Learning Vector Quantization (LVQ) was proposed to derive the CVs used in a 1-NN classifier [28][29]. Despite common roots, LVQ algorithms and unsupervised VQ algorithms such as SOM differ in their primary usages of CVs; the unsupervised algorithms attempt to obtain a set of CVs to best represent the data while LVQ algorithms attempt to reduce the misclassification rate by focusing on the decision boundaries between classes. These complementary properties allow us to combine unsupervised and supervised VQ methods  to obtain CVs to both reduce the misclassification rate and represent the data distribution .

Multiple studies explored the integration of the hierarchical feature-extracting capability of deep feature extractors with VQ  which were also later adapted to Continual Learning. TPCIL [30] proposed to retain the topology of the feature space to preserve old knowledge over the increments. IDLVQ [31] proposed to adapt a margin-based loss for the task of few-shot class incremental learning (FSCIL) - a special case of CIL, therefore not directly transferrable to CIL/NECIL - to create a large margin between classes to mitigate overlap. TOPIC [32] was also proposed for the FSCIL setting with the aim of preserving old knowledge by stabilizing a NG network. 

",0
85," **Optical Flow Estimation.** Traditionally, optical flow estimation [1][2][3][4][5] is treated as an energy minimization problem. Nowadays, deep models [6][7][8][9] formulate the optical flow estimation as a regression problem with an end-to-end trainable Convolutional Neural Network. More recently, deep learning based iterative refinement for optical flow [7][10][8][11][12] has resulted in a big breakthrough. RAFT  constructs a 4D multi-scale correlation volume and utilizes a convolutional GRU block as an update operator. And GMA [11] further proposes a global motion aggregation module to tackle the occlusion problem. Besides, recent methods [13][14][15][16][17] mainly focus on the recurrent decoder of RAFT. These methods are orthogonal to ours as we mainly focus on GIM pretraining of feature encoder and provide a rethinking for optical flow from the geometric matching consistent scene perspective.

**Geometric Image Matching.** Geometric image matching [18][19] tends to find correspondence among images with different views. Different from optical flow estimation, image matching usually assumes that the scene is static and the geometric displacements are due to the changeof viewpoints (camera pose). Among recent detector-free matching methods [20][21][22][23][24], Tang _et al_. [24] propose QuadTree attention to capture both fine-level details and long-range dependencies, which outperforms the Linear attention [25] used in [23]. In this work, we also employ a QuadTree attention-based network as our feature extractor to learn feature correlation between two-view images. Benefited from the matching pre-training, we can get much better flow estimation within non-occluded regions. Furthermore, global motion aggregation module [11] can help propagate the accurate flow within non-occluded regions to more challenging occluded ones, which can boost the performance of optical flow estimation a lot.

Besides, there is also a line of works [5][10][26][27][28] which try to reduce the gap between optical flow estimation and GIM. GMFlow [5] formulates the optical flow estimation as a dense global matching problem, while GMFlowNet [10] introduces the matching loss and matching initialization into the optical flow. On the other hand, Depthstill [27] constructs matching pairs based on estimated depth for direct optical flow training and achieves superior generalization to unseen real data. We should clarify that our work is different from them because MatchFlow focuses on the GIM based pre-training for better feature representations from the perspective of curriculum learning [29]. Moreover, MatchFlow outperforms these competitors with superior generalization on Sintel.

**Curriculum Learning**. Large datasets are one of the driving forces of deep learning. However, due to it's difficult to provide reliable ground truth optical flow labels, there is limited target data. Thus optical flow models typically rely on the pre-training on the synthetic FlyingChair [6] and FlyingThings3D [30] dataset with the curriculum learning [29]. In contrast, we give a rethinking to this pipeline and propose using GIM as a pre-training task for optical flow.

",1
86," **Optical Flow Estimation.** Traditionally, optical flow estimation [1][2][3][4][5] is treated as an energy minimization problem. Nowadays, deep models [6][7][8][9] formulate the optical flow estimation as a regression problem with an end-to-end trainable Convolutional Neural Network. More recently, deep learning based iterative refinement for optical flow [7][10][8][11][12] has resulted in a big breakthrough. RAFT  constructs a 4D multi-scale correlation volume and utilizes a convolutional GRU block as an update operator. And GMA [11] further proposes a global motion aggregation module to tackle the occlusion problem. Besides, recent methods [13][14][15][16][17] mainly focus on the recurrent decoder of RAFT. 

**Geometric Image Matching.** Geometric image matching [18][19] tends to find correspondence among images with different views. Different from optical flow estimation, image matching usually assumes that the scene is static and the geometric displacements are due to the changeof viewpoints (camera pose). Among recent detector-free matching methods [20][21][22][23][24], Tang _et al_. [24] propose QuadTree attention to capture both fine-level details and long-range dependencies, which outperforms the Linear attention [25] used in [23]. 

Besides, there is also a line of works [5][10][26][27][28] which try to reduce the gap between optical flow estimation and GIM. GMFlow [5] formulates the optical flow estimation as a dense global matching problem, while GMFlowNet [10] introduces the matching loss and matching initialization into the optical flow. On the other hand, Depthstill [27] constructs matching pairs based on estimated depth for direct optical flow training and achieves superior generalization to unseen real data. 

**Curriculum Learning**. Large datasets are one of the driving forces of deep learning. However, due to it's difficult to provide reliable ground truth optical flow labels, there is limited target data. Thus optical flow models typically rely on the pre-training on the synthetic FlyingChair [6] and FlyingThings3D [30] dataset with the curriculum learning [29]. 

",0
87," Previous work relates to ours from two main perspectives: Video Anomaly Detection methods (see Sec. 2.1), and diffusion models for motion synthesis (see Sec. 2.2).

Pioneer works analyze the trajectory of the agents in the frames to discriminate those distant from normality . Within recent literature, two major trends can be identified: latent- and reconstruction-based methods. VAD techniques also vary based on the type of input data they use, such as videos or human skeletal pose motions. MoCoDAD, as all the VAD works presented in this section, adheres to the OCC protocol, which simulates the scarcity of anomalies in real-world scenarios [1].

**Latent-based VAD** methods identify abnormality according to a score extracted from a learned latent space whereby normality is supposedly mapped into a constrained volume, and anomalies are those latents lying outside, with a largerscore (see [2][3][4] for an overview of latent-based AD). Sabokrou et al. [5] propose a two-staged cascade of deep neural networks. First, they employ a stack of autoencoders that detects points of interest (POIs) while excluding irrelevant patches (e.g., background). Second, they identify anomalies by densely extracting and modeling discriminative patches at POIs. Notice that this work constrains normality to belong to a single mode and anomalies outside, thus, addressing the openset'ness of anomalies, but it hampers the multimodal and diversity  aspect of normal motions. Contrarily, our work considers the multimodality of normal and abnormal motions.

Notably, Nguyen et al. [6] propose an image-based technique exploring multimodal anomaly detection via multi-headed VAEs. However, considering a fixed number of modes for reality amends multimodality only partially, as it misses to unleash its openset'ness. Differently, we adopt diffusion models for their improved mode coverage and generate multiple futures, not being constrained on a fixed number of heads (see Sec. 5).

**Reconstruction-based VAD** methods consider the original metric space of the input and leverage reconstruction as the proxy task to derive an anomaly score. These models are trained to encode and reconstruct the input from normal events, producing larger errors on anomalies not seen during training. [7][8] use sequences of frames and feed them to convolutional autoencoders. Gong et al. [9] ""memorize"" the most representative normal poses to discriminate new input samples. Liu et al. [10] tackle intensity and gradient loss, optical flow, and adversarial training. Luo et al. [11] use stacked RNNs with temporally-coherent sparse coding enforcing similar neighboring frames to be encoded with similar reconstruction coefficients. Barbalau et al. [12] builds upon [13] and integrates the reconstruction of the input frames, via multi-headed attention, into a multi-task learning framework. Besides [13][10], all works rely on a single reconstruction proxy task via non-variational architectures that learn discrete manifolds. However, normality and abnormality are multimodal and diverse, making it hard for these techniques to have an exact match (reconstruction) over the GT. Additionally, GANs used in [10] suffer from mode collapse [14] lacking to represent the multimodality of reality. Similarly to [6][12] can represent only a fixed number of modalities, which does represent the openset'ness of reality. MoCoDAD is a reconstruction-based approach and leverages diffusion processes [15] to account for the openset'ness of normalcy and anomalies in terms of pertinence to the GT.

**Skeleton-based VAD** methods exploit compact spatio-temporal skeletal representations of human motion instead of raw video frames. Morais et al. [16] use two GRU autoencoder branches to account for the global and local decomposition of the skeleton in a particular frame. Luo et al.  exploit stacked layers of ST-GCN [17] to accumulate joint information over the spatio-temporal dimensions of the frame and predict joints in the future. However, [17] uses a fixed adjacency matrix, depicting joint connections, for all ST-GCN layers, which hinders the exploration of intra-frame and intra-joint relationships, two factors that play a crucial role in improving the encoding of spatio-temporal features [18]. Markovitz et al. [19] utilize the encoder of an ST-GCN autoencoder to embed space-time skeletons into a latent vector. This vector is then fed to an end-to-end trainable deep-embedded clustering procedure which produces \(k\) clusters representing the multimodality of normalcy and anomalies. Flaborea et al. [20] propose COSKAD and force the normal instances into the same latent region driving the distances to a common center. MoCoDAD is also a skeleton-based approach that mitigates the choice of _k a priori_ to cover the multimodality of reality.

Diffusion models have marked a revolution in generative tasks such as image and video synthesis [21][22][23], but they have not been employed for VAD. Saadatnejad et al. [21] propose a two-step framework based on temporal cascaded diffusion (TCD). First, they denoise imperfect observation sequences and, then, improve the predictions of the (frozen) model on repaired frames. Tevet et al. [22] use a transformer encoder to learn arbitrary length motions [24][25] coherent with a particular conditioning signal \(c\). They experiment with constrained synthesis where \(c\) is a text prompt (i.e., text-to-motion) or a specific action class (i.e., action-to-motion) and unconstrained synthesis where \(c\) is not specified. Chen et al. [23] design a transformer-based VAE [25] to learn a representative latent space for human motion sequences. They apply a diffusion model in this latent space to generate vivid motion sequences while obeying specific conditions similar to [22]. Differently, MoCoDAD is a diffusion-based model that uses conditioning over a portion of the input (e.g., previous frames condition the generation of future ones).

Wyatt et al. [26] propose AnoDDPM, a diffusion model on images, which does not require the entire Markov chain (noise/denoise) to take place. They use decaying octaves of simplex noising functions to distinguish the corruption rate of low-frequency components from high-frequency ones. However, they add and remove noise without conditioning, identifying anomalies when noise removal diverges from the input. Differently, MoCoDAD is based on generating and comparing multimodal motions against the GT in terms of pertinence. Our proposed model is the first to exploit the multimodal generative and improved mode-coverage capabilities of diffusive techniques, via forecasting tasks, further to being first in adopting them for detecting video anomalies. Hence, to transfer DDPMs from video-basedAnomaly Detection to skeleton-based VAD we rely on a U-Net-shaped stack of STS-GCN [18] layers, which includes the spatio-temporal aspects of joints in sequences of human poses.

",1
88," Previous work relates to ours from two main perspectives: Video Anomaly Detection methods (see Sec. 2.1), and diffusion models for motion synthesis (see Sec. 2.2).

Pioneer works analyze the trajectory of the agents in the frames to discriminate those distant from normality . Within recent literature, two major trends can be identified: latent- and reconstruction-based methods. VAD techniques also vary based on the type of input data they use, such as videos or human skeletal pose motions. 

**Latent-based VAD** methods identify abnormality according to a score extracted from a learned latent space whereby normality is supposedly mapped into a constrained volume, and anomalies are those latents lying outside, with a largerscore (see [2][3][4] for an overview of latent-based AD). Sabokrou et al. [5] propose a two-staged cascade of deep neural networks. First, they employ a stack of autoencoders that detects points of interest (POIs) while excluding irrelevant patches (e.g., background). Second, they identify anomalies by densely extracting and modeling discriminative patches at POIs. Notice that this work constrains normality to belong to a single mode and anomalies outside, thus, addressing the openset'ness of anomalies, but it hampers the multimodal and diversity  aspect of normal motions. 

Notably, Nguyen et al. [6] propose an image-based technique exploring multimodal anomaly detection via multi-headed VAEs. However, considering a fixed number of modes for reality amends multimodality only partially, as it misses to unleash its openset'ness. 

**Reconstruction-based VAD** methods consider the original metric space of the input and leverage reconstruction as the proxy task to derive an anomaly score. These models are trained to encode and reconstruct the input from normal events, producing larger errors on anomalies not seen during training. [7][8] use sequences of frames and feed them to convolutional autoencoders. Gong et al. [9] ""memorize"" the most representative normal poses to discriminate new input samples. Liu et al. [10] tackle intensity and gradient loss, optical flow, and adversarial training. Luo et al. [11] use stacked RNNs with temporally-coherent sparse coding enforcing similar neighboring frames to be encoded with similar reconstruction coefficients. Barbalau et al. [12] builds upon [13] and integrates the reconstruction of the input frames, via multi-headed attention, into a multi-task learning framework. Besides [13][10], all works rely on a single reconstruction proxy task via non-variational architectures that learn discrete manifolds. However, normality and abnormality are multimodal and diverse, making it hard for these techniques to have an exact match (reconstruction) over the GT. Additionally, GANs used in [10] suffer from mode collapse [14] lacking to represent the multimodality of reality. Similarly to [6][12] can represent only a fixed number of modalities, which does represent the openset'ness of reality. 

**Skeleton-based VAD** methods exploit compact spatio-temporal skeletal representations of human motion instead of raw video frames. Morais et al. [16] use two GRU autoencoder branches to account for the global and local decomposition of the skeleton in a particular frame. Luo et al.  exploit stacked layers of ST-GCN [17] to accumulate joint information over the spatio-temporal dimensions of the frame and predict joints in the future. However, [17] uses a fixed adjacency matrix, depicting joint connections, for all ST-GCN layers, which hinders the exploration of intra-frame and intra-joint relationships, two factors that play a crucial role in improving the encoding of spatio-temporal features [18]. Markovitz et al. [19] utilize the encoder of an ST-GCN autoencoder to embed space-time skeletons into a latent vector. This vector is then fed to an end-to-end trainable deep-embedded clustering procedure which produces \(k\) clusters representing the multimodality of normalcy and anomalies. Flaborea et al. [20] propose COSKAD and force the normal instances into the same latent region driving the distances to a common center. 

Diffusion models have marked a revolution in generative tasks such as image and video synthesis [21][22][23], but they have not been employed for VAD. Saadatnejad et al. [21] propose a two-step framework based on temporal cascaded diffusion (TCD). First, they denoise imperfect observation sequences and, then, improve the predictions of the (frozen) model on repaired frames. Tevet et al. [22] use a transformer encoder to learn arbitrary length motions [24][25] coherent with a particular conditioning signal \(c\). They experiment with constrained synthesis where \(c\) is a text prompt (i.e., text-to-motion) or a specific action class (i.e., action-to-motion) and unconstrained synthesis where \(c\) is not specified. Chen et al. [23] design a transformer-based VAE [25] to learn a representative latent space for human motion sequences. They apply a diffusion model in this latent space to generate vivid motion sequences while obeying specific conditions similar to [22]. 

Wyatt et al. [26] propose AnoDDPM, a diffusion model on images, which does not require the entire Markov chain (noise/denoise) to take place. They use decaying octaves of simplex noising functions to distinguish the corruption rate of low-frequency components from high-frequency ones. 

",0
89," Due to the rapid development of convolutional neural networks (CNN), CNN-based methods [1][2][3][4] have become mainstream methods for SR tasks. SRCNN [1] pioneered the application of convolutional neural networks on the SR task, surpassing the performance of traditional methods. In EDSR [2], a very deep network was utilized and the batch normalization layers in the residual block were removed. SwinIR [5] first attempted to apply the Swin-Transformer [6] on the SR task, showing the potential of Transformer-based networks [7]. To enlarge the receptive field, the hybrid attention block (HAB) was proposed by HAT [8], which achieved state-of-the-art performance. However, these models have high requirements for memory and computational resources which are not easily attainable in real-world applications.

To realize lightweight SR for GPU servers, many approaches tried to reduce the number of parameters and FLOPs. CARN  attempted to apply group convolutions. IMDN [9] employed a progressive refinement module to improve the information extraction ability and reduced the number of layers and channels. For further improvement in the efficiency of feature extraction, in RFDN , \(1\times 1\) convolutions were utilized to replace the \(3\times 3\) convolutions on the split channel of the IMDB [9] module. RLFN [10] replaced the progressive refinement module with a simpler residual module, which further improved the running speed and achieved great performance. MemSR [11] optimized the network in terms of memory by removing the residual structure in the model and proposed a novel knowledge distillation method to improve the performance.

Due to the particularity of the mobile platforms, most models for GPU servers are not directly applicable to mobile devices whose applications require to be carried out in a timely fashion with restricted computational resources. To solve this problem, current methods mainly optimized the network from three aspects, _i.e._, neat network topology, computation reduction and operator substitution. ECBSR , which proposed an Edge-oriented Convolutional Block (ECB) to achieve better performance, used relatively neat topology for low inference latency on mobile devices and introduced reparameterization techniques to achieve computation reduction for SR task. Following , RepSR [12] optimized the performance and training efficiency of the reparameterization module in ECBSR to achieve further computation reduction. ABPN [13] first attempted to apply operator substitution and used the faster _repeat_ operator instead of nearest neighbor interpolation in global residual connections. Inspired by the reparameterization technique, we propose ET, which substitutes more types of operators to achieve low inference latency. Our proposed ETDS optimized the network from all three aspects, where ET optimized the network by achieving operator substitution and computation reduction while the dual stream network adopted a neat topology.

",1
90," Due to the rapid development of convolutional neural networks (CNN), CNN-based methods [1][2][3][4] have become mainstream methods for SR tasks. SRCNN [1] pioneered the application of convolutional neural networks on the SR task, surpassing the performance of traditional methods. In EDSR [2], a very deep network was utilized and the batch normalization layers in the residual block were removed. SwinIR [5] first attempted to apply the Swin-Transformer [6] on the SR task, showing the potential of Transformer-based networks [7]. To enlarge the receptive field, the hybrid attention block (HAB) was proposed by HAT [8], which achieved state-of-the-art performance. However, these models have high requirements for memory and computational resources which are not easily attainable in real-world applications.

To realize lightweight SR for GPU servers, many approaches tried to reduce the number of parameters and FLOPs. CARN  attempted to apply group convolutions. IMDN [9] employed a progressive refinement module to improve the information extraction ability and reduced the number of layers and channels. For further improvement in the efficiency of feature extraction, in RFDN , \(1\times 1\) convolutions were utilized to replace the \(3\times 3\) convolutions on the split channel of the IMDB [9] module. RLFN [10] replaced the progressive refinement module with a simpler residual module, which further improved the running speed and achieved great performance. MemSR [11] optimized the network in terms of memory by removing the residual structure in the model and proposed a novel knowledge distillation method to improve the performance.

Due to the particularity of the mobile platforms, most models for GPU servers are not directly applicable to mobile devices whose applications require to be carried out in a timely fashion with restricted computational resources. To solve this problem, current methods mainly optimized the network from three aspects, _i.e._, neat network topology, computation reduction and operator substitution. ECBSR , which proposed an Edge-oriented Convolutional Block (ECB) to achieve better performance, used relatively neat topology for low inference latency on mobile devices and introduced reparameterization techniques to achieve computation reduction for SR task. Following , RepSR [12] optimized the performance and training efficiency of the reparameterization module in ECBSR to achieve further computation reduction. ABPN [13] first attempted to apply operator substitution and used the faster _repeat_ operator instead of nearest neighbor interpolation in global residual connections. 

",0
91," **Scene-Text Understanding**. Most early STU works [1][2][3][4][5] have merely focused on Optical Character Recognition (OCR). We instead focus on scene-text understanding (STU) in the context of V&L tasks: VQA [6][7] and image captioning . The most common approach for these STU tasks is to fuse pre-extracted object detection features with off-the-shelf OCR signals as additional input [6][8][9][10][11][12][13]. These works often focus on specific challenges in downstream STU tasks, including dealing with noisy OCR signals, enabling the generation of rare words, or incorporating geometric information of OCR texts. In contrast, our work focuses on pre-training general-purpose STU models and shows the effectiveness of our objectives on multiple downstream STU tasks (SS3.1).

**V&L Pre-Training for STU**. One line of works incorporates OCR signals explicitly for pre-training [14][9][15]. TAP proposes an objective to learn the relative spatial position of two OCR texts. LOGOS [15] localizes a region that

The other line of works is OCR-free. Recently, extremely large image-text models have shown promising results on STU tasks, despite having no explicit STU objectives (_e.g_., GIT2 [16], Flamingo [17]). However, it would require an analysis of their private data and a prohibitive amount of resources to pinpoint what contributes to such strong results. Our study offers a complementary perspective to this OCR-free approach by pushing the limit of the OCR-heavy approach further than before and conducting more thorough experiments at a smaller scale.

",1
92," **Scene-Text Understanding**. Most early STU works [1][2][3][4][5] have merely focused on Optical Character Recognition (OCR). We instead focus on scene-text understanding (STU) in the context of V&L tasks: VQA [6][7] and image captioning . The most common approach for these STU tasks is to fuse pre-extracted object detection features with off-the-shelf OCR signals as additional input [6][8][9][10][11][12][13]. These works often focus on specific challenges in downstream STU tasks, including dealing with noisy OCR signals, enabling the generation of rare words, or incorporating geometric information of OCR texts. 

**V&L Pre-Training for STU**. One line of works incorporates OCR signals explicitly for pre-training [14][9][15]. TAP proposes an objective to learn the relative spatial position of two OCR texts. LOGOS [15] localizes a region that

The other line of works is OCR-free. Recently, extremely large image-text models have shown promising results on STU tasks, despite having no explicit STU objectives (_e.g_., GIT2 [16], Flamingo [17]). 

",0
93," To the best of our knowledge, all currently available systems for tracking machine learning models (such as DVC7, MLFlow (), WandB8, Neptune9, and the Hugging Face Hub ()) track a model checkpoint as a single large file (i.e., a blob of data). As in Git LFS, these systems track large files by using Git to track metadata about the file while keeping the contents in a storage system external to the repository. Consequently, these systems cannot be used for collaborative model development since they cannot take advantage of recent research on communication-efficient training and model merging.

Like AdapterHub (), Git-Theta enables the sharing of cheaply-communicable/storable updates that allow existing pre-trained models to perform a new tasks. Unlike Git-Theta, AdapterHub does not track continual changes to a given model and does not include functionality for merging different versions or histories of a model.

An earlier related system, ModelHub ([1]), aims to track the full lifecycle of a model, including its architecture, hyperparameters, training details, metrics, and more. ModelHub also includes a query language to find specific models based on the large amount of metadata it stores. Like Git-Theta, ModelHub includes some functionality for minimizing storage costs, but it still treats the model as a single large tensor of values and does not support merging. Philosophically, the two systems also differ significantly--ModelHub aims to be a complete system for tracking experiments, models, code, and metadata, whereas Git-Theta is a lightweight Git extension that supports existing workflows and is easily extendible.

Since Git-Theta focuses on tracking changes to a model made by distributed contributors, it is complementary to systems that implement similar functionality for datasets such as Dolt10, Pachyderm11, and XetHub ([2]).

",1
94," To the best of our knowledge, all currently available systems for tracking machine learning models (such as DVC7, MLFlow (), WandB8, Neptune9, and the Hugging Face Hub ()) track a model checkpoint as a single large file (i.e., a blob of data). As in Git LFS, these systems track large files by using Git to track metadata about the file while keeping the contents in a storage system external to the repository. Consequently, these systems cannot be used for collaborative model development since they cannot take advantage of recent research on communication-efficient training and model merging.



An earlier related system, ModelHub ([1]), aims to track the full lifecycle of a model, including its architecture, hyperparameters, training details, metrics, and more. ModelHub also includes a query language to find specific models based on the large amount of metadata it stores. Like Git-Theta, ModelHub includes some functionality for minimizing storage costs, but it still treats the model as a single large tensor of values and does not support merging. Philosophically, the two systems also differ significantly--ModelHub aims to be a complete system for tracking experiments, models, code, and metadata, whereas Git-Theta is a lightweight Git extension that supports existing workflows and is easily extendible.



",0
95," Recent years have witnessed increasing attention on multimodal machine translation (MMT) that translates a source sentence into the target language accompanied with an additional modality (). Given the additional modality and its relation to the source sentence, MMT can be roughly divided into image-guided translation (; ), video-guided translation (), speech translation ([2]; [1]), IT ([4]). Image-guided MMT aims to leverage visual context to aid textual machine translation ([3]; ). The significant difference between image-guided translation and image translation is that the latter embeds the source sentence in its visual modality in the image while the former has the image and the source sentence separated and the image is used to provide additional information for translating the source sentence.

In contrast to image-guided translation, IT has not yet been fully explored in the literature probably due the lack of publicly available datasets for IT. Both [4] and  propose end-to-end approaches to it. [4] uses a convolutional encoder to encode the image and Transformer decoder to generate target translation. The end-to-end IT model is able to locate characters in image, performs implicit tokenization on the source text, and then extracts latent semantic representations from them. This model can extract the latent token representations of image and text, and map into a shared space to implement the E2E IT. While they provide an initial definition of the IT task, they neither consider the modality gap nor verify the effect of the proposed models on real-world images.

For speech translation (ST), recent efforts have shifted towards end-to-end speech-to-text translation that directly translates a speech in the source language into a text in the target language ([6]; [7]). This is because end-to-end ST is of less error propagation and low latency compared with traditional cascaded ST (Inagumaet al., 2021; [1]). However, E2E ST suffers from the high cost of speech-to-text parallel data creation. Pre-training and multitask learning strategies have been explored to mitigate this data scarcity issue ([5]; [8]). In addition, similar to E2E IT, E2E ST is also confronted with the cross-modality issue, which can be mitigated by sharing the same semantic space for audio and text representations ([2]). Partially motivated by E2E ST, we propose an end-to-end framework for IT from the perspectives of pre-training with data of the MT task, sharing parameters across modalities, knowledge transfer via multitask learning, attempting to address the data scarcity and modality gap issues in IT.

",1
96," Recent years have witnessed increasing attention on multimodal machine translation (MMT) that translates a source sentence into the target language accompanied with an additional modality (). Given the additional modality and its relation to the source sentence, MMT can be roughly divided into image-guided translation (; ), video-guided translation (), speech translation ([2]; [1]), IT ([4]). Image-guided MMT aims to leverage visual context to aid textual machine translation ([3]; ). The significant difference between image-guided translation and image translation is that the latter embeds the source sentence in its visual modality in the image while the former has the image and the source sentence separated and the image is used to provide additional information for translating the source sentence.

In contrast to image-guided translation, IT has not yet been fully explored in the literature probably due the lack of publicly available datasets for IT. Both [4] and  propose end-to-end approaches to it. [4] uses a convolutional encoder to encode the image and Transformer decoder to generate target translation. The end-to-end IT model is able to locate characters in image, performs implicit tokenization on the source text, and then extracts latent semantic representations from them. This model can extract the latent token representations of image and text, and map into a shared space to implement the E2E IT. While they provide an initial definition of the IT task, they neither consider the modality gap nor verify the effect of the proposed models on real-world images.

For speech translation (ST), recent efforts have shifted towards end-to-end speech-to-text translation that directly translates a speech in the source language into a text in the target language ([6]; [7]). This is because end-to-end ST is of less error propagation and low latency compared with traditional cascaded ST (Inagumaet al., 2021; [1]). However, E2E ST suffers from the high cost of speech-to-text parallel data creation. Pre-training and multitask learning strategies have been explored to mitigate this data scarcity issue ([5]; [8]). In addition, similar to E2E IT, E2E ST is also confronted with the cross-modality issue, which can be mitigated by sharing the same semantic space for audio and text representations ([2]). 

",0
97," **Controllable Dialogue Generation** Currently, there have existed many studies on CDG ([6]; [1]; [2]). CTRL ([9]) used 55 kinds of attribute control codes to finetune an LM which is expensive and requires extensive annotated attribute labels. [8]; [4]; [5]; [10] addressed these limitations by employing an attribute discriminator to update the hidden activations or re-weight the next token distributions, resulting in a slow inference speed. Despite the progress, these models all focus on the single-attribute CDG where the attribute only contains coarse-grained discrete values, such as _happiness_ in emotion-controlled generation. It is also vital to explore multi-attribute CDG with multi-granularity attributes. Recently, some works ([7]; [3]) extend to multi-attribute controllable text generation by simply concatenating the prefixes trained for single attribute. However, they are only suitable for discrete attributes but not for fine-grained continuous attributes like personas ([2]). Besides, we find all these methods have a large performance drop from seen attribute values to unseen combinations. Therefore, in this paper, we are the first to explore the compositional generalization for multi-attribute CDG where a model could learn from seen attributes and generalize to out-of-distribution (OOD) combinations.

**Compositional Generalization in NLP** Compositional generalization has gradually attracted the interest of NLP researchers. The main application is in semantic parsing, involving grammar-based approaches ([14]), data augmentation strategies ([15]), disentangled representations ([13]), etc. Recently, a large-scale benchmark, STYLEPTB, is constructed to advance the development of compositional style transfer ([11]), and a template-based input representation is also performed on the data-to-text task ([12]). Overall, the application of compositional generalization in NLP tasks is not widespread and there is no related work on CDG at all.

**Prompt Learning** Prompt-based methods have achieved significant success in many NLP fields ([17]; [18]). [19] proposed the task-specific continuous prompts to finetune a NLG model. For controllable generation, [16]; [3]; [7] applied the prompt learning to represent each attribute value as an independent prefix. However, those methods are impractical for fine-grained attributes with a large value set. In contrast, we use the control codes to generate attribute-oriented prompts to guide the generation via a shared MLP layer.

",1
98," **Controllable Dialogue Generation** Currently, there have existed many studies on CDG ([6]; [1]; [2]). CTRL ([9]) used 55 kinds of attribute control codes to finetune an LM which is expensive and requires extensive annotated attribute labels. [8]; [4]; [5]; [10] addressed these limitations by employing an attribute discriminator to update the hidden activations or re-weight the next token distributions, resulting in a slow inference speed. Despite the progress, these models all focus on the single-attribute CDG where the attribute only contains coarse-grained discrete values, such as _happiness_ in emotion-controlled generation. It is also vital to explore multi-attribute CDG with multi-granularity attributes. Recently, some works ([7]; [3]) extend to multi-attribute controllable text generation by simply concatenating the prefixes trained for single attribute. However, they are only suitable for discrete attributes but not for fine-grained continuous attributes like personas ([2]). Besides, we find all these methods have a large performance drop from seen attribute values to unseen combinations. 

**Compositional Generalization in NLP** Compositional generalization has gradually attracted the interest of NLP researchers. The main application is in semantic parsing, involving grammar-based approaches ([14]), data augmentation strategies ([15]), disentangled representations ([13]), etc. Recently, a large-scale benchmark, STYLEPTB, is constructed to advance the development of compositional style transfer ([11]), and a template-based input representation is also performed on the data-to-text task ([12]). 

**Prompt Learning** Prompt-based methods have achieved significant success in many NLP fields ([17]; [18]). [19] proposed the task-specific continuous prompts to finetune a NLG model. For controllable generation, [16]; [3]; [7] applied the prompt learning to represent each attribute value as an independent prefix. However, those methods are impractical for fine-grained attributes with a large value set. 

",0
99," **Compressed vision task**. The main idea of introducing compressed video into current computer vision tasks is to utilizing the motion vector and residual on the compressed domain to avoid fully decode all frames from the video and save the storage space at the same time. Early work mainly base on MPEG-4 video codec [1][2][3][4]. CoViAR [1] proposed a back-tracking technique to trace motion vectors back to I-frame, which works on MPEG-4. MM-ViT [4] proposed a multi-modal transformer to process the I-frame, motion vector, residual and audio in the compressed video. Since the MPEG-4 codec is outdated, other works, e.g., MVCGC [5] and ATTP , is designed to work on other coedcs like H.264 and H.265 to ensure generalizability. Comparing with MPEG-4, H.264 and H.265 allow a more flexible yet complicated compression, which makes it more challenging to learn from compressed domain. MVCGC [5] proposed a self-supervised method to learn video representations by utilizing the mutual information between RGB video frames and motion vectors. ATTP  designed a lightweight deep neural network to process the compressed video and achieve real time action recognition on embedded AI devices. Similarly, our work

**Video captioning.** Video captioning aims to convert the content of videos into natural language descriptions, which requires the model to understand the objects in the video and the behavior of the objects. Some works focus on the design of the model structure. These methods usually extract features offline, and then models use these features to generate captions by designing different network architectures. HMN [6] proposed a hierarchical modular network that serves as a strong video encoder, which bridges videos and languages. ORG-TRL [7] proposes an object relational graph based encoder, which captures more detailed interaction features to enrich visual representation. SGN [8] designed a semantic grouping network to group video frames with discriminating word phrases of partially decoded caption. Some works explore additional information to help the model generate more accurate video captions. TextKG [9] propose a two-stream network capable of knowledge-assisted video description using knowledge graphs. Univl [10] learns powerful vision-and-language representations by pre-training the models on large-scale datasets, _e.g_., HowTo100M [11] and WebVid-2M [12]. Some other works focus more on end-to-end video captioning generation. SwinBERT [13] proposed an end-to-end transformer-based model, which takes video frame patches directly as inputs and then uses VidSwin to extract visual features. MV-GPT [14] designed an encoder-decoder model end-to-end to generate the video caption from video frames and transcribed speech directly. We propose an end-to-end video captioning model based on the compressed domain without decoding video frames and extracting features offline, which not only accelerates the generation of captions, but also performs favorably against the state-of-the-art methods.

",1
100," **Compressed vision task**. The main idea of introducing compressed video into current computer vision tasks is to utilizing the motion vector and residual on the compressed domain to avoid fully decode all frames from the video and save the storage space at the same time. Early work mainly base on MPEG-4 video codec [1][2][3][4]. CoViAR [1] proposed a back-tracking technique to trace motion vectors back to I-frame, which works on MPEG-4. MM-ViT [4] proposed a multi-modal transformer to process the I-frame, motion vector, residual and audio in the compressed video. Since the MPEG-4 codec is outdated, other works, e.g., MVCGC [5] and ATTP , is designed to work on other coedcs like H.264 and H.265 to ensure generalizability. Comparing with MPEG-4, H.264 and H.265 allow a more flexible yet complicated compression, which makes it more challenging to learn from compressed domain. MVCGC [5] proposed a self-supervised method to learn video representations by utilizing the mutual information between RGB video frames and motion vectors. ATTP  designed a lightweight deep neural network to process the compressed video and achieve real time action recognition on embedded AI devices. 

**Video captioning.** Video captioning aims to convert the content of videos into natural language descriptions, which requires the model to understand the objects in the video and the behavior of the objects. Some works focus on the design of the model structure. These methods usually extract features offline, and then models use these features to generate captions by designing different network architectures. HMN [6] proposed a hierarchical modular network that serves as a strong video encoder, which bridges videos and languages. ORG-TRL [7] proposes an object relational graph based encoder, which captures more detailed interaction features to enrich visual representation. SGN [8] designed a semantic grouping network to group video frames with discriminating word phrases of partially decoded caption. Some works explore additional information to help the model generate more accurate video captions. TextKG [9] propose a two-stream network capable of knowledge-assisted video description using knowledge graphs. Univl [10] learns powerful vision-and-language representations by pre-training the models on large-scale datasets, _e.g_., HowTo100M [11] and WebVid-2M [12]. Some other works focus more on end-to-end video captioning generation. SwinBERT [13] proposed an end-to-end transformer-based model, which takes video frame patches directly as inputs and then uses VidSwin to extract visual features. MV-GPT [14] designed an encoder-decoder model end-to-end to generate the video caption from video frames and transcribed speech directly. 

",0
101," **Supervised Point Cloud Completion.** Earlier efforts to address point cloud completion can be divided into surface reconstruction and template matching. Surface reconstruction methods [1][2] attempt to restore missing regions by fitting existing points to an implicit surface based on geometric cues, and then resample new points from the estimated surface. On the other hand, template matching techniques [3] retrieve a template shape from a database and deform it to fit the target shape. However, surface reconstruction-based methods are able to fill holes on the surface but are limited in handling severe geometric incompleteness, while template matching methods are computationally expensive and rely on the availability of a sufficient number of example shapes. Starting with the pioneering work PCN [4], deep learning-based methods [5][6][7][8] have gained significant attention in point cloud completion. However, the supervised training approach requires paired ground truth, which is difficult to obtain for real-world scans. As a result, these methods are often trained on synthetic datasets, which leads to impressive results on synthetic data but may not generalize well to real-world scans [9].

**Unpaired and Weakly-Supervised Completion.** To address the issue of data acquisition, Chen _et al._[10] proposed the first method, Pcl2Pcl, that can be trained without paired partial and complete point sets. This was achieved through a generative adversarial network , where the generator transforms a partial shape latent encoding into a representation indistinguishable from the latent variable obtained from real complete shapes by the discriminator. Following Pcl2Pcl, many methods [11][9][12][13] have been proposed to produce more accurate results. Nevertheless, complete shape repositories are still required, and combining unaligned real-world partial scans with complete shapes from other sources may result in poor outcomes due to alignment errors. Different from prior approaches, Gu _et al._ tackle the problem of point cloud completion by using unaligned real-world partial point clouds as their data source. The network is trained with multi-view geometric constraints as weak supervision cues. However, these methods require scans from multiple viewing angles, which are not always feasible to obtain.

**Self-Supervised Learning.** To mitigate the cost of dataset collection and annotation, self-supervised learning [14] have been proposed. For example, DINO [15] demonstrated improved classification performance using only self-supervised training, without any labels. Self-supervised learning has also gained popularity in point cloud studies. Building upon the work of He _et al._[16], Liu _et al._[17] proposed a self-supervised mask discrimination framework for pretraining transformers. For point cloud upsampling, SSPU-Net  leverages the consistency between input sparse and generated dense point clouds to train the network using only sparse clouds. Concurrently with our research, Hong _et al._[18] proposed a related point cloud completion scheme, but used the same data for training and testing to enable an adaptive closed-loop [19] optimization. In contrast, our approach uses distinct test samples.

",1
102," **Supervised Point Cloud Completion.** Earlier efforts to address point cloud completion can be divided into surface reconstruction and template matching. Surface reconstruction methods [1][2] attempt to restore missing regions by fitting existing points to an implicit surface based on geometric cues, and then resample new points from the estimated surface. On the other hand, template matching techniques [3] retrieve a template shape from a database and deform it to fit the target shape. However, surface reconstruction-based methods are able to fill holes on the surface but are limited in handling severe geometric incompleteness, while template matching methods are computationally expensive and rely on the availability of a sufficient number of example shapes. Starting with the pioneering work PCN [4], deep learning-based methods [5][6][7][8] have gained significant attention in point cloud completion. However, the supervised training approach requires paired ground truth, which is difficult to obtain for real-world scans. As a result, these methods are often trained on synthetic datasets, which leads to impressive results on synthetic data but may not generalize well to real-world scans [9].

**Unpaired and Weakly-Supervised Completion.** To address the issue of data acquisition, Chen _et al._[10] proposed the first method, Pcl2Pcl, that can be trained without paired partial and complete point sets. This was achieved through a generative adversarial network , where the generator transforms a partial shape latent encoding into a representation indistinguishable from the latent variable obtained from real complete shapes by the discriminator. Following Pcl2Pcl, many methods [11][9][12][13] have been proposed to produce more accurate results. Nevertheless, complete shape repositories are still required, and combining unaligned real-world partial scans with complete shapes from other sources may result in poor outcomes due to alignment errors. 

**Self-Supervised Learning.** To mitigate the cost of dataset collection and annotation, self-supervised learning [14] have been proposed. For example, DINO [15] demonstrated improved classification performance using only self-supervised training, without any labels. Self-supervised learning has also gained popularity in point cloud studies. Building upon the work of He _et al._[16], Liu _et al._[17] proposed a self-supervised mask discrimination framework for pretraining transformers. For point cloud upsampling, SSPU-Net  leverages the consistency between input sparse and generated dense point clouds to train the network using only sparse clouds. Concurrently with our research, Hong _et al._[18] proposed a related point cloud completion scheme, but used the same data for training and testing to enable an adaptive closed-loop [19] optimization. 

",0
103," Basis Learning.Early work showed that optical flow estimation due to camera motion can be constrained using a subspace formulation for flow [1]. Basis learning has been used as a regularization in low-level vision, unifying tasks such as depth, flow, and segmentation [2]. PCAFlow [3] builds a higher dimensional flow subspace from movies to represent flow as a weighted sum of flow bases. Recent work [4] learns the coefficients to combine eight pre-defined flow bases for homography estimation.

Motion as Input.Most of the work in motion segmentation focuses on the single-object case. While earlier work uses traditional methods to cluster pixels into similar motion groups [5][6][7], later methods train deep neural networks which take flow as input and predict segmentation as output [8][9]. Another work [10] uses the distinctiveness of motion in the case of foreground objects by proposing an adversarial setting to predict motion from context. Segmenting objects in camouflaged settings can be achieved by modeling background motion to remove its effect and highlight the moving foreground object [11]. Recent work uses consistency between two flow fields computed under different frame gaps for self-supervision [12].

The most relevant to our work is OCLR [13] which extends motion segmentation to multiple objects by relating motion extracted from multiple frames using a transformer in a layered representation. In this work, we show that better results can be achieved on real data even from a single image by modeling pixel-wise geometry.

Motion for Supervision.While using motion only as input works well where appearance fails, e.g. the camouflage datasets, RGB carries important information that might be missing in flow. DyStaB [14] trains a dynamic model by exploiting motion for temporal consistency and then uses it to bootstrap a static model which takes a single image as input. A single image network is used to predict a segmentation in [15] and then the motion of each segment is predicted with a two-frame motion network. While image warping loss is used in [15] for self-supervision, recent work [16][17] uses flow reconstruction loss by assuming the availability of flow at train time only. GWM [16] segments foreground objects by fitting an approximate motion model to each segment and then merging them using spectral clustering. The follow-up work [17] extends it to multiple objects by predicting probable motion patterns for each segment with a distribution. We also reconstruct flow for supervision but differently, we account for 3D to remove the ambiguity in reconstructing motion from a single image.

The most relevant to our work is the previous work that uses flow as a source of supervision for depth [18] or segmentation [16][19]. In this work, we model both depth and segmentation with supervision from motion.

Multi-Object Scene Decomposition.Our work is also related to scene decomposition approaches which are mostly evaluated on synthetic datasets. The earlier image-based decomposition approaches such as MONet [20] and IODINE [21] use a sequential VAE structure where the decomposition at a step can affect the remaining parts to be explained in the next step. GENESIS [22] follows an object-centric approach by accounting for component interactions, which is extended to more realistic scenarios with an autoregressive prior in the follow-up work [23]. Slot Attention [24] uses an iterative attention mechanism to decompose the image into a set of slot representations. A hierarchical VAE is used in [25] to extract symmetric and disentangled representations.

There are also video-based approaches to multi-object scene decomposition. SCALOR [26] focuses on scaling generative approaches to crowded scenes in terms of object density. SIMONe [27] learns a factorized latent space to separate object semantics that is constant in the sequence from the background which changes at each frame according to camera motion. SAVi [28] extends Slot Attention [24]to videos and SAVi++ [29] extends it to real-world driving scenarios with sparse depth supervision.

Self-Supervised Monocular Depth Estimation.Zhou et al. [30] train a pose network to estimate the pose between the frames in a sequence and jointly train it with the depth network. Godard et al. [31] improves the results with a better loss function and other design choices. Guizilini et al. [32] learn detail-preserving representations using 3D packing and unpacking blocks. Given instance segmentation masks, a line of work [33][34] models the motion of objects in the scene in addition to the camera motion to go beyond the static-scene assumption. While the object masks are supervised using ground truth masks in , the masks are learned without supervision as an auxiliary output in [35] for better depth estimation. While they require multiple frames during inference, our approach can estimate masks from a single image. Additionally, our method does not use camera intrinsics.

",1
104," Basis Learning.Early work showed that optical flow estimation due to camera motion can be constrained using a subspace formulation for flow [1]. Basis learning has been used as a regularization in low-level vision, unifying tasks such as depth, flow, and segmentation [2]. PCAFlow [3] builds a higher dimensional flow subspace from movies to represent flow as a weighted sum of flow bases. Recent work [4] learns the coefficients to combine eight pre-defined flow bases for homography estimation.

Motion as Input.Most of the work in motion segmentation focuses on the single-object case. While earlier work uses traditional methods to cluster pixels into similar motion groups [5][6][7], later methods train deep neural networks which take flow as input and predict segmentation as output [8][9]. Another work [10] uses the distinctiveness of motion in the case of foreground objects by proposing an adversarial setting to predict motion from context. Segmenting objects in camouflaged settings can be achieved by modeling background motion to remove its effect and highlight the moving foreground object [11]. Recent work uses consistency between two flow fields computed under different frame gaps for self-supervision [12].

The most relevant to our work is OCLR [13] which extends motion segmentation to multiple objects by relating motion extracted from multiple frames using a transformer in a layered representation. 

Motion for Supervision.While using motion only as input works well where appearance fails, e.g. the camouflage datasets, RGB carries important information that might be missing in flow. DyStaB [14] trains a dynamic model by exploiting motion for temporal consistency and then uses it to bootstrap a static model which takes a single image as input. A single image network is used to predict a segmentation in [15] and then the motion of each segment is predicted with a two-frame motion network. While image warping loss is used in [15] for self-supervision, recent work [16][17] uses flow reconstruction loss by assuming the availability of flow at train time only. GWM [16] segments foreground objects by fitting an approximate motion model to each segment and then merging them using spectral clustering. The follow-up work [17] extends it to multiple objects by predicting probable motion patterns for each segment with a distribution. 



Multi-Object Scene Decomposition.Our work is also related to scene decomposition approaches which are mostly evaluated on synthetic datasets. The earlier image-based decomposition approaches such as MONet [20] and IODINE [21] use a sequential VAE structure where the decomposition at a step can affect the remaining parts to be explained in the next step. GENESIS [22] follows an object-centric approach by accounting for component interactions, which is extended to more realistic scenarios with an autoregressive prior in the follow-up work [23]. Slot Attention [24] uses an iterative attention mechanism to decompose the image into a set of slot representations. A hierarchical VAE is used in [25] to extract symmetric and disentangled representations.

There are also video-based approaches to multi-object scene decomposition. SCALOR [26] focuses on scaling generative approaches to crowded scenes in terms of object density. SIMONe [27] learns a factorized latent space to separate object semantics that is constant in the sequence from the background which changes at each frame according to camera motion. SAVi [28] extends Slot Attention [24]to videos and SAVi++ [29] extends it to real-world driving scenarios with sparse depth supervision.

Self-Supervised Monocular Depth Estimation.Zhou et al. [30] train a pose network to estimate the pose between the frames in a sequence and jointly train it with the depth network. Godard et al. [31] improves the results with a better loss function and other design choices. Guizilini et al. [32] learn detail-preserving representations using 3D packing and unpacking blocks. Given instance segmentation masks, a line of work [33][34] models the motion of objects in the scene in addition to the camera motion to go beyond the static-scene assumption. While the object masks are supervised using ground truth masks in , the masks are learned without supervision as an auxiliary output in [35] for better depth estimation. 

",0
105," **Deep Learning on 3D Point Sets.** Deep learning on point sets/clouds has been widely investigated in several problems, including shape classification, object part segmentation, scene semantic segmentation, reconstruction and object detection [1][2][3][4][5][6][7][8][9][10][11][12]. Most recent works aim at directly manipulating 3D points without transforming coordinates into regular voxel grids. Since a point cloud is essentially a set of unordered points and invariant to permutations of its points, deep learning on point clouds mainly focuses on designing effective operations that do not rely on point orders. Because point cloud methods do not involve sequence modeling, directly applying them to 3D point lists, _e.g._, proteins, may lead to inferior accuracy.

**Deep Learning on Proteins.** Proteins exhibit multi-level structures. Deep-learning-based methods for protein representation learning mainly focus on the 1D primary and the 3D tertiary structures understanding. The primary structure refers to the sequence of amino acids in the polypeptide chain. The tertiary structure refers to the three-dimensional structure created by a single protein molecule (a single polypeptide chain). For the primary structure, because acids in polypeptide chains can be seen as words in sentences, approaches for natural language processing can be used for sequence-based protein representation learning [13][14][15][16][17][18][16]. For the tertiary structure, the 3D geometric information of amino acids or atoms is used to enhance protein representation [19][20][21][22][23][24][25][26][27]. Different from these methods, we propose a Transformer-style method to model primary and tertiary structures for proteins. Moreover, we employ different approaches to capture the 1D and 3D structures.

**Transformer.** Impressive progress has been made on natural language processing due to the success of Transformernetworks [28][29][30][31]. In computer vision, the community has used self-attention or Transformer to model images in a non-local manner [32][33][34]. In particular, Zhao _et al_. proposed a Point Transformer [35] to model point clouds. Fan _et al_. proposed a P4Transformer [36] for point cloud video understanding. Lai _et al_. proposed a Stratified Transformer [3] for point cloud segmentation. Feng _et al_. proposed a Structure Embedding Transformer (SEFormer) [37] for 3D object detection. Wang _et al_. proposed a Relation-Enhanced Transformer [38] for text-based point cloud localization. Inspired by these methods, we propose a Transformer-style PointListNet for 3D point list modeling. Different from these methods, we replace learning-based self-attention with rule-based distance-attention, thus more efficient to achieve the correlation among microparticles. Moreover, we integrate relative structure modeling into Transformer and employ regular and irregular methods to capture the sequence and geometry structures, respectively.

",1
106," **Deep Learning on 3D Point Sets.** Deep learning on point sets/clouds has been widely investigated in several problems, including shape classification, object part segmentation, scene semantic segmentation, reconstruction and object detection [1][2][3][4][5][6][7][8][9][10][11][12]. Most recent works aim at directly manipulating 3D points without transforming coordinates into regular voxel grids. Since a point cloud is essentially a set of unordered points and invariant to permutations of its points, deep learning on point clouds mainly focuses on designing effective operations that do not rely on point orders. Because point cloud methods do not involve sequence modeling, directly applying them to 3D point lists, _e.g._, proteins, may lead to inferior accuracy.

**Deep Learning on Proteins.** Proteins exhibit multi-level structures. Deep-learning-based methods for protein representation learning mainly focus on the 1D primary and the 3D tertiary structures understanding. The primary structure refers to the sequence of amino acids in the polypeptide chain. The tertiary structure refers to the three-dimensional structure created by a single protein molecule (a single polypeptide chain). For the primary structure, because acids in polypeptide chains can be seen as words in sentences, approaches for natural language processing can be used for sequence-based protein representation learning [13][14][15][16][17][18][16]. For the tertiary structure, the 3D geometric information of amino acids or atoms is used to enhance protein representation [19][20][21][22][23][24][25][26][27]. 

**Transformer.** Impressive progress has been made on natural language processing due to the success of Transformernetworks [28][29][30][31]. In computer vision, the community has used self-attention or Transformer to model images in a non-local manner [32][33][34]. In particular, Zhao _et al_. proposed a Point Transformer [35] to model point clouds. Fan _et al_. proposed a P4Transformer [36] for point cloud video understanding. Lai _et al_. proposed a Stratified Transformer [3] for point cloud segmentation. Feng _et al_. proposed a Structure Embedding Transformer (SEFormer) [37] for 3D object detection. Wang _et al_. proposed a Relation-Enhanced Transformer [38] for text-based point cloud localization. Inspired by these methods, we propose a Transformer-style PointListNet for 3D point list modeling. 

",0
107," Domain generalization aims to learn more generalized knowledge from existing multiple source domains and finally test on the unknown target domain. Over the years, great efforts have been made in many directions, such as Invariant Representation ([13]; [3]; [5]; [11]), Causal ([1]; ; [14]), and Optimization ([2]; [10]; [8]; ). To generalize well on the unknown target domains, previous works introduce a domain generation strategy, enhancing the performance of DNNs by generating new domains. [7] perturbs the input samples along the direction of the most significant domain change while maintaining semantics.  trains a domain transformation model to transform images to unseen domains by fooling a domain classifier. [6]; [4] simply use a style transfer like AdaIN () to argument data in style aspects to optimize the model.  train a data generator to generate new domains using optimal transport to measure the distribution divergence. Zhou et al. (2021; 20) achieves style augmentation in the feature level by mixing the CNN feature map's mean and std between instances of different domains. [9] focuses on addressing the uncertain nature of domain shifts by modeling feature statistics as uncertain distributions, which is also achieved through the use of AdaIN, where non-semantic factors are replaced with randomly chosen values from the modeled distributions. [12] address the problem of domain shift by developing two simple and efficient normalization methods that can reduce the non-semantic domain shift between different distributions, while  jointly learns semantic and variation encoders to disentangle the semantic and non-semantic factors. Our approach explores the non-semantic factor to create augmented samples, which to some extent, is similar to approaches of data augmentation.

Distributionally robust optimization is a promising approach to tackle distribution discrepancy by exploring unknown domains in a fixed uncertainty set ([17]). DRO has developed plenty of approaches with different methods to measuring distribution discrepancy, such as Wasserstein distance ([16]; [22]), \(f\)-divergence (; [20]; ; [19]) and maximum mean discrepancy (MMD) (). Unfortunately, employing DRO to DG has shown limited performance improvement in practice (). ([18]; [21]; ) have pointed out that in order to capture the unknown target domain, the uncertainty set is often overwhelmingly large, leading the learned model to make decisions with fairly low confidence in DRO. [15];  focuses on the low confidence problem, and use a Wasserstein distance is employed to determine the uncertainty set.  uses data geometry to construct more reasonable and effective uncertainty sets, while  constructs the uncertainty using the data topology. Our approach MODE tackles the low confidence problem by performing distribution exploration in a specific uncertainty subset (non-semantic factor) and uses Wasserstein distance ([16]; [22]) to measure the distribution discrepancy in DG.

",1
108," Domain generalization aims to learn more generalized knowledge from existing multiple source domains and finally test on the unknown target domain. Over the years, great efforts have been made in many directions, such as Invariant Representation ([13]; [3]; [5]; [11]), Causal ([1]; ; [14]), and Optimization ([2]; [10]; [8]; ). To generalize well on the unknown target domains, previous works introduce a domain generation strategy, enhancing the performance of DNNs by generating new domains. [7] perturbs the input samples along the direction of the most significant domain change while maintaining semantics.  trains a domain transformation model to transform images to unseen domains by fooling a domain classifier. [6]; [4] simply use a style transfer like AdaIN () to argument data in style aspects to optimize the model.  train a data generator to generate new domains using optimal transport to measure the distribution divergence. Zhou et al. (2021; 20) achieves style augmentation in the feature level by mixing the CNN feature map's mean and std between instances of different domains. [9] focuses on addressing the uncertain nature of domain shifts by modeling feature statistics as uncertain distributions, which is also achieved through the use of AdaIN, where non-semantic factors are replaced with randomly chosen values from the modeled distributions. [12] address the problem of domain shift by developing two simple and efficient normalization methods that can reduce the non-semantic domain shift between different distributions, while  jointly learns semantic and variation encoders to disentangle the semantic and non-semantic factors. 

Distributionally robust optimization is a promising approach to tackle distribution discrepancy by exploring unknown domains in a fixed uncertainty set ([17]). DRO has developed plenty of approaches with different methods to measuring distribution discrepancy, such as Wasserstein distance ([16]; [22]), \(f\)-divergence (; [20]; ; [19]) and maximum mean discrepancy (MMD) (). Unfortunately, employing DRO to DG has shown limited performance improvement in practice (). ([18]; [21]; ) have pointed out that in order to capture the unknown target domain, the uncertainty set is often overwhelmingly large, leading the learned model to make decisions with fairly low confidence in DRO. [15];  focuses on the low confidence problem, and use a Wasserstein distance is employed to determine the uncertainty set.  uses data geometry to construct more reasonable and effective uncertainty sets, while  constructs the uncertainty using the data topology. 

",0
109," **Test time adaptation** (TTA) [1][2][3][4] is proposed to learn the test distribution by leveraging unlabeled test images, which provide hints about distribution information. Test-time training [1] employs a manually designed self-supervised learning task to learn the test distribution, which requires altering the training stage and finetuning all the layers. To mitigate this issue, Tent [5] is proposed by only finetuning the batch normalization layers with an unsupervised entropy minimization loss. Following works try different unsupervised losses to help test time adaptation, such as consistency loss [2], contrastive loss [6] or log-likelihood ratio loss [7]. However, when applied to the test data with a large domain gap, these methods commonly fail due to the inaccurate estimation of statistics and produce small gradients. In contrast, our method can alleviate these issues and also succeed in the few-data scenarios.

**Domain generalization** (DG) has attracted significant attention recently for its ability to generalize to unseen domains by only learning from source domains [8][9]. To achieve the generalization ability, current methods primarily aim to learn invariant features across all domains [10][11][12][13], augment data [14][15][16][17] to learn diverse features or regularize network with training schemes or losses [18][19][20][21]. While these methods only consider the training stage, several methods alter model behaviors according to the test samples for better adaptation to the unseen domains. Instance Normalization [22] and AdaBN  are simple but effective modules that utilize test statistics to perform normalization. Du _et al._[23] generates accurate statistics for each test sample with a trained statistics prediction network. In addition to normalization, Pandey _et al._[24] train a generative network to generate the nearest neighbor for each test sample in the source latent space. ARM [25] applies a meta-learning training scheme to extract batch-specific features during training and test for adaptation. Despite their efforts to adapt the model at the test stage, they require modifying the training stage and cannot be applied to an already trained model. In contrast, we propose our DomainAdaptor, which can be employed in any trained model, making it more practical in the real world.

**Temperature scaling** has been studied in different fields. In knowledge distillation [26][27][28][29], it is used to soften the probability distribution over classes, providing additional class relationship information for the student model. In confidence calibration [30][31][32][33], the temperature is finetuned as a parameter to ensure the predicted class confidence accurately reflects the likelihood of its ground truth correctness. In self-supervised learning [34][35], the temperature is used in contrastive loss to penalize hard negative samples. Differently, in this work, we employ temperature to encourage effective learning by fully exploiting unlabeled test data.

",1
110," **Test time adaptation** (TTA) [1][2][3][4] is proposed to learn the test distribution by leveraging unlabeled test images, which provide hints about distribution information. Test-time training [1] employs a manually designed self-supervised learning task to learn the test distribution, which requires altering the training stage and finetuning all the layers. To mitigate this issue, Tent [5] is proposed by only finetuning the batch normalization layers with an unsupervised entropy minimization loss. Following works try different unsupervised losses to help test time adaptation, such as consistency loss [2], contrastive loss [6] or log-likelihood ratio loss [7]. However, when applied to the test data with a large domain gap, these methods commonly fail due to the inaccurate estimation of statistics and produce small gradients. 

**Domain generalization** (DG) has attracted significant attention recently for its ability to generalize to unseen domains by only learning from source domains [8][9]. To achieve the generalization ability, current methods primarily aim to learn invariant features across all domains [10][11][12][13], augment data [14][15][16][17] to learn diverse features or regularize network with training schemes or losses [18][19][20][21]. While these methods only consider the training stage, several methods alter model behaviors according to the test samples for better adaptation to the unseen domains. Instance Normalization [22] and AdaBN  are simple but effective modules that utilize test statistics to perform normalization. Du _et al._[23] generates accurate statistics for each test sample with a trained statistics prediction network. In addition to normalization, Pandey _et al._[24] train a generative network to generate the nearest neighbor for each test sample in the source latent space. ARM [25] applies a meta-learning training scheme to extract batch-specific features during training and test for adaptation. Despite their efforts to adapt the model at the test stage, they require modifying the training stage and cannot be applied to an already trained model. 

**Temperature scaling** has been studied in different fields. In knowledge distillation [26][27][28][29], it is used to soften the probability distribution over classes, providing additional class relationship information for the student model. In confidence calibration [30][31][32][33], the temperature is finetuned as a parameter to ensure the predicted class confidence accurately reflects the likelihood of its ground truth correctness. In self-supervised learning [34][35], the temperature is used in contrastive loss to penalize hard negative samples. 

",0
111," Retrieval in Seq2Seq TasksIn semantic parsing, many previous studies ([2]) have propose to employ paraphrase scores to retrieve or rerank MRs, which all follow the order of generating first and then scoring. [5] first generate a set of candidate MRs and choose the realization that best paraphrases the input. [7] propose a set of reranking scorer for neural semantic parsers. [3] combine a retrieval model and a meta-learner to employ the similar datapoints from the training data. [1] construct parallel sentence pairs through retrieval, and conduct unsupervised machine translation models. [8]; [4]; [6] enhance the representations of instances or the robustness of decoder by retrieval. Different from the common generate-then-score framework, the order of our RaAS framework is the reverse of them. We are the first to use retrieval results to obtain supervision for zero-shot semantic parsing.

Low Resource Semantic ParsingMany low resource semantic parsing methods have been proposed to reduce the demand for annotations([10]; [16]; [14]). Many weakly supervised learning are proposed ([18]; [9]; [17]), such as denotation-based learning ([13]; ), iterative searching ([19]). Semi-supervised semantic parsing is also proposed ([23]; ; [12]). One other strategy is to augment data. [21] construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase.  produce pseudo-labeled data. [15] create new ""recombinant"" training examples with SCFG. [11]; [22]; [20] explore the training / decoding methods of PLMs for low-resource semantic parsing. Different from previous work, our framework focuses on obtaining and facilitating supervision signals rather than model design or data synthesization.

",1
112," Retrieval in Seq2Seq TasksIn semantic parsing, many previous studies ([2]) have propose to employ paraphrase scores to retrieve or rerank MRs, which all follow the order of generating first and then scoring. [5] first generate a set of candidate MRs and choose the realization that best paraphrases the input. [7] propose a set of reranking scorer for neural semantic parsers. [3] combine a retrieval model and a meta-learner to employ the similar datapoints from the training data. [1] construct parallel sentence pairs through retrieval, and conduct unsupervised machine translation models. [8]; [4]; [6] enhance the representations of instances or the robustness of decoder by retrieval. 

Low Resource Semantic ParsingMany low resource semantic parsing methods have been proposed to reduce the demand for annotations([10]; [16]; [14]). Many weakly supervised learning are proposed ([18]; [9]; [17]), such as denotation-based learning ([13]; ), iterative searching ([19]). Semi-supervised semantic parsing is also proposed ([23]; ; [12]). One other strategy is to augment data. [21] construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase.  produce pseudo-labeled data. [15] create new ""recombinant"" training examples with SCFG. [11]; [22]; [20] explore the training / decoding methods of PLMs for low-resource semantic parsing. 

",0
113," Audio-Visual Cross-modal LearningCross-modal representation learning is a long-standing research topic, ranging from speech enhancement [1], speech source separation [2] to synchronization [3][4] and other speech disentanglement [5][6][7]. Among them, EVP [6] used cross-modal supervision to disentangle speech content and emotion from the audio signal with landmark as the intermediate representation. Recently, CMC  discussed how multi-view ""modality"" can be jointly unified to boost intrinsic representation through contrastive learning, rather than predictive (or reconstruction) learning, it also demonstratedthat the more views, the better. Concurrently, MMV [8] introduced different modality embedding graphs for effective cross-modal representations, again through contrastive learning. More recently, HCMoCo [9] extended similar ideas with a hierarchical strategy to learn different levels of representations for human-centric perception tasks. Although impressive results were reported, it is still unclear how it performs on face analysis tasks, not mention to on synthesis tasks. Our work shares a similar spirit but a different purpose, where we first pre-train a non-identity visual representation which then helps learn a lip and non-lip space, the latter further serves as the upper-bound learning target for subsequent audio-to-visual diffusion prior.

Face Reenactment & Talking Head GenerationFace Reenactment is designed to transfer part or full facial motion from a driving source to the target video with good ID-preserved appearance and background. It can be further divided into two categories depending on whether the driving source is from video [10][11][12][13][14][15][16][17][18] or audio [19][6][20][21][22][23]. Among them, audio-driven face reenactment generally aims to edit the mouth regions of the target video in order to match the input audio while leaving other facial attributes mostly unchanged, _i.e._, pose. EVP [6] tries to infer the non-rigid facial expression in addition to lip motion from the audio input. A closely related line of work is the audio-driven talking-head generations [24][25][26][27] where only one target reference face is given, hence, other face attributes including pose, expression, blink and gaze have to be either explicitly given [28][29] or partly inferred through statistical methods [25][30][26][27]. Specifically, Lu _et al_.  employed an auto-regressive model while Min _et al_. [30] leveraged normalized flow prior to predicting a natural-looking pose sequence from the input audio, both showed encouraging results. Compared with them, we aim to infer more diverse facial motions including pose, expression, and even blink and gaze in a holistic manner through an audio-to-visual diffusion prior model.

Diffusion Generative ModelsThe diffusion model [31][32][33], which is a likelihood-based model consisting of cascading denoising autoencoders, has recently shown great success in numerous generative tasks with different modalities including image [34][35][36][37], audio [38], video [39], and motion [40]. To name a few, DDPM [32] explored the diffusion model for unconditional image generation. GLIDE [35] introduced text-conditional diffusion model and showed that classifier free guidance has better performance than CLIP [41] guidance. DALLE-2 [36] modified GLIDE to generate semantically consistent images conditioned on a CLIP image embedding, and proposed a diffusion prior that produces the image embedding given a text caption. MDM [40] utilized a classifier-free diffusion-based generative model for text-to-motion and action-to-motion tasks, allowing motion completion and editing as well.

",1
114," Audio-Visual Cross-modal LearningCross-modal representation learning is a long-standing research topic, ranging from speech enhancement [1], speech source separation [2] to synchronization [3][4] and other speech disentanglement [5][6][7]. Among them, EVP [6] used cross-modal supervision to disentangle speech content and emotion from the audio signal with landmark as the intermediate representation. Recently, CMC  discussed how multi-view ""modality"" can be jointly unified to boost intrinsic representation through contrastive learning, rather than predictive (or reconstruction) learning, it also demonstratedthat the more views, the better. Concurrently, MMV [8] introduced different modality embedding graphs for effective cross-modal representations, again through contrastive learning. More recently, HCMoCo [9] extended similar ideas with a hierarchical strategy to learn different levels of representations for human-centric perception tasks. Although impressive results were reported, it is still unclear how it performs on face analysis tasks, not mention to on synthesis tasks. 

Face Reenactment & Talking Head GenerationFace Reenactment is designed to transfer part or full facial motion from a driving source to the target video with good ID-preserved appearance and background. It can be further divided into two categories depending on whether the driving source is from video [10][11][12][13][14][15][16][17][18] or audio [19][6][20][21][22][23]. Among them, audio-driven face reenactment generally aims to edit the mouth regions of the target video in order to match the input audio while leaving other facial attributes mostly unchanged, _i.e._, pose. EVP [6] tries to infer the non-rigid facial expression in addition to lip motion from the audio input. A closely related line of work is the audio-driven talking-head generations [24][25][26][27] where only one target reference face is given, hence, other face attributes including pose, expression, blink and gaze have to be either explicitly given [28][29] or partly inferred through statistical methods [25][30][26][27]. Specifically, Lu _et al_.  employed an auto-regressive model while Min _et al_. [30] leveraged normalized flow prior to predicting a natural-looking pose sequence from the input audio, both showed encouraging results. 

Diffusion Generative ModelsThe diffusion model [31][32][33], which is a likelihood-based model consisting of cascading denoising autoencoders, has recently shown great success in numerous generative tasks with different modalities including image [34][35][36][37], audio [38], video [39], and motion [40]. To name a few, DDPM [32] explored the diffusion model for unconditional image generation. GLIDE [35] introduced text-conditional diffusion model and showed that classifier free guidance has better performance than CLIP [41] guidance. DALLE-2 [36] modified GLIDE to generate semantically consistent images conditioned on a CLIP image embedding, and proposed a diffusion prior that produces the image embedding given a text caption. MDM [40] utilized a classifier-free diffusion-based generative model for text-to-motion and action-to-motion tasks, allowing motion completion and editing as well.

",0
115," **Vision-language models for grounding.** Contrastive language image pre-training [1] (CLIP) led to a range of follow up work performing open-vocabulary detection [2][3][4][5][6] or segmentation [7][8][9]. While these methods leverage dense human annotations for training, an alternate line of works [10][11][12][13] attempt to learn alignment between regions of images and language with only image level noisy captions for supervision. Their weak supervision allows better scalability (to more data) leading to learning more generic and transferable representations. In fact, multiple such works [10][11][14][8] perform zero-shot semantic segmentation. However, unlike [10][11] geared to segment a fixed count of foreground objects, our proposed CLIPpy can better segment arbitrary object counts and background classes. In contrast to  using generic image level features, CLIPpy explicitly learns local features during training. Moreover, CLIPpy requires no dense human annotations or task-specific fine-tuning in contrast to [14][8]. We also highlight how [10][11][14] perform grouping independent of language at inference - however CLIPpy can group conditioned on language, capturing variable object boundaries for different language prompts.

Multiple **contemporary works** also explore similar directions as CLIPpy, leveraging pre-trained vision-language models for various grouping tasks under weak supervision (no pixel level annotation) [15][16][17][18][19][20]. Combining self-supervised methods that emerge grouping [21] with CLIP models [1] for cross-modal alignment is explored in [15] gaining notable improvements at object boundaries. A clustering mechanism containing learnable centres similar to [10] is combined with reconstruction and super-pixel alignment losses to achieve grouping in [16]. Learning decoder networks over a frozen CLIP backbone [1] with text to image patch similarity losses are explored in [17][18] resulting in similar grouping behaviour. In contrast to these methods utilizing contrastive vision language training to emerge grouping, recent works [19][20] also showcase how text-to-image generative models (particularly Stable Diffusion [22]) can be leveraged to perform visual grouping.

**Zero-shot semantic segmentation.** A form of top-down grouping, this relatively new task [23][24][25][26][27][28][29][30] attempts to segment unseen classes, usually after a supervised training phase often involving dense annotation based supervision. Following two early representative works [24][25], most later approaches [28][31][32][33][30][34] formulate the task as a pixel-level zero-shot classification problem with a closed set vocabulary. While CLIPpy follows a similar pixel based formulation, in contrast, our method requires no dense human annotations for supervision, no task specific fine-tuning, and is open-vocabulary. Recent work [14][4] also explores region-level classification leveraging pre-trained CLIP models [1], but unlike CLIPpy perform grouping independent of language during inference.

**Unsupervised segmentation**. Analogous to bottom-up grouping, these works perform class-agnostic segmentation within the visual modality with no explicit language alignment [21][35][36][37][38]. This topic has a long, rich history in human visual perception [39] and computer vision [40], and has been explored as means of generalizing to new visual domains [41]. It is this goal that most closely inspires our work. Early efforts group pixels based on known spatially-local affinities [42][43][44], with subsequent methods leading to region proposal networks for object detection  and advances in semantic segmentation [45]. Recent methods employ self-supervision to learn perceptual grouping [46][35] or object-centric groupings [47][48][49][50][51]. Our proposed CLIPpy demonstrates competitive performance, but additionally aligns groups to the language modality explicitly.

**Learning robust visual representations**. For a long time, ImageNet [52] accuracy was believed to provide a reasonable proxy for quality of learned visual representations [53][54]. However, recent work highlights notable deficiencies in such learned representations [55][56][57] including sensitivity to low level textures, failure for domain shifts, and reliance on spurious correlations. These failures inspired a large literature to mitigate learning spurious correlations [58][59][60] by focusing on new optimization techniques. Progress on this issue may address parallel issues in fairness [61]. Resulting methods have largely focused on synthetic data, re-balancing data, and shaping learned embeddings [59]. Nonetheless, theoretical results suggest pessimistic bounds unless additional structure informs the problem (see refs. in [58]). Therein, the structured output predictions of proposed CLIPpy provide another promising solution.

",1
116," **Vision-language models for grounding.** Contrastive language image pre-training [1] (CLIP) led to a range of follow up work performing open-vocabulary detection [2][3][4][5][6] or segmentation [7][8][9]. While these methods leverage dense human annotations for training, an alternate line of works [10][11][12][13] attempt to learn alignment between regions of images and language with only image level noisy captions for supervision. Their weak supervision allows better scalability (to more data) leading to learning more generic and transferable representations. In fact, multiple such works [10][11][14][8] perform zero-shot semantic segmentation. However, unlike [10][11] geared to segment a fixed count of foreground objects, our proposed CLIPpy can better segment arbitrary object counts and background classes. 

Multiple **contemporary works** also explore similar directions as CLIPpy, leveraging pre-trained vision-language models for various grouping tasks under weak supervision (no pixel level annotation) [15][16][17][18][19][20]. Combining self-supervised methods that emerge grouping [21] with CLIP models [1] for cross-modal alignment is explored in [15] gaining notable improvements at object boundaries. A clustering mechanism containing learnable centres similar to [10] is combined with reconstruction and super-pixel alignment losses to achieve grouping in [16]. Learning decoder networks over a frozen CLIP backbone [1] with text to image patch similarity losses are explored in [17][18] resulting in similar grouping behaviour. **Zero-shot semantic segmentation.** A form of top-down grouping, this relatively new task [23][24][25][26][27][28][29][30] attempts to segment unseen classes, usually after a supervised training phase often involving dense annotation based supervision. Following two early representative works [24][25], most later approaches [28][31][32][33][30][34] formulate the task as a pixel-level zero-shot classification problem with a closed set vocabulary. 

**Unsupervised segmentation**. Analogous to bottom-up grouping, these works perform class-agnostic segmentation within the visual modality with no explicit language alignment [21][35][36][37][38]. This topic has a long, rich history in human visual perception [39] and computer vision [40], and has been explored as means of generalizing to new visual domains [41]. It is this goal that most closely inspires our work. Early efforts group pixels based on known spatially-local affinities [42][43][44], with subsequent methods leading to region proposal networks for object detection  and advances in semantic segmentation [45]. Recent methods employ self-supervision to learn perceptual grouping [46][35] or object-centric groupings [47][48][49][50][51]. 

**Learning robust visual representations**. For a long time, ImageNet [52] accuracy was believed to provide a reasonable proxy for quality of learned visual representations [53][54]. However, recent work highlights notable deficiencies in such learned representations [55][56][57] including sensitivity to low level textures, failure for domain shifts, and reliance on spurious correlations. These failures inspired a large literature to mitigate learning spurious correlations [58][59][60] by focusing on new optimization techniques. Progress on this issue may address parallel issues in fairness [61]. Resulting methods have largely focused on synthetic data, re-balancing data, and shaping learned embeddings [59]. Nonetheless, theoretical results suggest pessimistic bounds unless additional structure informs the problem (see refs. in [58]). 

",0
117," Heatmap-based pose estimation [1][2][3][4][5] dominated the area of human pose estimation in terms of performance. Some studies [1][2][5] constructed novel networks to extract better features. While others [6][7][8][9] built upon an optimization perspective trying to mitigate quantization errors. In summary, heatmap-based methods made full use of the spatial information of the feature map and obtain a preferable performance. However, efficiency is still a certain drawback of heatmap-based methods.

For regression-based methods, Deeppose  is firstly proposed to regress the joint coordinates directly. CenterNet [10] and DirectPose [11] are proposed to accomplish multi-person human pose estimation in a _one-stage_ object detection framework, which directly regresses the joint coordinates instead of the bounding box. SPM [12] introduced the root joints to indicate different person instances and hierarchical rooted human body joints representations to better predict long-range displacements for some joints. Recently, RLE [13] introduced a flow model to capture the underlying output distribution and gets a satisfying performance. Although these methods have made great efforts to find the implicit relationship of keypoints, their performance improvement is still insufficient due to the lack of explicit guidance of heatmaps.

Transformer is proposed in  and achieves great success in Natural Language Processing (NLP). Recent studies in vision tasks used Transformer as an alternative backbone to CNN for its ability to capture global dependencies. In the area of 2D human pose estimation, many efforts [14][1][15][4][5] have been done to incorporate the Transformers. TFPose [15] first introduced Transformer to the pose estimation framework in a regression-based manner. PRTR [14] proposed a two-stage and end-to-end regression-based framework using cascade Transformers and achieves SOTA performance in regression-based methods. TransPose [5] and TokenPose [1] introduced Transformer for heatmap-based human pose estimation achieving comparable performance while being more lightweight. In our work, we introduce the transformer module to assist in finding potential relationships between keypoints.

Knowledge Distillation (KD) is formally proposed in [16], which aims to transfer the teacher's learned knowledge to the student model. In 2D human pose estimation, FPD [17] first used knowledge distillation classically based on the Hourglass network. OKDHP [18] introduced an online pose distillation approach that distills the pose structure knowledge in a one-stage manner. ViTPose [4] also implemented a large-to-small model knowledge distillation to prove its knowledge transferability. However, all previous distillation works on human pose estimation ignore the knowledge transferring between heatmap-based and regression-based methods. In this work, for the first time, we propose a heatmap-to-regression distillation framework to take benefits from both schemes.

",1
118," Heatmap-based pose estimation [1][2][3][4][5] dominated the area of human pose estimation in terms of performance. Some studies [1][2][5] constructed novel networks to extract better features. While others [6][7][8][9] built upon an optimization perspective trying to mitigate quantization errors. In summary, heatmap-based methods made full use of the spatial information of the feature map and obtain a preferable performance. However, efficiency is still a certain drawback of heatmap-based methods.

For regression-based methods, Deeppose  is firstly proposed to regress the joint coordinates directly. CenterNet [10] and DirectPose [11] are proposed to accomplish multi-person human pose estimation in a _one-stage_ object detection framework, which directly regresses the joint coordinates instead of the bounding box. SPM [12] introduced the root joints to indicate different person instances and hierarchical rooted human body joints representations to better predict long-range displacements for some joints. Recently, RLE [13] introduced a flow model to capture the underlying output distribution and gets a satisfying performance. Although these methods have made great efforts to find the implicit relationship of keypoints, their performance improvement is still insufficient due to the lack of explicit guidance of heatmaps.

Transformer is proposed in  and achieves great success in Natural Language Processing (NLP). Recent studies in vision tasks used Transformer as an alternative backbone to CNN for its ability to capture global dependencies. In the area of 2D human pose estimation, many efforts [14][1][15][4][5] have been done to incorporate the Transformers. TFPose [15] first introduced Transformer to the pose estimation framework in a regression-based manner. PRTR [14] proposed a two-stage and end-to-end regression-based framework using cascade Transformers and achieves SOTA performance in regression-based methods. TransPose [5] and TokenPose [1] introduced Transformer for heatmap-based human pose estimation achieving comparable performance while being more lightweight. 

Knowledge Distillation (KD) is formally proposed in [16], which aims to transfer the teacher's learned knowledge to the student model. In 2D human pose estimation, FPD [17] first used knowledge distillation classically based on the Hourglass network. OKDHP [18] introduced an online pose distillation approach that distills the pose structure knowledge in a one-stage manner. ViTPose [4] also implemented a large-to-small model knowledge distillation to prove its knowledge transferability. However, all previous distillation works on human pose estimation ignore the knowledge transferring between heatmap-based and regression-based methods. 

",0
119," **Neural Scene Representations.** Using neural networks to implicitly represent 3D scenes [1][2][3][4][5] has achieved exciting progress recently. NeRF  and its variants [6][7][8][9][10][11] show impressive results on novel view synthesis [12][5][13][14] and many other applications including 3D reconstruction [15][11][16][17][18], semantic segmentation [19][20][18], generative model [21][22][23][24][25], and 3D content creation [26][27][28][29][30][31][32].

Implicit neural representations exhibit remarkable rendering quality, but they suffer from slow rendering speeds due to the numerous costly MLP evaluations required for each pixel. To address this challenge, many recent papers propose _hybrid_ representations that combine a fast explicit scene representation with learnable neural network components, providing significant speedups over purely implicit methods. Various explicit representations have been investigated, including sparse voxels [33][34][35][36], low-rank components [21][37][38], point clouds [39][39][40][35][41] and others [42][43][44][45]. However, these approaches assume static 3D scenes, leaving explicit representations for dynamic scenes unexplored. This paper provides an explicit model for dynamic scenes, substantially accelerating prior methods that rely on fully implicit methods.

**Neural Rendering for Dynamic Scenes.** Representing dynamic scenes by neural radiance fields is an essential extension of NeRF, enabling numerous real-world applications [46][47][48][49][25]. One line of research represents dynamic scenes by extending NeRF with an additional time dimension (T-NeRF) or additional latent code [50][51][52][50]. Despite the ability to represent general typology changes, they suffer from a severely under-constrained problem, requiring additional supervision like depths, optical flows or dense observations for decent results. Another line of research employs individual MLPs to represent a deformation field and a canonical field [53][54][55][56][57], where the canonical field depicts a static scene, and the deformation field learns coordinate maps to the canonical space over time. We propose a simple yet elegant solution for dynamic scene representation using six feature planes, making minimal assumptions about the underlying scene.

Recently, _MAV3D_[58] adopted our design for text-to-4D dynamic scene generation, demonstrating an exciting direction for dynamic scenes beyond reconstruction.

**Accelerating NeRFs.** Many works have been proposed to accelerate NeRF at diverse stages. Some methods improve _inference_ speeds of trained NeRFs by optimizing the computation [59][60][61][62]. Others reduce the _training_ times by learning a generalizable model [63][5][12][64]. Recently, rendering speeds during _both stages_ are substantially reduced by using explicit-implicit representations [21][37][36][65][43][66]. In line with this idea, we propose an explicit representation for dynamic fields to accelerate dynamic NeRFs.

Very recently, several concurrent works have aimed to accelerate dynamic NeRFs. [67][68][69][70][71] use _time-aware_ MLPs to regress spacetime points' colors or deformations from canonical spaces. However, they remain partially implicit for dynamic fields, as they rely on MLPs with time input to obtain spacetime features. In contrast, our paper proposes a more elegant and efficient explicit representation for dynamic fields without using time-aware MLPs. Like [72], _NeRFlayer_[73] uses a highly compact 3D grid at each time step for 4D field representation, which results in substantial memory costs for lengthy videos.

_Tensor4D_[74] shares a similar idea as ours, which represents dynamic scenes with 9 planes and multiple MLPs. _D-TensoRF_[75] regards dynamic fields as 5D tensors and applies CP/MM decomposition on them for compact representation. Our paper is most closely related to _\(K\)-Planes_[76], which also employs six feature planes for representation.

",1
120," **Neural Scene Representations.** Using neural networks to implicitly represent 3D scenes [1][2][3][4][5] has achieved exciting progress recently. NeRF  and its variants [6][7][8][9][10][11] show impressive results on novel view synthesis [12][5][13][14] and many other applications including 3D reconstruction [15][11][16][17][18], semantic segmentation [19][20][18], generative model [21][22][23][24][25], and 3D content creation [26][27][28][29][30][31][32].

Implicit neural representations exhibit remarkable rendering quality, but they suffer from slow rendering speeds due to the numerous costly MLP evaluations required for each pixel. To address this challenge, many recent papers propose _hybrid_ representations that combine a fast explicit scene representation with learnable neural network components, providing significant speedups over purely implicit methods. Various explicit representations have been investigated, including sparse voxels [33][34][35][36], low-rank components [21][37][38], point clouds [39][39][40][35][41] and others [42][43][44][45]. However, these approaches assume static 3D scenes, leaving explicit representations for dynamic scenes unexplored. 

**Neural Rendering for Dynamic Scenes.** Representing dynamic scenes by neural radiance fields is an essential extension of NeRF, enabling numerous real-world applications [46][47][48][49][25]. One line of research represents dynamic scenes by extending NeRF with an additional time dimension (T-NeRF) or additional latent code [50][51][52][50]. Despite the ability to represent general typology changes, they suffer from a severely under-constrained problem, requiring additional supervision like depths, optical flows or dense observations for decent results. Another line of research employs individual MLPs to represent a deformation field and a canonical field [53][54][55][56][57], where the canonical field depicts a static scene, and the deformation field learns coordinate maps to the canonical space over time. 



**Accelerating NeRFs.** Many works have been proposed to accelerate NeRF at diverse stages. Some methods improve _inference_ speeds of trained NeRFs by optimizing the computation [59][60][61][62]. Others reduce the _training_ times by learning a generalizable model [63][5][12][64]. Recently, rendering speeds during _both stages_ are substantially reduced by using explicit-implicit representations [21][37][36][65][43][66]. 

Very recently, several concurrent works have aimed to accelerate dynamic NeRFs. [67][68][69][70][71] use _time-aware_ MLPs to regress spacetime points' colors or deformations from canonical spaces. However, they remain partially implicit for dynamic fields, as they rely on MLPs with time input to obtain spacetime features. 

_Tensor4D_[74] shares a similar idea as ours, which represents dynamic scenes with 9 planes and multiple MLPs. _D-TensoRF_[75] regards dynamic fields as 5D tensors and applies CP/MM decomposition on them for compact representation. 

",0
121," **Related Animal Datasets.** The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments. There are several existing datasets for studying animal social behavior, including CRIM13 ([1]), Fly vs. Fly (), and CalMS21 (). These datasets contain video or pose data from interacting animals, as well as human-annotated behavior labels (Table 1); they all focus on a single species and setting. AnimalKingdom ([2]) is another recent animal behavior dataset that includes social and nonsocial behavior from multiple species, but is focused on human annotation-based action recognition only. Our dataset is unique in that it defines a range of downstream tasks for each organism; these tasks are motivated by scientific experiments, with the goal of to driving scientific discovery in biology.

**Related Human Datasets.** While animal video datasets remain comparatively rate, there are many video datasets designed for work in human action recognition. Human datasets typically have very different visual characteristics from animal datasets. Most notably, many human datasets that are used to benchmark self-supervised video representation learning, such as Kinetics ([5]), UCF101 ([3]) and HMDB51 ([4]), contain'spatially heavy' visual information that informs downstream action classification- that is, different actions have different backgrounds. Because of these differences in the visual appearance, agents' actions can be partly distinguished by these visual features alone, without models having to learn any temporal features of the agents' behavior. In contrast, our animal videos are all acquired against a stationary, neutral background, forcing models to use the temporal structure of the data to distinguish between actions.

**Related Problems in Multi-Agent Behavior.** While our dataset is composed of multi-agent data from biology, there are also multi-agent behavior datasets from other domains, such as from autonomous driving ([7]; [8]), sports analytics ([11]; [6]), and video games ([10]; [9]). These datasets often focus on forecasting, motion planning, and reinforcement learning, whereas our dataset is used for tasks from scientific applications, such as distinguishing animal strains via observed behaviors.

**Work in Animal Behavior Analysis.** In biology and neuroscience, computational models of behavior have the potential to significantly reduce human data annotation efforts, and to provide more detailed descriptions of the behavior in question (; ). Automated characterizations of animal behavior have been used to study the relationship between neural activity and behavior (), to characterize behavioral differences between species and between different strains within a species ([12]), and to quantify the effect of functional or pharmacological perturbations (; ). The input to these models may be video ([13]) or trajectory data (; [14]).

Supervised behavior models have been trained to identify human-defined behaviors-of-interest ([16]; [14]; ; ), often using frame-by-frame behavior annotations from domain experts. Another body of work discovers behaviors without human annotations, using unsupervised and self-supervised methods ([15]; ; ; ; ) that learn the latent structure of behavioral data. The learned representation may be continuous (), or discrete, such as when discovering behavior motifs ([15]; ; ). There currently does not exist a unified behavioral representation learning dataset that can compare these models across a broad range of behavior analysis settings. Here, we propose  for evaluating the performance of these representation learning methods.

**Work in Representation Learning.** Representation learning for visual ([21]; ; [17]; [18]; [20]) and trajectory data (; [19]) has been applied to a variety of tasks, such as for image classification (), speech recognition ([17]), and behavior classification (). In these works, many different unsupervised / self-supervised methods have been developed, employing various pretext tasks to pre-train a model, such as classifying image rotations ([21]), predicting future observations ([17]), contrastive learning with image augmentations (), and decoding programmatic attributes (). The quality of learned representations is often evaluated on downstream tasks.

_Behavioral Representation Learning._ For behavior analysis, applications of representation learning include discovering behavior motifs ([15]; ; ; ), identifying internal states (), and improving sample-efficiency of supervised classifiers (). These works use methods such as variational autoencoders (), autoregressive hidden Markov models (), and Uniform Manifold Approximation and Projection (UMAP) ([22]) to characterize the latent structure of behavior.

Notably, many groups have proposed methods for unsupervised behavior discovery ([15]; [23]; ; ; Hsu & ; ). These works use different methods to model the temporal structure of behavior, including wavelet transforms ([15]), autoregressive hidden Markov models (), and recurrent NNs (), as well as different methods for segmenting behavior, such as Gaussian mixture models (Hsu & ), k-means clustering (), and watershed transforms ([15]). Our goal is to develop a standardized dataset for evaluating these methods on a common set of behavior analysis tasks.

",1
122," **Related Animal Datasets.** The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments. There are several existing datasets for studying animal social behavior, including CRIM13 ([1]), Fly vs. Fly (), and CalMS21 (). These datasets contain video or pose data from interacting animals, as well as human-annotated behavior labels (Table 1); they all focus on a single species and setting. AnimalKingdom ([2]) is another recent animal behavior dataset that includes social and nonsocial behavior from multiple species, but is focused on human annotation-based action recognition only. 

**Related Human Datasets.** While animal video datasets remain comparatively rate, there are many video datasets designed for work in human action recognition. Human datasets typically have very different visual characteristics from animal datasets. Most notably, many human datasets that are used to benchmark self-supervised video representation learning, such as Kinetics ([5]), UCF101 ([3]) and HMDB51 ([4]), contain'spatially heavy' visual information that informs downstream action classification- that is, different actions have different backgrounds. Because of these differences in the visual appearance, agents' actions can be partly distinguished by these visual features alone, without models having to learn any temporal features of the agents' behavior. 

**Related Problems in Multi-Agent Behavior.** While our dataset is composed of multi-agent data from biology, there are also multi-agent behavior datasets from other domains, such as from autonomous driving ([7]; [8]), sports analytics ([11]; [6]), and video games ([10]; [9]). 

**Work in Animal Behavior Analysis.** In biology and neuroscience, computational models of behavior have the potential to significantly reduce human data annotation efforts, and to provide more detailed descriptions of the behavior in question (; ). Automated characterizations of animal behavior have been used to study the relationship between neural activity and behavior (), to characterize behavioral differences between species and between different strains within a species ([12]), and to quantify the effect of functional or pharmacological perturbations (; ). The input to these models may be video ([13]) or trajectory data (; [14]).

Supervised behavior models have been trained to identify human-defined behaviors-of-interest ([16]; [14]; ; ), often using frame-by-frame behavior annotations from domain experts. Another body of work discovers behaviors without human annotations, using unsupervised and self-supervised methods ([15]; ; ; ; ) that learn the latent structure of behavioral data. The learned representation may be continuous (), or discrete, such as when discovering behavior motifs ([15]; ; ). There currently does not exist a unified behavioral representation learning dataset that can compare these models across a broad range of behavior analysis settings. 

**Work in Representation Learning.** Representation learning for visual ([21]; ; [17]; [18]; [20]) and trajectory data (; [19]) has been applied to a variety of tasks, such as for image classification (), speech recognition ([17]), and behavior classification (). In these works, many different unsupervised / self-supervised methods have been developed, employing various pretext tasks to pre-train a model, such as classifying image rotations ([21]), predicting future observations ([17]), contrastive learning with image augmentations (), and decoding programmatic attributes (). The quality of learned representations is often evaluated on downstream tasks.

_Behavioral Representation Learning._ For behavior analysis, applications of representation learning include discovering behavior motifs ([15]; ; ; ), identifying internal states (), and improving sample-efficiency of supervised classifiers (). These works use methods such as variational autoencoders (), autoregressive hidden Markov models (), and Uniform Manifold Approximation and Projection (UMAP) ([22]) to characterize the latent structure of behavior.

Notably, many groups have proposed methods for unsupervised behavior discovery ([15]; [23]; ; ; Hsu & ; ). These works use different methods to model the temporal structure of behavior, including wavelet transforms ([15]), autoregressive hidden Markov models (), and recurrent NNs (), as well as different methods for segmenting behavior, such as Gaussian mixture models (Hsu & ), k-means clustering (), and watershed transforms ([15]). 

",0
123," In this section, we comprehensively review existing works on low-light image enhancement and analyze their limitations in producing satisfactory results.

Conventional Methods.Many conventional LLIE methods are based on Retinex theory, which formulates an image as a multiplication of reflectance and illumination (Eq. (1)). Some methods estimate the reflectance and illumination via a variational framework, the estimated illumination is further adjusted to restore the original low-light image [1][2]. Other Retinex-based methods usually optimize an energy function derived from the Maximum-a-Posteriori (MAP) framework to enhance low-light images, in which some heuristic priors are designed to constrain the properties of reflectance or illumination [3][4][5][6]. For example, Gaussian total variation in [4], structure-aware regularization in [3] and the additional noise term in [5]. However, these hand-craft priors are usually inaccurate and limited when applied to different cases, leading to apparent color deviation.

CNN-based Learning Methods.CNN has pushed forward the solution of LLIE by large margins [7]. Some methods integrate Retinex theory with CNNs [8][9][10][11][8]. For example, Wei _et al._ propose an end-to-end Retinex-Net, in which a decomposition module and an illumination adjustment module are used to learn reflectance and illumination [8]. Wu _et al._ propose URetinex-Net which unfolds an optimization problem into several learnable network modules [11]. Some methods design effective networks to predict normal-light images directly from low-light ones via a supervised learning strategy [12][13][14][15][16][17]. For instance, [14] learns a 3D lookup table, and [16] uses color consistency to constrain network training. There are still some unsupervised methods to solve LLIE [18][19][20]. One famous work is [20], in which a self-calibrated illumination learning framework is proposed for fast, flexible, and robust low-light image enhancement. Because CNNs are usually limited by inductive bias, the illumination still cannot be well recovered for some images.

Transformer-based Methods.Transformer-based methods usually combine self-attention and convolution to extract long- and short-range dependencies, so that better performance can be obtained [21][22][23]. However, the long-range context dependencies are usually obtained by building patch-level relationships, and these methods downsample images before calculating self-attention to reduce computation. Hence, they may lose some useful information, which is adverse to recover high-quality images.

In this paper, we propose a local-to-global hierarchical self-attention to completely express images. We would like to point out that the work [24] aiming at high-level tasks proposes a TNT structure to explore local attention to assist global attention. Our work is different from it in the following aspects: 1) TNT still focuses on patch-level attention, pixel-level dependencies are still not explored, i.e., it does not completely model images. While ours completely express images via pixel-level attention. 2) Local attention in TNT just provides auxiliary information for the patch embedding of global attention. While our local attention models pixel dependencies locally and the global attention extends pixel dependencies to the whole image. 3) TNT adopts a bypass structure to realize auxiliary information enhancement. While our local attentions are directly embedded as the input for global attention.

",1
124," In this section, we comprehensively review existing works on low-light image enhancement and analyze their limitations in producing satisfactory results.

Conventional Methods.Many conventional LLIE methods are based on Retinex theory, which formulates an image as a multiplication of reflectance and illumination (Eq. (1)). Some methods estimate the reflectance and illumination via a variational framework, the estimated illumination is further adjusted to restore the original low-light image [1][2]. Other Retinex-based methods usually optimize an energy function derived from the Maximum-a-Posteriori (MAP) framework to enhance low-light images, in which some heuristic priors are designed to constrain the properties of reflectance or illumination [3][4][5][6]. For example, Gaussian total variation in [4], structure-aware regularization in [3] and the additional noise term in [5]. However, these hand-craft priors are usually inaccurate and limited when applied to different cases, leading to apparent color deviation.

CNN-based Learning Methods.CNN has pushed forward the solution of LLIE by large margins [7]. Some methods integrate Retinex theory with CNNs [8][9][10][11][8]. For example, Wei _et al._ propose an end-to-end Retinex-Net, in which a decomposition module and an illumination adjustment module are used to learn reflectance and illumination [8]. Wu _et al._ propose URetinex-Net which unfolds an optimization problem into several learnable network modules [11]. Some methods design effective networks to predict normal-light images directly from low-light ones via a supervised learning strategy [12][13][14][15][16][17]. For instance, [14] learns a 3D lookup table, and [16] uses color consistency to constrain network training. There are still some unsupervised methods to solve LLIE [18][19][20]. 

Transformer-based Methods.Transformer-based methods usually combine self-attention and convolution to extract long- and short-range dependencies, so that better performance can be obtained [21][22][23]. However, the long-range context dependencies are usually obtained by building patch-level relationships, and these methods downsample images before calculating self-attention to reduce computation. Hence, they may lose some useful information, which is adverse to recover high-quality images.



",0
125," **Visual tracking paradigm.** The Siamese network [1][2][3][4][5] based tracking paradigms have drawn great attention recently, in which they formulate the tracking as per-frame target matching. Under the pair-wise matching framework, Siamese trackers are improved with the help of following techniques: powerful backbones [3][5], elaborated prediction networks [2][4][6], attention mechanism [7][8] and model fine-tuning [9][10]. Recent pure transformer-based trackers [11][12][13][14] leverage the vision transformer to unify the feature extraction and fusion, while still not consider how to effectively model the temporal dependency. Discriminative Correlation Filter (DCF) [15][16][17][18][19][20][21] is another popular tracking paradigm, which can optimize the target model by solving least-squares based regression. Though DCF can easily utilize the temporal information by updating the model online, it suffers from the complex handcrafted optimization.

**Temporal modelling in Siamese tracking.** Two representative paradigms are introduced to enhance the temporal modelling in Siamese trackers: the first one is to update templates using online mechanism  or deep-learning based networks [7][22]; the second one [23] is to propagate the target information from templates to search frame. Despite the improvements, both of them require extra hyper-parameters and redundant network modules to equip the original Siamese pipeline. In contrast to the sophisticated methods and tedious hyper-parameters mentioned above,VideoTrack is the first to encode the temporal information via a simple feedforward video transformer backbone, which is novel and conceptually neat.

**Video vision transformer.** Video transformers have recently been introduced as powerful video recognition models, motivated by the impressive performance of transformers in language and vision [24][25][11][26][12][27][28][29][30][31][32][33]. ViViT [34], Timesformer [35], VTN [36] and VideoSwin [37] are the pioneering works, which apply the pure-transformer based models for video recognition. The underlying reasons for their success lie in the characteristics of video: videos are sequential data while transformer attention can capture the global dependency among all the video segments. Considering visual tracking is highly sensitive to spatial/appearance information, rather than semantic/category, we modify the standard video transformer structure to exploit more static/dynamic appearance clues for tracking.

**Temporal modelling in video understanding.** Temporal context modelling is the key issue in video understanding task. 3D convolutional block [38][39][40] is the widely adopted technique, which expands 2D CNN into temporal domain. Then, non-local network [41] applies self-attention to capture long-range spatiotemporal dependencies on top of 2D CNN. Recently, video transformers [34][35] use self-attention as the exclusive building block to capture spatiotemporal context. In instance-level video understanding, temporal shift [42][43] and message token [44] mechanisms are equipped into video transformer to enhance the temporal modelling as well as reducing the computation cost. In this work, we empirically evaluate different temporal modelling methods and develop a disentangled dual-template scheme for VideoTrack model.

",1
126," **Visual tracking paradigm.** The Siamese network [1][2][3][4][5] based tracking paradigms have drawn great attention recently, in which they formulate the tracking as per-frame target matching. Under the pair-wise matching framework, Siamese trackers are improved with the help of following techniques: powerful backbones [3][5], elaborated prediction networks [2][4][6], attention mechanism [7][8] and model fine-tuning [9][10]. Recent pure transformer-based trackers [11][12][13][14] leverage the vision transformer to unify the feature extraction and fusion, while still not consider how to effectively model the temporal dependency. Discriminative Correlation Filter (DCF) [15][16][17][18][19][20][21] is another popular tracking paradigm, which can optimize the target model by solving least-squares based regression. Though DCF can easily utilize the temporal information by updating the model online, it suffers from the complex handcrafted optimization.

**Temporal modelling in Siamese tracking.** Two representative paradigms are introduced to enhance the temporal modelling in Siamese trackers: the first one is to update templates using online mechanism  or deep-learning based networks [7][22]; the second one [23] is to propagate the target information from templates to search frame. Despite the improvements, both of them require extra hyper-parameters and redundant network modules to equip the original Siamese pipeline. 

**Video vision transformer.** Video transformers have recently been introduced as powerful video recognition models, motivated by the impressive performance of transformers in language and vision [24][25][11][26][12][27][28][29][30][31][32][33]. ViViT [34], Timesformer [35], VTN [36] and VideoSwin [37] are the pioneering works, which apply the pure-transformer based models for video recognition. The underlying reasons for their success lie in the characteristics of video: videos are sequential data while transformer attention can capture the global dependency among all the video segments. 

**Temporal modelling in video understanding.** Temporal context modelling is the key issue in video understanding task. 3D convolutional block [38][39][40] is the widely adopted technique, which expands 2D CNN into temporal domain. Then, non-local network [41] applies self-attention to capture long-range spatiotemporal dependencies on top of 2D CNN. Recently, video transformers [34][35] use self-attention as the exclusive building block to capture spatiotemporal context. In instance-level video understanding, temporal shift [42][43] and message token [44] mechanisms are equipped into video transformer to enhance the temporal modelling as well as reducing the computation cost. 

",0
127," **Compositional Zero-Shot Learning.** Given descriptions only, we can recognize objects that are never seen before. In conventional Zero-Shot Learning (ZSL), models have access both to images of seen classes and descriptions of seen and unseen classes [1]. In contrast, CZSL presents no description of seen and unseen attribute-object compositions while all attributes and objects as concepts are seen during training. Recently, works in CZSL are divided into two main streams. One extracts attribute and object words or visual features independently from a composition during training, including learning attributes as linear transformations of objects , learning to hierarchically decompose compositions and recompose the concepts with learned visual concepts [2], learning independent prototypes of attributes and objects and compositing prototypes via graph network [3], and learning decomposed prototypes of visual concept features [4] via siamese contrastive embedding network [5]. The other is to learn a compositional space [6], a graph network [7][8], an episode-based cross-attention module [9], and a contrastive space [10] for contextuality modeling. Also, Yang _et al._[11] rethink the CZSL task in a decomposable causal way and learn three spaces for attribute, object, and composition classifications. Additionally, with pre-trained large vision language models like CLIP, Nayak _et al._[12] propose to tune soft prompts as concept embeddings.

Recent work in [13] addresses the problem of attribute diversity. They propose to learn translational attribute features conditionally dependent on the object prototypes. Specifically, they add generic object embedding as the object prototype to the concatenated attribute and object embedding. However, this approach makes the model concentrate more on the composition instead of the attribute, causing the attribute learning degrade to learning the contextuality between attribute and object. On the contrary, we explicitly focus on learning conditional attribute embeddings. The learned conditional attribute embeddings can be changed along with the objects and input images.

**Attribute Learning.** Learning features of attributes is explored by a large community including image search [14][15], sentence generation [16], and zero-shot classification [17][18]. Conventional attribute learning approaches map the attributes into high-dimensional space and train a discriminative classification head without considering the diverse nature of attributes [19]. Our work also learns high-dimensional embeddings to represent attributes. The main difference is that our learned attribute embeddings are conditioned on different objects and input images.

",1
128," **Compositional Zero-Shot Learning.** Given descriptions only, we can recognize objects that are never seen before. In conventional Zero-Shot Learning (ZSL), models have access both to images of seen classes and descriptions of seen and unseen classes [1]. In contrast, CZSL presents no description of seen and unseen attribute-object compositions while all attributes and objects as concepts are seen during training. Recently, works in CZSL are divided into two main streams. One extracts attribute and object words or visual features independently from a composition during training, including learning attributes as linear transformations of objects , learning to hierarchically decompose compositions and recompose the concepts with learned visual concepts [2], learning independent prototypes of attributes and objects and compositing prototypes via graph network [3], and learning decomposed prototypes of visual concept features [4] via siamese contrastive embedding network [5]. The other is to learn a compositional space [6], a graph network [7][8], an episode-based cross-attention module [9], and a contrastive space [10] for contextuality modeling. Also, Yang _et al._[11] rethink the CZSL task in a decomposable causal way and learn three spaces for attribute, object, and composition classifications. Additionally, with pre-trained large vision language models like CLIP, Nayak _et al._[12] propose to tune soft prompts as concept embeddings.

Recent work in [13] addresses the problem of attribute diversity. They propose to learn translational attribute features conditionally dependent on the object prototypes. Specifically, they add generic object embedding as the object prototype to the concatenated attribute and object embedding. However, this approach makes the model concentrate more on the composition instead of the attribute, causing the attribute learning degrade to learning the contextuality between attribute and object. 

**Attribute Learning.** Learning features of attributes is explored by a large community including image search [14][15], sentence generation [16], and zero-shot classification [17][18]. Conventional attribute learning approaches map the attributes into high-dimensional space and train a discriminative classification head without considering the diverse nature of attributes [19]. 

",0
129," There are two streams of work that are relevant for LMMP. In online revenue management literature, Gallego and Van  introduced the dynamic pricing problem where the demand is a known function of price (action).  and  extended the problem under unknown demands with multiple resource constraints. [1] proposed a Thompson sampling-based algorithm and extended it to contextual bandits with knapsacks. When the expected demand is a linear function of the price vector, the dynamic pricing problem is a special case of linear contextual bandits with knapsack (LinCBwK) proposed by .

The LinCBwk is a common generalization of bandits with knapsacks ([3]; [5]; [4]) and online stochastic packing problems (; ; [2]). Recently,  proved a logarithmic regret bound for LinCBwK when there exists a problem-dependent gap between the reward of the optimal action and the other actions. Instead of the gap assumption, we require non-degeneracy of the stochastic contexts (see Assumption 3 for a precise definition) to obtain a regret bound sublinear in \(d\) and extends to the case when the contexts are generated from \(J\) different class.

[6] proposed a variant of LinCBwK where the selected action must satisfy a single constraint with high probability in all rounds, i.e., LinCBwK with anytime constraints. [8] and [7] proposed a Thompson sampling-based algorithm and an upper confidence bound-based algorithm, respectively, for LinCBwK with a single anytime constraint. [9] highlighted the difference between global and anytime constraints and proposed a pessimistic-optimistic algorithm for the anytime constraints. We focus on the global constraints; however, we note that the extension to the anytime constraints is straightforward with minor modifications.

Let \(\mathbb{R}_{+}\) denote the set of positive real numbers. For two real numbers \(a,b\in\mathbb{R}\), we write \(a\wedge b:=\min\{a,b\}\) and \(a\lor b:=\max\{a,b\}\). For a natural number \(N\in\mathbb{N}\), let \([N]:=\{1,\ldots,N\}\).

",1
130," There are two streams of work that are relevant for LMMP. In online revenue management literature, Gallego and Van  introduced the dynamic pricing problem where the demand is a known function of price (action).  and  extended the problem under unknown demands with multiple resource constraints. [1] proposed a Thompson sampling-based algorithm and extended it to contextual bandits with knapsacks. When the expected demand is a linear function of the price vector, the dynamic pricing problem is a special case of linear contextual bandits with knapsack (LinCBwK) proposed by .

The LinCBwk is a common generalization of bandits with knapsacks ([3]; [5]; [4]) and online stochastic packing problems (; ; [2]). Recently,  proved a logarithmic regret bound for LinCBwK when there exists a problem-dependent gap between the reward of the optimal action and the other actions. 

[6] proposed a variant of LinCBwK where the selected action must satisfy a single constraint with high probability in all rounds, i.e., LinCBwK with anytime constraints. [8] and [7] proposed a Thompson sampling-based algorithm and an upper confidence bound-based algorithm, respectively, for LinCBwK with a single anytime constraint. [9] highlighted the difference between global and anytime constraints and proposed a pessimistic-optimistic algorithm for the anytime constraints. 

Let \(\mathbb{R}_{+}\) denote the set of positive real numbers. For two real numbers \(a,b\in\mathbb{R}\), we write \(a\wedge b:=\min\{a,b\}\) and \(a\lor b:=\max\{a,b\}\). For a natural number \(N\in\mathbb{N}\), let \([N]:=\{1,\ldots,N\}\).

",0
131," Our work is related to the many per-frame VIS methods, the recently proposed per-clip VIS methods, as well as the methods for learning query embeddings.

**Per-frame input based VIS methods.** A popular VIS pipeline [1][2][3][4][5][6][7][8][9] is to extend the representative image instance segmentation methods [10][11][12] by adapting a frame-to-frame instance tracker. For example, in [1][2][8], the clues such as category score, box/mask IoU and instance embedding similarity are integrated into the tracker. However, these trackers may struggle in distinguishing instances with similar appearance. Inspired by contrastive learning [13][14][15][16], IDOL [7] learns discriminative instance embeddings for multiple object tracking frame by frame, achieving state-of-the-art results on OVIS [17]. Besides, clip-to-clip trackers [18][5][17] propagates the predicted instance masks from a key frame to other frames using deformable convolution [18][19], non-local block [5], correlation [4][17], graph neural network [20], _etc_. By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance of per-frame methods.

**Per-clip input based VIS methods.** A clip-in clip-out VIS pipeline was firstly proposed in  to model a video clip as a single 3D spatio-temporal pixel embedding. In recent years, transformer based per-clip methods [21][22][23] have achieved impressive progress on the YouTube-VIS datasets [8]. VisTR [22] views the VIS task as a direct end-to-end parallel sequence prediction problem, but it needs a large memory to store spatio-temporal features. To solve the issue, IFC [21] transfers inter-frame information via efficient memory tokens, and SeqFormer  locates an instance in each frame and aggregates temporal information to predict video-level instances. To keep object temporal consistency, EfficientVIS [23] transfers inter-clip query embeddings via temporal dynamic convolution.

However, per-clip VIS methods do not perform well on the challenging OVIS videos [17] with occluded objects in crowded scenes. Actually, occlusion-aware models have been developed for related tasks [24][25][26]. For instance, a bilayer convolutional network is developed in [27] to infer the occluder and occluded instances in image segmentation. A repulsion detection loss is designed in [26] to distance the bounding box of an object from the surrounding non-target objects for detecting individual pedestrian in a crowd. Inspired by these works, we propose an inter-instance mask repulsion loss to distinguish the pixels of each instance from its nearby non-target instances.

**Query initialization.** Existing query-based VIS methods adopt zero-initialized (_e.g_., DETR ) or randomly-initialized (_e.g_., Deformable DETR [28]) inputs as initial queries. The initial queries cannot encode well the spatiotemporal priors of objects, making the query-based segmenter difficult to distinguish occluded instances with similar appearance. Actually, query initialization with contextual and positional information has been used in many computer vision tasks [29][30][31][32] for higher performance or faster convergence. However, it has not been well explored in VIS. In this paper, we thus propose a query initialization method to obtain temporally-aligned frame-level queries.

",1
132," Our work is related to the many per-frame VIS methods, the recently proposed per-clip VIS methods, as well as the methods for learning query embeddings.

**Per-frame input based VIS methods.** A popular VIS pipeline [1][2][3][4][5][6][7][8][9] is to extend the representative image instance segmentation methods [10][11][12] by adapting a frame-to-frame instance tracker. For example, in [1][2][8], the clues such as category score, box/mask IoU and instance embedding similarity are integrated into the tracker. However, these trackers may struggle in distinguishing instances with similar appearance. Inspired by contrastive learning [13][14][15][16], IDOL [7] learns discriminative instance embeddings for multiple object tracking frame by frame, achieving state-of-the-art results on OVIS [17]. Besides, clip-to-clip trackers [18][5][17] propagates the predicted instance masks from a key frame to other frames using deformable convolution [18][19], non-local block [5], correlation [4][17], graph neural network [20], _etc_. By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance of per-frame methods.

**Per-clip input based VIS methods.** A clip-in clip-out VIS pipeline was firstly proposed in  to model a video clip as a single 3D spatio-temporal pixel embedding. In recent years, transformer based per-clip methods [21][22][23] have achieved impressive progress on the YouTube-VIS datasets [8]. VisTR [22] views the VIS task as a direct end-to-end parallel sequence prediction problem, but it needs a large memory to store spatio-temporal features. To solve the issue, IFC [21] transfers inter-frame information via efficient memory tokens, and SeqFormer  locates an instance in each frame and aggregates temporal information to predict video-level instances. To keep object temporal consistency, EfficientVIS [23] transfers inter-clip query embeddings via temporal dynamic convolution.

However, per-clip VIS methods do not perform well on the challenging OVIS videos [17] with occluded objects in crowded scenes. Actually, occlusion-aware models have been developed for related tasks [24][25][26]. For instance, a bilayer convolutional network is developed in [27] to infer the occluder and occluded instances in image segmentation. A repulsion detection loss is designed in [26] to distance the bounding box of an object from the surrounding non-target objects for detecting individual pedestrian in a crowd. 

**Query initialization.** Existing query-based VIS methods adopt zero-initialized (_e.g_., DETR ) or randomly-initialized (_e.g_., Deformable DETR [28]) inputs as initial queries. The initial queries cannot encode well the spatiotemporal priors of objects, making the query-based segmenter difficult to distinguish occluded instances with similar appearance. Actually, query initialization with contextual and positional information has been used in many computer vision tasks [29][30][31][32] for higher performance or faster convergence. However, it has not been well explored in VIS. 

",0
133," Simulation Environments for Robotics:The robotics community has a long history of building simulators for safer and faster robot development [1][2][3][4][5]. Early works focused on modeling robot dynamics and physical forces for parameter identification and controller modelling [2][6]. Several works then developed accurate physics engines for improving robot design and motion planning [1][7][8], and for specific domains such as grasping , soft robotics [9], and SDVs . But to enable end-to-end testing of full autonomy systems, we must also simulate realistic sensor observations of the 3D environment for the robot to perceive, interact with its surroundings, and plan accordingly . Most prior sensor simulation systems use 3D-scanned or manually built synthetic environments for small indoor environments [7][3][10], and perform rasterization or ray-tracing [11] to simulate various sensor data [12][13]. For high-speed robots such as SDVs, simulators such as CARLA and AirSim [14] applied a similar approach. But due to the costly manual effort in creating scenes, these simulators have difficulty scaling to all the areas we may want to test in, have limited asset diversity (e.g., roads, vehicles, vegetation) compared to the real world, and generate unrealistic sensor data that require substantial domain adaptation for autonomy [15][16].

Novel View Synthesis:Recent novel view synthesis (NVS) work has achieved success in automatically generating highly photorealistic sensor observations [17][18][19][20]. Such methods aim to learn a scene representation from a set of densely collected observed images and render the scene from nearby unseen viewpoints. Some works perform geometry reconstruction and then warp and aggregate pixel-features from the input images into new camera views, which are then processed by learning-based modules [21][22]. Others represent the scene implicitly as a neural radiance field (NeRF) and perform volume rendering with a neural network [23][24][25][23]. These methods can represent complex geometry and appearance and have achieved photorealistic rendering, but focus on small static scenes. Several representations [26][27][28][29][30][31][32] partition the space and model the volume more efficiently to handle large-scale unbounded outdoor scenes. However, these works focus primarily on the NVS task where a dense collection of images are available and most test viewpoints are close to the training views, and focus on the static scene without rendering dynamic objects such as moving vehicles. In contrast, our work extends NVS techniques to build a sensor simulator from a single recorded log captured by a high-speed mobile platform. We aim to render image and LiDAR observations of dynamic traffic scenarios from new viewpoints and modified scene configurations to enable closed-loop autonomy evaluation.

Data-driven Sensor Simulation for Self Driving:Several past works have leveraged computer vision techniques and real world data to build sensor simulators for self-driving. Some works perform 3D reconstruction by aggregating LiDAR and building textured geometry primitives for physics-based rendering [33][34][35][36], but primarily simulate LiDAR or cannot model high-resolution images. Another line of work perform object reconstruction and insertion into existing images [37][38][25][39] or point clouds [40][41][42][43], but these methods are unable to render sensor data from new views for closed-loop interaction. DriveGAN [44] represents the scene as disentangled latent codes and generates video from control inputs with a neural network for differentiable closed-loop simulation, but is limited in its realism and is not temporally consistent. AADS [45] and VISTA 2.0 [46][47][48], perform multi-sensor simulation via image-based warping or ray-casting on previously collected sensor data to render new views of the static scene, and then insert and blend CAD assets into the sensor data to create new scenarios. These approaches, while promising, have visual artifacts for the inserted actors and rendered novel views, resulting in a large domain gap. Neural Scene Graphs (NSG) [19] and Panoptic Neural Fields (PNF) [17] represent the static scene and agents as multi-layer perceptrons (MLPs) and volume render photo-realistic images of the scene. However, the single MLP has difficulties modelling large scale scenes. These prior works also focus on scene editing and perception tasks where the SDV does not deviate significantly from the original recording. Instead, we focus on multi-sensor simulation for closed loop evaluation of autonomy systems, and specifically design our system to better handle extrapolation.

",1
134," Simulation Environments for Robotics:The robotics community has a long history of building simulators for safer and faster robot development [1][2][3][4][5]. Early works focused on modeling robot dynamics and physical forces for parameter identification and controller modelling [2][6]. Several works then developed accurate physics engines for improving robot design and motion planning [1][7][8], and for specific domains such as grasping , soft robotics [9], and SDVs . But to enable end-to-end testing of full autonomy systems, we must also simulate realistic sensor observations of the 3D environment for the robot to perceive, interact with its surroundings, and plan accordingly . Most prior sensor simulation systems use 3D-scanned or manually built synthetic environments for small indoor environments [7][3][10], and perform rasterization or ray-tracing [11] to simulate various sensor data [12][13]. For high-speed robots such as SDVs, simulators such as CARLA and AirSim [14] applied a similar approach. But due to the costly manual effort in creating scenes, these simulators have difficulty scaling to all the areas we may want to test in, have limited asset diversity (e.g., roads, vehicles, vegetation) compared to the real world, and generate unrealistic sensor data that require substantial domain adaptation for autonomy [15][16].

Novel View Synthesis:Recent novel view synthesis (NVS) work has achieved success in automatically generating highly photorealistic sensor observations [17][18][19][20]. Such methods aim to learn a scene representation from a set of densely collected observed images and render the scene from nearby unseen viewpoints. Some works perform geometry reconstruction and then warp and aggregate pixel-features from the input images into new camera views, which are then processed by learning-based modules [21][22]. Others represent the scene implicitly as a neural radiance field (NeRF) and perform volume rendering with a neural network [23][24][25][23]. These methods can represent complex geometry and appearance and have achieved photorealistic rendering, but focus on small static scenes. Several representations [26][27][28][29][30][31][32] partition the space and model the volume more efficiently to handle large-scale unbounded outdoor scenes. However, these works focus primarily on the NVS task where a dense collection of images are available and most test viewpoints are close to the training views, and focus on the static scene without rendering dynamic objects such as moving vehicles. 

Data-driven Sensor Simulation for Self Driving:Several past works have leveraged computer vision techniques and real world data to build sensor simulators for self-driving. Some works perform 3D reconstruction by aggregating LiDAR and building textured geometry primitives for physics-based rendering [33][34][35][36], but primarily simulate LiDAR or cannot model high-resolution images. Another line of work perform object reconstruction and insertion into existing images [37][38][25][39] or point clouds [40][41][42][43], but these methods are unable to render sensor data from new views for closed-loop interaction. DriveGAN [44] represents the scene as disentangled latent codes and generates video from control inputs with a neural network for differentiable closed-loop simulation, but is limited in its realism and is not temporally consistent. AADS [45] and VISTA 2.0 [46][47][48], perform multi-sensor simulation via image-based warping or ray-casting on previously collected sensor data to render new views of the static scene, and then insert and blend CAD assets into the sensor data to create new scenarios. These approaches, while promising, have visual artifacts for the inserted actors and rendered novel views, resulting in a large domain gap. Neural Scene Graphs (NSG) [19] and Panoptic Neural Fields (PNF) [17] represent the static scene and agents as multi-layer perceptrons (MLPs) and volume render photo-realistic images of the scene. However, the single MLP has difficulties modelling large scale scenes. 

",0
135," Equivariance in MLEquivariance is tied closely to geometry and symmetry. The entire subject of physics is founded on concepts surrounding symmetry. A wide range of natural phenomena admits equivariance inherently since the underlying mechanism is oftentimes geometrical. As a consequence, a lot of data that machine learning deals with has the equivariance property. For example, camera photography follows simple 3D geometry rules, thus a shift in camera position leads to a shift in the photograph; the molecules and point clouds have translation and rotation symmetry in 3D, thus an SE(3) transform should not change any property. Therefore, it is natural to consider equivariance in developing machine learning models.

Equivariance in 2D computer visionConvolutional neural networks (CNN) for 2D images are shown to have the approximate translation equivariance property due to the nature of convolution . There is a line of work developing rotation equivariant 2D CNNs [1][2][3][4][5]. The transformation group for 2D rotation is the Special Orthogonal group SO(2), and the Special Euclidean group SE(2) if the translation is allowed. The derivation of group equivariance constraint typically results in steerable filters constructed from 2D harmonic bases. The convolution filter weights are parameterized as a linear combination of the harmonic bases.

Equivariance can also be achieved by parameter sharing of the neural net weights [6]. However, this approach is only possible for limited kinds of groups, such as 90-degree rotations. 2D scale equivariant CNN has been studied [7][8][9]. This is typically done by applying the same convolution kernel on several scales or constructing steerable filters from the bases. Equivariant network design method can be generalized to other groups [10][11][12][13] and has rich theory in math and physics [14][15][16]. Equivariance can also be achieved by transforming the data to canonical coordinate systems [17][18][19]. In particular, [18]transforms the data to key canonical frames of the group and averages over those frames, while we average over a random sample of the cropping transform. Transformers are the current state-of-the-art neural net architecture [20]. People have sought to combine Transformer and equivariance, resulting in Lie-Transformer [21].

In terms of applications, there is good evidence that equivariance benefits image semantic segmentation [22][23], object detection to shifting  and rotation of images [24]. Equivariance is also useful for generative modeling, for example, for normalizing flow-based generative models [25][26], and variational autoencoders [27]. Equivariance to rotation is beneficial in digital pathology . Extension to time-equivariance for video is also possible [28].

**Equivariance in self-supervised learning.** Equivariance and invariance are useful in self-supervised learning. The popular contrastive learning algorithm relies on the invariance of representations between augmented views of the same image [29][30]. More recently, people are exploring ways to use equivariance in contrastive learning [31]. Leveraging equivariance to cropping transform results in dense contrastive learning at pixel-level: PixelPro [32], DenseCL [33], and at region-level: RegionCL , Det-Con [34]. Equivariance to 4-way rotation can be jointly used with the image-level contrastive objective to improve performance [35]. Self-supervised learning from equivariance between flow transformations of the input image is also effective [36] and between matching points for landmark representation learning [37].

These works are especially successful for downstream segmentation and detection tasks. However, the advancements in these work have yet to be thoroughly explored in the state-of-the-art depth or normal predictors to the best of our knowledge [38][39][40], where the dominant paradigm is still supervised training. Inspired by prior work in SSL and segmentation, our work brings in the powerful idea of equivariance to improve state-of-the-art supervised depth and normal predictors.

",1
136," Equivariance in MLEquivariance is tied closely to geometry and symmetry. The entire subject of physics is founded on concepts surrounding symmetry. A wide range of natural phenomena admits equivariance inherently since the underlying mechanism is oftentimes geometrical. As a consequence, a lot of data that machine learning deals with has the equivariance property. For example, camera photography follows simple 3D geometry rules, thus a shift in camera position leads to a shift in the photograph; the molecules and point clouds have translation and rotation symmetry in 3D, thus an SE(3) transform should not change any property. Therefore, it is natural to consider equivariance in developing machine learning models.

Equivariance in 2D computer visionConvolutional neural networks (CNN) for 2D images are shown to have the approximate translation equivariance property due to the nature of convolution . There is a line of work developing rotation equivariant 2D CNNs [1][2][3][4][5]. The transformation group for 2D rotation is the Special Orthogonal group SO(2), and the Special Euclidean group SE(2) if the translation is allowed. The derivation of group equivariance constraint typically results in steerable filters constructed from 2D harmonic bases. The convolution filter weights are parameterized as a linear combination of the harmonic bases.

Equivariance can also be achieved by parameter sharing of the neural net weights [6]. However, this approach is only possible for limited kinds of groups, such as 90-degree rotations. 2D scale equivariant CNN has been studied [7][8][9]. This is typically done by applying the same convolution kernel on several scales or constructing steerable filters from the bases. Equivariant network design method can be generalized to other groups [10][11][12][13] and has rich theory in math and physics [14][15][16]. Equivariance can also be achieved by transforming the data to canonical coordinate systems [17][18][19]. In particular, [18]transforms the data to key canonical frames of the group and averages over those frames, while we average over a random sample of the cropping transform. Transformers are the current state-of-the-art neural net architecture [20]. People have sought to combine Transformer and equivariance, resulting in Lie-Transformer [21].

In terms of applications, there is good evidence that equivariance benefits image semantic segmentation [22][23], object detection to shifting  and rotation of images [24]. Equivariance is also useful for generative modeling, for example, for normalizing flow-based generative models [25][26], and variational autoencoders [27]. Equivariance to rotation is beneficial in digital pathology . Extension to time-equivariance for video is also possible [28].

**Equivariance in self-supervised learning.** Equivariance and invariance are useful in self-supervised learning. The popular contrastive learning algorithm relies on the invariance of representations between augmented views of the same image [29][30]. More recently, people are exploring ways to use equivariance in contrastive learning [31]. Leveraging equivariance to cropping transform results in dense contrastive learning at pixel-level: PixelPro [32], DenseCL [33], and at region-level: RegionCL , Det-Con [34]. Equivariance to 4-way rotation can be jointly used with the image-level contrastive objective to improve performance [35]. Self-supervised learning from equivariance between flow transformations of the input image is also effective [36] and between matching points for landmark representation learning [37].



",0
137," 2D Generative ModelsIn past years, generative adversarial networks (GANs) [1][2][3] and likelihood-based approaches [4][5][6][7] enabled high-resolution photorealistic image synthesis. Due to their quality, GANs are used in a multitude of downstream applications ranging from steerable content creation [8][9][10][11][12] to data driven simulation [13][14][15][9]. Recently, autoregressive models and score-based models, e.g. diffusion models, demonstrate better distribution coverage while preserving high sample quality [16][17][18][19][20][21][22][23][24][25]. Since evaluation and optimization of these approaches in pixel space is computationally expensive, [23][25] apply them to latent space, achieving state-of-the-art image synthesis at megapixel resolution. As our approach operates on 3D scenes, computational efficiency is crucial. Hence, we build upon [23] and train our model in latent space.

Novel View SynthesisIn their seminal work , Mildenhall et al. introduce Neural Radiance Fields (NeRF) as a powerful 3D representation. PixelNeRF [26] and IBRNet [27] propose to condition NeRF on aggregated features from multiple views to enable novel view synthesis from a sparse set of views. Another line of works scale NeRF to large-scale indoor and outdoor scenes [28][29][30][31]. Recently, Nerfusion [31] predicts local radiance fields and fuses them into a scene representation using a recurrent neural network. Similarly, we construct a latent scene representation by aggregating features across multiple views. Different from the aforementioned methods, our approach is a generative model capable of synthesizing novel scenes.

3D Diffusion ModelsA few recent works propose to apply denoising diffusion models (DDM) [19][20][32] on point clouds for 3D shape generation [33][34][35]. While PVD [35] trains on point clouds directly, DPM [33] and LION [34] use a shape latent variable. Similar to LION, we design a hierarchical model by training separate conditional DDMs. However, our approach generates both texture and geometry of a scene without needing 3D ground truth as supervision.

3D-Aware Generative Models3D-aware generative models synthesize images while providing explicit control over the camera pose and potentially other scene properties, like object shape and appearance. SGAM  generates a 3D scene by autoregressively generating sensor data and building a 3D map. Several previous approaches generate NeRFs of single objects with conditional coordinate-based MLPs [36][37][38]. GSN [39] conditions a coordinate-based MLP on a ""floor plan"", i.e. a 2D feature map, to model more complex indoor scenes. EG3D [40] and Vox-GRAF [41] use convolutional backbones to generate 3D representations. All of these approaches rely on adversarial training. Instead, we train a DDM on voxels in latent space. The work closest to ours is GAUDI [42], which first trains an auto-decoder and subsequently trains a DDM on the learned latent codes. Instead of using a global latent code, we encode scenes onto voxel grids and train a hierarchical DDM to optimally combine global and local features.

",1
138," 2D Generative ModelsIn past years, generative adversarial networks (GANs) [1][2][3] and likelihood-based approaches [4][5][6][7] enabled high-resolution photorealistic image synthesis. Due to their quality, GANs are used in a multitude of downstream applications ranging from steerable content creation [8][9][10][11][12] to data driven simulation [13][14][15][9]. Recently, autoregressive models and score-based models, e.g. diffusion models, demonstrate better distribution coverage while preserving high sample quality [16][17][18][19][20][21][22][23][24][25]. Since evaluation and optimization of these approaches in pixel space is computationally expensive, [23][25] apply them to latent space, achieving state-of-the-art image synthesis at megapixel resolution. 

Novel View SynthesisIn their seminal work , Mildenhall et al. introduce Neural Radiance Fields (NeRF) as a powerful 3D representation. PixelNeRF [26] and IBRNet [27] propose to condition NeRF on aggregated features from multiple views to enable novel view synthesis from a sparse set of views. Another line of works scale NeRF to large-scale indoor and outdoor scenes [28][29][30][31]. Recently, Nerfusion [31] predicts local radiance fields and fuses them into a scene representation using a recurrent neural network. 

3D Diffusion ModelsA few recent works propose to apply denoising diffusion models (DDM) [19][20][32] on point clouds for 3D shape generation [33][34][35]. While PVD [35] trains on point clouds directly, DPM [33] and LION [34] use a shape latent variable. 

3D-Aware Generative Models3D-aware generative models synthesize images while providing explicit control over the camera pose and potentially other scene properties, like object shape and appearance. SGAM  generates a 3D scene by autoregressively generating sensor data and building a 3D map. Several previous approaches generate NeRFs of single objects with conditional coordinate-based MLPs [36][37][38]. GSN [39] conditions a coordinate-based MLP on a ""floor plan"", i.e. a 2D feature map, to model more complex indoor scenes. EG3D [40] and Vox-GRAF [41] use convolutional backbones to generate 3D representations. All of these approaches rely on adversarial training. 

",0
139," **Vision-based ego-car (BEV) feature learning** A rising tendency is conducting self-driving downstream tasks [1][2][3][4][5][6][7][8][9] under ego-car coordinate frame. To learn ego-car feature learning from onboard cameras, [10][11] project image features to ego-car coordinate based on depth estimation. OFT [1], LSS  and FIERY [6] predict depth distribution to generate the intermediate 3D representations. [5][12][2][3] resort neural network like Transformer to learn ego-car feature without depth. In order to learn the topology of the road network under the ego-car coordinate frame, we use the straightforward method of applying LSS [4] to extract ego-car features from the multiple onboard cameras.

**Road network extraction** With the emergence of deep learning [13][14][15][16][17][18], researchers in the field have explored the utilization of DNNs to decode and recover maps from aerial images and GPS trajectories [19][20]. Moreover, STSU [21] first detect centerline from front-view image with Transformer and then predict the association between centerline with MLP layer, followed with a final merge to estimate road network. Based on STSU, TPLR [22] introduces minimal cycle to eliminate ambiguity in its connectivity representation. Existing methods spend great effort to deal with problem in non-Euclidean domain but ignore the cooperation between Euclidean and non-Euclidean. However, we create a Euclidean-nonEuclidean unified representation.

**Non-Autoregressive generation** Non-Autoregressive gen eration for Neural Machine Translation (NMT) [23] has been proposed as a solution to speed up the one-by-one sequential generation process of autoregressive models. [24][25][26] utilized knowledge distillation to help NAT model capture target sequence dependency. [27][28][29][30] proposed iterative models that refine the output from the previous iteration or a noised target in a step-wise manner. [31] kept the auto-regressive approach for global modeling, but introduced the parallel output of a few successive words at each time step. [32][33][34][35] used latent variables as intermediates to reduce the dependency on the target sequence. The current state of non-autoregressive NMT models is far from perfect, with a significant gap in performance compared to their auto-regressive counterparts [36]. But in the case of our RoadNet Sequence, the auto-regressive dependency can be decoupled, leading to both acceleration and improved performance.

",1
140," **Vision-based ego-car (BEV) feature learning** A rising tendency is conducting self-driving downstream tasks [1][2][3][4][5][6][7][8][9] under ego-car coordinate frame. To learn ego-car feature learning from onboard cameras, [10][11] project image features to ego-car coordinate based on depth estimation. OFT [1], LSS  and FIERY [6] predict depth distribution to generate the intermediate 3D representations. [5][12][2][3] resort neural network like Transformer to learn ego-car feature without depth. In order to learn the topology of the road network under the ego-car coordinate frame, we use the straightforward method of applying LSS [4] to extract ego-car features from the multiple onboard cameras.

**Road network extraction** With the emergence of deep learning [13][14][15][16][17][18], researchers in the field have explored the utilization of DNNs to decode and recover maps from aerial images and GPS trajectories [19][20]. Moreover, STSU [21] first detect centerline from front-view image with Transformer and then predict the association between centerline with MLP layer, followed with a final merge to estimate road network. Based on STSU, TPLR [22] introduces minimal cycle to eliminate ambiguity in its connectivity representation. Existing methods spend great effort to deal with problem in non-Euclidean domain but ignore the cooperation between Euclidean and non-Euclidean. 

**Non-Autoregressive generation** Non-Autoregressive gen eration for Neural Machine Translation (NMT) [23] has been proposed as a solution to speed up the one-by-one sequential generation process of autoregressive models. [24][25][26] utilized knowledge distillation to help NAT model capture target sequence dependency. [27][28][29][30] proposed iterative models that refine the output from the previous iteration or a noised target in a step-wise manner. [31] kept the auto-regressive approach for global modeling, but introduced the parallel output of a few successive words at each time step. [32][33][34][35] used latent variables as intermediates to reduce the dependency on the target sequence. The current state of non-autoregressive NMT models is far from perfect, with a significant gap in performance compared to their auto-regressive counterparts [36]. But in the case of our RoadNet Sequence, the auto-regressive dependency can be decoupled, leading to both acceleration and improved performance.

",0
141," **Semantic-based Visual Localization.** Semantic segmentation is used in structure-based localization methods as a way to facilitate feature selection or matching [1][2][3], to filter 2D-3D matches by maximizing the semantic consistency between 2D images and 3D models [4][5] or to improve keypoint tracking . In these works, the pre-trained segmentation model is used mainly to filter matches or to improve SfM/VO, hence they still rely on keypoints descriptors. Similar to FGSN [4], our model learns robust image segmentation in a self-supervised manner exploiting label consistency between matched keypoints. Contrary to FGSN, our model provides both global and local representations, resulting in a full localization pipeline.

**Semantics-based retrieval and pose approximation.** To cope with extreme environmental, seasonal, and illumination changes in place recognition and image retrieval, several methods leverage image-to-image translation to handle the domain shift between the database and query images [6][7][8][9][10][11]. Other methods directly aim at obtaining image representation by leveraging weather-invariant semantic [12][13][14][15] or geometric information [16]. In particular, [14] describes images through histograms of semantic classes from pre-trained semantic segmentation, while LoST [13] performs a semantic-based pooling of convolutional features. Closest to our work, [7] and  train global representations within a deep metric learning framework and utilize semantic segmentation as an auxiliary task to infuse semantic information. Instead, we learn a finite set of (not necessarily semantic) classes to perform image segmentation from which we build local and global representations.

**Pose refinement.** Pose refinement approaches obtain an accurate camera pose estimate from an initial approximate pose via image alignment. In contrast to early methods based on handcrafted features [17] or pixel intensities [18], more recent methods learn deep features through direct feature alignment suitable for such pose refinement [19][20] or cast the camera pose localization as a metric learning problem. [14] proposed a semantic-based pose refinement relying on a pre-trained model, handcrafted global descriptors, and a geometric prior. Our pose refinement is inspired by PixLoc [19], except that instead of using multi-scale deep features, we align 1D features (labels) by minimizing a reprojection error as a function of label inconsistency.

**Privacy-preserving localization.** Storing the 3D maps on the cloud or sending images or descriptors from mobile devices to a server raise the question of privacy. As shown in [21], detailed and recognizable images of the scene can be obtained from sparse 3D point clouds with local descriptors. Geometry-based matching methods [22] do not rely on visual descriptors to localize, hence they are less subject to privacy issues. Still, [21] shows that depth and color are sometimes sufficient to recover details in a scene. Learning-based pose regression or scene point regression models [23][24] do not explicitly store the 3D map and, thus, partially avoid the privacy issues. Yet, according to [25], since these models memorize the scenes quite well, model inversion is often possible. Given a set of pre-selected scene landmarks, [26] learns to detect them and to regress the associated bearing vectors used by geometric camera pose estimation. However, this method still faces the same scaling issues as regression methods.

To address privacy, [27] propose to transform 3D point clouds into 3D line clouds, thus obfuscating the scene geometry. However, according to [28], a significant amount of information about the scene geometry is preserved in these line clouds, allowing to (approximately) recover image content.  propose a cloud-based mapping solution to preserve the privacy of users by hiding critical content of the input images. As the recovered pose may also be considered as sensitive, [29] perform a partial estimation of a 3DoF pose on a single dimension against a partial map. These partials maps are distributed on distinct servers so that the 6DoF pose can only be recovered on the user side. However, they do not tackle the privacy of the partial maps directly. In addition, these approaches do not consider private information contained in the query images, which could,, allow an attacker to track individuals and to study their behavior. Concerning privacy preservation of the query, [30][31] show that it is possible to reconstruct the original image from local image features. To address this, [32] propose to obfuscate the appearance of the original image by lifting the descriptors to affine subspaces. [33] propose to replace 2D points in the query image with randomly oriented 2D lines passing through the given point. This allows them to address privacy of both the query and the map. By relying on class labels, where only labels are kept in the map and hence making it impossible to recover fine details that could reveal private information, our SegLoc representations jointly tackles query and map privacy.

",1
142," **Semantic-based Visual Localization.** Semantic segmentation is used in structure-based localization methods as a way to facilitate feature selection or matching [1][2][3], to filter 2D-3D matches by maximizing the semantic consistency between 2D images and 3D models [4][5] or to improve keypoint tracking . In these works, the pre-trained segmentation model is used mainly to filter matches or to improve SfM/VO, hence they still rely on keypoints descriptors. 

**Semantics-based retrieval and pose approximation.** To cope with extreme environmental, seasonal, and illumination changes in place recognition and image retrieval, several methods leverage image-to-image translation to handle the domain shift between the database and query images [6][7][8][9][10][11]. Other methods directly aim at obtaining image representation by leveraging weather-invariant semantic [12][13][14][15] or geometric information [16]. In particular, [14] describes images through histograms of semantic classes from pre-trained semantic segmentation, while LoST [13] performs a semantic-based pooling of convolutional features. Closest to our work, [7] and  train global representations within a deep metric learning framework and utilize semantic segmentation as an auxiliary task to infuse semantic information. 

**Pose refinement.** Pose refinement approaches obtain an accurate camera pose estimate from an initial approximate pose via image alignment. In contrast to early methods based on handcrafted features [17] or pixel intensities [18], more recent methods learn deep features through direct feature alignment suitable for such pose refinement [19][20] or cast the camera pose localization as a metric learning problem. [14] proposed a semantic-based pose refinement relying on a pre-trained model, handcrafted global descriptors, and a geometric prior. 

**Privacy-preserving localization.** Storing the 3D maps on the cloud or sending images or descriptors from mobile devices to a server raise the question of privacy. As shown in [21], detailed and recognizable images of the scene can be obtained from sparse 3D point clouds with local descriptors. Geometry-based matching methods [22] do not rely on visual descriptors to localize, hence they are less subject to privacy issues. Still, [21] shows that depth and color are sometimes sufficient to recover details in a scene. Learning-based pose regression or scene point regression models [23][24] do not explicitly store the 3D map and, thus, partially avoid the privacy issues. Yet, according to [25], since these models memorize the scenes quite well, model inversion is often possible. Given a set of pre-selected scene landmarks, [26] learns to detect them and to regress the associated bearing vectors used by geometric camera pose estimation. However, this method still faces the same scaling issues as regression methods.

To address privacy, [27] propose to transform 3D point clouds into 3D line clouds, thus obfuscating the scene geometry. However, according to [28], a significant amount of information about the scene geometry is preserved in these line clouds, allowing to (approximately) recover image content.  propose a cloud-based mapping solution to preserve the privacy of users by hiding critical content of the input images. As the recovered pose may also be considered as sensitive, [29] perform a partial estimation of a 3DoF pose on a single dimension against a partial map. These partials maps are distributed on distinct servers so that the 6DoF pose can only be recovered on the user side. However, they do not tackle the privacy of the partial maps directly. In addition, these approaches do not consider private information contained in the query images, which could,, allow an attacker to track individuals and to study their behavior. Concerning privacy preservation of the query, [30][31] show that it is possible to reconstruct the original image from local image features. To address this, [32] propose to obfuscate the appearance of the original image by lifting the descriptors to affine subspaces. [33] propose to replace 2D points in the query image with randomly oriented 2D lines passing through the given point. This allows them to address privacy of both the query and the map. 

",0
143," **Self-supervised skeleton-based action recognition.** To meet the requirements of real-time recognition, it's necessary for the models to be able to directly extract dis criminative action representations from the label-free online videos. Recent progress in self-supervised skeleton-based action recognition can be summarized into two categories: pretext-task based learning [1][2][3][4][5] and contrastive learning [6][7][8][9][10]. For example, Zheng et al. [4] design a skeleton inpainting architecture to learn the long-term dynamics. Lin et al. [1] integrate multiple tasks such as jigsaw puzzle recognition to learn more general skeleton features. Xu et al. [2] propose reverse sequential predictions based on encoder-decoder structure to extract motion pattern. However, the representations learned by these methods may not be good enough, in the sense that they could be exclusively particular to the pre-designed tasks. Inspired by the success of contrastive learning in image classification (e.g., instance discrimination [11], SimCLR [12], and MoCo [13]), Rao et al.  first propose to perform contrastive learning among different augmentation-s of unlabeled skeleton data, to learn inherent action patterns. Thoker et al. [8] propose to generate contrastive pairs based on different input-representations of the skeleton sequences, i.e., graph, sequence, and image representation. By leveraging multiple views of the skeleton data, i.e., joint, bone, and motion, Li et al. [6] introduce SkeletonCLR and CrosSCLR to perform the single-view and cross-view contrastive learning. Guo et al. [7] further propose AimCLR to learn from extremely augmented skeleton sequences.

Nevertheless, none of the aforementioned methods concentrate on visual tempo, which is crucial for characterizing human action dynamics. To our best knowledge, Su et al. [10] propose motion consistency and continuity learning, which has overlap with our framework. However, the differences are obvious, which mainly lie in three aspects: (1) contrastive pairs are generated differently. In [10], speed-changed clips are considered as positive pairs, while we focus on relative visual tempo and regard these clips as negative pairs. (2) appearance information is modeled differently. In [10], appearance information is implicitly modeled in the contrastive learning process, while we design another Appearance-Consistency (AC) task specifically for spatial modeling. (3) learned features are enhanced differently. In [10], learned features are enhanced by designing a self-reconstruction based motion continuity module, while we introduce a novel Distribution-Consistency (DC) branch to guide the models focus on high-order semantics.

**Visual tempo modeling.** Visual tempo describes the speed of human movements, which has already been applied into various action recognition methods [14][15][16]. For example, Feichtenhofer et al. [14] first propose SlowFast network to explore the potential of different visual tempos. It consists of two pathways, operating at different frame rates, to capture both spatial semantics and motion dynamics. When it comes to video self-supervised learning [17][18], the potential of visual tempo in motion modeling is further verified. Yao et al. [17] utilize video playback rates as self-supervision signals and propose playback rate perception to learn spatiotemporal features in a collaborative discrimination-generation manner. The above methods usually require assigning visual tempos to each video clip according to different sampling rates, and then elaborate learning paradigms through reconstruction or prediction, but this is suboptimal in learning discriminative representations. Chen et al. [18] argue that relative speed is more in line with motion pattern. They propose a new video self-supervised learning framework (called RSPNet) to leverage the relative speed between two video clips to supervise the representation learning. Inspired by RSPNet, the proposed RVTCLR focuses on relative visual tempo learning with skeleton-specific modifications.

",1
144," **Self-supervised skeleton-based action recognition.** To meet the requirements of real-time recognition, it's necessary for the models to be able to directly extract dis criminative action representations from the label-free online videos. Recent progress in self-supervised skeleton-based action recognition can be summarized into two categories: pretext-task based learning [1][2][3][4][5] and contrastive learning [6][7][8][9][10]. For example, Zheng et al. [4] design a skeleton inpainting architecture to learn the long-term dynamics. Lin et al. [1] integrate multiple tasks such as jigsaw puzzle recognition to learn more general skeleton features. Xu et al. [2] propose reverse sequential predictions based on encoder-decoder structure to extract motion pattern. However, the representations learned by these methods may not be good enough, in the sense that they could be exclusively particular to the pre-designed tasks. Inspired by the success of contrastive learning in image classification (e.g., instance discrimination [11], SimCLR [12], and MoCo [13]), Rao et al.  first propose to perform contrastive learning among different augmentation-s of unlabeled skeleton data, to learn inherent action patterns. Thoker et al. [8] propose to generate contrastive pairs based on different input-representations of the skeleton sequences, i.e., graph, sequence, and image representation. By leveraging multiple views of the skeleton data, i.e., joint, bone, and motion, Li et al. [6] introduce SkeletonCLR and CrosSCLR to perform the single-view and cross-view contrastive learning. Guo et al. [7] further propose AimCLR to learn from extremely augmented skeleton sequences.

 

**Visual tempo modeling.** Visual tempo describes the speed of human movements, which has already been applied into various action recognition methods [14][15][16]. For example, Feichtenhofer et al. [14] first propose SlowFast network to explore the potential of different visual tempos. It consists of two pathways, operating at different frame rates, to capture both spatial semantics and motion dynamics. When it comes to video self-supervised learning [17][18], the potential of visual tempo in motion modeling is further verified. Yao et al. [17] utilize video playback rates as self-supervision signals and propose playback rate perception to learn spatiotemporal features in a collaborative discrimination-generation manner. The above methods usually require assigning visual tempos to each video clip according to different sampling rates, and then elaborate learning paradigms through reconstruction or prediction, but this is suboptimal in learning discriminative representations. Chen et al. [18] argue that relative speed is more in line with motion pattern. They propose a new video self-supervised learning framework (called RSPNet) to leverage the relative speed between two video clips to supervise the representation learning. Inspired by RSPNet, the proposed RVTCLR focuses on relative visual tempo learning with skeleton-specific modifications.

",0
145," **Adversarial Attacks.** Adversarial attacks reveal the vulnerability of current DNNs [1]. The classic adversarial attack methods are gradient-based, such as FGSM [2] and I-FGSM [3]. C&W [4] considers optimizing the distance between adversarial examples and benign samples, and proposed optimization-based attacks. Adversarial attacks can also be performed in the physical world [5][6]. As for defending against adversarial examples, adversarial training is a popular defense method that uses adversarial examples as extra training data to improve robustness [7].

**Increasing Attack Transferability.** An intriguing property of adversarial attacks is the transferability. Ensemble-based attack  uses multiple surrogate networks instead of one network. Ghost networks [8] generates different surrogate networks by perturbing skip connection and dropout layers. Optimization methods, such as MI [9], uses a momentum-based optimization, while VT [10] introduces gradient variance to control the stability of the localized gradients. RAP [11] generates adversarial examples located in a flat loss region. Data augmentation methods, such as DI [12], uses image transformation like resizing and padding, while TI [13] considers translating image pixels. SI [14] calculates gradients with the help of several scaled benign samples. Admix [15] calculates iterative gradients by mixing the benign images with randomly sampled images.

Various network architectures and features exhibit different relationships with adversarial attacks. DNNs' linearity is believed to cause adversarial vulnerability [2], and LinBP [16] skips the nonlinear activation during the backpropagation. SGM [17] uses more gradients through skip connections in residual networks. To better leverage the intermediate layers, one can train auxiliary classifiers based on feature spaces [18][19], maximize the distance between natural images and their adversarial examples in feature spaces , or fine-tune the existing adversarial examples in intermediate layer level by ILA [20].

**Style Transfer and Instance Normalization.** Style transfer can change the style of an image to match the style of another one [21][22]. Fast feedforward networks can perform stylization with arbitrary styles in a single forward pass [23][24]. Interestingly, style transfer has a wide range of applications. AdvCam [25] uses natural styles to hide non-\(L_{p}\) restricted perturbations. FSA [26] generates natural-looking adversarial examples by using optimized style changes. Style transfer has also been used to improve network robustness by exploring additional feature information [27]. Latent style transformations can detect adversarial attacks [28]. AMT-GAN [29] proposes an adversarial makeup transfer to protect facial privacy by preserving stronger black-box transferability.

The family of instance normalization (IN) including batch normalization [30], layer normalization [31], instance normalization [32], and group normalization . Normalizations are mainly used to reduce the covariate shift, and speed model training. Recently, normalizations have been found to be related to robustness. It has been shown that batch normalization makes DNNs use more non-robust but useful features [33][34]. AdvBN proposed adding an extra batch normalization into network training to increase training loss adversarially, which enables the network to resist various domain shifts [35]. Adjusting batch normalization statistics such as the running mean and variance in the inference phase, which are estimated during training, improves robustness and defense common corruption [36][37].

Among existing style-based attacks, FSA [26] differentiates style features and content features, which is similar to our method. However, there are three significant differences between FSA and our approach: 1) FSA proposes to hide adversarial perturbations in the optimized style, while we avoid relying on any style. 2) FSA aims at enhancing the natural looking of non-\(\ell_{p}\) restricted attacks, while we focus on the transferability of \(\ell_{p}\) restricted adversarial examples. 3) Both FSA and our work are inspired by AdaIN [23], but we use IN layer differently. FSA perturbs the IN layer to search malicious styles and requires a decoder. But we use randomized IN layers to augment attacks and don't need to train a decoder.

",1
146," **Adversarial Attacks.** Adversarial attacks reveal the vulnerability of current DNNs [1]. The classic adversarial attack methods are gradient-based, such as FGSM [2] and I-FGSM [3]. C&W [4] considers optimizing the distance between adversarial examples and benign samples, and proposed optimization-based attacks. Adversarial attacks can also be performed in the physical world [5][6]. As for defending against adversarial examples, adversarial training is a popular defense method that uses adversarial examples as extra training data to improve robustness [7].

**Increasing Attack Transferability.** An intriguing property of adversarial attacks is the transferability. Ensemble-based attack  uses multiple surrogate networks instead of one network. Ghost networks [8] generates different surrogate networks by perturbing skip connection and dropout layers. Optimization methods, such as MI [9], uses a momentum-based optimization, while VT [10] introduces gradient variance to control the stability of the localized gradients. RAP [11] generates adversarial examples located in a flat loss region. Data augmentation methods, such as DI [12], uses image transformation like resizing and padding, while TI [13] considers translating image pixels. SI [14] calculates gradients with the help of several scaled benign samples. Admix [15] calculates iterative gradients by mixing the benign images with randomly sampled images.

Various network architectures and features exhibit different relationships with adversarial attacks. DNNs' linearity is believed to cause adversarial vulnerability [2], and LinBP [16] skips the nonlinear activation during the backpropagation. SGM [17] uses more gradients through skip connections in residual networks. To better leverage the intermediate layers, one can train auxiliary classifiers based on feature spaces [18][19], maximize the distance between natural images and their adversarial examples in feature spaces , or fine-tune the existing adversarial examples in intermediate layer level by ILA [20].

**Style Transfer and Instance Normalization.** Style transfer can change the style of an image to match the style of another one [21][22]. Fast feedforward networks can perform stylization with arbitrary styles in a single forward pass [23][24]. Interestingly, style transfer has a wide range of applications. AdvCam [25] uses natural styles to hide non-\(L_{p}\) restricted perturbations. FSA [26] generates natural-looking adversarial examples by using optimized style changes. Style transfer has also been used to improve network robustness by exploring additional feature information [27]. Latent style transformations can detect adversarial attacks [28]. AMT-GAN [29] proposes an adversarial makeup transfer to protect facial privacy by preserving stronger black-box transferability.

The family of instance normalization (IN) including batch normalization [30], layer normalization [31], instance normalization [32], and group normalization . Normalizations are mainly used to reduce the covariate shift, and speed model training. Recently, normalizations have been found to be related to robustness. It has been shown that batch normalization makes DNNs use more non-robust but useful features [33][34]. AdvBN proposed adding an extra batch normalization into network training to increase training loss adversarially, which enables the network to resist various domain shifts [35]. Adjusting batch normalization statistics such as the running mean and variance in the inference phase, which are estimated during training, improves robustness and defense common corruption [36][37].

 

",0
147," **Attribute-Based CTG** focuses on generating sentences containing pre-specified attributes, such as sentiment and topic. As a vital demand for intelligent writing ([2]), existing efforts include fine-tuning PLMs and utilizing extra attribute classifiers. The first type usually fine-tunes separately and stores a full copy of PLM for each desirable attribute ([1]). To alleviate the storage problem, CTRL () provides 55 kinds of control codes (_i.e._, special keywords) to fine-tune one PLM for generating sentences of various styles. StylePTB () also proposes several style transfer tokens (_i.e._, a sequence of numbers) to guide a GPT-2 ([4]) to multiple styles transfer. GSum () introduces four guidance signals (_e.g._, keywords and relations) to enhance the controllability of PLMs in text summarization. Although they make successful attempts in attribute-based CTG, re-training whole PLMs could be expensive ([3]). To improve the flexibility and extensibility of the CTG model, the second type makes efforts in the inference stage. In short, utilizing extra attribute classifiers to guide PLMs in each generating step. PPLM () iteratively modifies latent representations of a GPT-2 referring to the gradient of attribute classifiers, yet notably increasing the inference time. To solve this problem, Fudge ([3]) uses an attribute predictor to adjust the output probabilities of a PLM. Similarly, GeDi () uses smaller PLMs as generative discriminators to hint a larger PLM generating sentences that satisfy desirableattributes. Despite their progress, the fluency of generating sentences tends to decrease compared with the original PLM (see SS 4.2) and extra inference time costs still existed. In comparison, utilizing Tailor, PLMs can benefit from the manner of controllability on single-attribute prompt combinations, with a negligible decrease on text quality.

**Prompt Learning** is a new paradigm in NLP summarised as ""Pre-train, Prompt and Predict"" (). In short, it guides a single PLM to solve various downstream tasks by reformulating these tasks into a text-to-text manner. Recently, the continuous prompt has attracted attention (; ; ), which usually forms as a set of continuous task-specific vectors to the input. Despite their encouraging progress, the prompt composition is rarely explored but undoubtedly important in prompt learning. In that case, a composable task could be accomplished by composing various subtasks with multiple sub-prompts (). To achieve it, PTR () introduces manual sub-prompts for entity recognition and relation classification, respectively. Then, these two kinds of prompts are composed by logic rules as a complete prompt for the relation extraction task. Unfortunately, the composition of continuous prompts is rarely explored yet has demonstrated great potential ([5]). The main difference between contrastive prefix [5] and Tailor is that the former needs attribute data to be occurred contrastively (e.g, positive and negative attribute data must be available at the same time), which might be limited for the single attribute. For multi-attribute, contrastive prefix trains a new prompt (twice the size of their single prompt) for each combination. Instead of it, Tailor only trains an extra prompt connector to enhance the combinations of single prompts. It can act as an efficient plug-and-play manner with extremely low training parameters to attribute-based CTG.

",1
148," **Attribute-Based CTG** focuses on generating sentences containing pre-specified attributes, such as sentiment and topic. As a vital demand for intelligent writing ([2]), existing efforts include fine-tuning PLMs and utilizing extra attribute classifiers. The first type usually fine-tunes separately and stores a full copy of PLM for each desirable attribute ([1]). To alleviate the storage problem, CTRL () provides 55 kinds of control codes (_i.e._, special keywords) to fine-tune one PLM for generating sentences of various styles. StylePTB () also proposes several style transfer tokens (_i.e._, a sequence of numbers) to guide a GPT-2 ([4]) to multiple styles transfer. GSum () introduces four guidance signals (_e.g._, keywords and relations) to enhance the controllability of PLMs in text summarization. Although they make successful attempts in attribute-based CTG, re-training whole PLMs could be expensive ([3]). To improve the flexibility and extensibility of the CTG model, the second type makes efforts in the inference stage. In short, utilizing extra attribute classifiers to guide PLMs in each generating step. PPLM () iteratively modifies latent representations of a GPT-2 referring to the gradient of attribute classifiers, yet notably increasing the inference time. To solve this problem, Fudge ([3]) uses an attribute predictor to adjust the output probabilities of a PLM. Similarly, GeDi () uses smaller PLMs as generative discriminators to hint a larger PLM generating sentences that satisfy desirableattributes. Despite their progress, the fluency of generating sentences tends to decrease compared with the original PLM (see SS 4.2) and extra inference time costs still existed. 

**Prompt Learning** is a new paradigm in NLP summarised as ""Pre-train, Prompt and Predict"" (). In short, it guides a single PLM to solve various downstream tasks by reformulating these tasks into a text-to-text manner. Recently, the continuous prompt has attracted attention (; ; ), which usually forms as a set of continuous task-specific vectors to the input. Despite their encouraging progress, the prompt composition is rarely explored but undoubtedly important in prompt learning. In that case, a composable task could be accomplished by composing various subtasks with multiple sub-prompts (). To achieve it, PTR () introduces manual sub-prompts for entity recognition and relation classification, respectively. Then, these two kinds of prompts are composed by logic rules as a complete prompt for the relation extraction task. Unfortunately, the composition of continuous prompts is rarely explored yet has demonstrated great potential ([5]). The main difference between contrastive prefix [5] and Tailor is that the former needs attribute data to be occurred contrastively (e.g, positive and negative attribute data must be available at the same time), which might be limited for the single attribute. For multi-attribute, contrastive prefix trains a new prompt (twice the size of their single prompt) for each combination.  

",0
149," Several important lines of work have considered the symmetries of the parametric representations of deep ReLU networks and their implications for learning. One focus area has been in designing optimization methods for neural networks that are invariant to scaling symmetries at individual neurons. Approaches for achieving this goal have include path normalization ([1]), manifold optimization (), proceeding in a different vector space ([2]), and projection onto a normalized manifold ().

Another fruitful direction of work has been in understanding how permutation symmetries in parameter space affect connectivity of the loss landscape. [4] consider when different parameter permutations of a trained network are connected via piecewise linear paths in parameter space with low loss.  show that linearly interpolating between different permutations of a network leads to flat regions of the loss landscape. Several recent works have shown that, if permutation symmetries are taken into account, then it is possible to interpolate between networks trained from different initializations, while maintaining a low loss barrier (; ; [3]).

In Armenta & , the authors define and study the _moduli space_ of neural network functions using quiver representation theory. This theory provides a framework for extracting global symmetries of parameter space of a network architecture from symmetries of the computational graph and the activation functions involved (see also Ganev & ), ; [5] build on these ideas to define _neural teleportation_ algorithms aimed at using symmetries in the loss landscape to improve the efficiency of gradient descent in finding a minimal-loss solution. In [6] the authors argue that symmetries in the loss landscape have associated conserved quantities that impact training dynamics.

A number of works have explicitly considered which symmetries are admitted by different ReLU networks. A number of authors consider the relationship between the parameters of a ReLU network and the geometry of its _bent hyperplane arrangement_ (aka _fold set_); [7], Rolnick & , and  use these properties to reverse-engineer the parameters of certain networks up to permutation and scaling symmetries, and Bui Thi Mai &  proves that for certain architectures there exist parameter settings without hidden symmetries. In particular, in Rolnick & , the authors provide a geometric condition on the bent hyperplane arrangement of a parameter ensuring that the parameters can be reverse-engineered up to permutation and scaling, hence has no hidden symmetries. It follows nearly immediately that all depth 2 networks and a positive measure subset of any depth 3 network have no hidden symmetries. In Bui Thi Mai & , the authors prove that a positive measure subset of parameters in every non-widening (\(n_{0}\geq n_{1}\geq\ldots n_{d}\)) architecture has no hidden symmetries. In the present work, we prove the complementary result that a positive measure subset of parameters in every architecture whose hidden layers are at least as wide as the input layer (that is, \(n_{0}\leq n_{\ell}\) for all \(\ell<d\)) has no hidden symmetries. An example of a family of architectures for which the question of the existence of parameters without hidden symmetries remains unresolved after the present work is an architecture of the form (\(n_{0},n_{1},n_{2},n_{3},n_{4}\)) with \(n_{0}<n_{1}\) and \(n_{2}<n_{0}\).

[8] study the _functional dimension_ of a network parameter setting - the dimension of the space of functions that can be achieved by infinitesimally perturbing the parameters - proving an upper bound on functional dimension that we conjecture is achieved for almost all parameter settings without hidden symmetries (cf. Lemma F.6).

",1
150," Several important lines of work have considered the symmetries of the parametric representations of deep ReLU networks and their implications for learning. One focus area has been in designing optimization methods for neural networks that are invariant to scaling symmetries at individual neurons. Approaches for achieving this goal have include path normalization ([1]), manifold optimization (), proceeding in a different vector space ([2]), and projection onto a normalized manifold ().

Another fruitful direction of work has been in understanding how permutation symmetries in parameter space affect connectivity of the loss landscape. [4] consider when different parameter permutations of a trained network are connected via piecewise linear paths in parameter space with low loss.  show that linearly interpolating between different permutations of a network leads to flat regions of the loss landscape. Several recent works have shown that, if permutation symmetries are taken into account, then it is possible to interpolate between networks trained from different initializations, while maintaining a low loss barrier (; ; [3]).

In Armenta & , the authors define and study the _moduli space_ of neural network functions using quiver representation theory. This theory provides a framework for extracting global symmetries of parameter space of a network architecture from symmetries of the computational graph and the activation functions involved (see also Ganev & ), ; [5] build on these ideas to define _neural teleportation_ algorithms aimed at using symmetries in the loss landscape to improve the efficiency of gradient descent in finding a minimal-loss solution. In [6] the authors argue that symmetries in the loss landscape have associated conserved quantities that impact training dynamics.

A number of works have explicitly considered which symmetries are admitted by different ReLU networks. A number of authors consider the relationship between the parameters of a ReLU network and the geometry of its _bent hyperplane arrangement_ (aka _fold set_); [7], Rolnick & , and  use these properties to reverse-engineer the parameters of certain networks up to permutation and scaling symmetries, and Bui Thi Mai &  proves that for certain architectures there exist parameter settings without hidden symmetries. In particular, in Rolnick & , the authors provide a geometric condition on the bent hyperplane arrangement of a parameter ensuring that the parameters can be reverse-engineered up to permutation and scaling, hence has no hidden symmetries. It follows nearly immediately that all depth 2 networks and a positive measure subset of any depth 3 network have no hidden symmetries. In Bui Thi Mai & , the authors prove that a positive measure subset of parameters in every non-widening (\(n_{0}\geq n_{1}\geq\ldots n_{d}\)) architecture has no hidden symmetries. 

[8] study the _functional dimension_ of a network parameter setting - the dimension of the space of functions that can be achieved by infinitesimally perturbing the parameters - proving an upper bound on functional dimension that we conjecture is achieved for almost all parameter settings without hidden symmetries (cf. Lemma F.6).

",0
151," To achieve the desired properties of a well-qualified AIF solution, related work from three aspects need to be discussed: _(i)_ **visual emotion analysis**, which plays an important role in producing corresponding concrete visualization with extracted emotional features and measuring whether the synthesized results could evoke specific emotional responses from human observers; _(ii)_ **style transfer**, which aims to create aesthetically pleasing images based on a reference of colors and textures, while maintaining the visual content of user-provided content images, thereby sharing similar goals with the AIF task; and _(iii)_ **vision transformer**, which has been demonstrated effective in modeling global token-to-token relationship between multi-modal inputs, making it a promising approach for interacting with user-provided images and texts in the AIF task.

Computer vision is increasingly focused on understanding emotion in context for more than two decades [1]. Before the advent of deep learning models, researchers developed a variety of handcrafted features for analyzing affective images [2][3][4][5][6][7], which are typically vulnerable and difficult to generalize to out-of-distribution scenarios. This situation has been improved since neural networks are used to adaptively predict emotions [8][9][10], where models could extract multi-grained emotional features  or focus on local image regions [11]. On the basis of previous works [12][13], we could design novel constraints to learning the inherent semantics of emotions and makes synthesized results favorable by human observers.

Early works adopt the optimization-based method [14] or design end-to-end models  to achieve style transfer for one specific style. To improve the efficiency of style transfer, researchers explore the approach to train multiple styles in one model [15][16][17], and further propose the first arbitrary style transfer model [18]. Using self-attention mechanisms to build long-range dependencies between regions [19][20] has received considerable attention in numerous studies devoted to improving the performance of arbitrary style transfer models [21][22][23][24]. Following them, StyTr2[25] further adopts the transformer architecture to extract and maintain the global information of in put images. Recently, researchers have attempted to replace the reference image with semantic textures of text to provide more flexible and user-friendly style guidance [26]. Compared to AIF algorithm, which is required to accurately understand _visually-abstract emotions_ from the text, image style transfer methods aim to create images based on a reference of _visually-concrete colors and textures_.

Transformer [27] has gained significant attention and popularity in recent years, leading to the development of unique feature fusion mechanisms for cross-modality tasks, _e.g_., text-to-image generation [28][29], visual grounding [30][31], and referring segmentation [32][33]. Meanwhile, researchers have explored the use of pure vision transformer models for a wide range of vision applications to achieve better performance, _e.g_. image classification [34], object detection [35], and semantic segmentation [36][37]. Great efforts have also been made to adapt vision transformer models to low-level vision problems, _e.g_., inpainting [38][39], super-resolution [40][41], and colorization [42]. For better concrete visualization of visually-abstract emotions and an in-depth understanding of the inherent properties of emotional words, we adopt the transformer architecture to interact with cross-modal inputs.

",1
152," To achieve the desired properties of a well-qualified AIF solution, related work from three aspects need to be discussed: _(i)_ **visual emotion analysis**, which plays an important role in producing corresponding concrete visualization with extracted emotional features and measuring whether the synthesized results could evoke specific emotional responses from human observers; _(ii)_ **style transfer**, which aims to create aesthetically pleasing images based on a reference of colors and textures, while maintaining the visual content of user-provided content images, thereby sharing similar goals with the AIF task; and _(iii)_ **vision transformer**, which has been demonstrated effective in modeling global token-to-token relationship between multi-modal inputs, making it a promising approach for interacting with user-provided images and texts in the AIF task.

Computer vision is increasingly focused on understanding emotion in context for more than two decades [1]. Before the advent of deep learning models, researchers developed a variety of handcrafted features for analyzing affective images [2][3][4][5][6][7], which are typically vulnerable and difficult to generalize to out-of-distribution scenarios. This situation has been improved since neural networks are used to adaptively predict emotions [8][9][10], where models could extract multi-grained emotional features  or focus on local image regions [11]. 

Early works adopt the optimization-based method [14] or design end-to-end models  to achieve style transfer for one specific style. To improve the efficiency of style transfer, researchers explore the approach to train multiple styles in one model [15][16][17], and further propose the first arbitrary style transfer model [18]. Using self-attention mechanisms to build long-range dependencies between regions [19][20] has received considerable attention in numerous studies devoted to improving the performance of arbitrary style transfer models [21][22][23][24]. Following them, StyTr2[25] further adopts the transformer architecture to extract and maintain the global information of in put images. Recently, researchers have attempted to replace the reference image with semantic textures of text to provide more flexible and user-friendly style guidance [26]. Compared to AIF algorithm, which is required to accurately understand _visually-abstract emotions_ from the text, image style transfer methods aim to create images based on a reference of _visually-concrete colors and textures_.

Transformer [27] has gained significant attention and popularity in recent years, leading to the development of unique feature fusion mechanisms for cross-modality tasks, _e.g_., text-to-image generation [28][29], visual grounding [30][31], and referring segmentation [32][33]. Meanwhile, researchers have explored the use of pure vision transformer models for a wide range of vision applications to achieve better performance, _e.g_. image classification [34], object detection [35], and semantic segmentation [36][37]. Great efforts have also been made to adapt vision transformer models to low-level vision problems, _e.g_., inpainting [38][39], super-resolution [40][41], and colorization [42]. 

",0
153," **Domain adaptation (DA).** DA assumes a setting in which labelled data is available for a source domain, and unlabelled data for a target domain. The goal is to maximize performance on the target domain. DA methods can be roughly divided into three types ([4]): _domain-invariant training_ (also called _feature alignment_) aims to ensure that the features generated by the model for the source and target domain are indistinguishable by some metric ([1]; ; [7]; [6]; ; [2]; [8]; [3]); _self-training_ involves generating pseudo-labels for the unlabelled data ([5]); and _self-supervision_ involves training an unsupervised/self-supervised model, later fine-tuned or jointly trained with supervision ().

**Source-Free Domain Adaptation (SFDA) and Test-time Adaptation (TTA).** These methods additionally assume that the source data itself is not available, e.g., because of resource, privacy, or intellectual property concerns. The distinction between SFDA and TTA is subtle: the latter is transductive, motivated by an online setup where adaptation happens on (unlabelled) target examples as they appear and evaluation is subsequently performed on the same examples. SFDA considers an offline adaptation phase and the adapted model is then evaluated on held-out examples. In practice, though, the methods developed for either are similar enough to be applicable to both. Related problems also include black-box (), online ([9]), continual ([10]), and universal ([11]) source-free domain adaptation.

Of the three types of DA methods discussed above, self-training most easily transfers to the SFDA and TTA settings ([12]; [17]), and we focus on this category since it's also the most generalizable to new modalities. Other methods use output prediction uncertainty for adaptation ([9]; [15]; [14]) or generative training to transform target domain examples or synthesize new ones (; ; [16]; [13]; [18]). Interestingly,  show that previous methods suffer from large hyperparameter sensitivity, and may _degrade_ the performance of the source model if not tuned in a scenario-specific manner; this violates the assumption that labelled target data is unavailable.

**Test-time Training (TTT).** TTT ([19]) is a related problem where, like in TTA, a pre-trained model is adapted on the target test examples using a self-supervised loss, before making a prediction on those examples. Unlike SFDA and TTA, though, TTT modifies the source training phase to incorporate a similar self-supervised loss there too.

**Domain generalization (DG).** In DG ([24]), like in SFDA, the target domain is unknown. However, unlike SFDA, no adaptation set is available. Instead the aim is to train a robust source model which works directly on new target distributions. Another important distinction is that DG assumes that information about the source domain is available during deployment on the target domain. A popular strategy for DG is to increase the source model's generalizability by exposing it to diverse ""conditions"" at training time via domain randomization ([20]) or adversarialdata augmentation ([23]; [21]), or to learn domain-_invariant_ representations by training to match all available training ""environments"" (; ), minimizing the worst-case loss over a set of such environments ([22]), or decomposing the latent space or model weights into domain-specific and domain-general components (; ).

",0
154," The LiDAR panoptic segmentation methods usually adopt the LiDAR semantic segmentation networks as the backbone and jointly optimize the semantic segmentation and instance segmentation tasks. The results of the two tasks are fused to generate the final panoptic segmentation results. The LiDAR semantic segmentation backbone is first introduced. Then, the panoptic segmentation methods, achieving instance segmentation upon the semantic segmentation backbone, are grouped into the proposal-based and proposal-free methods, and are further discussed.

LiDAR semantic segmentation backbone.The backbone network for LiDAR semantic segmentation can be divided into three groups: point-based, voxel-based, and 2D projection-based methods. The point-based methods [1][2][3][4] directly operate on the raw point clouds but are extremely time-consuming due to the expensive local neighbor searching. The voxel-based methods [5][6] discretize the point clouds into structured voxels, where sparse 3D convolutions are applied. These methods are still difficult to meet the real-time applications, although they achieve the highest accuracy. The 2D projection-based methods are more efficient since most of their computation is done in the 2D space, such as range view (RV) , bird's-eye view (BEV) [7], polar view [8], and multi-view fusion [9][10]. For fast LiDAR panoptic segmentation, the Panoptic-PolarNet [11], SMAC-Seg [12], and LPSAD [13] all adopt the 2D projection-based backbone. A recent real-time semantic segmentation method, namely CPGNet [10], is a 2D projection-based one that explores an end-to-end multi-view fusion framework by fusing the features from the point, BEV, and RV.

**Proposal-based methods.** The proposal-based methods conduct instance segmentation through a two-stage complicated process. They first detect the foreground instances and then refine the instance segmentation results independently within each detected bounding box. Based on the Mask R-CNN [14] for instance segmentation, the MOPT [15] and EfficientLPS [16] insert a semantic branch to achieve panoptic segmentation on the range view (RV). Recently, LidarMultiNet [17] unifies LiDAR-based 3D object detection, semantic segmentation, and panoptic segmentation in a single framework to reduce the computation cost by sharing a strong voxel-based backbone.

**Proposal-free methods.** The proposal-free methods usually apply the class-agnostic clustering to the _things_ points to conduct instance segmentation. The LPSAD [13] clusters points into instances by regressing the center offsets. The Panoster [18] directly predicts the instance IDs from a learning-based clustering module, where the time-consuming DBSCAN [19] is used to refine the instance segmentation results. The DS-Net [20] proposes a learnable dynamic shifting module that adjusts the kernel functions of the MeanShift [21] to handle instances with various sizes. The GP-S3Net [22] constructs a graph convolutional network (GCN) on the over-segmentation clusters to identify instances. The SMAC-Seg [12] proposes a sparse multi-directional attention clustering and center-aware repel loss for instance segmentation. These methods adopt the time-consuming clustering methods. To further accelerate the algorithm, recently, Panoptic-PolarNet [11], SCAN [23], and Panoptic-PHNet [24] adopt the BEV center heatmap and center offsets for instance segmentation. However, the BEV center heatmap is also costly and confuses on \(z\)-axis.

",0
155," **Deformation Transfer.** Deformation transfer is a long-standing problem in the computer graphics community [1][2][3][4][5]. Sumner _et al_. [1] apply an affine transformation to each triangle of the mesh to solve an optimization problem that matches the deformation of the source mesh while maintaining the shape of the target mesh. BenChen _et al_. [4] enclose the source and target shapes with two cages and transfer the Jacobians of the source deformation to the target shape. However, these methods need tedious human efforts to annotate the correspondence between the source and target shapes. More recently, several deep learning methods are developed to solve the deformation transfer task. However, they either require manually providing the correspondence [6] or cannot generalize [7][8] to stylized characters with different shapes. Gao _et al_. [8] propose a VAE-GAN based method to leverage the cycle consistency between the source and target shapes. Nonetheless, it can only work on shapes used in training. Wang _et al_. [9] introduce conditional normalization used in style transfer for 3D deformation transfer. But the method is limited to clothed-humans and cannot handle the large shape variations of stylized characters.

We argue that these learning-based methods cannot generalize to stylized characters because they rely on encoding their global information (_e.g._, body or parts), which is different from traditional works that focus on local deformation, _e.g._, the affine transformation applied to each triangle in [1]. Using a neural network to encode the global information easily leads to overfitting. For example, modelstrained on human meshes cannot generalize to a stylized humanoid character. At the same time, early works only focus on local information and cannot model global information such as correspondence between the source and target shapes, which is why they all need human effort to annotate the correspondence. Our method tries to learn the correspondence and deform locally at the same time.

**Skeleton-based Pose Transfer.** Besides mesh deformation transfer, an alternative way to transfer pose is to utilize skeletons. Motion retargeting is also a common name used for transferring poses from one motion sequence to another. Gleicher _et al_. [10] propose a space-time constrained solver aiming to satisfy the kinematics-level constraints and to preserve the characters' original identity. Following works [11][12] try to solve inverse-kinematics or inverse rate control to achieve pose transfer. There are also dynamics-based methods [13][14] that consider physics during the retargeting process. Recently, learning-based methods [15][16][17][18][19] train deep neural networks to predict the transformation of the skeleton. Aberman _et al_. [20] propose a pooling-based method to transfer poses between meshes with different skeletons.

All these works highly rely on the skeleton for pose transfer. Other works try to estimate the rigging of the template shape [21][22][23][24] when a skeleton is not available. But if the prediction of the skinning weights fails, the retargeting fails as well. Liao _et al_. [25] propose a model that learns to predict the skinning weights and pose transfer jointly using ground truth skinning weights and paired motion data as supervision, which limits the generalization of this method to categories where annotations are more scarce compared to humans (_e.g_., quadrupeds). Instead, our method uses posed human or animal meshes for training and deforms stylized characters of different shapes at inference.

**Implicit 3D shape representation.** Implicit 3D shape representations have shown great success in reconstructing static shapes [26][27][28][29][30][31] and deformable ones [32][33][34][35][36][37][33][29]. DeepSDF [29] proposes to use an MLP to predict the signed distance field (SDF) value of a query point in 3D space, where a shape code is jointly optimized in an auto-decoding manner. Occupancy flow [34] generalizes the Occupancy Networks [30] to learn a temporally and spatially continuous vector field with a NeuralODE [38]. Inspired by parameteric models, NPMs [36] disentangles and represents the shape and pose of dynamic humans by learning an implicit shape and pose function, respectively. Different from these implicit shape representation works that focus on reconstructing static or deformable meshes, we further exploit the inherent continuity and locality of implicit functions to deform stylized characters to match a target pose in a zero-shot manner.

",1
156," **Deformation Transfer.** Deformation transfer is a long-standing problem in the computer graphics community [1][2][3][4][5]. Sumner _et al_. [1] apply an affine transformation to each triangle of the mesh to solve an optimization problem that matches the deformation of the source mesh while maintaining the shape of the target mesh. BenChen _et al_. [4] enclose the source and target shapes with two cages and transfer the Jacobians of the source deformation to the target shape. However, these methods need tedious human efforts to annotate the correspondence between the source and target shapes. More recently, several deep learning methods are developed to solve the deformation transfer task. However, they either require manually providing the correspondence [6] or cannot generalize [7][8] to stylized characters with different shapes. Gao _et al_. [8] propose a VAE-GAN based method to leverage the cycle consistency between the source and target shapes. Nonetheless, it can only work on shapes used in training. Wang _et al_. [9] introduce conditional normalization used in style transfer for 3D deformation transfer. But the method is limited to clothed-humans and cannot handle the large shape variations of stylized characters.



**Skeleton-based Pose Transfer.** Besides mesh deformation transfer, an alternative way to transfer pose is to utilize skeletons. Motion retargeting is also a common name used for transferring poses from one motion sequence to another. Gleicher _et al_. [10] propose a space-time constrained solver aiming to satisfy the kinematics-level constraints and to preserve the characters' original identity. Following works [11][12] try to solve inverse-kinematics or inverse rate control to achieve pose transfer. There are also dynamics-based methods [13][14] that consider physics during the retargeting process. Recently, learning-based methods [15][16][17][18][19] train deep neural networks to predict the transformation of the skeleton. Aberman _et al_. [20] propose a pooling-based method to transfer poses between meshes with different skeletons.

All these works highly rely on the skeleton for pose transfer. Other works try to estimate the rigging of the template shape [21][22][23][24] when a skeleton is not available. But if the prediction of the skinning weights fails, the retargeting fails as well. Liao _et al_. [25] propose a model that learns to predict the skinning weights and pose transfer jointly using ground truth skinning weights and paired motion data as supervision, which limits the generalization of this method to categories where annotations are more scarce compared to humans (_e.g_., quadrupeds). 

**Implicit 3D shape representation.** Implicit 3D shape representations have shown great success in reconstructing static shapes [26][27][28][29][30][31] and deformable ones [32][33][34][35][36][37][33][29]. DeepSDF [29] proposes to use an MLP to predict the signed distance field (SDF) value of a query point in 3D space, where a shape code is jointly optimized in an auto-decoding manner. Occupancy flow [34] generalizes the Occupancy Networks [30] to learn a temporally and spatially continuous vector field with a NeuralODE [38]. Inspired by parameteric models, NPMs [36] disentangles and represents the shape and pose of dynamic humans by learning an implicit shape and pose function, respectively. 

",0
157," GAN and RL.There are works using ideas from RL to train GANs ([1]; [4]; [3]; [2]). The most relevant work is SeqGAN ([1]), which uses policy gradient to train the generator network. There are several main differences between their settings and ours. First, different GAN objectives are used: SeqGAN uses the JS divergence while we use IPM. In SeqGAN, the next token is dependent on tokens generated from all previous steps, while in diffusion models the next image is only dependent on the model output from one previous step; Also, the critic takes the whole generated sequence as input in SeqGAN, while we only care about the final output. Besides, in our work, rewards are mathematically derived from performing gradient descent w.r.t. IPM, while in SeqGAN, rewards are designed manually. In conclusion, different from SeqGAN, we propose a new policy gradient algorithm to optimize the IPM objective, with a novel analysis of monotonic improvement conditions and a new regularization method for the critic.

Diffusion and GAN.There are other works combining diffusion and GAN training: [5] consider multi-modal noise distributions generated by GAN to enable fast sampling; [7] considers a truncated forward process by replacing the last steps in the forward process with an autoencoder to generate noise, and start with the learned autoencoder as the first step of denoising and then continue to generate data from the diffusion model; Diffusion GAN ([6]) perturbs the data with an adjustable number of steps, and minimizes JS divergence for all intermediate steps by training a multi-step generator with a time-dependent discriminator. To our best knowledge, there is no existing work using GAN-style training to fine-tune a pretrained DDPM sampler.

Fast samplers of DDIM and more.There is another line of work on fast sampling of DDIM (), for example, knowledge distillation (; ) and solving ordinary differential equations (ODEs) with fewer steps ([8]; ). Samples generated by DDIM are generally less diverse than DDPM (). Also, fast sampling is generally easier for DDIM samplers (with deterministic Markov chains) than DDPM samplers, since it is possible to combine multiple deterministic steps into one step without loss of fidelity, but not for combining multiple Gaussian steps as one (). Fine-tuning DDIM samplers with deterministic policy gradient for fast sampling also seems possible, but deterministic policies may suffer from suboptimality, especially in high-dimensional action space ([9]), though it might require fewer samples. Also, it becomes less necessary since distillation is already possible for DDIM.

Moreover, there is also some recent work that uses sample quality metrics to enable fast sampling. Instead of fine-tuning pretrained models,  propose to optimize the hyperparameters of the sampling schedule for a family of non-Markovian samplers by differentiating through KID ([10]), which is calculated by pretrained inception features. It is followed by a contemporary work that fine-tunes pretrained DDIM models using MMD calculated by pretrained features ([12]), which is similar to the method discussed in Section 3.3 but with a fixed critic and a deterministic sampling chain. Generally speaking, adversarially trained critics can provide stronger signals than fixed ones and are more helpful for training ([11]). As a result, besides the potential issues discussed in Section 3.3, such training may also suffer from sub-optimal results when \(p_{0}^{\theta}\) is not close enough to \(q_{0}\) at initialization, and is highly dependent on the choice of the pretrained feature.

",1
158," GAN and RL.There are works using ideas from RL to train GANs ([1]; [4]; [3]; [2]). The most relevant work is SeqGAN ([1]), which uses policy gradient to train the generator network. 

Diffusion and GAN.There are other works combining diffusion and GAN training: [5] consider multi-modal noise distributions generated by GAN to enable fast sampling; [7] considers a truncated forward process by replacing the last steps in the forward process with an autoencoder to generate noise, and start with the learned autoencoder as the first step of denoising and then continue to generate data from the diffusion model; Diffusion GAN ([6]) perturbs the data with an adjustable number of steps, and minimizes JS divergence for all intermediate steps by training a multi-step generator with a time-dependent discriminator. 

Fast samplers of DDIM and more.There is another line of work on fast sampling of DDIM (), for example, knowledge distillation (; ) and solving ordinary differential equations (ODEs) with fewer steps ([8]; ). Samples generated by DDIM are generally less diverse than DDPM (). Also, fast sampling is generally easier for DDIM samplers (with deterministic Markov chains) than DDPM samplers, since it is possible to combine multiple deterministic steps into one step without loss of fidelity, but not for combining multiple Gaussian steps as one (). Fine-tuning DDIM samplers with deterministic policy gradient for fast sampling also seems possible, but deterministic policies may suffer from suboptimality, especially in high-dimensional action space ([9]), though it might require fewer samples. Also, it becomes less necessary since distillation is already possible for DDIM.

Moreover, there is also some recent work that uses sample quality metrics to enable fast sampling. Instead of fine-tuning pretrained models,  propose to optimize the hyperparameters of the sampling schedule for a family of non-Markovian samplers by differentiating through KID ([10]), which is calculated by pretrained inception features. It is followed by a contemporary work that fine-tunes pretrained DDIM models using MMD calculated by pretrained features ([12]), which is similar to the method discussed in Section 3.3 but with a fixed critic and a deterministic sampling chain. Generally speaking, adversarially trained critics can provide stronger signals than fixed ones and are more helpful for training ([11]). As a result, besides the potential issues discussed in Section 3.3, such training may also suffer from sub-optimal results when \(p_{0}^{\theta}\) is not close enough to \(q_{0}\) at initialization, and is highly dependent on the choice of the pretrained feature.

",0
159," **Emergent Behaviour from Large Scale Pretraining** has mainly been observed in Large Language Models (LLMs). Most notably, GPT-2 [1], GPT-3 [2], and Chat-GPT  have been shown to be capable of tasks such as zero-shot translation, question answering, arithmetic, as well as planning actions for embodied agents [3]. Fine-tuning LLMs can also lead to models that can generate code from docstrings [4] or solve math problems [5][6]. Only a few emergent zero-shot behaviours have been reported for VLMs like CLIP, mainly for classification [7] and OCR [8]. Generative VLMs like FLAMINGO [9] and BLIP [10] excel in captioning and visual question-answering tasks, but also have no way of solving pixel-level computer vision tasks.

**Prompting VLMs** is most commonly performed by prepending a set of learnable tokens to the text input [11][12], vision input [13][14][15], or both text and vision inputs [16][17], in order to easily steer a frozen CLIP model to solve a desired task. [18] learn augmentations in pixel space, such as padding around the image, or changing a patch of the image, which are optimized with gradient descent on a downstream task. [19] cast image inpainting as a visual prompting task, using a generative model trained on figures from academic papers. Coloring regions of an image has been used for the VCR task [20], where a model is fine-tuned on annotated images [21]. Colorful Prompt Tuning (CPT) [22] color regions of an image and use a captioning model to predict which object in an image an expression refers to by predicting its color. Similarly to CPT, we augment the input image in pixel space and perform zero-shot inference. However, we _annotate_ the image in a human-like manner and show that our method is more powerful and more flexible than CPT.

**Referring Expression Comprehension** (REC) aims to localize a target object in an image that corresponds to a textual description. Most approaches to REC start with object proposals, for example, generated with Faster-RCNN [23], and learn to score them [24][25][26][27][28]. REC is sometimes considered together with referring expression generation -- the task of generating a description of a given region. [26] use a comprehension model to guide a generator,whereas [29] jointly train a detector with a caption generator. Some works model the scene as a graph [25][27][28] or use language parsers and grammar-based methods [30][31], leading to a more interpretable result. More recently, transformer architectures have been used [32][33][34][35][36]. [32][33][34][36] perform text-modulated object detection, where a transformer decoder takes the referring expression as an input and predicts a bounding box. [35] train with a text-to-pixel contrastive loss, which allows for a text-driven segmentation or detection at test time.

**Unsupervised Referring Expression Comprehension** is a less explored area, only made possible with the introduction of large pre-trained models such as CLIP [7]. ReCLIP [37] crops object proposals and ranks them using CLIP before an ad-hoc postprocessing step to take into account relations such as left/right, smaller/bigger, etc. CPT [22] colors object proposal boxes and use a pre-trained captioning model [38] to auto-regressively predict which colored proposal corresponds to the query description. Pseudo-Q [39] generates descriptions for multiple objects in an image, which is used to train a REC network. However, this model is not fully unsupervised as the pseudo descriptions it uses are generated using a captioning model trained on COCO.

**Visual Reasoning Using Large Pretrained Models** has been an area of significant interest in the last few years. In addition to referring expression detection [37], CLIP [7] has been used for semantic segmentation [40][41]. [41] use CLIP to assign text labels to object parts after doing part co-segmentation in the latent space of a GAN. [40] utilize CLIP for open-vocabulary segmentation by using a general-purpose mask proposal network and CLIP as a classifier. CLIP has also been used for unsupervised object proposal generation [42] and open-set detection [43]. Semantic segmentation also emerges from image only [44][45] or image-text [46] self-supervision.

**Bias of VLMs** is an increasingly popular area of research, as downstream applications come with the risk of perpetuating biases and stereotypes existing in the training data. However, methods for assessing the bias of a VLM are still not well established. [47] measure the misclassification rate of CLIP of faces of people of different races with non-human and criminal categories, whereas [48][49][50] measure fairness in retrieval results. Here, we show a different kind of bias, where the addition of a red circle over a person can trigger a negative connotation.

",1
160," **Emergent Behaviour from Large Scale Pretraining** has mainly been observed in Large Language Models (LLMs). Most notably, GPT-2 [1], GPT-3 [2], and Chat-GPT  have been shown to be capable of tasks such as zero-shot translation, question answering, arithmetic, as well as planning actions for embodied agents [3]. Fine-tuning LLMs can also lead to models that can generate code from docstrings [4] or solve math problems [5][6]. Only a few emergent zero-shot behaviours have been reported for VLMs like CLIP, mainly for classification [7] and OCR [8]. Generative VLMs like FLAMINGO [9] and BLIP [10] excel in captioning and visual question-answering tasks, but also have no way of solving pixel-level computer vision tasks.

**Prompting VLMs** is most commonly performed by prepending a set of learnable tokens to the text input [11][12], vision input [13][14][15], or both text and vision inputs [16][17], in order to easily steer a frozen CLIP model to solve a desired task. [18] learn augmentations in pixel space, such as padding around the image, or changing a patch of the image, which are optimized with gradient descent on a downstream task. [19] cast image inpainting as a visual prompting task, using a generative model trained on figures from academic papers. Coloring regions of an image has been used for the VCR task [20], where a model is fine-tuned on annotated images [21]. Colorful Prompt Tuning (CPT) [22] color regions of an image and use a captioning model to predict which object in an image an expression refers to by predicting its color. 

**Referring Expression Comprehension** (REC) aims to localize a target object in an image that corresponds to a textual description. Most approaches to REC start with object proposals, for example, generated with Faster-RCNN [23], and learn to score them [24][25][26][27][28]. REC is sometimes considered together with referring expression generation -- the task of generating a description of a given region. [26] use a comprehension model to guide a generator,whereas [29] jointly train a detector with a caption generator. Some works model the scene as a graph [25][27][28] or use language parsers and grammar-based methods [30][31], leading to a more interpretable result. More recently, transformer architectures have been used [32][33][34][35][36]. [32][33][34][36] perform text-modulated object detection, where a transformer decoder takes the referring expression as an input and predicts a bounding box. [35] train with a text-to-pixel contrastive loss, which allows for a text-driven segmentation or detection at test time.

**Unsupervised Referring Expression Comprehension** is a less explored area, only made possible with the introduction of large pre-trained models such as CLIP [7]. ReCLIP [37] crops object proposals and ranks them using CLIP before an ad-hoc postprocessing step to take into account relations such as left/right, smaller/bigger, etc. CPT [22] colors object proposal boxes and use a pre-trained captioning model [38] to auto-regressively predict which colored proposal corresponds to the query description. Pseudo-Q [39] generates descriptions for multiple objects in an image, which is used to train a REC network. However, this model is not fully unsupervised as the pseudo descriptions it uses are generated using a captioning model trained on COCO.

**Visual Reasoning Using Large Pretrained Models** has been an area of significant interest in the last few years. In addition to referring expression detection [37], CLIP [7] has been used for semantic segmentation [40][41]. [41] use CLIP to assign text labels to object parts after doing part co-segmentation in the latent space of a GAN. [40] utilize CLIP for open-vocabulary segmentation by using a general-purpose mask proposal network and CLIP as a classifier. CLIP has also been used for unsupervised object proposal generation [42] and open-set detection [43]. Semantic segmentation also emerges from image only [44][45] or image-text [46] self-supervision.

**Bias of VLMs** is an increasingly popular area of research, as downstream applications come with the risk of perpetuating biases and stereotypes existing in the training data. However, methods for assessing the bias of a VLM are still not well established. [47] measure the misclassification rate of CLIP of faces of people of different races with non-human and criminal categories, whereas [48][49][50] measure fairness in retrieval results. 

",0
161," In segmentation-based methods, the task of identifying lane lines has been converted to a per-pixel prediction task. [1] first introduces a spatial mechanism passing messages between pixels row-wise and column-wise that fails to perform in real time. [2] further proposes a recurrent aggregator fully utilised lane shape priors to obtain better performance. On [3], additional affinity fields are predicted simultaneously with the binary segmentation map, which is used in the decoder to cluster lane pixels. Segmentation-based method can achieve high accuracy when lane lines are visible, but it's unstable in complex traffic scenarios and inefficient.

Anchor-based & detection-based methods define lane lines in a similar way. They divide an image into slices or cells, and then convert the lane detection task into either offsets' regression on each slice or a row-wise classification task.  first predicts lane lines via a simple linear layer using row-wise classification. [4] improves the representation of lane lines by converting cell representation into anchor representation, and identifying lane shape through regression of the offsets on every slice between anchors and ground truth. [5] further enhances this formulation by adding anchor-based pooling and a lane attention mechanism to it. [6] proposes a hybrid anchor system to improve the performance of UFLD. [7] proposes a conditional convolution and RIM migration to solve the instance-level discrimination problem on lane detection. [8] develops ROIGather to fuse lane context from different layers and, for the first time, changes the anchor-based formulation into an anchor-free manner, achieving state-of-the-art performance on multiple benchmarks.

Anchor-based and detection-based methods heavily rely on the position of anchors. On one hand, this can bring higher accuracy since anchors contain prior information on lane lines. On the other hand, these inherent properties lead to some shortcomings, such as the starting point of the anchor may not always be located on the three edges of the image, limiting its application.

Keypoint-based methods treat lane lines' prediction as a key point estimation task. Usually, the algorithm will first predict all the possible key points that most likely belong to lane lines, and follow up with a post-process of assigning different points to different lanes. [9] predicts key points on lane lines and distinguishes each instance by embedding features of predicted points. [10] predicts local key points in a bottom-up manner and refiners key points' location via its offsets between adjacent points. [11] clusters points via offsets between key points and start points, and a modified deformable convolution network [12] to extract holistic lane features. Lane instances are predicted by keypoint-based methods via low-efficient post-processing of key points from the heat map, moreover, the accuracy of the algorithm highly relies on the resolution of the input image, together with time-consuming post-processing, making keypoint-based methods hard to strike a balance between latency and accuracy.

",0
162," In this Section, we give a brief overview of related works on point cloud shape correspondence, including learning on point clouds, shape correspondence, and self-ensembling approaches.

**Learning on Point Clouds.** PointNet [1] learns from global information through multi-layer perceptrons and max-pooling operation. PointNet++ [2] devises a hierarchical architecture that recursively partitions the point cloud to extract local features more effectively. Recent works explore local aggregators via relations [3][4][5], and graphs [6][7]. PointCNN [8] transforms neighboring points to the canonical order to apply traditional convolution on point clouds. DGCNN [6] creates a graph in the feature space and designs EdgeConv [6] to learn the edge features of the graph in each layer. However, the methods are commonly based on some assumptions of implicit local geometry, which may result in sensitivity to point cloud disturbances.

**Shape Correspondence.** As an active research area in computer vision and graphics, point cloud shape correspondence methods roughly include spectral-based methods [9][10][11][12] and point-based methods [13][14][15][16]. Spectral-based methods require connectivity information to compute the LBO eigenvectors as basis functions and infer a linear transformation for shape correspondence. However, with regard to point cloud data, connectivity information is difficult to obtain directly while point-based methods directly process point clouds without connectivity information to find the dense point-to-point mapping between two point clouds with deformable 3D shapes. Deprelle et al. [13] propose representing shapes as the deformation and combination of learnable elementary 3D structures. Groueix et al.  employ an encoder-decoder architecture to obtain and constrain the similarity matrix with manually annotated labels. The deep learning methods train their neural networks in a data-driven manner and improve performance to a large extent. However, manually labeling the point-to-point cor respondence between two point clouds in 3D space takes much time and effort. Therefore, some unsupervised point cloud shape correspondence methods [14][16] are proposed to reduce the overhead of labeling. CorrNet3D [16] is the first unsupervised deep learning framework. DPC [14] designs several reconstruction losses to smooth point cloud representations. Due to the lack of annotation, the mismatching issue of symmetrical parts in point clouds with different orientations has become an undeniable problem in unsupervised shape correspondence area.

**Self-ensembling Approaches.** Self-ensembling approaches improve the model generalization by encouraging consensus among ensemble predictions of unknown samples with small perturbations. \(\Gamma\) model [17] consists of two identical parallel branches that respectively take raw images and corrupted images as inputs. In contrast to \(\Gamma\) model, \(\Pi\) model [18] integrates two parallel branches into a single branch. As an extension of the \(\Pi\) model, the temporal model [18] forces the consistency between the outputs and the aggregated outputs over previous training epochs. Mean Teacher [19] replaces network prediction average with network parameter average via the exponential moving average (EMA) strategy. We design a framework similar to Mean Teacher and adapt it for the unsupervised point cloud shape correspondence task. The proposed framework facilitates the network to yield consistent and accurate predictions under noise perturbations and orientation rotations.

",1
163," In this Section, we give a brief overview of related works on point cloud shape correspondence, including learning on point clouds, shape correspondence, and self-ensembling approaches.

**Learning on Point Clouds.** PointNet [1] learns from global information through multi-layer perceptrons and max-pooling operation. PointNet++ [2] devises a hierarchical architecture that recursively partitions the point cloud to extract local features more effectively. Recent works explore local aggregators via relations [3][4][5], and graphs [6][7]. PointCNN [8] transforms neighboring points to the canonical order to apply traditional convolution on point clouds. DGCNN [6] creates a graph in the feature space and designs EdgeConv [6] to learn the edge features of the graph in each layer. However, the methods are commonly based on some assumptions of implicit local geometry, which may result in sensitivity to point cloud disturbances.

**Shape Correspondence.** As an active research area in computer vision and graphics, point cloud shape correspondence methods roughly include spectral-based methods [9][10][11][12] and point-based methods [13][14][15][16]. Spectral-based methods require connectivity information to compute the LBO eigenvectors as basis functions and infer a linear transformation for shape correspondence. However, with regard to point cloud data, connectivity information is difficult to obtain directly while point-based methods directly process point clouds without connectivity information to find the dense point-to-point mapping between two point clouds with deformable 3D shapes. Deprelle et al. [13] propose representing shapes as the deformation and combination of learnable elementary 3D structures. Groueix et al.  employ an encoder-decoder architecture to obtain and constrain the similarity matrix with manually annotated labels. The deep learning methods train their neural networks in a data-driven manner and improve performance to a large extent. However, manually labeling the point-to-point cor respondence between two point clouds in 3D space takes much time and effort. Therefore, some unsupervised point cloud shape correspondence methods [14][16] are proposed to reduce the overhead of labeling. CorrNet3D [16] is the first unsupervised deep learning framework. DPC [14] designs several reconstruction losses to smooth point cloud representations. Due to the lack of annotation, the mismatching issue of symmetrical parts in point clouds with different orientations has become an undeniable problem in unsupervised shape correspondence area.

**Self-ensembling Approaches.** Self-ensembling approaches improve the model generalization by encouraging consensus among ensemble predictions of unknown samples with small perturbations. \(\Gamma\) model [17] consists of two identical parallel branches that respectively take raw images and corrupted images as inputs. In contrast to \(\Gamma\) model, \(\Pi\) model [18] integrates two parallel branches into a single branch. As an extension of the \(\Pi\) model, the temporal model [18] forces the consistency between the outputs and the aggregated outputs over previous training epochs. Mean Teacher [19] replaces network prediction average with network parameter average via the exponential moving average (EMA) strategy. 

",0
164," **Attribute Discovery** Several approaches have been successful in extracting attribute directions in StyleGAN's latent space in the past few years. InterfaceGAN [1] used an external classifier and human annotations to label sampled images in order to build a simple linear model that captures the attribute direction in a GAN's latent space. GANSpace [2] applies PCA to these intermediate representations to find the large factors of variation and then reprojects these directions onto a GAN's latent space. Similarly, SeFa [3] directly captures these directions via matrix factorization of the affine mapping weights in styleGAN, which identify directions of large changes without the need to sample the latent space. An alternative strategy is to directly learn the interpretable directions through a jointly-trained predictive model by assuming that the more predictive variations are more likely to be semantically meaningful [4]- or that using a Hessian penalty , or Jacobian [5], in the image space enables learning of directions. LatentCLR [6] used a similar optimization framework, but instead of training a separate predictive model, it leveragedthe GAN's internal representation and adopted a contrastive loss [7] for attribute discovery.

**Model Auditing** With increased awareness of the societal impact of machine learning models, there is an increased interest in characterizing and criticizing model behavior under the broad umbrella of auditing [8][9]. There has been relatively less work in auditing generative models. For example, [10] introduce a new performance metric for generative models that measures fidelity, diversity, and generalization. Another related work is from Bau et al., [11] who investigate what a GAN cannot generate, whereas our interest is in distinguishing a client GAN from a reference GAN.

**Interpretation of Domain Shift** Some of the most related work comes from methods that aim for characterizing domain shift [12][13], but these methods are limited to specific settings: either relying on human intervention [12] or needing a disentangled generator in the input [13]. An indirect way to obtain aligned attributes is via _aligned GANs_- GANs where one is fine-tuned from the other [14][15]. Here, the attribute direction will be inherent to the children models, eliminating the need to do joint discovery to identify similar attributes. However, obtaining an _aligned GAN_ through a separate fine-tuning process for attribute discovery across distributions is neither practical nor feasible.

",0
165," Dual-pixel depth estimation.Since dual-pixel cameras have been released, many works have been proposed to estimate depth from a dual-pixel image. Garg _et al_. [1] present a learning-based dual-pixel depth estimation using the affine invariant objective to estimate inverse depth. Zhang _et al_.  introduce a supervised learning method similar to [1] to estimate disparity by using two dual-pixel cameras. Recently, Pan _et al_. [2] propose a dual-pixel simulator and also presented the learning-based inverse depth estimation method, which is trained with their simulated data. Xin _et al_. [3] present an unsupervised optimization method based on the estimated defocus map, which can also be shown as an inverse depth map in dual-pixel data. These methods estimate unidirectional information of inverse depth maps [1][2][3] only, often assuming that the focus plane is fixed to the nearest or the farthest location of the scene [3].

Punnappurath _et al_. [4] show that the sign of a bidirectional dual-pixel disparity changes depending on the focus-plane depth in an image. Also, Wadhwa _et al_. [5] apply traditional stereo matching on the separated stereo images from a dual-pixel image to estimate bidirectional disparity. These two traditional methods [4][5] discover the bidirectional nature of dual-pixel disparity and anisotropic blur kernels. However, their performance often degrades due to severe defocus blur of a shallow depth of field and the affine ambiguity of bidirectional disparity w.r.t. focus-plane depth. A learning-based approach would be impactful in mitigating these challenges as the problem is severely ill-posed. However, as mentioned earlier, there is no dual-pixel dataset, including pairs of dual-pixel photographs and bidirectional disparities for every focus-plane depth.

In contrast, our self-supervised method estimates bidirectional disparities while implicitly imposing the reflective-symmetry constraint in anisotropic kernels.

Learning-based stereo.Learning-based stereo methods can be divided into two groups. First, _supervised_ learning-based approaches [6][7][8][9] have been proposed that train neural networks with traditional stereo image datasets [10][11][12]. These network architectures resemble many aspects of the traditional stereo algorithms, for instance, searching correspondences between two rectified images in a coarse-to-fine manner using an image pyramid. However, it is challenging to create a supervision dataset with ground-truth depth labels in the real world. Available datasets are still insufficient to cover the variety of daily stereo-imaging scenarios.

The other group of approaches is _self-supervised_-based to address the limitation of acquiring the true dense depth labels in stereo depth estimation [13][14][15][16][17]. The key advantages of the self-supervised approach are that it does not require any ground-truth depth labels for learning and that once a self-supervised network is pretrained with a large number of observation samples in advance, the network can infer depth at a faster speed than the traditional binocular stereo methods [18].

As mentioned earlier, there is no publicly available dual-pixel dataset with the ground-truth bidirectional disparity labels with a wide range of variation of focus-plane depths. We therefore adopt this self-supervised learning scheme for the dual-pixel disparity problem. Note that the traditional learning-based stereo scheme is not directly applicable to dual-pixel photography since the sign of a bidirectional disparity should change according to an arbitrary focus-plane depth, and the blur kernels should be anisotropic for left/right photodiodes, respectively. To the best of our knowledge, this is the first learning-based solution that can estimate a bidirectional disparity w.r.t. an arbitrary focus-plane depth in dual-pixel photography.

",1
166," Dual-pixel depth estimation.Since dual-pixel cameras have been released, many works have been proposed to estimate depth from a dual-pixel image. Garg _et al_. [1] present a learning-based dual-pixel depth estimation using the affine invariant objective to estimate inverse depth. Zhang _et al_.  introduce a supervised learning method similar to [1] to estimate disparity by using two dual-pixel cameras. Recently, Pan _et al_. [2] propose a dual-pixel simulator and also presented the learning-based inverse depth estimation method, which is trained with their simulated data. Xin _et al_. [3] present an unsupervised optimization method based on the estimated defocus map, which can also be shown as an inverse depth map in dual-pixel data. These methods estimate unidirectional information of inverse depth maps [1][2][3] only, often assuming that the focus plane is fixed to the nearest or the farthest location of the scene [3].

Punnappurath _et al_. [4] show that the sign of a bidirectional dual-pixel disparity changes depending on the focus-plane depth in an image. Also, Wadhwa _et al_. [5] apply traditional stereo matching on the separated stereo images from a dual-pixel image to estimate bidirectional disparity. These two traditional methods [4][5] discover the bidirectional nature of dual-pixel disparity and anisotropic blur kernels. However, their performance often degrades due to severe defocus blur of a shallow depth of field and the affine ambiguity of bidirectional disparity w.r.t. focus-plane depth. A learning-based approach would be impactful in mitigating these challenges as the problem is severely ill-posed. However, as mentioned earlier, there is no dual-pixel dataset, including pairs of dual-pixel photographs and bidirectional disparities for every focus-plane depth.



Learning-based stereo.Learning-based stereo methods can be divided into two groups. First, _supervised_ learning-based approaches [6][7][8][9] have been proposed that train neural networks with traditional stereo image datasets [10][11][12]. These network architectures resemble many aspects of the traditional stereo algorithms, for instance, searching correspondences between two rectified images in a coarse-to-fine manner using an image pyramid. However, it is challenging to create a supervision dataset with ground-truth depth labels in the real world. Available datasets are still insufficient to cover the variety of daily stereo-imaging scenarios.

The other group of approaches is _self-supervised_-based to address the limitation of acquiring the true dense depth labels in stereo depth estimation [13][14][15][16][17]. The key advantages of the self-supervised approach are that it does not require any ground-truth depth labels for learning and that once a self-supervised network is pretrained with a large number of observation samples in advance, the network can infer depth at a faster speed than the traditional binocular stereo methods [18].



",0
167," 3D object detection aims to detect instances in the 3D space, which is of great importance in various applications, such as autonomous driving, navigation robot, and augmented reality. Current LiDAR detectors [1][2][3] mainly voxelize the point cloud into a BEV or voxel representations and adopt traditional 2D convolutions to predict bounding boxes. To improve the inference speed, some works [4][5] attempt to detect objects directly through the range view. [6] combines both BEV and range-view features together to achieve better performance. VoxelNet [2] adopts PointNet [7] to extract local features through raw point clouds and fill them into the predefined voxel space. PointPillar [8] directly processes points inside a pillar to naturally formulate BEV feature representations, establishing an ultra-fast baseline detector. Inspired by CenterNet [9], CenterPoint [1] introduces a center-based label assignment strategy in 3D object detection, achieving competitive detection accuracy among various approaches. In addition to dense one-stage detectors, many works [10][11] apply an R-CNN-style two-stage detection paradigm. Point R-CNN [11] propose 3D RoIAlign to aggregate regional features based on the proposals and then refines the detections. PV-RCNN [12] and Voxel R-CNN [13] construct two-parallel branches to extract both point-level and voxellevel features to enjoy the best of each. However, independently localizing moving instances can introduce misalignment noise, 3D-MAN [14] and MPPNet [15] leverage multi-frame information to further enhance predictions with temporal knowledge.

Semi-supervised learning is an important task in leveraging easy-to-access unlabeled data to improve the supervised model. Most current SSL methods [16][17][18][19] involve adding additional supervision on unlabeled data to regularize the learning of the model. Among them, pseudo-labeling is a popular pipeline [20][21], where unlabeled data is firstly labeled with a supervised model, and then acts in a common training paradigm. In order to guarantee the quality of the generated labels, [21] often filter them with a hard threshold based on the classification score. In addition to hard pseudo-labels, NoisyStudent [22] explores soft supervision to avoid ambiguity problems. Besides, it injects different augmentations to the student and teacher models, to encourage consistency regularization. In light of this, plenty of approaches [23][24][25] enforce the model to predict similar results when applying various input permutations. Such strategies are also verified on 2D object detection, where [26] borrows the idea from FixMatch to achieve promising performance. MUM [27] introduces Mix/UnMix augmentation, enforcing students to reconstruct unmixed features for the mixed input images.

Semi-supervised learning (SSL) has been rapidly developed in both classification and object detection domains and obtains promising results in recent years. There are two main streams in SSOD: consistency learning and pseudo labeling. Consistency-based works [28][29] apply data augmentations/perturbations to the input, which forms natural regularization for the network predictions. Such a consistency-learning target enforces the model to acquire valid information from the unlabeled data, therefore improving the performance. SESS [30] is the first work to attempt such a paradigm on 3D object detection, where the classification and regression predictions are matched through L2 distance and supervised with the similarity loss on the teacher and student. Inspired by Mean Teacher [28], it also adopts the exponential moving average (EMA) technique to further boost the performance. Apart from the consistency-based approaches, pseudo-labeling is another solution [31][32]. Most PL works pay attention to the quality enhancement of pseudo labels. 3DIoUMatch [32] proposes to learn the IoU of the network predictions and utilize it to adaptively filter the low-quality pseudo labels. In order to remove the duplicate score threshold search, Proficient-Teacher [33] introduces a novel clustering-based box voting module, replacing the hand-crafted NMS process. Different from the previous approaches, our method views pseudo-labeling as a noisy learning problem, therefore delivering more generalization against noisy pseudo-data.

",1
168," 3D object detection aims to detect instances in the 3D space, which is of great importance in various applications, such as autonomous driving, navigation robot, and augmented reality. Current LiDAR detectors [1][2][3] mainly voxelize the point cloud into a BEV or voxel representations and adopt traditional 2D convolutions to predict bounding boxes. To improve the inference speed, some works [4][5] attempt to detect objects directly through the range view. [6] combines both BEV and range-view features together to achieve better performance. VoxelNet [2] adopts PointNet [7] to extract local features through raw point clouds and fill them into the predefined voxel space. PointPillar [8] directly processes points inside a pillar to naturally formulate BEV feature representations, establishing an ultra-fast baseline detector. Inspired by CenterNet [9], CenterPoint [1] introduces a center-based label assignment strategy in 3D object detection, achieving competitive detection accuracy among various approaches. In addition to dense one-stage detectors, many works [10][11] apply an R-CNN-style two-stage detection paradigm. Point R-CNN [11] propose 3D RoIAlign to aggregate regional features based on the proposals and then refines the detections. PV-RCNN [12] and Voxel R-CNN [13] construct two-parallel branches to extract both point-level and voxellevel features to enjoy the best of each. However, independently localizing moving instances can introduce misalignment noise, 3D-MAN [14] and MPPNet [15] leverage multi-frame information to further enhance predictions with temporal knowledge.

Semi-supervised learning is an important task in leveraging easy-to-access unlabeled data to improve the supervised model. Most current SSL methods [16][17][18][19] involve adding additional supervision on unlabeled data to regularize the learning of the model. Among them, pseudo-labeling is a popular pipeline [20][21], where unlabeled data is firstly labeled with a supervised model, and then acts in a common training paradigm. In order to guarantee the quality of the generated labels, [21] often filter them with a hard threshold based on the classification score. In addition to hard pseudo-labels, NoisyStudent [22] explores soft supervision to avoid ambiguity problems. Besides, it injects different augmentations to the student and teacher models, to encourage consistency regularization. In light of this, plenty of approaches [23][24][25] enforce the model to predict similar results when applying various input permutations. Such strategies are also verified on 2D object detection, where [26] borrows the idea from FixMatch to achieve promising performance. MUM [27] introduces Mix/UnMix augmentation, enforcing students to reconstruct unmixed features for the mixed input images.

Semi-supervised learning (SSL) has been rapidly developed in both classification and object detection domains and obtains promising results in recent years. There are two main streams in SSOD: consistency learning and pseudo labeling. Consistency-based works [28][29] apply data augmentations/perturbations to the input, which forms natural regularization for the network predictions. Such a consistency-learning target enforces the model to acquire valid information from the unlabeled data, therefore improving the performance. SESS [30] is the first work to attempt such a paradigm on 3D object detection, where the classification and regression predictions are matched through L2 distance and supervised with the similarity loss on the teacher and student. Inspired by Mean Teacher [28], it also adopts the exponential moving average (EMA) technique to further boost the performance. Apart from the consistency-based approaches, pseudo-labeling is another solution [31][32]. Most PL works pay attention to the quality enhancement of pseudo labels. 3DIoUMatch [32] proposes to learn the IoU of the network predictions and utilize it to adaptively filter the low-quality pseudo labels. In order to remove the duplicate score threshold search, Proficient-Teacher [33] introduces a novel clustering-based box voting module, replacing the hand-crafted NMS process. 

",0
169," Unimodal Scene Text Detection.Unimodal scene text detection represents the method directly adopts the bounding boxes annotation only . It can be roughly divided into two categories: Segmentation-based methods and regression-based methods. The segmentation-based methods usually conduct pixel-level [1][2][3][4][5][6], segment-level [7][8][9][10][11], or contour-level [12][13] segmentation, then grouping segments into text instances via post-processing. The regression-based methods [14][15][16][17][18][19][20][21] regards text as a whole object and regress the bounding boxes of the text instances directly.

Cross-modal Assisted Scene Text Detection.Unlike unimodal based scene text detection, cross-modal assisted scene text detection aims to make full use of cross-modal information including visual, semantic, and text knowledge to boost the performance. Wan _et al._[22] utilized an image-level text recognition pretraining tasks to enhance backbone via the proposed self-attention based text knowledge mining mechanism. Song _et al._[23], inspired by CLIP, designed three pretraining fine-grained cross-modality interaction tasks to align unimodal embeddings for learning better representations of backbone. Xue _et al._ jointly learned and aligned visual and partial text instances information for learning effective visual text representations via the proposed weakly supervised pretraining method. Long _et al._[24] proposed an end-to-end model to perform unified scene text detection and visual layout analysis simultaneously. The above methods explicitly leverage text or visual information to assist text detection. Instead, our method focuses on improving the performance results by turning a CLIP model into a scene text detector via leveraging pretrained text knowledge.

",1
170," Unimodal Scene Text Detection.Unimodal scene text detection represents the method directly adopts the bounding boxes annotation only . It can be roughly divided into two categories: Segmentation-based methods and regression-based methods. The segmentation-based methods usually conduct pixel-level [1][2][3][4][5][6], segment-level [7][8][9][10][11], or contour-level [12][13] segmentation, then grouping segments into text instances via post-processing. The regression-based methods [14][15][16][17][18][19][20][21] regards text as a whole object and regress the bounding boxes of the text instances directly.

Cross-modal Assisted Scene Text Detection.Unlike unimodal based scene text detection, cross-modal assisted scene text detection aims to make full use of cross-modal information including visual, semantic, and text knowledge to boost the performance. Wan _et al._[22] utilized an image-level text recognition pretraining tasks to enhance backbone via the proposed self-attention based text knowledge mining mechanism. Song _et al._[23], inspired by CLIP, designed three pretraining fine-grained cross-modality interaction tasks to align unimodal embeddings for learning better representations of backbone. Xue _et al._ jointly learned and aligned visual and partial text instances information for learning effective visual text representations via the proposed weakly supervised pretraining method. Long _et al._[24] proposed an end-to-end model to perform unified scene text detection and visual layout analysis simultaneously. The above methods explicitly leverage text or visual information to assist text detection. 

",0
171," Transformer and the self-attention mechanism have shown great progress in the field of Natural Language Processing [1][2] and successfully applied to vision tasks thanks to the pioneering work of Vision Transformer [3]. Following its path, researchers have extended Vision Transformer models along various directions, including data efficiency [4], position encoding [5], and optimization [6]. To better adapt Vision Transformers to downstream tasks, several works focused on investigating pyramid model structures, and show advanced performances over convolution-based approaches. PVT [7] considers sampling sparse locations in the feature map as key and value pairs. DAT [8] takes a further step and shifts fixed locations toward different directions in a data-dependent way. MViT [9][10] considers the pooling function on the input to obtain key and value pairs, which can be seen as a lower resolution of the feature map. Other approaches adopt an alternative strategy and restrict the attention to carefully designed patterns. Swin Transformer [11] designs non-overlapped windows and shifts windows between consecutive blocks. On this basis, CSwin Transformer [12] adopts a cross-shape window to further improve model capacity.

By constraining the attention receptive field of each query in its own neighboring pixels, local attention inherits the advantages from traditional convolution including local inductive bias and translation-equivariance [13]. Researchers follow this path and target improving the efficiency of local attention. HaloNet [14] combines window attention with local attention by first dividing the input into blocks and considering neighborhood windows instead of pixels. Another direction is to design CUDA kernels with high inference speed. SAN [15] designs a novel patchwise attention pattern and achieves better performances based on convolution architectures. NAT [16] adopts neighborhood attention and specifically considers situations for corner pixels. Nevertheless, current local attention models either use inefficient Im2Col function and endure huge increase in inference time, or rely on carefully written CUDA kernels that restrict applicability on CUDA-free devices.

",0
172," **Open-set Classification**: The open-set setting considers knowledge acquired during training phase to be incomplete, thereby new unknown classes can be encountered during testing. The pioneering explorations in ([1]) formalize the open-set classification task, and have inspired a number of subsequent works, which roughly fall into one of the following two groups.

The first group explores model regularization using unknown data. [3] manually collect unknown data to train a \((n+1)\)-way classifier with one additional class, where \((n+1)^{th}\) class represents the unknown class. Instead of manually collecting unknown data, [2] generate feature vectors of unknown data using a generative adversarial network (). [4] use MixUp technique () to synthesize known data into unknown data.

The second group approaches this problem by discriminative representation learning, which facilitates open-set classification by widening the margin between known and unknown classes. MSP ([7]) is a maximum posterior probability-based baseline and ODIN ([5]) enlarges the difference between known and unknown classes by adding temperature scaling and perturbations to MSP. More recently, different optimization objectives such as large margin loss ([11]) and gaussian mixture loss ([8]) are adopted to learn more discriminative representations. [6]; [10]; [9] also impose gaussian assumption to data distribution to facilitate distinct unknown data.

**Open-set Relation Extraction**: Open-set RE is a pressing but underexplored task. Most of the existing RE methods manually collect NOTA data and adopt a \((n+1)\) way classifier to deal with NOTA relations ([12]; [14]; [13]). However, the collected NOTA data with manual bias cannot cover all NOTA relations and thus these methods cannot effectively deal with open-set RE ().

Our method avoids the bias and the expensive cost of manually collecting NOTA data by automatically synthesizing negative data. Compared with general open-set classification methods, our method takes relational linguistic rules into consideration and outperforms them by a large margin.

",1
173," **Open-set Classification**: The open-set setting considers knowledge acquired during training phase to be incomplete, thereby new unknown classes can be encountered during testing. The pioneering explorations in ([1]) formalize the open-set classification task, and have inspired a number of subsequent works, which roughly fall into one of the following two groups.

The first group explores model regularization using unknown data. [3] manually collect unknown data to train a \((n+1)\)-way classifier with one additional class, where \((n+1)^{th}\) class represents the unknown class. Instead of manually collecting unknown data, [2] generate feature vectors of unknown data using a generative adversarial network (). [4] use MixUp technique () to synthesize known data into unknown data.

The second group approaches this problem by discriminative representation learning, which facilitates open-set classification by widening the margin between known and unknown classes. MSP ([7]) is a maximum posterior probability-based baseline and ODIN ([5]) enlarges the difference between known and unknown classes by adding temperature scaling and perturbations to MSP. More recently, different optimization objectives such as large margin loss ([11]) and gaussian mixture loss ([8]) are adopted to learn more discriminative representations. [6]; [10]; [9] also impose gaussian assumption to data distribution to facilitate distinct unknown data.

**Open-set Relation Extraction**: Open-set RE is a pressing but underexplored task. Most of the existing RE methods manually collect NOTA data and adopt a \((n+1)\) way classifier to deal with NOTA relations ([12]; [14]; [13]). However, the collected NOTA data with manual bias cannot cover all NOTA relations and thus these methods cannot effectively deal with open-set RE ().



",0
174," Existing UDA approaches [1][2][3][4][5][6][7] have addressed distribution shifts effectively by adapting to target domains at training time. UDA approaches generally assume that 1) source data is available during adaptation, and 2) we already know which target domain the models are adapted to. However, these assumptions sometimes do not hold in real-world scenarios. To address such concerns, approaches that adapt a model at test time have been proposed, not requiring access to the source data during adaptation [8][9][10][11][12][13][14][15][16]. Several methods [14][15] perform adaptation in an offline manner, predicting test samples after iterating multiple epochs over the entire set of the test samples (_i.e.,_ test-time training). These approaches modify the training procedure to have self-supervised losses (_e.g.,_ rotation prediction or contrastiveloss) and utilize them as proxy losses for adaptation. However, as also pointed out in Wang _et al._[8], it is not guaranteed that optimizing the proxy losses helps in improving the main task since they are not directly related to classifying images into categories. Addressing such concerns, test-time adaptation (TTA) methods [8][9][10][11][12][17] have been proposed. These approaches do not require any modification of the training procedures, allowing the algorithms to be applicable to a given pretrained deep learning network. TENT [8], a recent seminal work in TTA, proposed to update the modulation parameters in batch normalization [18] layers while minimizing the entropy loss, effectively mitigating distribution shifts.

Feature alignment is widely adopted in UDA studies to mitigate distribution shifts [19][20][21][22]. However, most of these approaches do not consider categorical information but rather match the source and target distributions globally. This may harm class discrimination performance since it does not guarantee class-to-class matching between two distributions [23]. Tacking the problem, various studies have proposed to align distributions in a class-discriminative manner [23][24][25][26][27][28][29]. This point of view is also relevant to test-time adaptation, and we design an effective loss that simultaneously mitigates the distribution gap while improving class discriminability.

",1
175," Existing UDA approaches [1][2][3][4][5][6][7] have addressed distribution shifts effectively by adapting to target domains at training time. UDA approaches generally assume that 1) source data is available during adaptation, and 2) we already know which target domain the models are adapted to. However, these assumptions sometimes do not hold in real-world scenarios. To address such concerns, approaches that adapt a model at test time have been proposed, not requiring access to the source data during adaptation [8][9][10][11][12][13][14][15][16]. Several methods [14][15] perform adaptation in an offline manner, predicting test samples after iterating multiple epochs over the entire set of the test samples (_i.e.,_ test-time training). These approaches modify the training procedure to have self-supervised losses (_e.g.,_ rotation prediction or contrastiveloss) and utilize them as proxy losses for adaptation. However, as also pointed out in Wang _et al._[8], it is not guaranteed that optimizing the proxy losses helps in improving the main task since they are not directly related to classifying images into categories. Addressing such concerns, test-time adaptation (TTA) methods [8][9][10][11][12][17] have been proposed. These approaches do not require any modification of the training procedures, allowing the algorithms to be applicable to a given pretrained deep learning network. TENT [8], a recent seminal work in TTA, proposed to update the modulation parameters in batch normalization [18] layers while minimizing the entropy loss, effectively mitigating distribution shifts.

Feature alignment is widely adopted in UDA studies to mitigate distribution shifts [19][20][21][22]. However, most of these approaches do not consider categorical information but rather match the source and target distributions globally. This may harm class discrimination performance since it does not guarantee class-to-class matching between two distributions [23]. Tacking the problem, various studies have proposed to align distributions in a class-discriminative manner [23][24][25][26][27][28][29]. 

",0
176," Evaluation is a long-standing task in the field of NLG ([1]), which becomes more critical with the rapid development of PLMs. There are two main categories of automatic evaluation metrics, i.e., untrained and trained metrics ([3]). Untrained metrics without training on specific datasets of evaluation tasks or related tasks aim to measure the relationship among source texts, generated texts, and reference texts via n-gram overlap ([5]; ; [8]), semantic similarity ([12]; ), or language modeling / masked language modeling scores ([6]; [10]; ). In comparison, trained metrics are commonly trained on the evaluation datasets to fit human scores ([7]; [2]) or distinguish human-written texts from negative samples ([9]; [4]), aiming to achieve higher correlations with human judgments on specific datasets. Among these metrics, there are some similar works which re-frame NLG evaluation as QA tasks and adopt the generated answers or generation probabilities as evaluation results ([11]; [4]).

The most similar work to our method is UniEval ([4]). UniEval re-frames NLG evaluation as a Boolean QA task and trains the evaluation model on the pseudo data constructed from the evaluation dataset and other related datasets in a unified Boolean QA format. Compared with UniEval, our method is untrained since we transform NLG evaluation to an instruction-style QA task that can be solved by instruction-tuned PLMs without further training. Also, our method can provide some evidence (i.e., the answers to decomposed subquestions) to interpret how the model reaches the evaluation result, instead of only providing a final evaluation score.

Instruction learning ([16]) which trains PLMs to follow human instructions has attracted much attention recently since it shows the strong zero-shot cross-task generalization ability. To improve instruction understanding, existing works adopt instruction tuning ([14]) which trains PLMs on massive tasks describedvia instructions with multi-task learning, such as FLAN [14]; [17], T0 [18], and InstructGPT [15]. Other works systematically study instruction tuning in specific areas such as dialogue systems [19] and multi-modal learning [13].

In comparison, our work is the first to explore the potential of instruction-tuned PLMs in the evaluation of NLG without further training. We show that equipped with well-designed input prompts and suitable question decomposition, instruction-tuned PLMs can sequentially measure the quality of each sentence and finally recompose all the sub-questions with their answers to obtain surprisingly great evaluation results in an unsupervised fashion.

",1
177," Evaluation is a long-standing task in the field of NLG ([1]), which becomes more critical with the rapid development of PLMs. There are two main categories of automatic evaluation metrics, i.e., untrained and trained metrics ([3]). Untrained metrics without training on specific datasets of evaluation tasks or related tasks aim to measure the relationship among source texts, generated texts, and reference texts via n-gram overlap ([5]; ; [8]), semantic similarity ([12]; ), or language modeling / masked language modeling scores ([6]; [10]; ). In comparison, trained metrics are commonly trained on the evaluation datasets to fit human scores ([7]; [2]) or distinguish human-written texts from negative samples ([9]; [4]), aiming to achieve higher correlations with human judgments on specific datasets. Among these metrics, there are some similar works which re-frame NLG evaluation as QA tasks and adopt the generated answers or generation probabilities as evaluation results ([11]; [4]).

The most similar work to our method is UniEval ([4]). UniEval re-frames NLG evaluation as a Boolean QA task and trains the evaluation model on the pseudo data constructed from the evaluation dataset and other related datasets in a unified Boolean QA format. 

Instruction learning ([16]) which trains PLMs to follow human instructions has attracted much attention recently since it shows the strong zero-shot cross-task generalization ability. To improve instruction understanding, existing works adopt instruction tuning ([14]) which trains PLMs on massive tasks describedvia instructions with multi-task learning, such as FLAN [14]; [17], T0 [18], and InstructGPT [15]. Other works systematically study instruction tuning in specific areas such as dialogue systems [19] and multi-modal learning [13].



",0
178," According to the lane representation, there are mainly three kinds of approaches for lanes detection, including segmentation-based [1][2][3], anchor-based [4][5][6][7], and parameter-based methods[8][9][10][11]. In order to estimate the lane representation in the 3D space, some methods first conduct the estimation in the front-view space and then use inverse perspective mapping (IPM) to bridge the front-view space to BEV space for 3D results. The IPM correspondence is based on the planar road assumption, which is hard to handle road undulates, like a hilly road. To tackle this issue, some methods estimate the 3D lanes in the BEV space to resolve complex road conditions [12][13][14][15][16][17][18][19][20]. They usually build a dense correspondence by transforming the front-view feature to the BEV feature map. The dense correspondence builds an effective information flow for the BEV feature to improve the quality of the 3D lane.

To enhance the BEV feature, some methods introduce auxiliary tasks for the joint learning[12][13][15][16]. The others focus on designing a more effective transformation [14][17][16]. RobustLane [14] uses an attention mechanism to aggregate better global information and thus provides a better global smoothness. RTVLane [16] introduces a geometry consistency between 2D and 3D space to guide the learning of BEV features. Recently, PersFormer [17] uses a transformer to build the dense correspondence between the multi-scale front-view features and BEV feature through cross-attention, which achieves SOTA performance. Building a proper correspondence between the front-view space and the BEV space is the key to improving the 3D lane performance, but a dense correspondence is actually redundant. We observe that lanes only occupy a very small ratio of the dense correspondence. Thus, we build a sparse correspondence that only focuses on the points mostly belonging to a lane. The sparse correspondence efficiently brings the high-resolution lane information from the front-view space, ensuring fine-grained details in high-resolution BEV space. Besides, it also guides the information flow to learn better features beneficial for 3D lane results.

The coarse-to-fine design is widely adopted in many methods to hierarchically improve results [5][3][21][22][23][24][25][26][27][28][29]. The general design consists of a coarse result estimation using low-frequency information and a hierarchical refinement using high-frequency information at high resolution. Due to the sparsity of high-frequency information, many methods [21][22][23][24] propose to only focus on the valuable regions and operate on sparse point sets rather than regular grids to keep the balance between efficiency and accuracy. They mainly focus on common objects and use local features to recover fine-grained structures, like corners and boundaries. Different from common objects, 3D lanes have a uniquely long and thin structure, requiring a globally smooth and locally accurate estimation. In this paper, we fuse the local and global information referring to the coordinates of sparse points and jointly refine the global and local structures of lanes.

",1
179," According to the lane representation, there are mainly three kinds of approaches for lanes detection, including segmentation-based [1][2][3], anchor-based [4][5][6][7], and parameter-based methods[8][9][10][11]. In order to estimate the lane representation in the 3D space, some methods first conduct the estimation in the front-view space and then use inverse perspective mapping (IPM) to bridge the front-view space to BEV space for 3D results. The IPM correspondence is based on the planar road assumption, which is hard to handle road undulates, like a hilly road. To tackle this issue, some methods estimate the 3D lanes in the BEV space to resolve complex road conditions [12][13][14][15][16][17][18][19][20]. They usually build a dense correspondence by transforming the front-view feature to the BEV feature map. The dense correspondence builds an effective information flow for the BEV feature to improve the quality of the 3D lane.

To enhance the BEV feature, some methods introduce auxiliary tasks for the joint learning[12][13][15][16]. The others focus on designing a more effective transformation [14][17][16]. RobustLane [14] uses an attention mechanism to aggregate better global information and thus provides a better global smoothness. RTVLane [16] introduces a geometry consistency between 2D and 3D space to guide the learning of BEV features. Recently, PersFormer [17] uses a transformer to build the dense correspondence between the multi-scale front-view features and BEV feature through cross-attention, which achieves SOTA performance. Building a proper correspondence between the front-view space and the BEV space is the key to improving the 3D lane performance, but a dense correspondence is actually redundant. 

The coarse-to-fine design is widely adopted in many methods to hierarchically improve results [5][3][21][22][23][24][25][26][27][28][29]. The general design consists of a coarse result estimation using low-frequency information and a hierarchical refinement using high-frequency information at high resolution. Due to the sparsity of high-frequency information, many methods [21][22][23][24] propose to only focus on the valuable regions and operate on sparse point sets rather than regular grids to keep the balance between efficiency and accuracy. They mainly focus on common objects and use local features to recover fine-grained structures, like corners and boundaries. Different from common objects, 3D lanes have a uniquely long and thin structure, requiring a globally smooth and locally accurate estimation. 

",0
180," We give a short introduction to neuroimaging tasks, terminology, and methods. We then provide an overview of fundamental methods for adapting a model to multiple domains, including multi-task learning, few-shot learning, fine-tuning, and data synthesis.

Neuroimage analysis employs computational techniques to study the structure and function of the human brain. Common imaging techniques are structural magnetic resonance imaging (MRI), functional MRI, diffusion tensor imaging (DTI), computed tomography (CT), and Positron emission tomography (PET). Each imaging method can create diverse images with different characteristics and contrasts, which are further diversified depending on the properties of the acquisition site [1], device, protocol, imaging sequence [2], and use of contrast agents [3][4].

To analyze these images, a variety of processing tasks are most often combined in a processing pipeline. Common processing tasks include anatomical segmentation [5][6][7], skull stripping [8], defacing [9], registration [10][11][12][13][14], modality transfer [15][16], in-painting , super-resolution [17][18][19], reconstruction, and de-noising [20], bias field removal [21], surface fitting [22] and parcellation [23].

Multiple toolboxes provide a suite of interoperable software components, most implementing classical optimization strategies. Widely used toolboxes include Freesurfer , FSL , SPM [24], CIVET , BrainSuite , HCP pipeline , and BrainIAK [25]. Deep-learning-based methods are starting to be included because of their improved accuracy and shorter runtime [26]. While these methods provide solutions for common neuroimaging applications, most are limited to a single task and few modalities. Developers need to manually update the pipelines to include new processing tasks and to support a wider variety of image modalities. This process requires extensive technical expertise and computational resources, often not available to the clinical neuroscientists focusing on scientific questions.

Multi-Task Learning (MTL) frameworks solve multiple tasks simultaneously by exploiting similarities between related tasks [27]. MTL can improve performance and reduce computational cost and development time compared to designing task-specific solutions [28][29]. In neuroimaging, MTL networks were recently proposed for the simultaneoussegmentation and classification of brain tumors by training a single network with separate prediction heads associated with the different tasks [30]. This strategy is challenging to scale as the number of tasks increases, requires prior determination of the set of tasks, and importantly does not enable generalization of the model to new tasks. With Neuralizer, we build on these methods to achieve scalable MTL, without the need for multiple network heads, and importantly with the ability to generalize to new tasks and modalities.

To tackle problems in the limited data scenarios frequent in medical imaging, neural networks can be pre-trained on a related task with high data availability and then fine-tuned for specific tasks. For example, a common approach involves taking a Res-Net [31] trained on ImageNet [32] and fine-tuning part of the network for a new task [33][34][35]. For medical imaging, networks pre-trained on large sets of medical images are available [36], and fine-tuning them to new tasks results in shortened training time and higher accuracy [37][38]. However, fine-tuning also requires machine learning expertise and computational resources, most often not available in clinical research. Additionally, in scenarios with small datasets, fine-tuning models trained on large vision datasets can be harmful [39].

Few-shot models generate predictions from just a few labeled examples [40][41][42], or in the case of zero-shot methods [43], none at all. Many of these methods require training or fine-tuning. In computer vision, several methods pass a query image, along with a set of support images and labels as input to the model [40][44][45][46]. Natural image segmentation methods [47][48] use single image-label pairs [49][50] as support or aggregate information from a larger support set [51]. Recent few-shot learning methods in the medical image segmentation setting [52][53][54] operate on a specific anatomical region in a single image modality . Similar Prior-Data Fitted Networks (PFNs) are fitted to multiple datasets at once to learn the training and prediction algorithm [55]. During training, this strategy draws a dataset, a set of data points and their labels from it, masks one of the labels and predicts it. The resulting model aims to generalize to new datasets. PFNs have only been applied to low-dimensional and tabular data [56]. Our solution builds on ideas from these methods, but aims to solve a much larger range of diverse image-to-image tasks on neuroimages of many modalities.

Data augmentation increases the diversity of training data by augmenting or modifying existing data [57]. It improves model robustness to input variability that may not be available in the original training data. In neuroimaging, arbitrary image modalities can be simulated by synthesis of images without requiring any real data [26][12][58]. In meta-learning, data augmentation can further be used to generate entirely new tasks [59][60][61]. We use data augmentations and further expand existing methods by developing rich neuroimaging task augmentations for generalization to unseen neuroimaging tasks.

",1
181," We give a short introduction to neuroimaging tasks, terminology, and methods. We then provide an overview of fundamental methods for adapting a model to multiple domains, including multi-task learning, few-shot learning, fine-tuning, and data synthesis.

Neuroimage analysis employs computational techniques to study the structure and function of the human brain. Common imaging techniques are structural magnetic resonance imaging (MRI), functional MRI, diffusion tensor imaging (DTI), computed tomography (CT), and Positron emission tomography (PET). Each imaging method can create diverse images with different characteristics and contrasts, which are further diversified depending on the properties of the acquisition site [1], device, protocol, imaging sequence [2], and use of contrast agents [3][4].

To analyze these images, a variety of processing tasks are most often combined in a processing pipeline. Common processing tasks include anatomical segmentation [5][6][7], skull stripping [8], defacing [9], registration [10][11][12][13][14], modality transfer [15][16], in-painting , super-resolution [17][18][19], reconstruction, and de-noising [20], bias field removal [21], surface fitting [22] and parcellation [23].

Multiple toolboxes provide a suite of interoperable software components, most implementing classical optimization strategies. Widely used toolboxes include Freesurfer , FSL , SPM [24], CIVET , BrainSuite , HCP pipeline , and BrainIAK [25]. Deep-learning-based methods are starting to be included because of their improved accuracy and shorter runtime [26]. While these methods provide solutions for common neuroimaging applications, most are limited to a single task and few modalities. Developers need to manually update the pipelines to include new processing tasks and to support a wider variety of image modalities. This process requires extensive technical expertise and computational resources, often not available to the clinical neuroscientists focusing on scientific questions.

Multi-Task Learning (MTL) frameworks solve multiple tasks simultaneously by exploiting similarities between related tasks [27]. MTL can improve performance and reduce computational cost and development time compared to designing task-specific solutions [28][29]. In neuroimaging, MTL networks were recently proposed for the simultaneoussegmentation and classification of brain tumors by training a single network with separate prediction heads associated with the different tasks [30]. This strategy is challenging to scale as the number of tasks increases, requires prior determination of the set of tasks, and importantly does not enable generalization of the model to new tasks. 

To tackle problems in the limited data scenarios frequent in medical imaging, neural networks can be pre-trained on a related task with high data availability and then fine-tuned for specific tasks. For example, a common approach involves taking a Res-Net [31] trained on ImageNet [32] and fine-tuning part of the network for a new task [33][34][35]. For medical imaging, networks pre-trained on large sets of medical images are available [36], and fine-tuning them to new tasks results in shortened training time and higher accuracy [37][38]. However, fine-tuning also requires machine learning expertise and computational resources, most often not available in clinical research. Additionally, in scenarios with small datasets, fine-tuning models trained on large vision datasets can be harmful [39].

Few-shot models generate predictions from just a few labeled examples [40][41][42], or in the case of zero-shot methods [43], none at all. Many of these methods require training or fine-tuning. In computer vision, several methods pass a query image, along with a set of support images and labels as input to the model [40][44][45][46]. Natural image segmentation methods [47][48] use single image-label pairs [49][50] as support or aggregate information from a larger support set [51]. Recent few-shot learning methods in the medical image segmentation setting [52][53][54] operate on a specific anatomical region in a single image modality . Similar Prior-Data Fitted Networks (PFNs) are fitted to multiple datasets at once to learn the training and prediction algorithm [55]. During training, this strategy draws a dataset, a set of data points and their labels from it, masks one of the labels and predicts it. The resulting model aims to generalize to new datasets. PFNs have only been applied to low-dimensional and tabular data [56]. 

Data augmentation increases the diversity of training data by augmenting or modifying existing data [57]. It improves model robustness to input variability that may not be available in the original training data. In neuroimaging, arbitrary image modalities can be simulated by synthesis of images without requiring any real data [26][12][58]. In meta-learning, data augmentation can further be used to generate entirely new tasks [59][60][61]. 

",0
182," Many real-world network control problems rely heavily on convex optimization (Boyd & ; Hillier & ). This is often due to the relative simplicity of constraints and cost functions; for example, capacity constraints on edges may be written as simple linear combinations of flow values, and costs are linear in quantities due to the linearity of prices. In particular, linear programming (as well as specialized versions thereof) is fundamental in problems such as flow optimization, matching, cost minimization and optimal production, and many more. While algorithmic improvements have made many convex problem formulations tractable and efficient to solve, these methods are still not able to handle (i) nonlinear dynamics, (ii) stochasticity, or (iii) the curse of dimensionality in time-expanded networks. In this work, we aim to address these challenges by combining the strengths of direct optimization and reinforcement learning.

Nonlinear dynamics typically requires linearization to yield a tractable optimization problem: either around a nominal trajectory, or iteratively during solution. While sequential convex optimization often yields an effective approximate solution, it is expensive and practically guaranteeing convergence while preserving efficiency may be difficult (Dinh & ). Stochasticity may be handled in many ways: common strategies are distributional assumptions to achieve analytic tractability ([3]), building in sufficient buffer to correct via re-planning in the future (), or sampling-based methods, often with fixed recourse (). Addressing the curse of dimensionality relies on limiting the amount of online optimization; typical approaches include limited-lookahead methods ([1]) or computing a parameterized policy via approximate dynamic programming or reinforcement learning ([2]; Bertsekas & ; Sutton & ). However, these policies may be strongly sub-optimal depending on representation capacity and state/action-space coverage. In contrast to these methods, we leverage the strong performance of optimization over short horizons (in which the impact of nonlinearity and stochasticity is typically limited) and exploit an RL-based heuristic for future returns which avoids the curse of dimensionality and the need to solve non-convex or sampled optimization problems.

Our proposed approach results in a bi-level optimization problem. Bi-level optimization--in which one optimization problem depends on the solution to another optimization problem, and is thus nested--has recently attracted substantial attention in machine learning, reinforcement learning, and control ([5]; ; ; Amos & ; ; [4]). Of particular relevance to our framework are methods that combine principled control strategies with learned components in a hierarchical way. Examples include using LQR control in the inner problem with learnable cost and dynamics ([7]; [6];

[10]), learning sampling distributions in planning and control ([8]; Power & ; Amos & ), or learning optimization strategies or goals for optimization-based control (Sacks & ; [11]; [4]; [9]; [12]).

Numerous strategies for learning control with bi-level formulations have been proposed. A simple approach is to insert intermediate goals to train lower-level components, such as imitation ([8]). This approach is inherently limited by the choice of the intermediate objective; if this objective does not strongly correlate with the downstream task, learning could emphasize unnecessary elements or miss critical ones. An alternate strategy, which we take in this work, is directly optimizing through an inner controller, thus avoiding the problem of goal misspecification. A large body of work has focused on exploiting exact solutions to the gradient of (convex) optimization problems at fixed points ([6]; [10]; [13]). This allows direct backpropagation through optimization problems, allowing them to be used as a generic component in a differentiable computation graph (or neural network). Our approach leverages likelihood ratio gradients (equivalently, policy gradient), an alternate zeroth-order gradient estimator ([14]). This enables easy differentiation through lower-level optimization problems without the technical details required by fixed-point differentiation.

",1
183," Many real-world network control problems rely heavily on convex optimization (Boyd & ; Hillier & ). This is often due to the relative simplicity of constraints and cost functions; for example, capacity constraints on edges may be written as simple linear combinations of flow values, and costs are linear in quantities due to the linearity of prices. In particular, linear programming (as well as specialized versions thereof) is fundamental in problems such as flow optimization, matching, cost minimization and optimal production, and many more. While algorithmic improvements have made many convex problem formulations tractable and efficient to solve, these methods are still not able to handle (i) nonlinear dynamics, (ii) stochasticity, or (iii) the curse of dimensionality in time-expanded networks. 

Nonlinear dynamics typically requires linearization to yield a tractable optimization problem: either around a nominal trajectory, or iteratively during solution. While sequential convex optimization often yields an effective approximate solution, it is expensive and practically guaranteeing convergence while preserving efficiency may be difficult (Dinh & ). Stochasticity may be handled in many ways: common strategies are distributional assumptions to achieve analytic tractability ([3]), building in sufficient buffer to correct via re-planning in the future (), or sampling-based methods, often with fixed recourse (). Addressing the curse of dimensionality relies on limiting the amount of online optimization; typical approaches include limited-lookahead methods ([1]) or computing a parameterized policy via approximate dynamic programming or reinforcement learning ([2]; Bertsekas & ; Sutton & ). However, these policies may be strongly sub-optimal depending on representation capacity and state/action-space coverage. 

Our proposed approach results in a bi-level optimization problem. Bi-level optimization--in which one optimization problem depends on the solution to another optimization problem, and is thus nested--has recently attracted substantial attention in machine learning, reinforcement learning, and control ([5]; ; ; Amos & ; ; [4]). Of particular relevance to our framework are methods that combine principled control strategies with learned components in a hierarchical way. Examples include using LQR control in the inner problem with learnable cost and dynamics ([7]; [6];

[10]), learning sampling distributions in planning and control ([8]; Power & ; Amos & ), or learning optimization strategies or goals for optimization-based control (Sacks & ; [11]; [4]; [9]; [12]).

Numerous strategies for learning control with bi-level formulations have been proposed. A simple approach is to insert intermediate goals to train lower-level components, such as imitation ([8]). This approach is inherently limited by the choice of the intermediate objective; if this objective does not strongly correlate with the downstream task, learning could emphasize unnecessary elements or miss critical ones. An alternate strategy, which we take in this work, is directly optimizing through an inner controller, thus avoiding the problem of goal misspecification. A large body of work has focused on exploiting exact solutions to the gradient of (convex) optimization problems at fixed points ([6]; [10]; [13]). This allows direct backpropagation through optimization problems, allowing them to be used as a generic component in a differentiable computation graph (or neural network). 

",0
184," **Medical Image Segmentation and Diagnosis.** U-Net  and its variants [1][2][3][4] have been promoting the development of medical image segmentation. A recent self-configuring U-Net (nnUNet) [5] further surpassed existing approaches in various medical image segmentation tasks with minimal manual parameter tuning. Semantic segmentation serves as the core for downstream clinical tasks of disease detection , differential diagnosis [6], survival prediction [7], therapy planning , and treatment response assessment . Therefore, developing a reliable segmentation method is critical to improving safety in real-world clinical use. After the publication of Vision Transformers (ViTs) [8], integrating subsequent transformer blocks into the backbone of network architecture [9][10][11] has been investigated. ViTs achieved improved results over traditional U-Net, particularly for multi-class semantic segmentation tasks. This work greatly focuses on exploring the real-world OOD localization detection problem over medical image segmentation. Current solutions provide limited performance, so we study a novel architecture combining Transformer and nnUNet for improving segmentation performance under clinical tasks, utilizing segmentation to detect and diagnose minority tumors [6].

**Mask Transformers.** Unlike using Transformers directly as network backbones for natural and medical image segmentation [12][13][14][15][16], Mask Transformers seek to enhance the CNN-based backbone with stand-alone transformer blocks. MaX-Deeplab [17] interprets object queries in DETR  as memory-encoded queries for end-to-end panoptic segmentation. MaskFormer [18] further applies this design to semantic segmentation by unifying the CNN and the transformer branches. Afterward, Mask2Former [19] technically improves over its predecessor. Recently, CMT-Deeplab [20] and KMaX-Deeplab [21] propose to interpret the queries as clustering centers and add regulatory constraints for learning the cluster representations of the queries. The design of Mask Transformers is intuitively suitable for medical image segmentation, especially for the semantic segmentation and diagnosis of tumors. This task requires the network to be locally sensitive to image textures for tumor segmentation and can globally understand organ-tumor morphological information for tumor sub-type recognition. To our knowledge, we are the first to adapt Mask Transformers for medical image segmentation and further explore its usage of recognizing outliers via queries.

**OOD Detection and Localization.** OOD Detection aims to detect the out-of-distribution conditions (outliers) that are unseen in the training data. Maximal softmax probability (MSP) [22] serves as a strong baseline. After that, various approaches improved OOD detection from multiple aspects [23][24][25][26]. These approaches focus on image-level OOD detection, and efforts have also been made to localize OOD objects or regions on a large image, e.g., urban driving scenes [27][28][29][22][30][31][32]. Despite the advance of OOD detection and localization on natural images, its application on real-world medical images is challenging. Since the difference between foregrounds in real-world medical images is subtle, their OOD detection/localization becomes a typical near-OOD problem [33][34][35][36]. Therefore, the existing OOD solutions could hardly be recommended for clinical practice [37]. Recent work, HOD , paces one step toward real-world OOD detection of rare diseases in dermatology classification.

",1
185," **Medical Image Segmentation and Diagnosis.** U-Net  and its variants [1][2][3][4] have been promoting the development of medical image segmentation. A recent self-configuring U-Net (nnUNet) [5] further surpassed existing approaches in various medical image segmentation tasks with minimal manual parameter tuning. Semantic segmentation serves as the core for downstream clinical tasks of disease detection , differential diagnosis [6], survival prediction [7], therapy planning , and treatment response assessment . Therefore, developing a reliable segmentation method is critical to improving safety in real-world clinical use. After the publication of Vision Transformers (ViTs) [8], integrating subsequent transformer blocks into the backbone of network architecture [9][10][11] has been investigated. ViTs achieved improved results over traditional U-Net, particularly for multi-class semantic segmentation tasks. This work greatly focuses on exploring the real-world OOD localization detection problem over medical image segmentation. 

**Mask Transformers.** Unlike using Transformers directly as network backbones for natural and medical image segmentation [12][13][14][15][16], Mask Transformers seek to enhance the CNN-based backbone with stand-alone transformer blocks. MaX-Deeplab [17] interprets object queries in DETR  as memory-encoded queries for end-to-end panoptic segmentation. MaskFormer [18] further applies this design to semantic segmentation by unifying the CNN and the transformer branches. Afterward, Mask2Former [19] technically improves over its predecessor. Recently, CMT-Deeplab [20] and KMaX-Deeplab [21] propose to interpret the queries as clustering centers and add regulatory constraints for learning the cluster representations of the queries. The design of Mask Transformers is intuitively suitable for medical image segmentation, especially for the semantic segmentation and diagnosis of tumors. This task requires the network to be locally sensitive to image textures for tumor segmentation and can globally understand organ-tumor morphological information for tumor sub-type recognition. 

**OOD Detection and Localization.** OOD Detection aims to detect the out-of-distribution conditions (outliers) that are unseen in the training data. Maximal softmax probability (MSP) [22] serves as a strong baseline. After that, various approaches improved OOD detection from multiple aspects [23][24][25][26]. These approaches focus on image-level OOD detection, and efforts have also been made to localize OOD objects or regions on a large image, e.g., urban driving scenes [27][28][29][22][30][31][32]. Despite the advance of OOD detection and localization on natural images, its application on real-world medical images is challenging. Since the difference between foregrounds in real-world medical images is subtle, their OOD detection/localization becomes a typical near-OOD problem [33][34][35][36]. Therefore, the existing OOD solutions could hardly be recommended for clinical practice [37]. Recent work, HOD , paces one step toward real-world OOD detection of rare diseases in dermatology classification.

",0
186," **Gaze and Head Redirection.** Methods for redirecting gaze directions can be broadly classified into two categories: warping-based methods and generator-based methods. Deepwarp [1] presented a deep network to learn warping maps between pairs of eye images with different gaze directions, which required large amounts of data with annotations. Yu _et al._[2] utilized a pretrained gaze estimator and synthetic eye images to reduce the reliance on annotated real data. Yu _et al._[3] further extended the warping-based methods in an unsupervised manner by adding a gaze representation learning network. As for the generator-based methods, He _et al._[4] developed a GAN-based network for generating eye images with new gaze directions. FAZE  proposed an encoder-decoder architecture to transform eye images into latent vectors for redirection with rotation matrix multiplication, and then decode the edited ones back to the synthetic images with new gaze directions. ST-ED [5] further extended the encoder-decoder pipeline from gaze redirection only to both head and gaze redirection over full face images by disentangling latent vectors, and achieving precise redirection performance. However, ST-ED generates images with a restricted face range (no hair area) with a size of \(128\times 128\). We further improve the redirection task by covering the full face range with \(1024\times 1024\) resolution.

**Latent Space Manipulation.** Numerous methods investigated the latent space working with StyleGAN [6][7] to achieve semantic editing in image space due to its meaningful and highly disentangled properties. As for the supervised methods, InterFaceGAN [8] determined hyperplanes for the corresponding facial attribute editing based on provided labels. StyleFlow [9] proposed mapping a sample from a prior distribution to a latent distribution conditioned on the target attributes estimated by pretrained attribute classifiers. Given the unsupervised methods, GANSpace [10], SeFa [11] and TensorGAN [12] leveraged principal components analysis, eigenvector decomposition and higher-order singular value decomposition to discover semantic directions in latent space, respectively. Other self-supervised methods proposed mixing of latent codes from other samples for local editing [13][14], or incorporating the language model CLIP [15] for text-driven editing [16].

**Domain Adaptation for Gaze Estimation.** Domain gaps among different datasets restrict the application range of pretrained gaze estimation models. To narrow the gaps, a few domain adaptation approaches [17] were proposed for the generic regression task. SimGAN [18] proposed an unsupervised domain adaptation method for narrowing the gaps between real and synthetic eye images. HGM [19] designed a unified 3D eyeball model for eye image synthesis and cross-dataset gaze estimation. PnP-GA [20] presented a gaze adaptation framework for generalizing gaze estimation in new domains based on collaborative learning. Qin _et al._[21] utilized 3D face reconstruction to rotate head orientations together with changed eye gaze accordingly to enlarge overlapping gaze distributions among datasets. These adaptation methods typically rely on restricted face or eye images to alleviate interference from untargeted attributes. Our work incorporates the redirection task in a predefined meaningful feature space with controllable attributes to achieve high-resolution and full-face redirection.

",1
187," **Gaze and Head Redirection.** Methods for redirecting gaze directions can be broadly classified into two categories: warping-based methods and generator-based methods. Deepwarp [1] presented a deep network to learn warping maps between pairs of eye images with different gaze directions, which required large amounts of data with annotations. Yu _et al._[2] utilized a pretrained gaze estimator and synthetic eye images to reduce the reliance on annotated real data. Yu _et al._[3] further extended the warping-based methods in an unsupervised manner by adding a gaze representation learning network. As for the generator-based methods, He _et al._[4] developed a GAN-based network for generating eye images with new gaze directions. FAZE  proposed an encoder-decoder architecture to transform eye images into latent vectors for redirection with rotation matrix multiplication, and then decode the edited ones back to the synthetic images with new gaze directions. ST-ED [5] further extended the encoder-decoder pipeline from gaze redirection only to both head and gaze redirection over full face images by disentangling latent vectors, and achieving precise redirection performance. However, ST-ED generates images with a restricted face range (no hair area) with a size of \(128\times 128\). 

**Latent Space Manipulation.** Numerous methods investigated the latent space working with StyleGAN [6][7] to achieve semantic editing in image space due to its meaningful and highly disentangled properties. As for the supervised methods, InterFaceGAN [8] determined hyperplanes for the corresponding facial attribute editing based on provided labels. StyleFlow [9] proposed mapping a sample from a prior distribution to a latent distribution conditioned on the target attributes estimated by pretrained attribute classifiers. Given the unsupervised methods, GANSpace [10], SeFa [11] and TensorGAN [12] leveraged principal components analysis, eigenvector decomposition and higher-order singular value decomposition to discover semantic directions in latent space, respectively. Other self-supervised methods proposed mixing of latent codes from other samples for local editing [13][14], or incorporating the language model CLIP [15] for text-driven editing [16].

**Domain Adaptation for Gaze Estimation.** Domain gaps among different datasets restrict the application range of pretrained gaze estimation models. To narrow the gaps, a few domain adaptation approaches [17] were proposed for the generic regression task. SimGAN [18] proposed an unsupervised domain adaptation method for narrowing the gaps between real and synthetic eye images. HGM [19] designed a unified 3D eyeball model for eye image synthesis and cross-dataset gaze estimation. PnP-GA [20] presented a gaze adaptation framework for generalizing gaze estimation in new domains based on collaborative learning. Qin _et al._[21] utilized 3D face reconstruction to rotate head orientations together with changed eye gaze accordingly to enlarge overlapping gaze distributions among datasets. These adaptation methods typically rely on restricted face or eye images to alleviate interference from untargeted attributes. 

",0
188," **Implicit Neural Representations for 3D Surfaces.** Implicit neural representations have emerged a few years ago as an effective tool to represent surfaces whose topology is not known a priori. They can be implemented using (clipped) _signed distance functions_ (SDF) [1] or _occupancies_[2]. When an explicit representation is required, it can be obtained using Marching Cubes [3] and this can be done while preserving differentiability [4][5][6]. However, they can only represent watertight surfaces.

Thus, to represent open surfaces, such as clothes, it is possible to use inflated SDFs surrounding them. However, this entails a loss in accuracy and there has been a recent push to replace SDFs by _unsigned distance functions_ (UDFs) [7][8][9]. One difficulty in so doing was that Marching Cubes was not designed with UDFs in mind, and obtaining explicit surfaces from these UDFs was therefore non-trivial. This has been addressed in  by modifying the Marching Cubes algorithm to operate with UDFs. We model garment with UDFs and use  to mesh them. Other works augment signed distance fields with covariant fields to encode open surface garments [10][11].

**Draping Garments over 3D Bodies.** Two main classes of methods coexist, physics-based algorithms [12][13][12][14][15] that produce high-quality drapings but at a high computational cost, and data-driven approaches that are faster but often at the cost of realism.

Among the latter, template-based approaches [16][17][18][19][20][11][16] are dominant. Each garment is modeled by a specific triangulated mesh and a draping function is learned for each one. In other words, they do not generalize. There are however a number of exceptions. In [21][22] the mesh is replaced by 3D point clouds that can represent generic garments. This enables deforming garments with arbitrary topology and geometric complexity, by estimating the deformation separately for each point. [23] goes further and allows differentiable changes in garment topology by sampling a fixed number of points from the body mesh. Unfortunately, this point cloud representation severely limits possible downstream applications.

In recent approaches [24][25], a space of garments is learned with clothing items modeled as inflated SDFs and one single shared network to predict their deformations as a 3D displacement field. This makes deployment in real-world scenarios easier and allows the reconstruction of garments from images and 3D scans. However, the inflated SDF scheme reduces realism and precludes post-processing using standard physics-based simulators or other cloth-specific downstream applications. Furthermore, both models are fully supervised and require a dataset of draped garments whose collection is extremely time-consuming.

Alleviating the need for costly ground-truth draped garments is tackled in [16][20], by introducing physics-basedlosses to train draping networks in a self-supervised manner. The approach of [20] relies on a mass spring model to enforce the physical consistency of static garments deformed by different body poses. The method of [16] also accounts for variable body shapes and dynamic effects; furthermore, it incorporates a more realistic and expressive material model. Both methods, however, require training one network per garment, a limitation we remove.

",1
189," **Implicit Neural Representations for 3D Surfaces.** Implicit neural representations have emerged a few years ago as an effective tool to represent surfaces whose topology is not known a priori. They can be implemented using (clipped) _signed distance functions_ (SDF) [1] or _occupancies_[2]. When an explicit representation is required, it can be obtained using Marching Cubes [3] and this can be done while preserving differentiability [4][5][6]. However, they can only represent watertight surfaces.

Thus, to represent open surfaces, such as clothes, it is possible to use inflated SDFs surrounding them. However, this entails a loss in accuracy and there has been a recent push to replace SDFs by _unsigned distance functions_ (UDFs) [7][8][9]. One difficulty in so doing was that Marching Cubes was not designed with UDFs in mind, and obtaining explicit surfaces from these UDFs was therefore non-trivial. This has been addressed in  by modifying the Marching Cubes algorithm to operate with UDFs.  Other works augment signed distance fields with covariant fields to encode open surface garments [10][11].

**Draping Garments over 3D Bodies.** Two main classes of methods coexist, physics-based algorithms [12][13][12][14][15] that produce high-quality drapings but at a high computational cost, and data-driven approaches that are faster but often at the cost of realism.

Among the latter, template-based approaches [16][17][18][19][20][11][16] are dominant. Each garment is modeled by a specific triangulated mesh and a draping function is learned for each one. In other words, they do not generalize. There are however a number of exceptions. In [21][22] the mesh is replaced by 3D point clouds that can represent generic garments. This enables deforming garments with arbitrary topology and geometric complexity, by estimating the deformation separately for each point. [23] goes further and allows differentiable changes in garment topology by sampling a fixed number of points from the body mesh. Unfortunately, this point cloud representation severely limits possible downstream applications.

In recent approaches [24][25], a space of garments is learned with clothing items modeled as inflated SDFs and one single shared network to predict their deformations as a 3D displacement field. This makes deployment in real-world scenarios easier and allows the reconstruction of garments from images and 3D scans. However, the inflated SDF scheme reduces realism and precludes post-processing using standard physics-based simulators or other cloth-specific downstream applications. Furthermore, both models are fully supervised and require a dataset of draped garments whose collection is extremely time-consuming.

Alleviating the need for costly ground-truth draped garments is tackled in [16][20], by introducing physics-basedlosses to train draping networks in a self-supervised manner. The approach of [20] relies on a mass spring model to enforce the physical consistency of static garments deformed by different body poses. The method of [16] also accounts for variable body shapes and dynamic effects; furthermore, it incorporates a more realistic and expressive material model. 

",0