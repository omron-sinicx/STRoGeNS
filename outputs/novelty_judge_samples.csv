,BART,LED,LLAMA2,GPT
3331," **Facial Landmark Datasets.** Existing facial landmark datasets [1][2][3][4][5][6][7][8][9][10][11][12] can be roughly divided into two categories: 2D and 3D datasets. 2D datasets [2][1][3] are constructed by manually annotating 2D facial landmarks in images. However, the annotation process is time-consuming and labor-intensive, which limits the scalability of the datasets. 3D facial datasets [7][6] are collected from 3D face scanners, which provide high-quality 3D scans of the human face. These datasets provide dense annotations of facial landmarks, but the annotation quality is limited due to the lack of 3D annotations. In this work, we propose a 3D-aware facial landmark dataset by leveraging the recent advances in generative visual models and neural rendering.

**Synthetic Data Generation.** Synthetic data has been widely used in various computer vision tasks [13][14][15][16][17][18][19][20][21][22][23]. For example, [16] proposed a learning-to-simulate framework, which learns a mapping from synthetic images to real images. [17] proposed an adversarial training framework to learn a mapping between simulated and unsupervised images. DECAF [18] proposed causal-aware generative networks to generate fair synthetic data. [20] proposed to learn an appearance-based gaze estimator from one million synthesised images. Recently, [23] proposed StyleRig, which uses CycleGAN [22] to generate photo-realistic portrait images. Different from previous works, we leverage the recent progress in 3D image synthesis [24] to construct a synthetic dataset for facial landmark detection. We propose a novel multi-view consistent learning strategy to enforce 3D consistency while training on the synthetic dataset. We demonstrate the superiority of the proposed approach against state-of-the-art methods on several real and synthetic datasets, and show that our approach can significantly improve the performance of existing facial landmark detectors on both synthetic and real datasets. We refer the readers to [25][26] for more related works on facial landmark tracking and alignment, respectively.

 proposed the first deep learning-based facial landmark detector, which is based on cascaded cascaded networks [27][28][29][30]. [31][32] extended the cascaded framework to 3D and proposed a face alignment in full pose range (FITR) framework. [11] proposed the 3D Total Solution (3DTS) framework to simultaneously estimate facial landmarks and estimate facial pose.  proposed a multi-task learning framework to jointly estimate face alignment, expression, and pose estimation. [15] proposed FaceForensics++ to detect manipulated facial images.  presented a face detection framework based on generative adversarial networks (GANs).

Different from the existing works, our work focuses on 3D consistent training on synthetic data, which can be used to improve 3D/2D landmark detection accuracy.

 presented a generative model-based approach"," **Facial Landmark Datasets.** Existing datasets for facial landmark detection can be roughly divided into two categories: 1) 2D datasets [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15] and 3D data [16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32]. The first category of datasets [9][8] is the most commonly used dataset for training landmark detection algorithms. It provides annotations of up to 21 facial landmarks per image. The second category [3][10] provides annotations for 3D facial landmark localization. The 3D landmark detection dataset [12] provides 3D annotations of 3D/2D landmarks in the wild. The third category [15][20] contains annotations of 2D/3D facial landmarks in a variety of facial expressions.

**Synthetic Data.** Synthetic data has been widely used in computer vision to bridge the domain gap between real and synthetic data [17][16][18]. For example, [17] proposed to use adversarial training to learn from synthetic images. [18] proposed a GAN-based fair synthetic data generator for tabular data. [16] proposed reinforcement learning-based method for automatically adjusting the parameters of any non-differentiable simulator with the actual underlying goal of maximizing accuracy of maximizing the accuracy of a model trained on that data. Recently, [20] proposed UnityEyes, a novel method to rapidly synthesize large amounts of variable eye region images as training data. In this work, we leverage the recent advances in generative visual models and neural rendering to construct a synthetic dataset and propose a novel multi-view consistent training strategy to improve 3D-aware facial landmark detector.


","[ Facial landmark detection has been an active research topic in the past decades [1][2][3][4][5][6][7][8][9][10][11]. We focus on exploring approaches based on 2D vision and 3D model fusion as these two types of approaches are generally applicable to both real and synthetic images and can benefit from accurate 3D geometric information.

**2D-based learning approaches.** The first generation of algorithms utilized expert annotations to train a 2D discriminative face detector, such as LBP , HOG , and SVM . With the recent success of deep learning, many approaches [12] are proposed to generate more discriminative features for landmark localization. The advancement of large-scale deep learning methods such as VGG , ResNet [13] and encoder-decoder structure [14] further enable end-to-end 2D learning. These works are empirically demonstrated to be capable of detecting landmarks on unseen faces with a single image without additional pre-training [12][15]. They have been applied to face forgery [15] and facial emotion detection [16] for solving challenging problems with large-scale real images.

However, these methods are trained and tested only on real images and hence may fail in the presence of domain gap, _e.g_., when training with real images and testing with synthetic images. Several prior works have addressed this issue by adjusting training image generation process. Chang _et al_. [17] incorporated domain adaptation into the model by adversarial training with a generator. Harley _et al_. [16] treated the data generation process as a reinforcement learning problem and introduced an agent to learn the parameters of a differentiable simulation engine. DeeplabFaceNet  proposed an augmentation strategy by embedding patches into an existing auto-encoder. Meanwhile, some works have achieved decent results on in-the-wild landmark detection by augmenting real images with large-scale synthetic datasets [18][19]. They are limited by the annotation and the capability of the existing 2D methods.

**3D-based learning approaches.** Several pioneering approaches [15][16] attempted to learn from a real-to-synthetic image pairing by directly imposing a data term on 2D datasets. However, these approaches have limited 3D consistency and can hardly generalize to synthetic face alignment datasets. While many methods [20][21][22][23] proposed to solve this issue by integrating 3D facial landmark detection with CycleGAN [22] or DCGAN , the face landmark detection problem has not yet been fully solved. A new dataset with 3D ground truths [24] has been recently proposed for real-to-synthetic image generation, but it has no 3]","<>
Accurate facial landmark detection on in-the-wild images is crucial for various applications such as human-computer interaction, entertainment, and medical applications. The task of facial landmark detection has seen significant advancements, with a focus on enforcing 3D consistency while detecting 3D/2D facial landmarks. In the past, limitations existed due to the lack of multi-view in-the-wild training data [1]. To address this, recent advances in generative visual models and neural rendering have paved the way for rapid progress towards high-quality 3D image synthesis [5]. Leveraging such approaches has enabled the construction of synthetic datasets and the proposal of novel multi-view consistent learning strategies to improve 3D facial landmark detection accuracy on in-the-wild images, demonstrating superiority over state-of-the-art methods [15]. Additionally, there have been efforts to develop large-scale, detailed 3D face datasets and algorithms for predicting elaborate riggable 3D face models from a single image input [6]. These developments have significantly contributed to the enhancement of 3D-aware facial landmark detection [12].

Automatic facial point detection, a fundamental aspect of face analysis, has been the focus of several research efforts. The 300 Faces in-the-Wild Challenge aimed to compare the performance of different methods on a new-collected dataset, facilitating the development of facial landmark localization systems using the same evaluation protocol and markup [3]. Furthermore, the Annotated Facial Landmarks in the Wild (AFLW) database has emerged as a substantial resource, providing a large-scale collection of labeled images for facial landmark localization, thus overcoming limitations in existing databases [8]. Addressing large-pose face alignment challenges, recent works have been dedicated to developing a 3D total solution for face alignment, catering to the need for robust alignment in varying face poses [31][32]. These collective endeavors have significantly provoked advancements in facial landmark detection and alignment, further enhancing the robustness and accuracy of landmark localization algorithms for various facial poses and expressions [26][27][30].

In the realm of synthetic data and its role in facial analysis, researchers have explored the potential of using synthetic data alone for face-related computer vision tasks [19]. The innovations in learning-by-synthesis methods, as demonstrated by UnityEyes, have shown promise in rapidly synthesizing large amounts of variable eye region images as training data, leading to effective gaze estimation, even in challenging in-the-wild scenarios [20]. Moreover, approaches such as DAD-3DHeads have played a crucial role in addressing the need for large-scale dense and diverse datasets for 3D head alignment and reconstruction, resulting in state-of-the-art performance in various tasks related to 3D facial analysis [12]. Additionally, efforts to bridge the domain gap between synthetic and real image distributions through adversarial training have resulted in significant progress in image-to-image translation, further impacting the field of facial analysis [22]. These advancements highlight the growing significance of synthetic data and its utility in enhancing facial analysis tasks."
11," **Layout Generation.** Traditional approaches for layout generation are based on heuristics [1][2] and rule-based methods [3][4][5][6][7][8][9]. Recently, deep generative models have been applied to layout generation [10][11][12][13][14][15][16][17][18]. For example, CanvasVAE [10] and LayoutGAN [16] use variational auto-encoders (VAEs) to model the distribution of the layout data, where the VAE encodes the layout into a low-dimensional latent space and the GAN decodes the latent code into a high-dimensional image. However, these methods are not able to generate high-quality layouts with optional constraints, such as type or position of a specific element. To address this issue, LayoutVAE and its variants [14][17] propose to condition the layout generation on a label set, which can be regarded as a special case of our conditional generation setting. In this work, we propose a new discrete diffusion model that can be applied to a broad range of layout generation tasks.

**Controllable Layout Generation**. Controllable layout generation aims at synthesizing plausible arrangement of element bounding boxes with various constraints, including type, position, and layout type. Previous works on layout generation can be divided into two categories. The first category is based on optimization-based approaches [11][20][21], where the layout is optimized based on a set of heuristic rules. For instance, Vinci [7] optimizes the layout of an advertising poster based on the design principles of the poster. The second category is to use generative methods to generate the layout conditioned on the input constraints [12][15]. For instance [12] proposes to use a VAE-based generative model with a transformer encoder [19] to encode the input layout data into a latent code and then decode it back to the image space. For the conditional generation, [13] proposes a coarse-to-fine model that first generates a coarse layout and then fine-tunes it with the given constraints. In contrast, our model is able to learn to progressively infer a noiseless layout from the initial input, where we model the corruption process by modality-wise discrete diffusion. Moreover, we inject the layout constraints in the form of masking or logit adjustment during inference, which enables our model to generate layouts with various types of constraints. We show in the experiments that our LayoutDM outperforms both task-specific and task-agnostic baselines on several layout tasks, which is the first work that can generate diverse and controllable layouts in a single model that is built based on discrete diffusion models. Our work is also related to [17], where a transformer-based model is proposed to refine the UI layout aesthetics. Different from these works, we focus on the controllability of the generated layouts, which has not been studied in the literature before. We propose to inject layout constraints into the model by masking/logit adjustment, and show that our model can"," Layout Generation.Automated layout generation has been a long-standing research topic in the graphic design community [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30]. In this work, we mainly focus on controllable layout generation, which aims at synthesizing plausible layouts with various constraints, such as the type or position of a specific element.

**Controllable Layout Generation**. Controllable layouts have been studied in various domains, including graphic design [4][4], UI design [5][5], and presentation [6][10]. For example, [4] proposed a system to automatically generate visual-textual presentation layouts by investigating a set of aesthetic design principles and a computational framework integrating high-level aesthetic principles and low-level image features. [6] presented a hierarchical sequence-to-sequence approach to generate document-level layouts from scientific documents. [10] proposed to learn a sequence of compositional VAE-based models to generate vector graphic documents from text descriptions. [8] presented an interactive system to optimize the layout of web page layouts. [3] proposed an energy-based model to automatically create graphic design layouts using design principles derived from design principles. [7] proposed Vinci, an intelligent system that supports the automatic generation of advertising posters by matching the user-specified product images and layouts for generating an aesthetic poster. [9] presented to generate infographics by automatically imitating infographics from example-based infographics. [5] presented Rico, a data-driven layout dataset for mobile app design. [20] presented interactive furniture layout system that assists users by suggesting furniture arrangements that are based on interior design guidelines. [21] presented furniture arrangement optimization system that extracts hierarchical and spatial relationships for various furniture objects, encoding them into priors associated with ergonomic factors, and optimizing a cost function whose optimization yields realistic furniture arrangements. [17] proposed RUITE, a transformer-based layout refinement engine that optimizes layouts using a Transformer Encoder [19]. [14] proposed LayoutVAE, a variational autoencoder based framework for generating full image layouts given a label set. [16] proposed the LayoutGAN model that synthesizes layouts by modeling geometric relations of different types of 2D graphic elements and uses self-","[ **Layout generation.** Layout generation is a long-standing research problem with wide applications in the realms of user interface design, graphic design and scene generation [1]. Most of early works study the single-page design, and very few studies the multi-page layout [2][3][4][5][6][7]. The former often aims at handling the layout with high content density such as data visualization, data infographic, or mobile app design [8][9][10]. The latter usually deals with the layout with content diversity and layout context, such as scientific poster, landscape, product or living room scenes [11][12][13][14][15][16][17].

**Controllable generation.** For controllable layout generation, several methods [14][18] incorporate attribute generation with a coarse-to-fine model. RUITE [17] is the first to investigate generating the layout of user interface by unconditional autoregressive generation. Several methods use heuristic or predefined parameters to constrain the generation [19][20][21].

**Diffusion models.** Diffusion models have been applied for image generation [22][23][24][25]. D3PM [26] and ArMaxFlow [27] are the first to apply diffusion models for discrete generation, where both ArMaxFlow and D3PM are two-layer models with no modality-specific diffusion (meaning a modality-agnostic diffusion parameter \(\theta\)) for all the modalities. VQ-Diffusion [28] extends D3PM to make it modality-specific by conditioning the diffusion on the modality of the noise. RePaint [29] and SDEdit [30] are the first to introduce latent diffusion to image generation. However, their diffusion process is done in the continuous state space. Our LayoutDM is the first to introduce latent diffusion to discrete state space. Unlike SDEdit, RePaint and the aforementioned unconditional generators, our proposed LayoutDM is able to model conditional generation.

]","<In the field of automated layout generation, various techniques have been proposed to address the challenges of synthesizing plausible and controllable layouts. Some approaches focus on automated layout techniques for information presentations [2], aiming to determine the sizes and positions of visual objects as part of presentations. Others have explored learning layouts for single-page graphic designs [3] and automatic generation of visual-textual presentation layouts [4], demonstrating the potential for creating visually appealing layouts in different contexts. Additionally, research efforts have resulted in the development of systems such as LayoutVAE [10] and LayoutGAN [16] that aim to generate graphic layouts with wireframe discriminators, indicating a trend towards using generative adversarial networks in layout generation.>

<Furthermore, the use of deep generative models has been explored in the context of document-to-slide generation, as evidenced by the work on DOC2PPT [6]. This approach highlights the potential of leveraging hierarchical sequence-to-sequence techniques to summarize key concepts and arrange them in visually pleasing manners. Similarly, content-aware generative modeling of graphic design layouts has been investigated [11], focusing on capturing the effect of visual and textual contents on layouts, thus demonstrating an understanding of the interaction between content and layout in design. To empower users in generating aesthetic layouts, intelligent systems like Vinci have been introduced to support the automatic generation of advertising posters [7]. The pursuit of automating the creation of aesthetically compelling layouts aligns with the goal of controllable layout generation as showcased in LayoutDM.>

<In the domain of synthetic scene layout generation, research efforts have been directed towards exploring autoregressive transformers for indoor scene synthesis [18], which facilitates the creation of diverse and plausible synthetic indoor environments based on room types and floor plans. Moreover, the introduction of structured denoising diffusion models in discrete state-spaces [26] signifies a shift towards addressing diffusion-like generative models for discrete data, thus expanding the scope of layout generation to different data representation spaces. These advancements underscore the diverse approaches and methodologies employed in layout generation, paving the way for controllable layout generation models such as LayoutDM to draw inspiration from various techniques and adapt them to specific layout tasks.>"
2929," **Generative Adversarial Networks (GANs).** GANs [1][2][3][4][5] are powerful generative models capable of synthesizing high-fidelity images. However, they suffer from mode collapse and mode-collapse, limiting their ability to generate high-resolution images.

**Latent Diffusion Models (LDMs).** Recently, diffusion models [6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28] have shown promising results on image generation. These models are based on a two-stage process: first, a diffusion model is trained to denoise an input image, then a latent code is decoded back to the input image. In this work, we leverage the latent code of the diffusion model to embed a watermark into the model's latent space. The watermark is then embedded into the decoder of the generative model, allowing it to be used for active content tracing. We show that the proposed method can be applied to a wide range of generation tasks, including text-to-image and text-image editing. We also evaluate the robustness of the watermark to image modifications, showing that it is both effective and robust to image manipulation.
**Generative Image Detection.** Generative image detection aims to identify whether an image is generated by a GAN or not [29][30][31][32][33][34][35][36][37][38][39][40][41][42][43]. In this paper, we focus on the task of image detection, which aims to determine whether a generated image is from a generative or non-generative model. The goal is to identify the source of an image, allowing for future detection and/or identification. In contrast, our method aims to trace the content of the generated image, which is a more challenging task due to the high-dimensional nature of the image space. We evaluate our method on a variety of image generation tasks and show that it can achieve high accuracy at a high false positive rate (FPR) below 10\({}^{-6}\).

Deep Watermarking.Deep watermarking [44][45][46][47][48][49][50][51][52][53][54][55][56][57][58] is a technique for embedding a secret into a neural network, which can be used to protect the privacy of the network. The key idea is to embed the secret into the network's weights, so that the network can be easily identified by comparing the hidden weights with those of the original network. In the context of generative image modeling, deep watermarks have been used for attribute attribution [54] and watermark disclosure [55]. In [54], the authors propose to attribute the generated images to a pre-trained diffusion model. In [55], they propose to embed watermarks into the latent space of the model, and use the extracted watermarks to attribute generated"," Generative Adversarial Networks (GANs)  and Variational Auto-Encoders (VAEs)  have been shown to be effective at generating high quality images [1][2][3][4][5][6][7][8][9][10][11]. Diffusion models [12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28] have been used to synthesize images conditioned on text prompts [23][23]. These methods have been applied to various generation tasks such as image inpainting [21][23], image colorization [22], image editing [23], video generation [17], and text-to-image generation [18][20]. In this work, we focus on text-conditioned image generation.

Generative adversarial networks have been widely used in image forensics [29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][68]. In particular, the work of [39] shows that each GAN leaves its specific fingerprint in the images it generates, just like real or fake images. In contrast, we show that latent diffusion models leave watermarks in the same way.

","[ Generative Models for Image SynthesisMost works on image synthesis involve training GANs [1][2][3] or style-based models [4][5][6][7]. In the post-GAN era, progress comes from diffusion models [8][9][10][11][12][13][14][15][16][17][18][19][20]. Like GANs, these models produce realistic images from a random noise initialization. However, they do not need to train a discriminator, and they can reach higher resolution with only 2D transformations.

A second stream of work focuses on image editing with diffusion models. This includes inpainting [21], image-to-image translation [22], image editing [23][24][25], or image editing from image-to-image translation [26][27]. This work also proposed to learn to model a per-pixel attention to better sample from the image formation process [28].

DeepFake DetectionDiffusion models [29][30][31][32][33][34][35] have led to high-fidelity images, which have sparked interest in deepfake detection [36][37][38][39][40][41][42][43]. The lack of high-quality datasets for training these detectors is a significant barrier [40]. Given these shortcomings, we contribute a new dataset and benchmark to further enable this work.

WatermarkingThe goal of watermarking is to embed secret information into images. The watermarked image can be stored along with its owner, allowing the owner to prove its legitimacy [44]. In digital watermarking, a target image is split into blocks and encrypted to avoid directly stealing the secret image content. Secret information is embedded by changing the transformation that the decryptor applies to the blocks .

A neural network can embed information into an image, making it more detectable [45][46][47][48]. The resulting watermark is very fragile and requires restoration before inspection. This is particularly a concern in practice, as the watermarked image can be distorted in the meantime. Besides, it is usually difficult to ensure the watermark survives the post-processing steps, such as cropping, rotation, etc., that most real applications require. We propose a different approach, where the watermark is in the latent space of the generative model. It is sturdier than a watermarked image, as we have a back-end restoration process to obtain a clear image. Furthermore, the watermark does not require the re-insertion step.

Deepfake AttributionPrevious works propose to detect the source of an image or video through an image's visual]","<The use of generative image modeling has become increasingly prevalent in various applications, raising concerns about responsible deployment and ethical considerations [43]. One area of focus is on active content tracing methods that combine image watermarking and Latent Diffusion Models to conceal an invisible watermark in generated images [1]. The incorporation of a pre-trained watermark extractor and a statistical test allows for future detection and/or identification of the concealed watermark [1]. This approach has shown robustness to image modifications and has been evaluated on various generation tasks [43]. Furthermore, recent advances in generative adversarial networks (GANs) have led to significant improvements in image generation, offering a potential framework for comparison with latent diffusion models and watermarking techniques [1][2].

Another significant advancement in generative modeling is the StyleGAN architecture, which has set new standards for generative modeling in terms of image quality, controllability, and distribution metrics [3]. The architecture's ability to disentangle high-level attributes and stochastic variations in generated images has improved the state-of-the-art in traditional distribution quality metrics, interpolation properties, and latent factor disentanglement [2]. Additionally, improvements in image quality and the ease of image attribution have been achieved through redesigned generator normalization, progressive growing, and path length regularization [3].

The deployment of text-to-image generation models, such as CogView and Make-A-Video, has also presented challenges in detecting and attributing fake images generated by neural networks [7][8]. These models have demonstrated exceptional performance in generating high-quality images and videos from text prompts [8]. However, concerns about misuse and misinformation have emphasized the need for reliable detection and attribution methods to address deepfake phenomena [6][7].

Recent research has also explored techniques for detecting and simulating artifacts in GAN-generated fake images, leveraging the use of supervised and unsupervised approaches for image forensics and model attribution [39][40]. The development of digital watermarking and steganography approaches has emerged as a potential solution for the protection and attribution of generative models while addressing ethical concerns related to their deployment and potential misuse [44][45]. By embedding watermarks and latent fingerprints into deep neural networks, it may be possible to attribute generated content to specific models and ensure responsible disclosure of generative models [48][49]. Ultimately, these advancements in digital watermarking, steganography, and image attribution techniques hold promise for addressing ethical concerns and ensuring responsible deployment of generative models in various applications [51].>"
2913," **Single-view 3D Human Pose Estimation.** Single-view methods [1][2][3][4] estimate 3D human pose from a single RGB image, which can be divided into two categories: single-person and multi-person. Single-person methods [5][6][7][8][9][10][11][12] directly estimate the 3D pose of each person in the image. Multi-person approaches [13][14][15][16][17] first estimate the 2D poses of all the people in the scene and then triangulate them to obtain the final 3D poses.

**Uncalibrated Multi-View 3D HPE.** Most of the existing multi-view HPE methods [14][13][15] are calibrated, which assume that all cameras are synchronized and calibrated. However, it is difficult to apply them to uncalibrated scenes, where cameras may not be synchronized and there may be occlusions between people. To address this problem, [14] proposes to estimate the camera pose from the estimated 2D heatmaps and then use the estimated camera pose as the calibration pattern. [15] proposes a generalizable human pose triangulation (GHP) module that can be embedded into existing calibrated methods to improve the generalization ability. [16] proposes FLEX, which is parameter-free and can be used for uncalibration scenes. [17] proposes an extrinsic camera calibration method for a moving person, which assumes that the camera poses of the moving person can be estimated from the detected 2D keypoints. Our method differs from these methods in two aspects. First, our method does not require the keypoints of the person to be detected. Second, we propose a novel Probabilistic Triangulation module that is end-to-end trainable, enabling the gradients to be directly back-propagated from 3D heatmap to 2D pose heatmap, enabling training with 2D ground truth heatmaps only. This allows our method to achieve comparable results with state-of-the-art calibrated methods, which demonstrates the effectiveness of our proposed method. Note that our method is orthogonal to [16][15], which can also be integrated into our method for further performance improvement. We leave it for future work to explore how to combine our method with these methods to further improve the performance and generalizability. Note also that we do not compare our method against [16], which is a calibration-free method, in Sec. 4.3.2.3 and 4.4.3, as our method can be integrated with any calibrated methods. We will leave it as future work for further investigation.
**Probabilistic Perspective-n-Point (PnP) Optimization.** Our method is also related to the probabilistic PnP optimization methods [18][19][20], which aim to learn the probability distribution of the perspective-angle of the camera. [18] proposes MonoRUn, which estimates the camera and 3D object poses in a unified framework and"," **Multi-view 3D Human Pose Estimation.** Multi-view human pose estimation has been extensively studied in the past few years, which can be roughly divided into two categories: single-view and multi-view methods. Single-view approaches [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] estimate 3D human pose from 2D keypoints detected in the 2D view. These methods can be categorized into two types: the one-stage methods and the two-stage ones. The two-step methods first detect 2D joints in the source view and then estimate the 3D pose of each person in the reference view. For example, [8] proposes a cross-view fusion scheme to fuse 2D features from multiple views. [10] uses a multi-way matching algorithm to cluster the detected 2D poses in all views and then finds the cross view correspondences across the keypoints. [9] proposes to use the semantic feedback from the smart edge sensors coupled with a backend through a semantic feedback loop. [12] presents a hierarchical scheme to concisely represent query embeddings of multi-person skeleton joints and introduces an input-dependent query adaptation approach. [13] presents two learnable triangulation methods that combine 3D information from multiple 2D feature maps from the backbone. [11] proposes an epipolar transformer to leverage 3D-aware features to improve 2D pose estimation.
-based methods [14][17] estimate the camera extrinsics and 3D poses of all cameras with a linear algorithm.

**Uncalibrated Multi-View 3D Pose Estation.** The main challenge in multi-camera 3D HRP estimation is the calibration process. Existing methods [8][10] are limited to fixed camera pose and therefore suffer from the generalization ability. [17] proposes the extrinsic camera calibration method for a room-scale multi-image system. [14] proposes human pose as calibration pattern to handle the case of uncalibrated cameras. [15] introduces a stochastic framework for human pose reconstruction and generalization across different camera arrangements. [16] proposes FLEX to reconstruct a single consis-tent skeleton with temporally coherent joint rotations and bone lengths. [18] proposes MonoRUn to estimate dense 3D correspondences between 2D and","[ **Single-view 3D human pose estimation.** Recent deep learning methods focus on end-to-end 3D human pose estimation from a single image, with many methods achieving very competitive performance [1][2][3][4][5]. [5] is one of the first methods to propose a deep learning approach for 3D human pose estimation. It combines keypoint-based detection, 2D pose estimation and 3D pose estimation to recover 3D poses from a single image. Wang _et al_.  applies the Neural Network for Human Pose Estimation (NNHPE) [2] for 3D human pose estimation. To solve the intra-class uncertainties in 2D pose heatmaps, they use one-way and two-way image-to-image correspondence losses. Inspired by spatial-temporal transformer architecture, S3DHPE  uses 3D temporal convolution to fuse the multi-view 3D human pose. Chen _et al_.  presents a fully-convolutional model that learns both pose representation and 3D pose regression from multi-view inputs.

**Multi-view 3D human pose estimation.** Multi-view 3D human pose estimation [6][7][8][9][10] models pose as a latent variable with fixed camera parameters. There are two types of methods. The first type [11][12] employs the multi-view detector  to estimate the 2D poses from each view. Then these 2D poses are clustered into different people, and triangulated to obtain the 3D pose of each person. The second type [13][14][15] predicts the camera pose for each view first, and then uses the predicted camera poses to obtain 2D poses. The results are fused using back-projection to obtain the final 3D pose estimation. However, these methods depend heavily on the camera calibration and, thus, cannot be directly applied to the real-world setting where the camera is usually not known in advance.

Recently, some works have attempted to generalize the multi-view 3D human pose estimation in uncalibrated settings. [16] adopts an end-to-end trainable manner for human pose triangulation. However, its naive triangulation is inefficient, leading to worse performances.

Liu _et al_. [17] propose to estimate camera parameters by moving human bodies. Specifically, they use an estimated camera parameter \(\{\theta_{1},\theta_{2},\theta_{3}\}\) to estimate a reference pose and then reconstruct the pose of a person. The estimated camera parameters are then used to estimate the pose. However, this method does not enable online inference, as the person pose is estimated without the estimated camera parameters.

Recently, Li _et al_. [14] also proposed to estimate]","Step 1. Extracted important sentences from the cited paper:

1. **Existing multi-view methods are restricted to fixed camera pose and therefore lack generalization ability.** 
2. **This paper presents a novel Probabilistic Triangulation module that can be embedded in a calibrated 3D human pose estimation method, generalizing it to uncalibration scenes.**
3. **Specifically, We maintain a camera pose distribution and then iteratively update this distribution by computing the posterior probability of the camera pose through Monte Carlo sampling.**
4. **Extensive experiments on Human3.6M and CMU Panoptic demonstrate that our method outperforms other uncalibration methods and achieves comparable results with state-of-the-art calibration methods.**
5. **We demonstrate qualitative and quantitative evaluations using synthesized and real data.**

Step 2: Summarization and selection of topics:
1. Limitations of existing multi-view methods due to fixed camera pose.
2. Introduction of Probabilistic Triangulation module for uncalibrated 3D human pose estimation.
3. Iterative update of camera pose distribution using Monte Carlo sampling.
4. Performance comparison with state-of-the-art calibration methods.
5. Evaluation using synthesized and real data.

Step 3. Writing related work based on topics:

<Related work has been an active research area in computer vision and graphics, with a particular focus on multi-view 3D human pose estimation. The limitation of existing multi-view methods due to fixed camera pose is a well-documented challenge [1] that has hindered their generalization ability. In addressing this limitation, recent work has introduced the concept of Probabilistic Triangulation modules for uncalibrated 3D human pose estimation, presenting a significant advancement in the field [2]. This novel approach maintains a camera pose distribution and iteratively updates it through Monte Carlo sampling, a methodology that has shown promising results in overcoming the restrictions imposed by fixed camera poses [3]>

<The introduction of the Probabilistic Triangulation module has enabled the generalization of calibrated 3D human pose estimation methods to uncalibrated scenes, resulting in improved performance compared to existing uncalibration methods and comparable results with state-of-the-art calibration methods [4]. The extensive experiments conducted on benchmark datasets such as Human3.6M and CMU Panoptic have validated the efficacy of this approach, demonstrating its superiority over traditional methods in both qualitative and quantitative evaluations, including synthesized and real data [5]. These findings underscore the significant contributions of the Probabilistic Triangulation module in addressing the limitations of existing multi-view methods and advancing the state of the art in 3D human pose estimation.>

This related work covers the essential aspects of the Probabilistic Triangulation module and its contribution to uncalibrated multi-view 3D human pose estimation, highlighting its improved performance and generalization abilities compared to existing methods in the field."
4584," Continual Learning (CL) aims to develop ML models that can learn from continuous streams of data and mitigate catastrophic forgetting [1][2][3][4]. Incremental Learning (IL) can be broadly classified into three categories: _rehearsal-based_ methods [5][6][7][8][9][10][11][12][13][14], _memory-based methods_[15][16][17][18][19][20][21][22][23][24], and _joint training_ methods.

**Rehearsals-based Methods.** These methods store a small number of exemplars from the previous tasks in a memory buffer and replay them when learning a new task. GEM [1] and EWC [4] are two of the earliest works in this category. EWC uses the Fisher information matrix to estimate the importance of each parameter for the previous task, while GEM uses the gradient of the loss with respect to the new task to compute the episodic memory. iCaRL [5] uses a cross-entropy loss to learn a unified classifier for both the old and the new tasks. To address the imbalance between old and new classes, Cui _et al_. [6] propose a class-balanced distillation loss to mitigate the bias towards the old classes. To overcome the limited capacity of the memory buffer, MCT [20] proposes to use a memory-augmented network to generate pseudo-exemplars for the old tasks. In addition to exemplars, some methods use knowledge distillation [9][11] to transfer the knowledge from the old task to the novel task. In FOSTER [14], the authors propose a feature boosting and compressing method to improve the generalisation ability of the model.

The memory-based approaches suffer from a trade-off between the memory size and the performance of the incremental model. In contrast, the memory-less methods do not require any memory at all. The memory-free methods [19][22] do not use any exemplars at all for the incremental task. Instead, they use the feature extractor to extract features from the past tasks and store them in the memory. The extracted features are then used to train the classifier on the current task. The classifier is then trained on the newly generated features. In ACIL [19], a memory transformer [16] is used to store the features extracted from the last few layers of the neural network. In DyTox [17], a transformer-based memory module is proposed to store and retrieve the features stored in the previous layers. In DER [12], a dynamic module is introduced to dynamically expand the representation of the network for incremental tasks. The feature expansion module is trained to adaptively select the best feature representation for each task at each training iteration. In this paper, we focus on the exemplar-based class-incremental learning methods as they are the current state-of-the-art in terms of accuracy and computational efficiency. We show that these methods are not well suited for real-world use cases."," **Continual Learning (IL).** IL aims to learn a model that can learn new tasks without catastrophic forgetting [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35]. In this paper, we focus on Class-IL, which has been studied extensively in the literature [18][18]. Class IL has been extensively studied in the past few years [18], with a wide range of approaches proposed. We categorise them into three main categories: **1) **Memory-based methods**[18][25], which store a subset of past data into a memory bank and use it to prevent forgetting when training future tasks. **2) **Replay-based Methods**[22][24] store a small subset of exemplars of the previous classes and use them to re-train the current model. **3) **Dynamic-expansion methods**, which use dynamic expansion of the model parameters to adapt to new tasks [17][14].


We categorise the memory-based approaches into two main categories. The first category of methods [18] stores exemplars from the previous tasks into the memory bank, and re-use them to train the model for the new task. For example, [19] uses a memory transformer network to combine the exemplars with the model predictions. [19], [20] optimises the exemplar selection and parameterisation in an end-to-end manner, and [21] uses reinforcement learning to optimise the policy function of the memory network. The second category [24] stores a subset from the old classes into a self-sustaining representation, and uses it to train a linear classifier for new tasks. For instance, [23] uses an exemplar generator to extract pseudo-features of the past classes, and use these pseudo features to fine-tune the model.

**Joint Training.** Joint Training [28] is a method that uses a shared backbone model to learn from a set of incremental tasks, and trains the model with a shared encoder/decoder architecture. [27] proposes to use the GPT-3 model [27], which is the current state-of-the-art model for IL, to","[ IL is a widely studied area within the field of deep learning (see [1][2][3] for a brief overview). The main purpose is to train models that can learn from streaming data and maintain the knowledge learned during training.

The past decade has seen a growing number of IL methods addressing the aforementioned problem of catastrophic forgetting, from knowledge distillation , prioritisation [4], multiple tasks [3], stability/plasticity trade-offs [5][6] and ensemble methods [7]. These approaches, however, tend to have a hard time achieving an optimal balance of stability and plasticity while managing to preserve the knowledge learnt so far. Recent work [8] demonstrates that different learning tasks lead to different IL algorithms and unified learning task settings do not necessarily exist.

Recent work has successfully incorporated IL techniques into real-world scenarios. In particular, many approaches [9][10][11][12][13] have been developed for IL with few-shot data [14]. In particular, self-supervised tasks , attention-based models [15][16], memory expansion based methods [17], token expansion-based approaches [18][19], exemplar-based models [20][21][22], self-regularisation based approaches [23] have been introduced in the literature.

To analyse current IL approaches, it is necessary to consider two different directions: Multi-shot IL where the task under evaluation has access to abundant data during training [18][24], and Exemplar-Free IL, where the data available to the model is limited to just the data examples from the new class . Both of these settings require new training data to learn new tasks and, typically, this is obtained using exemplars from the data examples of the new classes that were previously seen. We want to go beyond this typical setting and challenge our approach and approaches used by current IL algorithms in a realistic industrial scenario.

Other approaches have investigated more recent considerations to IL, such as energy consumption [25][26] and model size [27][28][29][30]. In particular, Bifano [25] introduced a new criterion for comparing energy consumption of various models on the same task. This has since been extended to include several metrics [31][32] and models with variable parameters [33]. However, these approaches are often restricted to few-shot tasks, do not take realistic class sizes into consideration and still focus more on the accuracy over other metrics, such as energy consumption. In contrast to these approaches, our research intends to provide a realistic perspective on IL and also incorporate additional metrics such as energy consumption into IL experiments.

When considering the industrial set-up, the relevance of an optimal representation of data is also of interest [34][35][36]. Of relevance is also the ability to predict and learn in an un]","<Incremental Learning (IL) has gained attention for developing Machine Learning (ML) models that can learn from continuous streams of data and mitigate catastrophic forgetting [1]. However, the current state-of-the-art IL implementations have been criticized for focusing excessively on accuracy metrics, neglecting evaluation in terms of energy consumption and computational load [1]. To bridge this gap, recent efforts have surfaced to introduce more realistic evaluation scenarios for Continual Learning methods, emphasizing energy consumption and overall computational load [1, 25]. This is essential for a comprehensive understanding and applicability of IL approaches in practical use cases, especially in industrial setups [1]. Additionally, the introduction of novel datasets such as InVar-100 (Industrial Objects in Varied Contexts) has facilitated the simulation of visual environments in industrial setups, allowing for various experiments for IL [1].

Recent advances have seen a shift towards addressing the challenge of catastrophic forgetting in class-incremental learning by proposing new methods such as Gradient Episodic Memory (GEM) and synaptically intelligent networks [2, 4]. These approaches aim to alleviate forgetting while enabling beneficial transfer of knowledge to previous tasks [2, 4]. Furthermore, the introduction of methods such as Dynamic Network Expansion and self-sustaining representation expansion has shown promise in achieving a stability-plasticity trade-off, improving accuracy for past as well as new classes without storing historical data [24, 25]. These developments demonstrate a move towards more effective handling of catastrophic forgetting in class-incremental learning scenarios.

In the context of class-incremental learning, methods such as iCaRL have been introduced to enable incremental learning in a class-incremental way, allowing for the addition of new classes progressively without the need for fixed data representations, thereby opening possibilities for deep learning architectures [5]. Additionally, the development of frameworks like ACIL (Analytic Class-Incremental Learning) has demonstrated competitive accuracy performance while ensuring privacy in class-incremental learning [32]. These advancements highlight the ongoing efforts to enhance the performance and privacy-preserving capabilities of class-incremental learning methods.

The environmental impact of training machine learning models has also come under scrutiny, leading to the development of tools such as the Machine Learning Emissions Calculator and methods for quantifying the carbon footprint of computation [25, 26]. These endeavors are essential for raising awareness and encouraging responsible practices in the machine learning community. Furthermore, the implications of scaling language and vision transformers to achieve remarkable performance across various tasks, including few-shot learning, have set the stage for exploring the potential of large-scale models in different domains [27, 25, 28, 29]. Additionally, ongoing research on continual learning for automatic speech recognition in production-scale systems underscores the applicability of incremental learning across diverse domains [31].>"
1374," **Video Recognition.** Early video recognition methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18] mainly focus on designing efficient video recognition models. Among them, two-stream networks [2][2] and 3D convolution networks [9][8] are the most popular methods, which first extract frame-level features from RGB frames and then fuse them with optical flow to obtain the temporal information. However, these methods are computationally expensive due to the heavy computation cost. To reduce the computation cost, some works [5][7] propose to use the temporal attention mechanism to capture temporal information, while some other works [6][10] utilize the temporal difference module to reduce the computational cost. Besides, some other methods [13][18][16] focus on the temporal saliency detection and aggregation. For example, NSNet [18] proposes a non-saliency suppression mechanism to suppress the non-discriminative frames, while TDN [10] introduces a temporal difference network to capture the difference between two consecutive frames.

**Vision Transformer for Video Understanding.** Transformer [19][20][21][22][23][24] has been widely used in video understanding due to its superior performance in capturing long-range dependencies. ViViT [22] is the first pure-transformer-based model for video understanding. Later, Video Swin Transformer (VidT) [23] is proposed to further improve the efficiency of ViT by using shifted windows [20] and multi-scale attention [24]. Besides, TimeSformer [21] proposes to use self-attention mechanism to model temporal information in videos. In this paper, we also adopt a pure-Transformer based architecture for video recognition. However different from previous works, we focus on exploring the bidirectional cross-modal knowledge of pre-trained VLMs to enhance the video recognition performance. We propose a novel framework to explore bidirectionality in the video domain, which can be applied to various video recognition tasks with a parameter-free manner.


**Vision-Language Models for Image-Video understanding.** Recently, large-scale vision-language models (VLMs) [25][26][27][28][29][30][31][32][33][34] have demonstrated impressive transferability on various image-text pairs. CLIP [25] pre-trains a powerful VLM on 400 million images and 400 million text-video pairs, which is then fine-tuned on image classification tasks to achieve state-of-the-art performance. FrozenCLIP [29] extends CLIP to video recognition by freezing the last few layers of the frozen CLIP model and freezing the rest of the parameters. Frozen-V2 [30] further expands the model size by expanding the CLIP-base model to a large number of video-specific sub-networks. Some works [31][33] explore the parameter-efficient image-to-"," **Video Recognition.** Most existing video recognition methods are based on deep Convolutional Neural Networks (CNNs) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34]. In particular, CNN-based methods have achieved state-of-the-art performance in video recognition. However, they suffer from the high computational cost and the lack of temporal modeling capability. To address these issues, many methods have been proposed to reduce the computational cost of video recognition, such as frame sampling [13][15], dynamic inference [16][16], temporal sampler [17][13], adaptive focus [14], multi-agent reinforcement learning [15], and temporal attention [18]. However, these methods are designed for video classification, which is a more challenging task. In this paper, we propose a novel framework to explore bidirectional cross-modal knowledge for video recognition with pre-trained VLMs.

**Pre-trained Vision-Language Models.** Pre-trained vision-language models (VLMs) [25] have demonstrated impressive transferability on various visual recognition tasks [25]. Recently, several works [29][29] have shown that pre-training a VLM on large-scale image-text pairs is an efficient way to transfer knowledge to video recognition tasks. For example, EVL [29] proposes a lightweight Transformer decoder and a query token to collect frame-level spatial features from the VL encoder. EVL  proposes a cross-frame attention mechanism to exchange information across frames. UATVR [26] proposes an auxiliary probabilistic inference procedure to aggregate the video-to-text and video-text embeddings for more flexible inference. Cap4Video [27] proposes to directly generate associated captions from videos using zero-shot video captioning with knowledge from web-scale VL models. However these methods only explore the bidirectionality of the pre-train VLM, and do not explore the video attributes. In contrast, our framework explores the video attribute association mechanism, which leverages the video knowledge to generate textual auxiliary attributes for complementing video recognition in a parameter-free manner.

","[ **Video Recognition.** Most existing works focus on modeling spatio-temporal features using 2D CNNs. [1] and [2] propose two-stream architectures to use both 2D and 3D information to improve video representation. [3] and [4] learn the temporal sequence from two views: verb and noun. Other works propose 3D convolutional layers to build 3D spatio-temporal features [5][6][7][8][9][10][11][12]. Recently, [13][14][15][16][17][18] propose efficient methods based on a class-agnostic sampler to select informative segments for efficient video recognition. Differently, we are interested in learning the pretrained VLMs, which need to adapt both image-text and video representations. We use the output of a well-trained VLM to supervise the fine-tuning of video models, which differs from the above works.

**Transformer-based Methods.** The recent success of the Transformer architecture [19] on vision tasks has encouraged researchers to explore transformer-based models in video. [20][21][22][23][24] mainly learn the spatial or temporal features in a self-attention manner. [22][24] propose pure-transformer based models to learn video features. ISFA [21] combines the transformer and non-local blocks for stronger contextualization. [23] achieves better results on temporal features by directly learning local visual tokens. Besides transformer-based methods, there are some works exploring dense visual-textual alignment in a MLP-based manner.  proposes CLIP-v, which encodes audio into a few visual tokens, while  use a sparse transformer-based model for sparse alignment to speed up the inference. Our proposed BIKE exploits the bidirectional visual-textual knowledge for video representation and introduces several parameter-free mechanisms for enhanced visual features.

**Video-textual Knowledge Transfer.** Video knowledge has shown powerful generalization to other downstream tasks. Recently, [25] propose CLIP, which shows impressive performance on video captioning and text-video retrieval.  achieve strong performance on action detection by utilizing temporal information in videos.  improve video representation by adding the hierarchical language codes. [26][27][28] propose effective retrieval methods to conduct multi-modal matching between videos and captions. [29] adopts an efficient incremental training process to speed up the large-scale pretraining of CLIP. However, previous works usually focus on the image-text domain, while the real-world video datasets lack good textual information. Our BIKE addresses the aforementioned problem by exploring bidirectional visual-textual knowledge in a novel transformer-]","<In recent years, the use of pre-trained vision-language models (VLMs) has gained significant attention for video recognition tasks. The ability of VLMs to capture cross-modal relationships between visual and textual domains has shown promising results in enhancing video recognition models. Traditional approaches for video recognition have predominantly focused on architectures such as SlowFast Networks [1], Two-Stream Convolutional Networks [2], and TEA [5] for action recognition. These methods leverage spatial and temporal information from videos to capture motion, context, and features for accurate classification. However, these approaches may not fully exploit the potential of cross-modal knowledge exploration offered by pre-trained VLMs.

The proposed framework, BIKE, leverages pre-trained VLMs to explore bidirectional knowledge transfer for video recognition. BIKE's Video Attribute Association mechanism takes advantage of Video-to-Text knowledge, generating textual auxiliary attributes for enhancing video recognition. Similarly, the Temporal Concept Spotting mechanism utilizes Text-to-Video expertise to capture temporal saliency and enhance video representation, leading to state-of-the-art performance on popular video datasets. This aligns with recent research on the integration of vision and language for video understanding, as well as the exploration of bidirectional knowledge transfer for enhanced video recognition [6] [10] [11].

Furthermore, the use of dynamic frame sampling strategies [13] and adaptations of image models for efficient video action recognition [32] [33] have also attracted attention in the community. These methods aim to optimize video recognition by leveraging techniques such as non-saliency suppression [18], uncertainty-adaptive retrieval [26], and token clustering [28]. The adoption of transformer-based video recognition models [20] [21] and the study of different architectures such as TimeSformer [21] and ViViT [22] have also contributed to the advancement of video understanding techniques. These approaches demonstrate the transition towards novel paradigms in video action recognition, signaling a shift from traditional end-to-end learning approaches.

Lastly, the emerging trend of integrating language supervision for video recognition, as seen in approaches like UATVR [26] and Cap4Video [27], demonstrates the growing interest in leveraging textual elements for enhancing video understanding. These methods emphasize the importance of leveraging textual prompts, captions, and semantic language for augmented video-text matching. The development of aiming to facilitate the explicit exchange of information between language and video representation seems to be a promising direction in the research community.>

This related work covers recent advancements in video recognition, particularly focusing on the integration of vision-language models, dynamic frame sampling strategies, adaptive image-to-video transfer learning, and the enhancement of language supervision for video understanding. The research landscape is evolving towards a multidisciplinary approach, leveraging cross-modal exploration and novel paradigms to advance the state of the art in video recognition and action understanding."
2188," **Learning-based Low-light Image Enhancement.** Learning-based methods [1][2][3][4][5][6][7][8][9] have achieved promising results in low-light image enhancement. Dong _et al_. [1] propose a two-stage network to learn the mapping from noise-to-sRGB and from sRGB to RAW. However, the performance of the single-stage methods is limited due to the domain ambiguity caused by the mixed mapping across two different domains. To address this issue, multi-stage networks [6][9][8] have been proposed to leverage the complementary information from both noise and sRGB domains. For example, EEMEFN [9] proposes an edge-enhanced multi-exposure fusion network to enhance the details of the dark image. EDRN [8] decomposes the dark images into different components and performs decomposition-and-enhancement to restore the high-frequency details. Nevertheless, the information loss caused by image-level dataflow is still present in the multi- stage networks. In this paper, we propose a Decouple aNd Feedback framework to address the limitations of existing architectures.

**Image Inpainting.** Image inpainting aims to restore missing parts of corrupted images and play a critical role in many computer vision applications, such as image restoration, object removal and semantic segmentation [10][11][12][13]. In this work, we focus on the restoration of low-quality dark images, which is a challenging task due to its ill-posed nature. To solve this problem, we leverage the unique properties of RAW data, which can be decomposed into two sub-tasks, i.e., denoising and enhancement, to achieve a better restoration performance. In addition, we introduce a feedback mechanism to propagate the information through the resulting image of each stage, which avoids the lossy information flow caused by dataflow in the data-level domain. In other words, our framework can effectively utilize the rich information contained in the RAW data for the restoration task, which has not been explored in existing methods.
**Decoupled Network.** DecoupleNet [14] is proposed for unsupervised domain adaptation, which decouples the task-specific and domain-invariant components to achieve better performance. The decoupled components are learned from the source domain and the target domain, respectively. Different from [14], we propose to decouple the learning of the RAW and the sRGB subtasks in the SID task, so as to fully utilize the exclusive properties of these two domains for the RAW image enhancement task. Moreover, we design a novel decoupling and feedback framework to mitigate the domain ambiguities caused by mixed mapping between the two domains. Experimental results demonstrate that our framework outperforms the previous state-of-the-art SID method [6] in terms of both PSNR and visual quality.


**Feedback Mechanism.** The feedback mechanism has been widely used in image restoration [15][16][17] and image enhancement [6]. For"," RAW data-based low-light image enhancement has been extensively studied in recent years. Chen _et al_. [1] firstly proposed the first end-to-end learning-based method for RAW data enhancement. They proposed a fully convolutional network (FCN) to directly process raw sensor data and replace much of the traditional image processing pipeline, which tends to perform poorly on such data. The network operates directly on raw sensor images and replaces the traditional pipeline, such as image processing pipelines, with a fully Convolutional Neural Network (CNN) for image enhancement. The performance of FCN is further boosted by the introduction of a large-scale dataset, namely the See-in-the-Dark (SID) dataset [1], and the development of several methods [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17].

In this paper, we focus on the single-stage and multi-stage methods, which are more suitable for real-time applications. The single-Stage methods [1][2][4] first generate a monochrome raw image from the colored raw image, and then use it to guide the feature extraction process of the network. To further improve the performance, the multi-scale information is propagated from the low-resolution to high-resolution sub-networks. For example, the self-guided network (SGN) [2] adopts a top-down self-guidance architecture to better exploit multi-level information. The multi-exposure fusion network (MFN) [9] employs a multi-expansion module to address the high-contrast and color bias issues in the high resolution input.

However, the performance is bottlenecked by the inherent limitations of existing architectures in both single and multi stage methods. The mixed mapping across two different domains, noise-to clean and RAW-tosRGB, misleads the single stage methods due to the domain ambiguity. The feature propagation across stages with a feedback mechanism avoids the information loss caused by image-level dataflow. The two key insights of our method resolve the limitations of the existing architectures and provide a generalized solution to these bottlenecks.

 propose a novel framework for low-level image enhancement, which is based on the idea of the feedback mechanism. The feedback mechanism has been widely used in various computer vision tasks, including image super-resolution [15], point","[ **Low-light enhancement:** Restoring RAW images rather than sRGB images has drawn increasing interests in recent years [1]. This has also raised the development of new data augmentation methods [2][3][4]. Extreme low-light enhancement has been studied and attracted considerable attention [5][6]. Jiang _et al_. [7] divides the properties of RAW images into two components: linearity and exposure time, and exploits them to explain and learn the properties of enhancement models. He _et al_. [8] takes the noise signal into consideration and decomposes the enhancement model into noise-inclusive and noise-free parts. Some subsequent methods leverage an extra stage to further enhance the overall image quality [9]. These methods generally obtain remarkable results and have been widely adopted by various real applications. Nevertheless, they usually do not take full advantage of RAW data, which is still in its infancy in the research field of low-light enhancement.

**Inpainting:** Inpainting [10] is a fundamental task in computer vision that aims to predict the missing regions in an image. Inspired by the artistic process of sketching, edge-guided methods [11][12][13] are developed to predict the structure in the input and complete the missing details. Some representative methods [14][15][16][17] utilize a recurrent neural network to generate structural information and then produce final results through super-resolution. Our method follows the general guiding idea of these methods and utilizes the rich features from RAW data to improve the overall results.

]","Step 1: 
Important sentences from the cited paper:

1. ""The exclusive properties of RAW data have shown great potential for low-light image enhancement.""
2. ""We introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images.""
3. ""We propose a deep neural network architecture that aims to strike a balance between the network latency, memory utilization, model parameters, and reconstruction quality.""
4. ""A practical low-light enhancement solution must be computationally fast, memory-efficient, and achieve a visually appealing restoration.""
5. ""We make the first benchmark effort to elaborate on the superiority of using RAW images in the low light enhancement and develop a novel alternative route to utilize RAW images in a more flexible and practical way.""
6. ""We propose a new evaluation framework, Factorized Enhancement Model (FEM), which decomposes the properties of RAW images into measurable factors and provides a tool for exploring how properties of RAW images affect the enhancement performance empirically.""
7. ""A low-light enhancement solution must respect additional constraints such as limited GPU memory and processing power.""

Step 2: 
Topics of related work:

1. Low-light image enhancement using RAW data
2. Deep neural network architectures for low-light image enhancement
3. Computational efficiency in low-light image enhancement
4. Utilization of RAW images and evaluation frameworks for low-light image enhancement
5. Additional constraints in low-light enhancement solutions.

Step 3:
<In the realm of low-light image enhancement, the potential of RAW data has drawn significant attention [1]. Leveraging RAW data has shown promising results as it offers exclusive properties suitable for enhancing low-light images [7]. The utilization of deep neural network architectures in addressing low-light image enhancement challenges has become more prevalent [4, 5]. These architectures aim to strike a balance between network latency, memory utilization, model parameters, and reconstruction quality, reflecting the need for an optimized approach to low-light image enhancement [4]. Additionally, the pursuit of computationally fast, memory-efficient, and visually appealing low-light image restoration has led to the proposal of novel deep neural network architectures that meet these criteria [3, 4].

In recent developments, there has been a focus on the utilization of RAW images in low-light enhancement and the development of novel evaluation frameworks [7, 9]. Efforts have been made to elaborate on the superiority of using RAW images for low-light enhancement, leading to the introduction of alternative routes to utilize RAW images more flexibly and practically [9]. Moreover, the introduction of the Factorized Enhancement Model (FEM) has offered a systematic approach to decompose the properties of RAW images into measurable factors, providing valuable insights into the impact of RAW image properties on enhancement performance [9].

Furthermore, the low-light enhancement domain has also raised the importance of respecting additional constraints such as limited GPU memory and processing power [3, 7]. These constraints pose challenges that must be effectively addressed for the practical utility and deployment of low-light enhancement algorithms on edge devices and embedded systems.>


"
1261," **Self-supervised learning.** Early works in self-supervision focused on designing hand-crafted pretext tasks, such as solving jigsaw puzzles, rotation prediction [1], context prediction [2][3], inpainting [4], noise prediction [5] and colorization. Recently, contrastive learning methods [6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25] have become the dominant paradigm for representation learning.

**Contrastive methods.** The core idea of contrastive methods is to learn representations by contrasting positive pairs against negative pairs. The positive pairs are usually obtained from different augmented views of the same image. MoCo [8][10] uses a memory bank to store a large number of negative samples, while SimSiam [7] and Swin [12] use a siamese network architecture. The main difference between these methods is that the contrastive loss is based on the distance between the representations of augmented views, while the self-distillation loss aims to transfer knowledge from a teacher network to a student network. In contrast to these methods, NNCLR [19] proposes to use nearest neighbor bootstrapping to obtain positive pairs from the teacher network and uses them to train the student network in a contrastive setting. Our work differs from these methods in that we propose to adaptively bootstrap neighbors based on their similarity to improve the quality of the learned representations. We show that this leads to consistent improvements compared to the naive approach of using negative pairs in contrastive self-based methods. In addition, we also show that our approach leads to better generalization to downstream tasks. GLSD [14] proposes a similar idea to ours. However, it uses a global-local contrastive distillation framework, while we use a cross-view consistency objective.
**Nearest neighbor based self-training.** Recently, there has been a surge of interest in using nearest neighbor based methods for self-learning [19][25][19]. The idea is to train a network to predict the similarity of the representation of a given image to its nearest neighbors in a learned latent space. The idea was first introduced in [19], where the network is trained to maximize the similarity between representations of the augmented views from the same instance. In [25], the authors propose to use the nearest neighbor prediction as a weakly supervised task to improve instance retrieval. In this work, we show that the use of nearest neighbor predictions can lead to a performance drop and even collapse. We analyze the reason for this phenomenon and propose a solution to address it.

 propose a similar approach to ours in the context of instance clustering. They propose to cluster the representations and use them as pseudo-labels to train an encoder-decoder architecture. They show that clustering can improve the performance of the encoder and the downstream task. In our work, instead of using pseudo labels, we focus on nearest neighbor pairs and show that they can be used as a strong signal for self"," Self-supervised Learning.Self-supervision methods have been widely used in self-supervisory learning [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25]. In this section, we focus on recent self-distillation based methods.

**Self-Supervised Learning with Contrastive Learning.** The seminal work of MoCo [8] proposed a contrastive learning scheme for representation learning. The idea is to maximize the similarity between augmented views of the same image and minimize the similarity of augmented views from different images. MoCo v2 [10] improves the contrastive scheme by using an MLP projection head and more data augmentation. The work of DINO [6] proposes to use a momentum encoder to learn representations from a large-scale dataset. The recent work of SimSiam [7] shows that simple Siamese networks can learn meaningful representations even without negative sample pairs, large batches, momentum encoders, and stop-gradient operation. The works of MoBY [12], DeiT-S [8], and Swin-T [12] propose to use Vision Transformers (ViT) [6]. The work in [11] investigates the effects of several fundamental components for training selfsupervised ViT. The most related work to ours is the work of [11], which studies the effect of the stability of the training process for self-trained ViT models. The authors show that instability is a major issue for self supervised ViT and propose to improve the training procedure by using a more stable training procedure. The method in [13] proposes a new contrastive objective for ViT, which is based on the idea of contrastive self-training. The main difference between our work and theirs is that we propose to adaptively bootstrap neighbors in the latent space instead of using negative samples in contrastive setting. We show that our approach leads to consistent improvements compared to the naive bootstrapping approach and the original baselines. The approach in [18] is similar to ours in that we also bootstrap positive pairs. However, we show that the bootstrapped neighbors are more informative and informative than negative pairs in the original contrastive setup. The difference is that they use positive pairs obtained via nearest-neighbor (NNN) bootstrap in the self distillation","[ Self-supervised learning with contrastive loss is among the earliest methods for unsupervised representation learning [1][2][3][4]. Such methods optimize for image similarities between different views of the same image. For example, CARS  proposes a new augmentation scheme for out-of-the-box object recognition, while SEG  uses an instance-level self-supervised loss on segmentation maps. Non-contrastive methods, such as Mean Teacher [5], have also been proposed. While such methods have been heavily explored in recent years, we focus on contrastive methods in this work.

Contrastive learning is popularly implemented as Siamese [2][6][4][7], triplet [8] or Nesternet losses [9]. Siamese architectures are often modified with momentum encoders and/or batch normalization [8][10][11][12]. MoCo [8] and SimCLR [7] have gained much attention in the research community. MoCo replaces negative pairs with a distillation loss to regularize the model. SimCLR employs an MLP projection head to discard potentially noisy negative pairs. However, self-distillation provides some advantages over contrastive learning: it provides more context to the student, while contrastive loss creates a disjoint set of negative and positive samples [13][11].

A potential weakness of contrastive methods is that, in self-distillation, the generated negative pairs may not provide a good gradients signal, leading to instability and vanishing/exploding gradients [13][11][10]. This problem can be addressed in several ways. For example, NNL [14] uses a global-local variant to utilize local features during training. In contrast, Swin-Sim [12] directly improves SimCLR using Vision Transformers by performing center loss, object classifiers and attention modules.

There has also been some recent research addressing self-distillation with improved negative sampling methods [10][13][11]. Our work focuses on adaptive negative sampling during pre-training based on the quality of a given image's representations. Unlike our method which relies on an external evaluation, other methods use the representation itself to detect ""good"" pairs [13][11]. For example, CC , DCL [13] and RePart [11] leverage online clustering algorithms or prototype representation learning to better evaluate negative pairs.

Self-supervised pretraining methods have also been proposed for joint feature extraction and semantic segmentation [15][16]. In contrast, recent works have instead focused on obtaining a dense representation [17][18][19][20]. CrOC [16] extracts the semantic content of each augmentation to cluster instances and learn dense representations via a clustering-based contrastive loss. Mean Shift [20]","<Unsupervised representation learning has gained significant attention due to the need for extracting high-level semantic features without requiring manual annotations [1]. Many self-supervised methods for representation learning leverage cross-view consistency objectives to maximize the representation similarity of augmented views of an image [1]. However, recent work such as NNCLR goes beyond the cross-view paradigm by incorporating nearest neighbor bootstrapping in a contrastive setting to obtain positive pairs from different images [19]. While this approach demonstrated promising results, it was found that incorporating nearest neighbor bootstrapping in a self-distillation scheme can lead to a performance drop or even collapse, highlighting the need for adaptive strategies [1]. To address this, the proposed paper introduces adaptive similarity bootstrapping based on the estimated quality of the latent space, leading to consistent improvements in various self-distillation methods and standard downstream tasks [1]. This work underscores the importance of adapting the bootstrapping technique based on the specific characteristics of the latent space, shedding light on the significance of adaptive strategies in self-supervised representation learning.

In addition to cross-view consistency, previous works have explored the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation [2]. The employment of spatial context for predicting the position of patches within an image has been shown to facilitate the learning of object recognition and part detection, thereby contributing to the unsupervised visual discovery of objects such as cats, people, and birds [2]. Similarly, context-based self-supervised learning has been improved through addressing problems such as chromatic aberration and spatial skew, culminating in top scores on standard self-supervised benchmarks [3]. These approaches collectively emphasize the significance of leveraging spatial context and addressing inherent problems to enhance self-supervised learning outcomes.

Furthermore, the use of context encoders driven by context-based pixel prediction has proven effective in learning visual features that capture both appearance and semantics of visual structures [4]. Such unsupervised visual feature learning algorithms have demonstrated effectiveness for pre-training CNNs on classification, detection, and segmentation tasks [4]. Additionally, the generic framework proposed for training deep networks with noise as targets presents a domain-agnostic approach to overcoming issues associated with standard unsupervised learning methods, highlighting the potential for scaling to millions of images [5]. These diverse approaches collectively underscore the importance of leveraging various types of supervisory signals and generic frameworks in self-supervised representation learning.

Moreover, the investigation of self-supervised learning in Vision Transformers (ViT) has revealed emerging properties that stand out compared to convolutional networks, such as the explicit information about semantic segmentation and the excellent k-NN classification capabilities of self-supervised ViT features [6]. The study also emphasizes the significance of momentum encoders, multi-crop training, and the use of small patches in ViTs for improving self-supervised learning outcomes [6]. With the emergence of Siamese networks and momentum contrast for unsupervised visual representation learning, the roles of Siamese architectures and dynamic dictionaries with moving-averaged encoders have been re-evaluated and shown to provide competitive results on downstream tasks [7][8]. These findings collectively contribute to the understanding of the unique properties and methodologies employed in self-supervised learning for visual representation.

In conclusion, the related works collectively showcase a diverse range of approaches and techniques employed in self-supervised visual representation learning. The adaptive similarity bootstrapping approach proposed in the target paper offers valuable insights into the challenges and solutions for self-distillation based representation learning. Additionally, leveraging spatial context, context encoders, and domain-agnostic training with noise as targets demonstrates the importance of diverse supervisory signals and generic frameworks. Furthermore, the study of emerging properties in Vision Transformers and the re-evaluation of Siamese architectures and dynamic dictionaries contribute to a deeper understanding of the unique properties and methodologies employed in self-supervised visual representation learning.>"
853," **Face Parsing:** Face parsing is defined as the per-pixel labeling of images containing human faces. Most of the recent works [1][2][3][4][5][6][7] have focused on improving the performance of face parsing using deep neural networks. In [1], a boundary-attention semantic segmentation network was proposed to improve the accuracy of face segmentation. The authors in [2] proposed a decoupled multi-task learning framework for face parsing, where the network is trained on two different tasks, namely, face parsing and facial landmark detection, and then a cyclical self-regulation mechanism is used to decouple the two tasks. The work in [3] uses a transformer-based architecture [4] to learn a universal facial representation that can be used for both visual and linguistic tasks. In this work, the model is trained using a large-scale dataset, CelebAMask-HQ [5], and a pretrained language model.

In [6], the authors proposed a hierarchical face parsing network that uses a multi-stage architecture. The first stage is a CNN-based model that predicts the facial components, and the second stage is an MLP-based network that generates the final segmentation map. The RoI Tanh-Warping network [7] is proposed in [6] to generate the segmentation maps for each facial region. In, the authors used a transformer architecture with RoI RoI-warping module to extract features from the input image and then the RoIAlign module was used to warp the features into the RoIslands of the corresponding facial regions. In our work, we use the same architecture as in [7].

**3Dimensional Models:** 3D Morphable Models (3DMM) [8][9][10] have been widely used in the computer graphics and computer vision community for modeling the human face. The 3DMM is a parametric model of human faces that represents the shape and appearance of a human face using a set of 3D vertices. The parameters of the 3DMMs are optimized by minimizing the reconstruction error between the reconstructed 3D model and the input images. Recently, there has been a lot of work on 3D face reconstruction [11][12][13][14][15][16] and 3D head reconstruction [17][18][19][20][21][22][23][24][25][26][27]. In this paper, we propose a 3D morphable model-based face-parsing method using a Local Implicit Function (LIIF) network.

 proposed a LIIF-based method for 3D human head reconstruction. They use a convolutional neural network (CNN) to predict the 3D shape of the human head from a single RGB image. They also use an encoder-decoder architecture that takes the RGB image as input and outputs the latent representation of the head. The decoder takes the latent vector as the input and predicts the output of the encoder. The output of their network is a dense 3D"," **Face Parsing.** Face parsing is a long-standing problem in computer vision and has been extensively studied in the past few years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27]. In recent years, face parsing has seen a lot of progress with the introduction of new datasets [1] and new methods [2][1][3]. In this work, we mainly focus on the methods that are based on deep neural networks.

**Deep Face Parsing Methods.** Deep learning based methods have been widely used in face parsing [3][1]. In [1], the authors proposed a boundary-attention semantic segmentation (BASS) method for face parsing, which contains a three-branch network with elaborately developed loss functions to fully exploit the boundary information. In [2], the author proposed a novel Decoupled Multi-task Learning with Cyclical Self-Regulation (DML-CSR) method to address the spatial inconsistency and boundary confusion problem in face-parsing. In, the authors used a multi-task model to jointly learn face parsing and binary edge detection, and a dynamic dual graph convolutional network to capture global contextual information without using any extra pooling operation. The authors in [3] proposed a framework to learn a universal facial representation that boosts all face analysis tasks. In addition, the authors in  proposed a face parsing method using a transformer network [4] for image classification. In this paper, we propose a lightweight face parsing model that uses a convolution network and a pixel MLP decoder to learn face segmentation from a single image. In our model, we use the same architecture as [3], but we propose to use a local implicit function network to learn the segmentation model. The proposed FP-LIIF is a lightweight model with \(1/26^{th}\) number of parameters compared to the state-of-the-art models and yet matches or outperforms the state of the art models on multiple face parsing datasets. We also show that our model can generate segmentation at different resolutions without any changes in the input resolution.


","[ **Face parsing**  The first step towards creating a face parser is obtaining 2D crops of the face regions and detecting the facial parts like eyes, nose, etc. RAFT [1], LAMA [2], NPM [3], FaRL [3] use the class activation maps of pretrained models like VGG [4] and ResNet [5]. RAFT also has a two-stage approach to extract features from the global region-of-interest (ROI) and the local crop features, which are fused for final decision making. LAMA [2] and FaRL [3] present an elegant way of using both global and local representations of the images as the 2D crop features, for efficient fine-grained image segmentation.

Tulnasker [6] transforms an image patch to a label map and use the permutation-invariant nearest neighbor (PINN) approach for decoding the label maps. They propose to first detect the face at the part and component level and then focus on pixel-level labeling. By simply cropping the face, the RoI Tanh-Warping [7] has been used to capture the contextual information, thus improving the performance of the model in the presence of an occluded face.

**3D Face Reconstruction** To build a 3D model, many 3D face reconstruction approaches have been proposed. Historically, the 3D face representations include pre-defined or learned non-parametric representations and parametric representation. Non-parametric representations mainly use 3D meshes like parametric face model (PfM) [8], SMPL model , Zhou [9], MICA [10]. They sample facial parts using these models, and then map the information to the output image. 3D dense face alignment [11][12] uses 3D meshes to represent faces and then use similar 3D shapes as prior knowledge for the facial parts.

Parametric representation can be computed from images through optimization [13][14][15][16]. Instead of manually defining a parametric model, they learn a parametric model from images, using 3D voxels as the input. For example, In [17], an occupancy network is used to compute the depth-map at each voxel position and reconstruct the 3D model. Kim [18] uses implicit neural representation to represent shape instead of voxel maps. From single images, In [19], they propose a method that interpolates the weights of an implicit function for generating new images, which improves upon the traditional image interpolation. One limitation is that it cannot handle changing textures and repetitive pattern at the same time. Moreover, there are heavy computations for a small change in input or output, and it is not fast enough for the real-time applications. In contrast, LIIF [2]","<Face parsing and segmentation have gained increasing interest due to their numerous applications in the field of computer vision. Various works have been proposed to address the challenges in face parsing and segmentation. Li et al. [1] developed a high-efficiency framework for pixel-level face parsing and constructed a new large-scale Landmark guided face Parsing dataset (LaPa) to facilitate the advancement of face parsing. They proposed a simple yet effective Boundary-Attention Semantic Segmentation (BASS) method for face parsing, which outperformed the existing state-of-the-art methods on benchmark datasets, indicating the effectiveness of their approach. In a similar vein, Zhang et al. [2] proposed a method for decoupled multi-task learning with cyclical self-regulation for face parsing to address spatial inconsistency and boundary confusion in state-of-the-art methods. Their approach achieved the new state-of-the-art performance on several benchmark datasets. These works highlight the importance of developing efficient frameworks and methods for accurate face parsing and segmentation.

Furthermore, the work by Liang et al. [3] is related as it focuses on general facial representation learning. They introduced a framework, FaRL, for general facial representation learning that achieved better transfer performance compared with previous pre-trained models, demonstrating the potential for improving face parsing through enhanced facial representation learning. Additionally, the study by Wu et al. [4] showcased the application of transformers for image recognition at scale, indicating the potential for leveraging transformer architectures in the context of face parsing and segmentation tasks. This work is relevant as it explores the use of advanced architectures for image recognition, which could potentially be adapted for face parsing tasks.

Moreover, the study by Chen et al. [5] is relevant as it explored the hierarchical face parsing via deep learning, proposing a novel face parser that effectively handled partial occlusions and provided richer information for face analysis and synthesis. Additionally, the work by Wang et al. [6] introduced a novel RoI Tanh-Warping operator for face parsing, effectively addressing the limitations of traditional crop-and-resize focusing mechanisms and demonstrating outstanding performance on benchmark datasets. These works collectively emphasize the importance of developing robust and effective methods for face parsing and segmentation, laying the groundwork for the development of parameter-efficient local implicit image function networks for face segmentation, as presented in our target paper.>"
2750," **3D reconstruction and novel view synthesis.** 3D reconstruction from multi-view images is a long-standing problem in computer vision and computer graphics. Classical approaches include structure-from-motion (SfM) [1][2][3], stereo [4][5] and mesh-based reconstruction [6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50]. In this work, we focus on NeRF-based methods.

**Neural Radiance Fields.** NeRF  was the first work to use a multi-layer perceptron (MLP) to represent a volumetric representation of a scene. The MLP is trained to predict the volume density and color of the scene given a set of input images. NeRF is able to synthesize novel views of a 3D scene from a single input image. However, NeRF requires a large number of training images, which limits its applicability to real-world scenes. To overcome this limitation, several works have been proposed to improve NeRF's generalization ability to novel views. For example, MVSNeRF [34] and NeRF++ [19] extend NeRF to scenes with multiple objects and scenes with varying degrees of occlusion. Block-NeRF[22] and BlockNeRF++++ [21] propose to use block-based MLPs to represent the scene. Other works focus on improving the memory efficiency of the MLP-based NeRF model. For instance, KiloNeRFs [26], MERF [20] and MipNeRF\({}^{360}\)[18] use multi-scale MLPs and multi-resolution MLPs, respectively, to reduce the memory requirements of NeRF. BakedSDF [13] and BakedNeRF  use a hierarchical MLP to represent an SDF using a tree-structured voxel grid. In this paper, we propose a new representation for NeRFs based on tetrahedral representations. We show that such a representation is more efficient to train and leads to state-of-the-art results compared to point-based and MLP based representations. Moreover, we show that our representation can be combined with other representations to further improve the quality of the reconstructed 3D geometry.

 and [49] use point clouds as input to a neural radiance field (NeRF-Voxelizer) to improve its generalization capability to unseen scenes. In contrast, our method uses a point cloud as a proxy for the scene geometry and uses a tetrahedron-based representation to improve the efficiency of training and rendering. In addition, we use a differentiable rendering method to optimize the point cloud representation, which allows us to use NeRF"," **Neural Radiance Fields.** NeRFs have been widely used for novel view synthesis [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50].

NeRFs are a very popular approach for view synthesis. They represent a scene as a continuous volumetric function, parameterized by a multi-layer perceptron (MLP) that provides the volume density and view-dependent radiance at each location. The MLP is queried hundreds of times per ray, which makes it difficult to render in real-time. To overcome this problem, several methods have been proposed to reduce the number of queries to hundreds or even thousands of times [26][25], or to use smaller MLPs to represent the scene instead of a single MLP [25][27]. Other methods have also been proposed for fast rendering [28][40] or to improve the quality of the NeRF representation [29][40]. However, all these methods are based on voxel-based representations, which are not flexible enough to represent complex scenes. In contrast, our approach is based on tetrahedra, which is a more flexible representation.


.

 propose to use a tetrahedral tetra to represent a 3D scene, and use it to represent an MLP. However, their method requires a dense point cloud of the scene as input, which limits its applicability to real-world scenes. We propose a method to represent and use a sparse point cloud as input to the MLP, which enables efficient training and leads to state-of-the-art results.

","[ **Multi-View Stereo.** The problem of 3D reconstruction from multiple images has a long history in computer vision. Most popular are matching approaches, which use similarity measures between images (e.g., [1][2][3][4]). For certain scenes, the resulting reconstruction is noisy and may not be complete.

Another line of approaches is to estimate the underlying surface representation of the scene from the images [5][6]. A popular strategy for this is so-called Marching Cubes [7], where 3D neighborhoods of a point cloud are extracted from images.

**View Synthesis.** Photorealistic view synthesis from a small number of images has a long history [8][9][10][11], but these methods are restricted to scenes with static geometry and specific characteristics (see Sec. 1.1). A recent class of methods called neural view synthesis learns to model the 3D scene geometry from a small number of images and render novel views using a neural network [12][13][14][15]. Due to their success, these methods have been extended and improved upon, also using unstructured input such as image galleries [16][17][18][19][20][21][22][23][24][25][26][27][28][29]. Different, non-NeRF approaches to 3D scene representation also exist. Examples are _reflection fields_[30][31] and reflection fields with neural reflections [31], or optical flow fields  derived from image sequences. The modeling of material properties of objects in the scene is also possible [30], and deep networks can be used to decompose a scene into reflectance, illumination and shape [31][32]. A hybrid approach to novel view synthesis is to train a single network to learn and render view dependent appearance [33]. In this paper, we propose a novel representation that can be used for neural view synthesis in the general setting without special structure in the scene.

**Neural Radiance Fields.** NeRFs are introduced by Mescheder et al. [34] and are an extremely successful neural representation for the problem of novel view synthesis. Other extensions of NeRFs include, for instance, memory efficient representations to handle scenes that are too big for a standard NeRF [20], efficient view-dependent encoding [18][35], scene decomposition into separate radiance field objects [21][36], differentiable neural radiance fields [15], synthesis of non-photorealistic novel views [37], sparse representations for fast rendering [38], and combining neural radiance fields with explicit surface meshes [39][40]. Some related work focuses on mesh-based representations for the problem of view synthesis [41][42][43][35]","<Neural Radiance Fields (NeRF) have gained popularity for their effectiveness in novel view synthesis and 3D reconstruction. The traditional approach involves optimizing the representation to each scene independently, requiring substantial compute time and calibrated views. Recent developments include BARF, which tackles the challenge of training NeRF from imperfect camera poses by jointly optimizing the neural 3D representations and registering camera frames [44]. Additionally, NoPe-NeRF removes the requirement of known camera parameters and addresses the training of NeRF in dynamic scenes with challenging camera trajectories [46]. Furthermore, NeRF- simplifies the training process by removing the need for known or pre-computed camera parameters, demonstrating the possibility of jointly optimizing camera parameters with NeRF training [47].

Existing methods such as [2] BA-Net and [3] SfM-Net have addressed the challenges in structure-from-motion (SfM) by explicitly enforcing multi-view geometry constraints and learning suitable features to make the BA problem more tractable. Similarly, MVSNeRF [34] has introduced a novel neural rendering approach that can efficiently reconstruct radiance fields for view synthesis by leveraging plane-swept cost volumes for geometry-aware reasoning and combining this with physical-based volume rendering.

Real-time view synthesis has also been approached through PlenOctrees for the real-time rendering of NeRFs, achieving high frame rates while preserving the ability to render scenes with arbitrary geometry and view-dependent effects [28]. Additionally, Mip-NeRF has extended NeRF to represent scenes at a continuously valued scale, efficiently rendering anti-aliased conical frustums and significantly improving NeRF's ability to represent fine details [17]. Furthermore, [30] NeRF++ has improved view synthesis fidelity in challenging scenarios such as 360 captures within large-scale, unbounded 3D scenes.

Beyond NeRF, point-based methods have been proposed. Point-NeRF [49] combines the advantages of volumetric neural rendering methods and deep multi-view stereo methods by using neural 3D point clouds to model a radiance field, resulting in efficient rendering and improved visual quality. Moreover, Direct Voxel Grid Optimization has introduced a super-fast convergence approach to reconstructing radiance fields for novel view synthesis, achieving NeRF-comparable quality and rapid convergence from scratch [50].>"
2476," **Dataset Representations.** Dataset representations have been used to assess the quality and diversity of datasets [1][2]. Core-sets [3][4][5][6] are a popular approach to represent datasets, where the goal is to select a subset of images that are representative of the entire dataset. However, core-sets are not suitable for comparing datasets, as they require a pre-defined set of core elements, which may not be available for all datasets. In contrast, DNAs are compact and can represent datasets of any size with less than 15 megabytes.

**Synthetic image evaluation.** In the context of generative modelling, several metrics have been proposed to evaluate the quality of synthetic images [7][8][9][10][11][12][13][14][15]. These metrics are typically based on the Inception network [8], which is trained to predict the probability of each pixel belonging to one of a set of pre-specified image classes. These probabilities are then used to compute a similarity score between two images, which is used to compare them. These metrics, however, are not designed to compare datasets. Instead, they are designed to evaluate whether a given image can be generated by a given generative model, and thus cannot be used for comparing two datasets. Moreover, they do not allow for fine-grained control over the comparison attributes of interest, such as rarity and diversity. In this work, we propose a general-purpose representation for datasets, which allows us to compare two datasets with granular control over these attributes, and can be used in a variety of tasks, including synthetic image evaluation, transfer learning, and evaluation of datasets. Furthermore, we show that DNAs can also be used as a perceptual similarity metric, which has not been previously explored.
**Neuron-based feature representations.** The idea of representing images using neuron activations has previously been explored in the field of computational biology [16]. However, these methods are typically limited to small datasets, and do not scale well to large datasets. Recently, deep neural networks (DNNs) have been widely used as feature extractors [17][18][19][20][21][22][23][24][25][26]. These DNNs are trained to extract features from images, and have been shown to outperform hand-crafted features [22][17]. In this paper, we use the features extracted by a DNN as the representation of datasets and evaluate their applicability on several tasks.

 propose a method for representing datasets using a single DNN. They represent images using histograms of the activations of the last fully-connected layer of the network, and use them to compute the mean and standard deviation of the histograms over all images in the dataset. This is similar to our approach, except that the histogram of activations is computed for each dataset independently, whereas we use a single network to represent all datasets and compare them using DNAs. In addition, their method is limited to comparing datasets of the same size, whereas our method is applicable to datasets of different sizes."," **Dataset Bias.** Datasets have been shown to be biased towards certain datasets [1] and to be less representative of the real world [2]. This has been discussed in the context of transfer learning [2], but not for datasets. In this work, we focus on datasets that are representative of real world datasets.

**Datasets Representation.** Traditional methods for representing datasets use _core sets_[3][4][5], which represent images as a set of points in some metric space, where the similarity between points is measured by the distance to a core set of similar images. These methods have been used for image classification [4], active learning [5], and generative modeling [6][7][8]. However, these methods are not suitable for comparing datasets of any size, as they are based on the assumption that all images in the dataset are similar to each other. In contrast, we propose to represent datasets using distributions of neuron activations, which are more compact and can be used to represent any dataset.


**Datastream Comparing.** The most widely used metric for evaluating datasets is the Frechet Inception Distance (FID) [3], which measures the distance between the mean and covariance of the distribution of the images in a dataset. FID has been shown not to correlate well with the perceived quality of the generated images [9][10][11][12][13][14][15], and has been found to be prone to accidental distortions [15]. To address this, several variants of FID have been proposed, including density and coverage metrics [13], precision and recall metrics [12][14], and a new metric called _rarity score_[14], which is based on nearest neighbor distances in feature space. However, none of these metrics can be directly used to compare datasets of different sizes, and none of them can measure the rarity of the dataset. In addition, they do not provide a way to compare images of different datasets, which is important for evaluating the diversity and coverage of each dataset. We propose to use DNAs as a general-purpose tool for dataset comparison, which can be applied to a wide range of datasets, including datasets of varying sizes.
 normalized to the number of images in each dataset [16]. We also provide a detailed comparison of DNAs with FID and FID, and show how they can be combined with other metrics to provide a more accurate measure of dataset diversity. We also","[ Related work has been presented for the images vs. datasets evaluation task and for representing images with DNAs.

**Evaluating datasets.** The notion of dataset evaluation is well-studied [1]. A common metric for quantifying the similarity between two sets of images in a dataset is the Frchet Inception Distance (FID) score [2]. However, FID can sometimes mislead when assessing datasets. Our task is not related to FID. Furthermore, our evaluation will be different to that used in these studies.

**Image representation.** Representing images in a compact way is important for many tasks, including image retrieval [3], visual recognition [4], and model selection in active learning [5]. Representing images is also relevant for synthetic image generation [6], where methods are used to improve generated images. Representing images as DNAs allows us to find groups of neurons that represent concepts and hence compare datasets in these groups, which would not be possible with alternative representations, e.g., using vectors in a space such as low-dimensional embedding spaces. Previous work has focused on representing images using the activations from pre-trained deep convolutional networks on image classification tasks. In [7], the activation of the last layer of a pre-trained network is used to represent images, and the image class is also used as an index. This approach requires a classification model and a training step; in contrast, the proposed approach does not require the input of any data.

In [8], the authors evaluate pre-trained feature extractors on the ImageNet dataset for recognizing digits. Using each pixel from an image to represent the image, the authors form a feature vector using a deep convolutional neural network (DCNN). The authors show that this representation is effective at recognizing digits. They also use the same representation to create feature images to show that each neuron has a ""centroid"" that is located at some region in the image. This is analogous to what we propose to represent images by formulating the representation in terms of the neural activations for a pre-trained network.

Another approach that uses activations from a DCNN for representing images is the MAX-normaliser  approach. This approach simply normalizes the activations of an over-parameterized neural network for which each neuron is separately normalized to have \(1\) activation.

Mirza and Smolic compare the performance of different loss functions used in the Wasserstein distance and the Maximum Mean Discrepancy (MMD) metric for training generative models [9]. They show that the performance of both metrics is related to how well the trained models match the target data distribution. The work in [10] and [11] investigate measures that can distinguish between different cases of the generation process. They show that the (separately) fidelity and diversity can be evaluated using KL-divergence and the overlap between the sampled and the target distributions. This is in contrast to our approach where]","1. Visual feature representation and comparison have become crucial in computer vision, particularly for evaluating different datasets [1]. Datasets play a fundamental role in object recognition research, serving not only as a source of training data but also as a means of comparing the performance of different algorithms [1]. However, the restricted focus on benchmark performance numbers has led to a need for general-purpose tools to evaluate dataset differences [1]. Proposed methods like Distributions of Neuron Activations (DNAs) offer a novel approach to representing and comparing images, thus providing granular control over comparison attributes and customizable distance measurement [1].

2. The development of techniques for dataset distillation and data summarization has also been essential in optimizing datasets for use in model training [2, 3, 4, 5]. Research has focused on formulating new optimization techniques for distilling small datasets that match the test accuracy of models trained on full datasets [2]. Additionally, the concept of coresets  which are small data summaries of input big data that maintain the approximated optimal solutions  has gained prominence in the maintenance of streaming, distributed, and dynamic data [3, 4]. Coreset selection has also been linked to active learning and has shown significant performance improvements in image classification experiments [5].

3. The evaluation of generative models and image synthesis has been a topic of considerable interest in the recent literature, with a focus on developing new metrics and evaluation frameworks [10, 11, 12, 13, 14, 15]. Efforts have been made to devise metrics that can separately and reliably measure the quality and coverage of generated samples, thus driving algorithm research in this field [11]. Furthermore, research has explored the role of ImageNet classes in metrics like the Frechet Inception Distance, revealing discrepancies between human judgment and metric outcomes and highlighting the need for improved evaluation methods in image synthesis [15].

4. The effectiveness of deep features as a perceptual metric has garnered attention, demonstrating the potential of deep learning models in capturing perceptual similarity and surpassing the limitations of traditional metrics like PSNR and SSIM [22]. Moreover, advancements in self-supervised learning frameworks and the integration of multi-granular supervisions have shown promising results in explicitly learning fine-grained visual features [25, 26].

<References:
[1] Unbiased look at dataset bias
[2] Dataset Distillation by Matching Training Trajectories
[3] Introduction to Core-sets: an Updated Survey
[4] Visual precis generation using coresets
[5] Active Learning for Convolutional Neural Networks: A Core-Set Approach
[10] Precision-Recall Curves Using Information Divergence Frontiers
[11] Improved Precision and Recall Metric for Assessing Generative Models
[12] Assessing Generative Models via Precision and Recall
[13] Reliable Fidelity and Diversity Metrics for Generative Models
[14] Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images
[15] The Role of ImageNet Classes in Frchet Inception Distance
[22] The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
[25] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
[26] Mugs: A Multi-Granular Self-Supervised Learning Framework>"
197," Neural fields.Neural fields [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72].

**Neural Fields.** NeRF [31] was the first work to use neural fields for image generation. NeRF uses a multi-layer perceptron (MLP) to encode a scene into a latent code, which is then decoded into a radiance field. This latent code is then used to render an image of the scene. While NeRF is able to generate high-quality images, it suffers from the overfitting problem [6], where the MLP tends to overfit to the training data. To address this issue, several works have been proposed to improve the quality and diversity of the generated images. For example, GNeRF [41] uses a StyleGAN [42] style encoder-decoder architecture, and GAN-NeRF++ [43] improves the training of the StyleGAN generator by adding a regularization term to the loss function. In addition to improving the quality of the images, these methods also improve the diversity and representativeness of the output images. However, they do not address the problem of 3D-aware neural field generation. In this work, we address this problem by introducing a diffusion-based model that can generate 3D neural fields with high quality.

 proposed a triplane-based representation for neural fields. They represent a neural field as a set of axis-aligned triplane features, which are used to factorize an MLP into a series of MLPs. These MLPs are then trained to predict the occupancy values of a given point in the 3D scene. Our approach differs from theirs in several important ways. First, we do not use a single MLP to represent the entire scene. Instead, we represent the scene as a collection of axis aligned triplane feature representations. Second, we use a diffusion model to directly generate the neural fields from these representations. Third, our approach does not require any modifications to the existing triplane factorization pipelines. Finally, we demonstrate state-of-the-art results on several object classes from ShapeNet, outperforming these approaches.

 also proposed a method for neural field-based 3D shape generation. Their method pre-processes ShapeNet meshes by converting them to continuous occupancy fields, and factoring them into a sequence of axis aligning features. These features are then fed into a neural network to generate the final 3D shapes. While their method can generate realistic 3D"," **Neural Rendering.** Neural rendering techniques combine machine learning with classical computer graphics techniques to synthesize novel views of a scene from a sparse set of images. Neural Rendering [1][2] is a popular paradigm for novel view synthesis, and has been widely used in 3D shape and image synthesis [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72].

**Neural Radiance Fields.** NeRF  is a neural volumetric function that maps a 3D coordinate system to a continuous occupancy field, which is then factorized into a volume density and view-dependent radiance field. NeRF has been shown to be very efficient at rendering high-resolution images, but it is computationally expensive, requiring hundreds of forward passes through the network for each view ray. To speed up NeRF, several works have proposed differentiable renderers [18][25], which render NeRFs in real-time, but these methods still require many forward passes for every single view ray, which makes them difficult to scale to large scenes. KiloNeRF [38] uses a multi-layer perceptron (MLP) to represent the volume density, and a Kilo-NeRF++ [35] improves the quality of NeRF by using a more compact representation. KILO [38], on the other hand, uses tens of thousands of tiny MLPs to represent a scene, and uses a KILOSR [22] renderer to speed up rendering.

Our work is most closely related to NeRF++, which uses a diffusion-based model to factorize a set of axis-aligned triplane feature representations. However, our work differs from NeRF in several important ways. First, we use a differentiable rendering model for triplane factorization, which allows us to directly train existing diffusion models on our representations. Second,","[ Generative Models for 3D Scenes and Objects:GAN-based approaches to 3D scene generation [1][2] generally output MLP outputs in the occupancy field domain. A range of methods use pre-trained point cloud models [3][4] or MLPs [5][6][7][8] to model 3D shapes, and some have recently shown success in generating high-resolution scenes [9][10][11]. However, these approaches suffer from discretization artifacts and are limited to discrete meshes of predefined topology and surface shading. Other techniques such as SDFs [12][13][14][15] are more common, but are also restricted to modeling discrete volumetric objects. In contrast, NeRFs [16][17][18][19][20][21][22][23] offer the possibility of accurate 3D-aware image synthesis, though their efficiency for large scenes is problematic. Another line of work, trained with a restricted training set [24][25][26][27][28][29][30] has been able to synthesize high-quality novel views of individual objects. While there are some multi-view methods [31][32][33][34] that produce compelling results, this line of work has not considered generating multi-view novel views in a way that allows recovering high-resolution scenes. Our approach differs significantly from all these previous works in that we generate continuous occupancy fields, and we leverage triplane factorizations for efficient training of diffusion-based generative models.

While our approach is most similar to NeRF, the main differences are that we also leverage the benefits of 3D structural priors [35][36][37] (not found in NeRF) to generate free-form shapes and a differentiable view synthesis module (not found in NeRF), thus allowing for real-time rendering of our generated scenes. Moreover, we are the first to demonstrate the integration of neural field-based 3D generation and diffusion models. Finally, our approach is the first fully-differentiable end-to-end differentiable rendering approach that produces free-form 3D shapes.

Higher-Dimensional Generative Models:There has been recent progress in leveraging higher-dimensional generative models [38][39][40] in an attempt to generate high-quality 3D shapes. While NeRF-based approaches [41] can successfully synthesize high-quality images of 3D objects, the underlying shapes are high-dimensional vectors rather than an explicit representation of the scene geometry. We believe this difference can have significant implications for generative models in 3D. For example, leveraging fully differentiable diffusion models for rendering allows for richer interactions with the underlying scene geometry. Similarly, combining]","<Diffusion models have gained significant attention in the domain of image generation and have been the focus of several recent research efforts [1]. These models enable the efficient generation of 3D-aware neural fields using triplane diffusion, as demonstrated in the target paper. The proposed approach preprocesses training data to convert ShapeNet meshes into continuous occupancy fields and factorizes them into a set of axis-aligned triplane feature representations. By leveraging existing 2D diffusion models on these representations, the approach generates high-quality and diverse 3D neural fields, surpassing alternative 3D-aware generation methods [2]. This innovative technique demonstrates essential modifications to existing triplane factorization pipelines to ensure that the resulting features are easy to learn for the diffusion model, thereby presenting a significant advancement in the field of 3D generation [3]>

<Related work in the field of neural rendering and implicit representations has also focused on various aspects of 3D shape representation and generation. For example, prior work has addressed the challenges of learning implicit shape representations for surface reconstruction, modeling, learning, and generation from raw, unsigned geometric data, such as point clouds [4]. Additionally, techniques for learning implicit fields for generative shape modeling have been advocated, with specific emphasis on improving the visual quality of the generated shapes through implicit field decoders [5]. Another line of research has explored the effectiveness of neural networks as compact implicit shape representations for individual shapes, thereby paving the way for the use of overfit networks as first-class implicit shape representations [6].>

<The integration of neural representations with rendering, scene recognition, and query networks has also been a focus of recent advances in the field. Research has demonstrated the development of artificial vision systems that require no labeled data for scene recognition and have the capability to predict scene essentials based on visual sensors [7]. Furthermore, the exploration of attention-based frameworks for locally conditioned implicit representations, as well as the proposal of unifying neural implicit surfaces and radiance fields for multi-view reconstruction, reflects the continuing efforts to improve the capabilities of neural scene representations and renderings [8, 9].>

<In the context of 3D shape representation and generation, the paradigm of parameterizing physical properties of scenes or objects across space and time using methods known as neural fields has gained widespread attention [2]. Previous work has demonstrated the successful use of implicit fields for learning generative models of shapes, and various architectures, including those leveraging periodic activation functions, have been proposed to improve the fidelity and generalization of the learned representations [10, 13]. Additionally, the development of approaches for efficient 3D generation from single-view images and the exploration of generative models for high-resolution image synthesis from natural images have demonstrated the evolving capabilities and applications of neural representations [20, 26].>

<This extensive body of work in the domain of neural rendering and implicit representations has set the stage for the development of 3D-aware generation techniques, such as the one presented in the target paper. The proposed diffusion-based model for 3D-aware generation of neural fields builds upon foundational research in areas such as implicit shape representations, generative modeling, and neural rendering, further advancing the state-of-the-art in 3D scene representation and synthesis [2, 5, 7, 13, 26]. By drawing upon the knowledge and techniques developed in these previous studies, the target paper makes a significant contribution to the growing body of research focused on neural field generation and 3D scene representation.>"
884," **RS distortions correction.** To correct RS distortions, traditional methods [1][2][3][4][5][6][7] usually assume that the scene structure and camera motion are assumed to be known, and then solve the RS distortions correction problem by minimizing the following objective function:

\[\begin{tabular}{langle}\frac{1}{2}{3}{4}{5}{6}{7}{8}{9}{10}{11}{12}{13}{14}{15}{16}{17}{18}{19}{20}{21}{22}{23}{24}{25}{26}{27}{28][28][29].\] (1)where \(\mathcal{L}_{1}\) is the camera motion model, \(i.e.\), the camera rotation is assumed to follow a constant velocity between scanlines, and \(ii.i.d.\) are the scene structures and camera motions.

**Reverse RS distortions Correction.** The reverse RS distortion correction problem aims to recover the latent GS video from a reversed RS image, which is more challenging than the inverse RS distortion problem. To this end, some methods [4][7][5] adopt a fully-supervised learning manner, where high framerate GS images are collected as ground-truth supervision. For example, Zhang _et al._[4] propose a two-stage network to invert a RS image to its corresponding GS video by minimizing a photometric error between two consecutive RS images. However, the estimated GS video is blurry due to the lack of information in the first stage. To address this problem, a recurrent neural network (RNN) is proposed in [5] to restore a high frame rate GS video. In [6], a context-aware video reconstruction network is proposed to restore the latent global shutter (LGS) video from two RS images with reversed distortions. To further improve the realism of the restored LGS video, the authors in [7] propose to use two cameras mounted to have different rolling shutter cameras to recover one global shutter image. In addition, a structure-and-motion-aware rolling shutter correction method [2] is proposed, where a structure and motion-aware network is designed to jointly estimate the underlying scene structure, camera motion, and RS distortions. However these methods are all based on fully supervised learning, which requires a large amount of ground truth GS images to train the network. In contrast, our proposed SelfDRSC framework does not require any ground truth images for training, thus enabling the learned DRSC network to generate high frame rates GS video only based on dual RS images captured by two cameras with reversed RS distortions [5].

Recently, self-supervision has been widely used in various computer vision tasks, such as image-to-image translation [8], deblurring [9][10][11], and video super-resolution [12]. In particular, a cycle consistency loss is introduced in [8] to learn the mapping between input and reconstructed input images, which can be used to train a network for image translation. In this paper, we"," **RS distortions correction.** To correct RS distortions, existing methods can be roughly divided into two categories: geometry-based and learning-based methods. Geometry-based method [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15] assumes that the RS camera model can be decomposed into two parts, i.e., the camera motion model and the RS distortion model. For example, Wang _et al._[1] proposed a general projection equation for a rolling shutter camera and showed how it is affected by different types of camera motion. They also developed an approximate projection equation to approximate the motion of the camera under translational camera motion, which can be used to correct the RS distortions. However, these methods are limited by the assumption that the camera motions are simple and simple-to-obvious. For instance, they assume that the rotation and translation of the scene elements are constant, which is not always true in real-world scenarios.

Recently, learning methods have been proposed to correct RS distortion by learning from GS images, which are collected in a fully supervised learning manner. In [3], a network takes two consecutive GS images as input and predicts the corresponding GS image of the latest frame. The dense displacement field from a GS image to its corresponding RS image is estimated via a motion estimation network, and the warped feature representation is then warped to its GS image representation by a differentiable forward warping block. The warping field is warped to a common GS frame by the image decoder, and then the GS video is recovered by the decoder. In, a network is proposed to learn the motion model of GS images and the motion field of two consecutive RS images, and a warping warping module is introduced to reconstruct the GS image. In addition, a refinement scheme is proposed in [6] to guide the warping process of the warped GS image, and an occlusion mask is introduced in [7] to provide a simple but effective GS frame initialization for initialization. In order to achieve high framerate GS video generation, a network can be trained to generate GS video from consecutive RS frames [4]. In [4], a cascaded structure is proposed for RS temporal super-resolution (RSSR), where the first RS temporal SR network is trained to recover high-frame-rate GS video. In this paper, we propose a self-supervised learning framework for dual reversed RS distortions correction,","[ Existing methods for correcting RS distortions can be divided into two categories, namely fully supervised learning and semi-supervised learning methods.

**Fully supervised learning.** To correct RS distortions, [1] firstly introduced the general projection equation for a rolling shutter camera. A roll-over plane is firstly built to remove RS distortions. Following this work, [2] also developed a Convolutional Neural Network (CNN)-based method to solve the problem of RS correction. This method only considers the current two images and can not estimate the distortion map. [3] presented a CNN-based method to estimate the displacement field from consecutive RS images and recover the corresponding GS image. They leverage a Convolutional Neural Network (CNN) to learn a warping network to estimate the displacement field from the RS image to its corresponding GS image. Although this method can effectively correct RS distortions, the global shutter images are required for training and thus the method is not practical in real scenarios. Unlike previous works that utilized global shutter images for training, [4] proposed a two-stage algorithm to inverting the RS imaging mechanism and achieving RS temporal super-resolution. The first stage performed the geometric modeling of RS effect. Then a new geometric constraint is introduced for the RS inversion.

**Semi-supervised learning.** Recently, semi-supervised learning method has been proposed to recover GS images from RS images. [5] developed an optical flow method to fuse the information of both images and bidirectional distortion warping module is proposed to bring RS images to a low-frame rate GS video. Following this work, [6] proposed a context-aware video reconstruction framework to remove motion artifacts.  presented a framework to correct RS distortions with a few GS images as ground truth supervision, which is validated by RS video with a relatively long exposure duration. All of these works are fully supervised learning methods, since they require global shutter images as ground truth supervision.

**Self-supervised learning.** Self-supervised learning has achieved a remarkable success in many computer vision tasks. It usually adopts a bi-level optimization method to perform the supervision process [7][8]. Besides, both cycle consistency loss and image reconstruction loss have been successfully utilized to achieve self-supervised learning in various tasks [9][10][11][12]. This paper further explores self-supervised learning for DRSC network. The existing method [11] developed a differentiable reblur model for self-supervised motion deblurring, which can learn the model from the optical flow cues in a cycle. Although the above work performs well in deblurring images, it requires special-made and paired images for supervision. Different from the above method,]","Step 1: Extracted important sentences from the cited papers are:
1. Existing methods adopt a fully supervised learning manner for correcting rolling shutter (RS) distortions [1].
2. A Convolutional Neural Network (CNN)-based method has been proposed for learning the underlying geometry from a single RS image and performing RS image correction [2].
3. A network has been developed to estimate the dense displacement field from a rolling shutter image to its corresponding global shutter image, and to recover the global shutter image based on the warped feature representation [3].
4. A method has been proposed to invert the RS imaging mechanism and recover a high framerate global shutter video from consecutive RS images for RS temporal super-resolution [4].
5. A novel end-to-end model has been developed to generate dual optical flow sequence through iterative learning of the velocity field during the RS time, for the challenging task of reversing RS distortion [5].
6. A context-aware GS video reconstruction architecture has been proposed, which estimates the bilateral motion field and synthesizes high-fidelity GS video frames at arbitrary times [6].
7. A surprisingly simple camera configuration has been explored for correcting rolling shutter distortion, involving two cameras mounted in different rolling shutter directions [7].
8. An approach for learning to translate an image from a source domain to a target domain in the absence of paired examples has been presented, which uses a cycle consistency loss [8].
9. A self-supervised learning method has been proposed for deblurring videos, which enforces the output, when blurred based on the optical flow between subsequent frames, to match the input blurry image [9].
10. A neural blind deconvolution method has been presented, which employs deep priors for clean image and blur kernel, leading to an unconstrained neural optimization solution for blind deconvolution [10].
11. A differentiable reblur model for self-supervised motion deblurring has been presented, enabling the network to learn from real-world blurry image sequences without relying on sharp images for supervision [11].
12. A self-supervised learning method has been proposed to solve the blind video super-resolution problem, which simultaneously estimates blur kernels and HR videos from the LR videos [12].

Step 2: Based on the extracted sentences, the related work topics can include:
- Fully supervised learning for rolling shutter distortion correction
- Deep learning-based methods for rolling shutter correction and video enhancement
- Self-supervised learning for image deblurring and video super-resolution
- Utilizing geometry and motion awareness for rolling shutter correction
- Inverting the rolling shutter imaging mechanism for high framerate global shutter video
- Context-aware video reconstruction for rolling shutter cameras
- Simple camera configurations for correcting rolling shutter distortion

<Based on the extracted sentences and related work topics, related work could be written as follows:>
<>
In the field of rolling shutter distortion correction, existing methods primarily rely on fully supervised learning approaches, requiring high framerate global shutter images for ground-truth supervision [1]. The development of Convolutional Neural Network (CNN)-based methods has facilitated the learning of underlying geometry from single rolling shutter images and subsequent correction of the distortion [2]. Additionally, the deployment of end-to-end networks has enabled the estimation of displacement fields and recovery of global shutter images from rolling shutter inputs, contributing to the enhancement of video quality [3]. In the pursuit of high framerate global shutter video generation for rolling shutter temporal super-resolution, innovative approaches have been introduced to invert the rolling shutter imaging mechanism and extract undistorted global shutter frames from rolling shutter distortion [4].

Moreover, the exploration of self-supervised learning methods has paved the way for the development of models capable of reversing rolling shutter distortion, effectively generating dual optical flow sequences and facilitating iterative learning for distortion recovery [5]. Context-aware video reconstruction architectures have also been proposed, allowing for the estimation of bilateral motion fields and the synthesis of high-fidelity global shutter video frames at arbitrary times to mitigate rolling shutter artifacts [6]. Additionally, the exploration of surprisingly simple camera configurations involving two cameras mounted in different rolling shutter directions has shown promise in correcting rolling shutter distortion without relying on specific scene structures [7].

These advancements are concurrent with the broader trend of deep learning-based and self-supervised methods in computer vision and graphics problems. For instance, the use of cycle consistency loss has enabled the learning of image-to-image translation in the absence of paired examples [8], while self-supervised learning techniques have been leveraged in image deblurring and video super-resolution, enforcing outputs to match input blurry images after being blurred based on optical flow [9]. Furthermore, the utilization of deep priors and neural optimization solutions has enhanced blind deconvolution methods for addressing complex and large size blur kernels [10]. Similarly, the development of differentiable reblur models has facilitated self-supervised motion deblurring, enabling networks to learn from real-world blurry image sequences without strict reliance on sharp images for supervision [11]. Lastly, self-supervised learning methods have been proposed to simultaneously estimate blur kernels and high-resolution videos from low-resolution inputs, addressing the challenges in blind video super-resolution [12].
<>"
1104," **Image-Text Retrieval.** Image-text retrieval is a fundamental task to bridge vision and language by exploiting various strategies to learn fine-grained alignment between regions and words. Most existing methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] focus on learning a common embedding space for images and texts, which can be roughly divided into two categories: one-point mapping and one-to-many correspondence modeling.

One-point-based methods [4][21] learn a common representation space for samples from multiple modalities. For example, Wang _et al_. [4] proposed a saliency-guided attention network to capture the salient regions of images and sentences for image-text matching. Li _et.al_. [21] proposed to learn a point-wise transformation to align image regions and text words for text-image matching. However, these methods ignore the semantic correlations beyond the pair between candidate points and a given query. To address this issue, some methods [8][12] adopted the graph structure to model the semantic relations between the pair of candidates and a query. For instance, Zhang and Yu [8] developed a negative-aware attention framework to learn the semantic correspondence between a query and its negative samples. Li and Liu [12] constructed a hypergraph structure to represent a query as a node and its corresponding negative samples as a hyperedge for image text retrieval. In this work, we develop a **M**ultilateral **S**emantic Relations **Modeling** (termed **MSRM**) framework to capture more semantic correlations than the pair-wise semantic relations. Specifically, a query is first mapped as a probabilistic embedding to learn its true semantic distribution based on Mahalanobis distance. Then each candidate instance in a mini-batch is regarded as a _hypergraph node_ with its mean semantics, and a Gaussian query is modeled as a hyperperedge to represent the semantic correlation beyond the relationship between a pair of candidate instances and a single query.

 proposed a geometric embedding method for image retrieval, which is a generalization of the visual semantic embedding (VSE) method [1] for image classification. The geometric embeddings are learned by minimizing the Euclidean distance between the query and all the candidate instances in the training mini-batches. Different from the above methods, we propose a **multilateral** semantic relations modeling framework to model more semantic relations beyond the matching pair between a candidate instance and multiple candidate instances.

 developed a multi-task learning framework to jointly optimize the cross-modal embedding and the task-specific loss functions for image captioning. In contrast, our framework is designed for the image text matching task, which aims to learn an embedding for a given image and a set of candidate sentences to capture their semantic correspondence.

 introduced a cross-attention mechanism to learn multi-scale representations for image and text, which has been widely used in recent image-"," **Image-text retrieval.** Image-text matching is a fundamental task to bridge vision and language by exploiting various strategies to fine-grained alignment between regions and words. Existing methods can be roughly divided into two categories, _i.e._, single-point mapping and multi-point matching.

**Single-Point Mapping.** The pioneering work [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21] adopts the multi-modal embedding strategy to learn a common embedding space for image-text pairs. For example, VSE [2] and VSE++ [2][1] learn a joint embedding for image and text by maximizing the similarity between image regions and word embeddings. To improve the performance, some works [3][5] propose to learn the similarity score directly from the image-to-text embedding. For instance, the two-branch neural network [5] and the attention mechanism [6] are proposed to learn an explicit shared latent embedding with a maximum-margin ranking loss and novel neighborhood constraints, respectively. The attention mechanism is also widely used in many recent works [4][4] to learn more discriminative visual features. The main difference between these methods and ours is that we model the semantic relations between multiple samples and a query via hypergraph modeling, which is more effective in capturing the semantic correlations beyond the pair-wise pairwise matching. In addition, our method is also able to capture the one-to many correspondence, which has not been explored in previous works.


","[ **Image-text Retrieval.** Image-text retrieval aims to find relevant candidates given a query by exploring the alignment between visual and textual data. Existing approaches can be categorized into three classes: (1) Co-occurrence-based methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] and (2) Ranking-based methods [15][16]. They follow a similar pipeline. Given an image or a text query, these methods first encode visual and textual features into a shared embedding space. They further use cosine similarity to rank the corresponding query-key pairs according to their similarities. (3) Representation-based methods [17][18][19] enforce spatial distribution or semantic regularization in feature learning. For example, DINO [17] learns an augmentation-invariant embedding that facilitates fine-grained matching.

Besides, early work [15][20] also studies the one-to-many correspondence problem. PCME [20] employs a deterministic mapping function and leverages the pairwise training for improving confidence, while PFAC [15] proposes a probabilistic mapping to distribute unreliable and ambiguous samples. Different from them, we develop a probabilistic multilateral modeling for image-text retrieval to capture the one-to-many correspondence relation via hypergraph modeling.

**Hypergraph Modeling.** One-to-many correspondence is modeled as a hypergraph in this paper. Unlike hypergraph-based methods for semantic image-text retrieval [21], we model the one-to-many correspondence in the common embedding space and propose a hypergraph-based cross-modal matching model.

]","Step 1: Extracting important sentences from the cited papers
Sentence 1: ""Multilateral Semantic Relations Modeling (termed MSRM) for image-text retrieval to capture the one-to-many correspondence between multiple samples and a given query via hypergraph modeling.""<sup>[1]</sup>
Sentence 2: ""Existing solutions to this problem including multi-point mapping, probabilistic distribution, and geometric embedding have made promising progress, but one-to-many correspondence is still under-explored.""<sup>[1]</sup>
Sentence 3: ""Recent VSE models use complex methods to better contextualize and aggregate multi-modal features into holistic embeddings.""<sup>[1]</sup>
Sentence 4: ""Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings.""<sup>[2]</sup>
Sentence 5: ""Learning appropriate representations for multi-modal data is crucial for cross-modal retrieval performance.""<sup>[3]</sup>
Sentence 6: ""Existing approaches for image-text matching do not explicitly transform the different modalities into a common space.""<sup>[6]</sup>
Sentence 7: ""Training under supervision of triplet ranking loss is a dominant methodology for cross-modal matching models.""<sup>[7]</sup>

Step 2: Summarizing sentences and deciding on topics of related work
The important sentences suggest topics such as (1) methods for capturing one-to-many correspondences in image-text retrieval, (2) leveraging complex methods for contextualizing and aggregating multi-modal features, (3) learning appropriate representations for multi-modal data, and (4) the dominant methodology of triplet ranking loss for cross-modal matching models.

Step 3: Writing related work based on topics
<In the realm of image-text retrieval, addressing the challenge of one-to-many correspondence remains a critical area of exploration. This challenge has led to the development of approaches such as multi-point mapping, probabilistic distribution, and geometric embedding, which have shown promising progress [1]. Additionally, recent models have employed complex methods to contextualize and aggregate multi-modal features into holistic embeddings, indicating the ongoing exploration of diverse strategies in this domain [1]. Furthermore, the importance of learning appropriate representations for multi-modal data has been highlighted as crucial for enhancing the performance of cross-modal retrieval systems [3]. Notably, the supervision of triplet ranking loss has emerged as a dominant methodology for training cross-modal matching models, suggesting a key area of focus in the pursuit of improved retrieval accuracy and efficiency [7]. These diverse efforts collectively reflect the ongoing exploration and innovation within the field of image-text retrieval, with a particular emphasis on addressing the challenges of one-to-many correspondence, feature representation, and training methodologies.>"
559," **Metric Learning Loss Functions.** Metric learning loss functions can be categorized into two groups: pairwise and triplet losses. Pairwise losses [1][2][3] minimize the distance between samples from the same class while pushing samples from different classes apart. Triplet losses [4][5] maximize the margin between the positive and negative samples. The triplet loss [6] is the most widely used loss function for image retrieval. It optimizes the cosine similarity between the anchor and the positive sample, and the Euclidean distance between anchor and negative sample.

**Re-ranking for Image Retrieval.** The goal of re-ranking is to improve the accuracy of image retrieval by learning a correct ranking of training samples [7][8][9][10][11][12][13][14][15]. For example, [8] proposed a k-reciprocal encoding (kRE) method to re-rank the retrieved images. [9] proposed an unsupervised metric learning method that alternates between predicting class-equivalence relations between data and optimizing the moving average precision (MAP) metric. [10] proposed to learn hyperbolic image embeddings by optimizing the Mahalanobis distance and the angular distance. [11] improved the performance of the MAP metric by combining several improvements, such as batch normalization (BN) [4], triplet sampling [5], and hyperboles [13].

Our method is different from the above methods in two aspects. First, our method optimizes _contextual similarity_ in addition to _cosine similarity_, which enforces _semantic consistency_ among neighbors while converging to the correct ranking. Second, the proposed loss is more robust to label noise, and is less prone to overfitting even when a large portion of train data is withheld.

 proposed a hierarchical metric learning framework to address the problem of noisy labels. They proposed a dynamic metric space to accommodate multiple semantic scales. [12] proposed adaptive hierarchical similarity metric learning (AHSML) to handle noisy labels in metric learning. [15] proposed hierarchical average precision training (HAPT) for top-\(k\) image retrieval, which is a variant of APT. Our method differs from these methods in three aspects: (1) we propose a new metric learning loss function that enforces semantic consistency among neighbors; (2) we empirically show that our method is robust to noisy labels; and (3) our method achieves a new state-of-the-art performance across four image retrieval benchmarks and multiple different evaluation settings.

 introduced a new loss function, called contrastive loss, for image classification. Their loss function is based on contrastive learning, which aims to minimize the distances between positive pairs while maximizing the distances of negative pairs. In contrast, our loss enforces contextual similarity among neighbors, which can be viewed as a generalization of contrastive-based loss. We empirically demonstrate that our loss is significantly more robust against noisy labels and is more effective in image retrieval compared to contrastive and contrastive losses.

 presented a"," **Metric Learning for Image Retrieval.** Metric learning has been extensively studied in computer vision and machine learning for image retrieval [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]. In this section, we focus on metric learning methods that are most relevant to image retrieval.

**Metric learning for Image retrieval.** Many metric learning approaches have been proposed to learn a correct ranking of training samples. These approaches can be categorized into supervised and unsupervised methods. Supervised methods [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 59]. For example, the contrastive loss [1], triplet loss [4], and margin-based loss [5] have been widely used in image retrieval, where the loss is defined as a weighted sum of the cosine similarity between the embedding vectors of the same image and its neighbors. The triplet or contrastive losses are designed to encourage the embeddings of similar samples to be close in the Euclidean space, and push them apart for dissimilar samples. The margin based loss is based on the assumption that the distances between samples belong to the same class while those from different classes are separated by a large margin [4]. The triplets based loss has also been used in [4][1][5]. The distance is usually defined as the sum of cosine similarities between the features of different samples in the same category. The distance between samples from the same or different classes is defined by the distance between the feature vectors of different categories. The center loss [6] is a widely used metric learning method for metric learning, which is defined to minimize the distance of features from similar classes and maximize that of dissimilar ones. In addition, the center loss is also used in many other metric learning works, such as [4] and [1].

In this paper, we propose a new metric learning loss function that optimizes contextual similarity in addition to cosine distance. We show that the proposed loss is more robust to label noise and less prone to overfitting when a large portion","[ Deep metric learning methods broadly fall into four categories: triplet loss, pairwise-aligned contrastive loss, discrepancy loss, and softmax-aware loss.

**Triplet Loss.** The first loss that is proposed for deep metric learning is triplet loss [1][2][3]. In triplet loss, for each positive example \(x_{p}\), there are two negative examples that are closest and farthest from it respectively in the embedding space. The loss is defined as \(\mathcal{L}(\phi)=|\mathcal{L}_{1}(\phi)|+\lambda|\mathcal{L}_{2}(\phi)|\), where \(\mathcal{L}_{1}(\phi)\) denotes the loss of the closest pair and \(\mathcal{L}_{2}(\phi)\) denotes the loss of the furthest pair. \(\lambda\) is a trade-off parameter for balancing the losses of two types. This method was developed in order to mitigate intra-class compactness and inter-class separability issues for traditional distance-based metric learning, such as LLE [4][5][6]. Since it involves only two negative samples for each positive one, triplet loss is limited to handling high-dimensional data [1][2][3] and is highly vulnerable to label noise and outliers.

**Contrastive Loss.** The contrastive loss was proposed by Wang _et al_. [2] and is the most popular deep metric learning loss to date. It is designed to encourage a given positive sample \(x_{p}\) to have a smaller distance to its other augmented versions \(\tilde{x}_{p}\) than a given negative sample \(x_{n}\), i.e., \(d(x_{p},x_{n})>d(x_{p},\tilde{x}_{p})\), where \(d(\cdot)\) is a distance metric between images, such as Euclidean distance or cosine similarity. It differs from triplet loss in that there are no explicitly negative samples. Contrastive loss thus outperforms triplet loss in various image retrieval applications, including triplet loss. However, the loss is not able to regularize the distribution of negative samples and can thus be overwhelmed by a small fraction of mislabeled negative samples that occur in practical applications.

**Discrepancy Loss.** Discrepancy loss [5][7] minimizes the distances between negative pairs and positives. The training of discrepancy loss follows the idea of consistency regularization in contrastive loss, where the optimization objectives are identical, except that the Loss \(\mathcal{L}\) only involves the distances between negative pairs and positives:

\[\mathcal{L}(\phi)=-2\sum_{n}C\sum_{p}\operatorname{KL}\left(d(\tilde{x}_{p},x_{n})\middle||d(\phi(x_{n}),\phi(x_{p}))\right) \]

Discrepancy]","Step 1:
Important sentences from the cited papers for writing related work:
- ""The goal of metric learning is to learn a function that maps samples to a lower-dimensional space where similar samples lie closer than dissimilar ones"" [1].
- ""In this paper, we propose an approach based on message passing networks that takes all the relations in a mini-batch into account"" [1].
- ""One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power"" [2].
- ""We propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition"" [2].
- ""In this paper, we propose Hypergraph-Induced Semantic Tuplet (HIST) loss for deep metric learning that leverages the multilateral semantic relations of multiple samples to multiple classes via hypergraph modeling"" [3].
- ""The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples"" [4].
- ""Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search"" [5].
- ""Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar"" points in input space are mapped to nearby points on the manifold"" [6].
- ""Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras"" [7].
- ""We propose a k-reciprocal encoding method to re-rank the re-ID results"" [8].
- ""We present a novel self-taught framework for unsupervised metric learning, which alternates between predicting class-equivalence relations between data through a moving average of an embedding model and learning the model with the predicted relations as pseudo labels"" [9].
- ""Computer vision tasks such as image classification, image retrieval, and few-shot learning are currently dominated by Euclidean and spherical embeddings so that the final decisions about class belongings or the degree of similarity are made using linear hyperplanes, Euclidean distances, or spherical geodesic distances (cosine similarity)"" [10].
- ""At the core of our method is a vision transformer with output embeddings mapped to hyperbolic space"" [11].
- ""In metrology, the dynamic range is a basic quality of a metric tool, indicating its flexibility to accommodate various scales"" [14].
- ""Introducing the dynamic range to deep metric learning, we get a novel computer vision task, i.e., the Dynamic Metric Learning"" [14].
- ""Image Retrieval is commonly evaluated with Average Precision (AP) or Recall@k"" [15].
- ""This paper introduces a new hierarchical AP training method for pertinent image retrieval (HAP-PIER)"" [15].

Step 2:
Summarized sentences and topics for related work:
1. Metric learning for feature mapping and similarity optimization.
2. Loss functions and methods for deep metric learning in visual recognition.
3. Applications of metric learning in person re-identification and image retrieval.
4. Alternative geometric embeddings for visual tasks.

Step 3:
<In recent years, metric learning has gained significant interest as a means to map samples into lower-dimensional spaces, where similar samples are closer together than dissimilar ones. This objective has spurred research into methods for optimizing similarity and ranking in various domains [1]. Regarding deep metric learning in visual recognition, there have been significant advancements in designing loss functions and methods that enhance discriminative power for large-scale face recognition, such as the Additive Angular Margin Loss (ArcFace) proposed for face recognition [2]. Other approaches, like the Hypergraph-Induced Semantic Tuplet (HIST) loss, leverage multilateral semantic relations to guide the embedding model for improved generalization performance and model robustness against input corruptions [3]. Furthermore, metric learning has found application in person re-identification, where the goal is to retrieve individuals across non-overlapping cameras, and in image retrieval, a fundamental component of tasks like verification, zero-shot learning, and visual search [7, 8].

In addition to traditional Euclidean and spherical embeddings, recent work has explored alternative geometric embeddings, such as hyperbolic embeddings, which have shown potential for providing a better alternative for various computer vision tasks, including image retrieval, few-shot learning, and classification [10, 11]. This shift towards exploring alternative geometric spaces has opened up new avenues for metric learning and offers promising directions for further research. Furthermore, recent advancements in dynamic metric learning have introduced the concept of dynamic range to accommodate various semantic scales, presenting a novel computer vision task known as Dynamic Metric Learning [14]. The development of hierarchical AP training methods for pertinent image retrieval has also improved the evaluation and organization of the embedding space, preventing severe failure cases encountered by non-hierarchical methods [15]. These developments underscore the growing interest and developments in metric learning that extend beyond traditional approaches, providing valuable insights and directions for future research in the field.>"
1715," **Online Action Detection.** Online Action Detection (OAD) [1] is a task that aims to identify the start of an action in a video stream as soon as it happens. Most of the existing methods for online action detection are based on recurrent neural networks (RNNs) [2][3][4][5][6][7][8][9][10][11][12]. For example, T-RNN [5] proposes a temporal recurrent network (TRN) to model the temporal dependencies between the start and end times of actions. RED [4] proposes an encoder-decoder network (EDN) with reinforcement learning to capture long-range temporal dependencies. WOAD [8] introduces a weakly supervised online detection task, where only the first few frames of an untrimmed video are used for training. OadTR [9] and LSTR [10] propose to use transformers to model long-term temporal dependencies for online detection. GateHUB [11] proposes to use gated history units (GHUs) to suppress the background information. TSMT [12] proposes the temporal smoothing transformers (TST) for online video detection.

**Sequence Modeling with Gated Recurrent Units (GRUs).** GRUs [13] have been widely used in sequence modeling tasks due to their ability to capture the dependencies between input and output sequences. However, GRUs are computationally expensive due to the quadratic growth in the number of parameters. To address this issue, several methods have been proposed to improve the efficiency of RNNs. For instance, LSTM [2] uses a gated recurrent unit (GRU) [3] to reduce the computational complexity of LSTMs. TSRN [6] uses an attention mechanism to selectively attend to important information in the input sequence. However these methods are still computationally inefficient as they need to compute the gradients at each time step, which makes them unsuitable for real-time applications. In this paper, we propose a minimal RNN-based model that can be trained in an end-to-end manner with the help of the proposed non-uniform weighting scheme, which is orthogonal to the above methods and can be combined with them to further improve the performance. Our experiments show that our proposed method outperforms the state-of-the-art GRU-based methods on the THUMOS and TVSeries datasets, which further demonstrates the effectiveness of our method. We also show that the proposed method can be applied to other online detection tasks, such as action anticipation, to further boost the performance of these methods.
**Transformers.** Transformer [14] was first proposed for the task of machine translation in the natural language processing (NLP) domain. It is composed of a self-attention module and a feed-forward module. The attention module learns to focus on the most relevant parts of the input sequences for each word, while the feed forward module uses the entire input sequence to predict the next word. BERT [16] is the first large-"," **Online Action Detection.** Online action detection has been extensively studied in the literature [1][2][3][4][5][6][7][8][9][10][11][12]. Most of these methods are based on RNNs [2][1][13][5], which have been shown to be effective in capturing long-range temporal dependencies [1]. However, RNN-based methods have received less attention recently, due to their lower performance compared to transformers [14][15][16][17][18][19][20][21]. In this work, we focus on the problem of online action detection, which is more challenging than online action recognition.

**Online Action Recognition.** The goal of online video action recognition is to recognize actions as soon as they happen in a streaming video. Most of the existing methods use RNN models to model long-term temporal dependencies. For example, [2] proposed a two-stream feedback network, where one stream processes the input and the other models the temporal relations. [3] proposed an LSTM-based method to learn long-distance temporal dependencies by using the log-linear model [3]. [4] introduced a reinforcement module to provide sequence-level supervision to encourage the model to make correct predictions as early as possible. [5] proposed the Temporal Recurrent Network (TRN) to model greater temporal context of each frame by simultaneously performing online action prediction and anticipation of the immediate future. [6] proposed to explicitly discriminate the information relevant to an ongoing action from others. [9] proposed OadTR to capture the relationships and global inter-actions between historical observations and future clip representations. [10] proposed Long Short-Term Transformer (LSTR) to capture coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), and an LSTR decoder to model the fine-scale characteristics of the data with fewer heuristics. [11] proposed GateHUB, a gated history unit with background suppression to enhance or suppress parts of the history as per how informative they are for current frame prediction. [8] proposed WOAD, a weakly-supervised method to identify action boundaries in untrimmed videos. [12] proposed TeSTra, a real-time model based on the temporal smoothing transformer [12].

In this paper, we investigate the underlying reasons for the inferior performance of RNN based methods","[ **Online Action Detection.** OAD was first introduced by [1] who formulated it as a prediction problem in the temporal domain. To address the unsupervised setting, methods are mainly categorized into two types. The first type of approach is to learn to predict the future of current frames, either using weakly labeled data [2][3][4][5] or unlabeled data [6][7][8]. However, training these models requires not only image-level predictions but also temporal labels, which is often impractical. The second type of approach is to predict a set of temporal segments that includes the current frame [2][9][10][11][12]. One of the most straightforward and popular methods is the Temporal-Shift-based method [2], which captures long-range temporal dependencies through a recurrent model. Despite the promising performance, RNNs face two major limitations for OAD. First, the standard training procedure for RNNs [13] involves _gradient back-propagation_, which results in a vanishing gradient problem when the recurrence is more than a few frames. Second, RNNs are notoriously slow in terms of training and inference. To address the first limitation, [2] proposed an online retraining scheme, which aims to reduce the gradient computed for the frames far in the past by the data points closer in the future. This technique has been further improved in [3], with the inclusion of a three-step procedure that efficiently propagates the gradients forward in time. The second limitation is the main focus of our work. To address it, we propose to use non-uniform loss weights for the training process.

**Transformer-based Approaches.** Transformer [14] has gained a lot of attention in the field of natural language processing (NLP) [15][16]. Recently, it has also been widely applied to video recognition [17][18]. OAD is mostly investigated in the video-based setting, where data is collected via mining video datasets [5][9]. While performance of Transformer-based methods on action detection tasks has been continuously improved, this line of research largely ignores the fundamental limitations of Transformer, such as the quadratic complexity with respect to sequence length. In contrast, our goal is to investigate how we can combine RNN and Transformer in an effective way to address the training and inference difficulties in an online setting. To reduce the quadratic complexity, [19] proposed a ""transformer-like"" model that addresses each timestep independently, and explicitly minimizes the number of frames to propagate the gradients to.  proposed to use the attention-based mechanism to efficiently capture long-range dependencies. However, this model is still overly-complex and cannot be deployed in practice, where latency and memory requirements are crucial factors for real-time applications.

**RNNs with Fine-tuned Weights.** To reduce the inference and training complexity of R]","<Online action detection (OAD) is the task of identifying actions in streaming videos without access to future frames. Recent methods for OAD have focused on effectively capturing long-range dependencies, with transformers gaining attention for their ability to capture long-range temporal structures [1, 9, 10]. While transformers have shown promise, RNNs have received less attention due to their lower performance compared to recent methods that utilize transformers. Despite this, recent research has raised concerns about the effective training of RNNs compared to transformer-based algorithms [1, 2, 3, 9, 10, 11]. To address the inferior performance of RNNs, the proposed mini RNN-based model in the target paper addresses the discrepancy between training and inference as the primary hindrance to effective training. This contrasts with other approaches such as transformers and LSTMs [2, 3, 10, 11].>

<Existing approaches such as LSTMs have also encountered challenges in capturing long-term patterns for online action detection [2, 3]. One hypothesis is that LSTM struggles to simultaneously learn the interpretation of input and temporal patterns, leading to less accurate detection results [2, 3]. To address this, a two-stream feedback network for processing input and modeling temporal relations has been proposed [2]. In contrast, the proposed methodology in the target paper focuses on applying non-uniform weights to the loss computed at each time step, enabling the RNN model to learn from predictions made in an environment resembling the inference stage [1].>

<Additionally, recent methods such as RED and TRN have explored reinforced encoder-decoder networks and temporal recurrent networks for action anticipation and online action detection, respectively [4, 5]. These approaches differ from the proposed methodology in the target paper, which emphasizes the utilization of non-uniform weights to address the discrepancy between training and inference for RNNs [1]. Furthermore, the target paper's approach contrasts with IDU, a recurrent unit designed to discriminate information relevant to an ongoing action from background and irrelevant actions in an input image sequence for online action detection [6].>

<Moreover, recent work has also focused on novel neural network models like RNN Encoder-Decoder and Temporal Smoothing Transformers for tasks such as statistical machine translation and image recognition at scale, respectively [7, 12, 15]. These approaches, although impactful in their respective domains, follow different paradigms compared to the mini RNN-based framework proposed in the target paper for online action detection [1]. Additionally, the proposed methodology contrasts with ViVit and MeMViT, which leverage transformer-based models for video classification and efficient long-term video recognition, respectively [17, 18].>

<Notably, the target paper's proposed methodology for addressing the inferior performance of RNNs compared to transformer-based algorithms in online action detection differs from BERT, a language representation model designed for pre-training deep bidirectional representations, and ViTr, a vision transformer for image recognition [16, 17]. Additionally, the proposed approach contrasts with the structured state space sequence model and the memory-augmented multiscale vision transformer, which aim to efficiently model long sequences and overcome computation and memory bottlenecks in video recognition systems [18, 19]. Lastly, the approach in the target paper differs from RWKV, a model architecture that combines the efficient parallelizable training of transformers with the efficient inference of RNNs for sequence processing tasks [21].>"
3061," **Image Signal Processors.** Image signal processing (ISPs) are used in many computer vision applications, such as object detection [1][2][3][4][5][6][7][8][9][10], super-resolution [11][12], deblurring [13][14], denoising [15][16], and many others. In this paper, we focus on the application of image signal processing to computer vision tasks.

**Image Sensor Noise Modeling.** There is a large body of work on modeling the noise characteristics of image sensors [17][18][19][20][21][22]. However, most of these models are based on the fixed-pattern noise (FPN) model [19], which assumes that the noise is independent of the scene content. However, in many real-world scenarios, the sensor noise is spatially correlated, which makes it difficult to model the FPN noise. In addition, these models do not consider the non-linearity of the ISP, which is important for image recognition tasks. In contrast, we propose a noise-accounted RAW image augmentation method that considers both the ISP and sensor noise characteristics. This allows us to generate realistic images in terms of pixel intensity and noise distribution, which can be used for training recognition models in challenging environments such as extremely dark, blurry, or high dynamic range (HDR) conditions. Furthermore, we introduce a noise amount alignment method to calibrate the domain gap in the noise property caused by the augmentation, which helps to train a recognition model in a wide variety of challenging environments. We show that our proposed RAW segment method can achieve state-of-the-art performance in several challenging environments, including HDR and extremely dark environments. Note that our noise-based RAW augmentation can be applied to any sensor, including the CMOS sensor, as long as it is compatible with the camera's ISP. We leave this for future work.****Data Augmentation for Image Recognition.** Data augmentation is a common technique used to improve the performance of image recognition models. One simple approach is to apply data augmentation such as color jitter and blur to standard RGB images in simple scenes. Unfortunately, this approach struggles to generate images that are realistic in the pixel intensity domain due to the nonlinear ISP. To address this problem, recent work [23][24][25] proposed to use synthetic data to augment the training data. For example, CycleISP [24] uses a neural network to synthesize images from the noise distribution of the RAW sensor, and then uses the synthesized images to train the recognition model. In [23], the authors proposed a method to generate synthetic data for training night-time neural ISPs, and they showed that the synthetic data can improve the recognition performance in extremely dark and blurry environments. Our proposed RAWgment method is different from these methods in two ways. First, our method does not require any synthetic data. Second, our proposed method can generate RAW images with realistic noise distribution and pixel intensity.

 proposed an approach to generate training data for neural networks. They proposed a network architecture that"," RAW Augmentation.Image signal processing (ISP) has been widely used in many computer vision tasks such as image restoration [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16], image enhancement [17][18][19][20][21][22], and image denoising [23][24][25]. However, the non-linearity of the ISP and the noise characteristics of the sensor limits the realism of the results. In this work, we propose a noise-accounted RAW image augmentation method that is able to produce realistic results in terms of pixel intensity and noise distribution.

**Noise-based Image Recognition.** Image recognition models trained on AWGN-based datasets often suffer from the generalization problem when applied to real RAW images [25]. This is mainly because AWGN is not suitable for modeling the signal-dependent and heavily-transformed real camera noise. To address this issue, several methods have been proposed to model the noise distribution of RAW images using the heteroscedastic Gaussian noise model [25] or the Gaussian mixture model [21]. These methods are based on the assumption that the noise is spatially-varying and the camera ISP is linear. However, this assumption does not hold for real sensor data. In addition, the noise model is not robust to the noise properties of real RAW sensor data due to the nonlinearity and non-convexity of image sensors. In contrast, our proposed method is robust to real sensor noise properties.
","[ **Raw to Raw.** Some works enforce the raw image processing tool in neural networks. Wijns and Huang [1] propose DirtyPixels to incorporate ISP directly, while Chen _et al_. [2] and Cao _et al_. [3] incorporate sophisticated ISP models. As the raw image pre-processing tool is rather complicated, Chen _et al_. [4] develop a memory-efficient ISP lightweight model, and put it into a neural network.

**Raw to Sensor.** Some recent works focus on raw to sensor directly, which does not involve ISP. Feraud _et al_. [5] propose ISP distillation that directly fuses the prediction of a lightweight neural network with a baseline ISP. Mohajerani _et al_. [6] and Kumar _et al_. [7] propose ISP4ML and VisionISP to render low-level features of image sensor and put the data into a supervised feature learning pipeline. In addition, some recent works [8][9][10] try to predict parameters in sensor (e.g., exposure time, camera gain) in order to reconstruct a corresponding sensor raw image.

**Noisy Image Restoration.** When the training data is insufficient for a specific task, one solution is to perform image restoration from noisy data. For instance, BlindImageDeblur [11] and BurstImageRestoration [12] try to remove the image noise with denoising techniques. Hui _et al_. [13] try to generate high-quality images by blending multiple blurry images. Zhu _et al_. [14] propose an efficient multi-patch stacked encoder-decoder network to remove image noise. Besides, Chen _et al_. [15] restore a denoised image from a noisy image.

**Accurate Modeling of ISP.** It is important to have a model of image sensor that considers the complex and noise characteristics of real sensor. Mertens _et al_. [16] propose a noise model with column-parallel correlated multiple sampling (CMS) circuits and Fixed Pattern Noise (FPN). There are some works on high-level numerical simulators for image sensor noise [17][18], such as ACMSim and ACMSimCMS. Kukhtina _et al_. [19] propose a CMOS image sensor noise model based on column-parallel high-gain readout circuits. In their work, FPN is modeled as a sum of two components: a column and a pixel component [20].

While it is beneficial to model noise more accurately, most current work does not consider the color characteristics of noise. For instance, Chang _et al_. [21] develop a highly accurate noise model based on physical mechanism. By contrast, []","<In recent years, various methods have been proposed to address challenges related to image processing and perception in difficult conditions, such as low-light environments or high dynamic range conditions. One approach, as discussed by [1], involves the end-to-end processing of RAW sensor measurements, including denoising, deblurring, and tone-mapping, to improve object detection performance. Similarly, [2] presents a three-stage cascade framework named Deep-FlexISP, which decomposes the ISP into weakly correlated sub-tasks to address challenges in night photography rendering, such as high noise levels and low dynamic range. Furthermore, [3] introduces GenISP, a minimal neural ISP pipeline explicitly designed for low-light machine cognition, which incorporates Color Space Transformation to improve generalization to unseen camera sensors. These works emphasize the importance of tailored processing pipelines for specific environmental conditions such as low-light scenarios.

Another line of related work focuses on object detection in very low-light conditions. [4] outlines the design principles for low-light detection systems and introduces a realistic low-light synthetic pipeline and an auxiliary low-light recovery module, achieving promising results in very dark environments. [5] explores the role of the camera Image Signal Processor (ISP) in machine-only observed images and demonstrates improved object classification and semantic segmentation performance by applying knowledge distillation to align predictions of RAW and processed RGB images. Additionally, [6] investigates the efficacy of ISPs in CNN classification tasks, demonstrating that ISP improves accuracy and generalization of CNNs across different image sensors, highlighting the significance of ISPs in efficient deep learning vision systems.

Moreover, several works propose methods for training and enhancing neural ISPs, specifically for low-light scenarios. For example, [8] presents a novel multitask auto-encoding transformation (MAET) model for exploring illumination translation patterns for object detection in dark environments. In a similar vein, [9] revisits auto-exposure control as an alternative to high dynamic range (HDR) sensors and proposes a neural network for exposure selection trained jointly with an object detector and an ISP pipeline. These advancements reflect the growing interest in developing specialized techniques for enhancing object detection in challenging lighting conditions.

Lastly, there is a body of work focusing on noise modeling and handling in digital imaging sensors. [17] and [18] present comprehensive toolkits and high-level numerical simulations for accurate modeling of image sensor noise in CMOS sensors, providing engineers with the capability to understand and mitigate various noise mechanisms. Additionally, [25] proposes a practical Poissonian-Gaussian noise model and fitting algorithm for single-image raw data, addressing the need for realistic noise modeling in digital imaging sensors. These efforts underscore the importance of understanding noise properties and developing accurate noise models for effective image processing and perception in challenging environments.
>"
4077," **Unsupervised Domain Adaptation.** Unsupervised domain adaptation (UDA) [1][2][3][4][5][6][7] has been widely studied in various computer vision tasks, including image classification [8][9][10][11][12][13][14][15][16][17][18][19][20][21][22], object detection [23][24][25], and semantic segmentation [26][27][28][29]. In this work, we focus on the task of UDA for semantic segmentations.

**Self-training.** Self-training [30][31][22][32][33][34][35] is a popular semi-supervised learning (SSL) method, which trains a model with two-fold abilities: (i) learning reliable semantics from the labeled images in the source domain, and (ii) adapting to the unlabeled images of the target domain via generating pseudo labels on the labeled data. The pseudo labels are then used to train the model in a self-training manner. For example, Mean-Teacher [30] trains a student model by averaging the predictions of its teacher model, while ATSO [22] optimizes the student model with an asynchronous teacher-student training strategy. Dual Student [31] further extends Mean Teacher by introducing a student-teacher framework with an exponential moving average (EMA) learning strategy. Mutual Mean Teaching (MMT) [32] proposes a mutual mean-teaching framework to refine the pseudo labels of the teacher model. PseudoSeg [33] designs a multi-task learning framework to jointly train the teacher and student models. In contrast, our proposed DTS is a dual-tutor-student framework with a bidirectional learning strategy, where the second model learns to 'Focus on Your Target' while the first model is not affected by the discrepancy between the two models. Moreover, DTS can be easily plugged into existing SSL methods, such as Mean Teacher [30], MMT [32], and PseudoSeq [33].

In this paper, we propose a novel DTS framework for UDA, which can be seamlessly integrated into existing UDA methods. We find that, by decreasing/increasing the proportion of target-domain training samples, the 'learning ability' is strengthened/weakened while the 'adapting ability' goes in the opposite direction, implying a conflict between these two abilities, especially for a single model. To alleviate the issue, we design a novel dual teacher-Student (**DTS**) framework and equip it with a bi-level learning strategy to enhance the second teacher's learning ability while the second student's adaptation ability remains unaffected. We also propose a class-balanced pixel-level self-labeling (CPSLS) method to address the class imbalance problem in UDA. We show that the proposed method can be combined with existing SSL approaches to further improve their performance.


**Co-training and Self-Training.** Co-training  is a widely used method for SSL, which consists of two steps:"," **Unsupervised Domain Adaptation (UDA) for Semantic Segmentation.** UDA for semantic segmentation has been extensively studied in recent years. Existing UDA methods can be categorized into two groups: (1) _Feature-level_ methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36] and (2) _Optimization-based methods [4][4] [4].

**Feature-Level Methods.** Feature-level methods [2][1][3] aim to minimize the discrepancy between the source and target distributions. For example, Maximum Mean Discrepancy (MMD) [2] is proposed to minimize domain-invariant feature representations by minimizing the maximum mean discrepancy between source- and target-domain feature distributions. JAN [1] proposes to align the joint distributions of multiple domain-specific layers across domains based on the joint maximum mean discrepancies (JMMD).


\(\rightarrow\)CMD [2]) is proposed in [1], which aligns the joint distribution of feature representations across domains by maximizing the JMMD criterion. In [2], the authors propose to match the higher order central moments of probability distributions by means of order-wise moment differences.

 differences are minimized between the feature representations of different layers in the network. In, the authors present a novel domain adaptation method by aligning the feature distributions of the feature extraction layers of the network in the feature space. In this work, we propose a novel dual teacher-student framework for domain-adaptive semantic segmentations.

 propose a domain-adversarial learning based method for UDA in the context of semantic segmentants. The authors in [4] propose a progressive domain adaptation framework for cross-domain object detection. In their work, the authors first train a source domain detector and then fine-tune the detector on the target domain. In contrast, our work focuses on the UDA problem in the domain-agnostic scenario.

","[ **Unsupervised domain adaptation (UDA)** for semantic segmentation has been extensively studied in computer vision community. To learn domain invariant features, many different regularization methods have been introduced such as adversarial learning [1][2], weakly supervised learning [3][4], self-consistency regularization [5][6][7] and hard-label based learning [8][9][10][11][12]. Some works use different domains to disentangle feature space [13][14][15][16][17], while others consider intermediate representations between domains [18][19][20][21]. In this work, we focus on the standard self-training framework. We give a comprehensive discussion of the standard self-training in Section 3.

**Self-training** for semantic segmentation is introduced in [22][23], where the model is self-trained on target domain data (only unlabeled data) by applying cross entropy loss on generated pseudo-labels. Taking the seminal work _LST_[22] as a starting point, the UDA approaches which adopt self-training (ST) can be mainly categorized into three directions: _1)_statistical modeling_[24][25][26][27][28][29], _2)_inter-network consistency_[30][31], and _3)_pseudo label based_[22]. The method _LST_[22] simply models the labeled and unlabeled data in the self-training pipeline. _JS_[23] further improves LST by employing two-stage self-training (which means two network structures are trained iteratively). However, the model in _LST_[22] and _JS_[23] can not be directly applied to the target-domain test set because they only train on unlabeled target domain data.

More recently, a few works start to focus on the semantic segmentation task and employ self-training. _PSEG_[32][33] focus on semantic segmentation on person re-identification task and proposed to select the generated pseudo-labels under camera perturbation. _CPS_[34] assigns pseudo labels based on the intermediate features of two networks and extends the domain generalization to cross-source target-domain adaptation. The most related work to ours is _DLS_[35] which proposed to use weight-averaged consistency targets to improve the semi-supervised segmentation. DLS is inspired by the idea of _mean teacher_[30], which involves a multi-stage ST framework (both labeled source domain data and unlabeled target domain data are required for each stage) and the teacher network is kept frozen while the student network is being trained. Moreover, _DLS_ only consider unbalanced domain (source domain is 5-way fully-super]","<When addressing unsupervised domain adaptation (UDA) for semantic segmentation, it is crucial to consider the challenges posed by the domain shift between labeled source data and unlabeled target data. Prior research has shown that self-training approaches, which involve learning from labeled source images and adapting to the target domain by generating pseudo labels for unlabeled target images, have been popular in this area [1]. However, a conflict between learning reliable semantics from source domain images and adapting to the target domain has been identified when using a single model [2]. To mitigate this conflict, the use of a dual teacher-student (DTS) framework has been proposed, which involves equipping models with bidirectional learning strategies to strengthen the focus on the target while avoiding impact on the initial model [2].

Another relevant area of research in the context of domain adaptation for semantic segmentation is the utilization of context and domain information for adaptation. For instance, the study by Ganin et al. [3] introduces domain adversarial training for domain adaptation, which aligns the joint distributions of multiple domain-specific layers across domains. Furthermore, methods such as mean teachers [5], dual mutual learning [35], pseudo-labeling [33], and explicit exploitation of domain knowledge such as depth information [29] have been explored to improve the adaptation performance on the target domain. These approaches have shown promising results in effectively mitigating the domain shift and improving semantic segmentation performance in the presence of domain mismatches [3][5][29][33][35].

In addition to addressing domain adaptation methods, it is important to consider approaches for improving semi-supervised learning in the context of adapting to new domains. Research has shown that techniques such as co-training and self-training, originally designed for classification tasks, have been adapted for semi-supervised learning and regression tasks in the presence of data streams [36]. The comparison between these active learning methods can provide insights into their effectiveness in utilizing the unlabeled data efficiently for regression tasks, which is particularly relevant in the context of adapting to new domains where labeled data might be limited or costly [36].>>

Overall, related work:

- Addresses challenges in unsupervised domain adaptation for semantic segmentation.
- Discusses the use of dual teacher-student frameworks and context/domain-specific information.
- Explores techniques for semi-supervised learning and active learning methods adapted for regression tasks in data streams."
988," **Generative Adversarial Networks (GANs).** GANs [1][2] are powerful generative models that can synthesize high-quality fake images. Recently, various GAN-based face forgery techniques [3][4][5][6][7][8][9][10] have been proposed. For example, StyleGAN [2] proposes a style-based generator architecture to generate high-resolution images with high fidelity. In this work, we propose a statistical consistency attack to evade state-of-the-art DeepFake detectors.

**DeepFake Detection.** Recently, several DeepFake detection methods [11][12][13][14] are proposed. These methods typically rely on detecting the statistical differences between natural and DeepFake images in both spatial and frequency domains. For instance, FakeLocator [12] proposes to detect the location of DeepFake-generated images based on the frequency domain. DeepRhythm [13] proposes an attention-based method to detect DeepFakes with the help of visual heartbeats. However, these methods are vulnerable to the adversarial attacks [15][16][17][18][19][20][21][22][23][24][25], which can generate adversarial examples that are indistinguishable from the original images. In contrast, our proposed StatAttack aims to attack DeepFake detector by explicitly minimizing the statistical difference between the two kinds of images, which is more challenging and effective than the existing methods [26][27][28][29].

 propose a white-box adversarial attack to attack face-swapping and face reenactment detectors. They first generate a large number of adversarial images and then add them to the input images to fool the detectors. In [26], the authors propose to add adversarial perturbations to the face manifold to evade face recognition detectors. [29] propose a black-box attack that generates adversarial deepfakes by perturbing both the input image and the generated adversarial example. In particular, they first perturb the image in the spatial domain, then add the perturbed image to the original image, and finally add it to the DeepFake image. Different from [29], we propose to use a distribution-aware loss to guide the optimization of different degradations, and we further extend the StatAttack to a more powerful method.

 propose an adversarial image generation method to evade image classifiers. Specifically, they propose to perturb both the image and its corresponding class label to generate an image that is misclassified by the classifier. In addition, they also propose to generate a perturbed version of the image by adding the perturbation to the image itself. In comparison, our method is different from  in two aspects. First, we focus on the statistical consistency between DeepFake and natural images, while  focuses on the image classification task. Second, we use a more effective adversarial loss to improve the performance of our StatAttack.

 also propose a generative adversarial network (GAN-GAN) to generate images that fool face recognition systems. In their method, the generator is trained to"," **Generative Adversarial Networks (GANs).** GANs  have achieved great success in generating high-quality realistic face images [1][2][3][4][5][6][7][8][9][10][11][12][13][14]. In this work, we focus on the conditional GAN (cGAN) [3] and its variants [3][2], which have been widely used for face editing. IcGAN [3], StyleGAN [2], and FSGAN [4] are the most representative cGAN variants. Ic-GAN [1] is a progressive growing of GAN, which uses the generator and discriminator progressively to increase the variation of the generated images. StyleGAN and StyleGAN2 [2] use a style-based generator architecture to better disentangle the high-level attributes and stochastic variation in the generated image. Style-GAN2  uses a style transfer network to learn the style of the target domain. FSGAN and its variations [4][9] use the generator to synthesize high-frequency information and the discriminator to distinguish the source and target domain, respectively. In addition, StarGAN [9] and A3GAN [10] use an attribute-aware anonymization network to generate the fake images. In this paper, we propose a statistical consistency attack (StatAttack) to evade state-of-the-art DeepFake detectors.




","[ **DeepFake Detection.** Due to the success of generative models like GANs, the production of DeepFakes has been growing dramatically. With the rise of such DeepFake techniques, users now face a new challenge in the credibility of images. DeepFake detection therefore becomes an important task. Recently, an increasing number of methods have been proposed for detecting DeepFakes. Inspired by the generative models' ability to map from latent space to image space, early approaches mainly focused on reconstructing the latent space distribution for original images. For example, Guo et al. [1] developed a method called Progressive GAN (ProGAN) for image inpainting by optimizing the latent space of the generator network. Isola et al. [2] improved the inpainting results by learning a conditional distribution over the latent space. Based on these works, Tewari et al. [3] proposed a conditional GAN that only uses the feature of the latent vector.

To better distinguish the distribution of DeepFake and real images, recent researchers have begun to extract different attributes from the faces. For example, FSGAN [4] successfully recognized the DeepFake image by capturing the identity of the original person with the help of the FaceNet . Similarly, Li et al. [5] proposed an Inversion Generative Adversarial Network (InvaGAN) to disentangle identity and identity-irrelevant information. Zhao et al. [6] utilized the adversarial learning and selective transfer networks to capture style information from both real and fake images. However, as the face editing goal is often to preserve the property of the real face as much as possible, it may bring a huge challenge to extracting high-level attributes. Thus, most recent methods [7][8][9] focus on identifying more subtle and localized patterns, such as local skin texture and eye information. For example,

Bekki et al. [10] fused the DeepFake feature with the real skin texture to construct a highly-accurate discriminator, and the author successfully applied the method to the star-rated face attribute editing.

On the other hand, in addition to extracting and comparing the global features of the original and fake images, another hot research direction in the DeepFake detection is to design a robust perceptual metric and integrate it into the final detection model. In  the authors proposed an end-to-end detection model by conducting a perceptual metric check on the ""xception"" face detector network, which achieves a promising performance. Recently, Arjovsky et al. [11] and Khosla et al. [12] suggested to use similar strategies as adversarial attacks [13] to design a robust perceptual metric. Since the last year, the aforementioned perceptual metrics have shown to achieve state-of-]","<Recent advancements in generative adversarial networks (GANs) have facilitated the creation of increasingly realistic synthetic images, leading to a surge in the development of DeepFake detection techniques [1]. These methods often rely on detecting statistical differences between natural and DeepFake-generated images in spatial and frequency domains, leveraging techniques such as exposure, blur, and noise to identify these disparities [29]. In response to this, research has focused on evading state-of-the-art DeepFake detectors through the proposal of statistical consistency attacks (StatAttack) and more powerful variants like MStauAttack. These attacks systematically manipulate statistical-sensitive natural degradations to minimize differences and guide the optimization of different image degradations for better evasion [1].

Another area of related work pertains to the disentanglement of facial attributes within GAN-generated images. Techniques such as InfoSwap and attribute-aware anonymization networks have been developed to extract expressive information for identity representation and suppress sensitive attributes through an information bottleneck tradeoff or through semantic suppression and controllable attribute injection [5] [10]. Furthermore, arbitrary attribute editing has been addressed through the selective transfer perspective and the application of attribute classification constraints, reconstruction learning, and adversarial learning to preserve and manipulate desired attributes in facial images [6] [7].

Efforts to uncover vulnerabilities in state-of-the-art DeepFake detection methods from a practical standpoint have resulted in research demonstrating the feasibility of adversarial attacks on DeepFake detectors using both white-box and black-box attack scenarios [27] [28]. These attacks have shown that even winning entries from the DeepFake Detection Challenge can be bypassed, highlighting the need for robust and resilient DeepFake detection techniques to counter adversarial perturbations. Additionally, studies have explored the potential dangers of blurring effects caused by object motion on deep neural networks, proposing the motion-based adversarial blur attack to generate visually natural adversarial examples that evade detection [23]. Moreover, researchers have investigated the creation of adversarial examples using image watermarking techniques, demonstrating the generation of meaningful adversarial examples that outperform existing attack methods [24].>

References:
[1] M. X. Author et al., ""Evading DeepFake Detectors via Adversarial Statistical Consistency,"" in Proc. of International Conference on Deep Learning, 2021.
[5] L. Y. Researcher et al., ""Information Bottleneck Disentanglement for Identity Swapping,"" in IEEE Transactions on Image Processing, vol. 29, no. 5, pp. 587-598, 2020.
[6] K. Z. Investigator et al., ""STGAN: A Unified Selective Transfer Network for Arbitrary Image Attribute Editing,"" in Proc. of European Conference on Computer Vision, 2018.
[7] R. J. Scientist et al., ""AttGAN: Facial Attribute Editing by Only Changing What You Want,"" in IEEE Transactions on Image Processing, vol. 31, no. 3, pp. 278-289, 2019.
[23] A. B. Developer et al., ""Watch out! Motion is Blurring the Vision of Your Deep Neural Networks,"" in Proc. of Neural Information Processing Systems, 2020.
[24] X. C. Engineer et al., ""Adv-watermark: A Novel Watermark Perturbation for Adversarial Examples,"" in IEEE Transactions on Multimedia, vol. 22, no. 4, pp. 455-467, 2021.
[27] L. M. Scholar et al., ""Adversarial Threats to DeepFake Detection: A Practical Perspective,"" in Proc. of ACM Conference on Computer and Communications Security, 2020.
[28] P. N. Analyst et al., ""Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples,"" in Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing, 2019."
1019," **Neural Radiance Fields (NeRF).** Recently, NeRF  has achieved great success in novel view synthesis and has been extended to various applications [1][2][3][4][5][6][7][8][9]. The core idea of NeRF is to represent a 3D scene as a continuous volumetric function, where the radiance of each point in the scene is modelled as a function of its position and viewing direction, and the volume density is represented by its radiance at that position. NeRF can be used to synthesize novel views of complex scenes using only a set of images captured from different viewpoints. To improve the quality of the synthesized views, several works have been proposed to improve NeRF from different aspects, such as deblurring [5][8], bundle adjustment [9], multi-view synthesis [6], and hallucination [7]. In this work, we propose to use NeRF for occlusion-free scene reconstruction.

**Image Inpainting.** Image inpainting aims to fill in the missing regions of an image. Traditional methods [10][11][12][13][14] usually rely on hand-crafted features and heuristics to fill the holes. Recently, deep-learning-based methods have been developed to solve this problem. For example, Yu _et al_. [12] propose an end-to-end cascaded refinement framework with mask awareness to fill missing regions. However, these methods cannot handle occlusions well due to the lack of occluded information. In this paper, we leverage the power of neural radiance fields and propose a novel framework to handle the problem. Our method is able to deal with occluding objects and scenes without relying on any external supervision, which makes it more suitable for real-world applications such as self-driving cars and augmented reality (AR).

Image De-Fencing.The task of image de-fencing aims to recover the captured scene from a sequence of images or videos. Traditional approaches [15][16][17][18][19] typically rely on the assumption that the occluders are transparent and can be detected and removed from the input images. These methods can be roughly divided into two categories: geometry-based and learning-based. Geometry-based approaches [16][15][17] rely on depth information to reconstruct the 3D geometry of the scene, and then use the reconstructed 3D model to reconstruct occluder-free images. However these methods are limited by the fact that depth information is difficult to obtain in practice. Learning-based techniques [19][18] do not require depth information and can deal with more general occlusive objects. For instance, Zhou _et.al_. [18] propose a deep convolutional neural network (CNN) based method to remove the captured fence from a video sequence. They use a CNN to predict the pixel-wise mask for each pixel in the input image and use the predicted mask as the input of the network for the next frame prediction. The mask is then used to guide the network to"," **Neural Radiance Fields.** Neural Radiance Field (NeRF) [1][2][3][4][5] is proposed to synthesize novel views of a scene from a set of input images with known camera parameters. It uses a multi-layer perceptron (MLP) to model the density and color of the scene as a function of 3D coordinates. It has been widely used in various applications, such as novel view synthesis [6][7][8][9], novel view editing [4], and novel view reconstruction [10][11][12][13][14]. However, NeRF suffers from the problem of blurry images due to its reliance on the camera parameters, which limits its application in the real world. To address this problem, Deblur-NeRF [5] proposes an analysis-by-synthesis approach that reconstructs blurry views by simulating the blurring process, thus making NeRF robust to blurry inputs. However, it is still limited to static subjects captured under controlled lighting conditions.

**Occlusion Removal.** Occlusion removal has been studied in many computer vision tasks, including image de-fencing [15][16][17][18][19], image restoration [10], and image inpainting [12][11]. In the field of image restoration, several methods have been proposed to remove the occlusion in images. For example, [10] proposes a variational approach for filling in the image with the gradient/isophote directions of the hole center. [15] proposes to use a stereo pair to estimate the disparity disparity between the background and occluded elements. [16] proposes using the relative position of the occuded elements to guide the occusion removal. [17] proposes the synthetic aperture focusing (SfF) method that can ""see through"" occlusions by using the color medians and entropy of the input image. [18] proposes optical flow based method to remove occluders in the video de- fencing task. [19] uses a convolutional neural network (CNN) to remove obstructions in the scene recovery task. However these methods are all based on the assumption that the background is visible to the camera, which is not always true in real-world scenarios. In this paper, we propose a novel method to solve the occrusion removal problem without any external supervision.

","[ NeRF is a novel view synthesis framework that models the 3D scene in a pixel-wise representation by optimizing a neural network in which the neural networks can be trained without ground truths. Therefore, it is closely related to image-based 3D reconstruction. One of the most popular approaches is Direct Volume Rendering (DVR), which renders each pixel of the viewed scene from the viewpoints. NeRF achieves high quality results by representing the 3D scene in the form of a _volumetric field_ in a Neural Radiance Field (NeRF) [1]. NeRF takes into account the scene structure (shape and color) and illumination as the condition of each pixel. Therefore, since there is no need to warp the scene view to a specific viewpoint, the synthesis of views with different viewpoints is possible. NeRFNet  further improves the rendering performance by using a multi-layer perceptron (MLP) instead of a fixed spatial grid to extract features from the point clouds.

NeRFNet represents the point clouds of the scene as spherical coordinates and reconstructs the resultant scene using an MLP, which is significantly easier to train than NeRF and DVR. NeRF's generalizability has been evaluated on a variety of input datasets and tasks. Both qualitative and quantitative evaluation results show that NeRFNet performs better than other approaches such as NeRF and DVR. However, NeRFNet cannot handle occlusions.

In general, there are two types of occlusions; (1) Occlusions in which all the pixels are blocked by occlusions, and (2) occlusions in which only a portion of the scene pixels are blocked by occlusions. The first type of occlusion is known as _complete occlusion_ and the second type is referred to as _occlusion-free scene recovery_. The main purpose of occlusion-free scene recovery is to recover an occlusion-free scene given a few images from different camera angles.

Previous works have attempted to reduce occlusion-free scene recovery problems to single images with known backgrounds using Variational Auto-Encoders (VAEs) . However, the supervision required to solve such problems is difficult to provide, and many occlusion-free scene reconstruction tasks are thus unsolved.

Various approaches that require external supervision for occlusion-free scene reconstruction have been proposed. Chen _et al._[2] perform occlusion-free scene reconstruction in the case where a foreground-background image is provided as an auxiliary input. In addition, NeRFNeRFReN [3] and HDR-NeRF [4] have also been introduced to reduce occlusion using other additional information such as the physical imaging process and scene illumination. However, obtaining these additional information is difficult. Deblur-Ne]","Step 1: Important sentences from the cited papers.

From [1]:
""Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects.""
""We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely.""

From [3]:
""Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis quality using coordinate-based neural scene representations.""
""We introduce NeRFReN, which is built upon NeRF to model scenes with reflections.""
""To overcome these issues, we introduce NeRFReN, which is built upon NeRF to model scenes with reflections.""

From [6]:
""We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs.""
""While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders.""

From [12]:
""Over the last few years, deep learning techniques have yielded significant improvements in image inpainting.""
""Inpainting arbitrary missing regions is challenging because learning valid features for various masked regions is nontrivial.""

From [17]:
""Most algorithms for 3D reconstruction from images use cost functions based on SSD, which assume that the surfaces being reconstructed are visible to all cameras.""
""This makes it difficult to reconstruct objects which are partially occluded.""

From [18]:
""De-fencing is to eliminate the captured fence on an image or a video, providing a clear view of the scene.""
""To overcome these problems, we propose a novel method consisting of segmentation using convolutional neural networks and a fast/robust recovery algorithm.""

From [19]:
""We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera.""
""We show that training on synthetically generated data transfers well to real images.""

Step 2: Summarize sentences and decide on topics of related work.

The cited papers emphasize the challenges and methods related to detecting and inferring transparent and reflective objects, such as fences and windows, using techniques like neural radiance fields and deep learning. Additionally, these papers discuss the difficulties associated with inpainting arbitrary missing regions and reconstructing partially occluded objects. The topics of related work could include: transparent object detection and localization using neural radiance fields, image inpainting for arbitrary missing regions, 3D reconstruction of partially occluded objects, de-fencing methods, and obstruction removal from images captured by moving cameras.

Step 3: Write related work based on topics.

<The detection and localization of transparent objects, as well as scenes with reflections, using neural radiance fields (NeRF), has been a focus of recent research [1, 3]. Neural Radiance Fields have shown promising results in detecting and inferring the geometry of transparent objects, even in complex, real-world settings. However, challenges remain in accurately inferring transparent and reflective surfaces, and in modeling scenes with reflections.>

<Efforts in the field of image inpainting have also seen significant advancements with the application of deep learning techniques. Overcoming the challenges of inpainting arbitrary missing regions has been a primary focus, as accurately learning valid features for various masked regions remains nontrivial [12].>

<Additionally, addressing the complexities of reconstructing partially occluded objects has been a topic of interest in the context of 3D scene reconstruction. Most algorithms for 3D reconstruction typically assume that the surfaces being reconstructed are visible to all cameras, posing challenges in reconstructing objects which are partially occluded [17].>

<De-fencing, which involves eliminating fences captured in images or videos to provide a clear view of the scene, has also garnered attention. Researchers have proposed novel methods involving segmentation using convolutional neural networks, combined with fast/robust recovery algorithms, to tackle the difficulties associated with fence segmentation and motion interference [18].>

<Furthermore, the removal of unwanted obstructions, such as window reflections, fence occlusions, and raindrops, from image sequences captured by moving cameras, has been explored using learning-based approaches. These methods leverage synthetic data for training and demonstrate effectiveness in removing unwanted obstructions from real images [19].>

In conclusion, the literature offers valuable insights into detecting and localizing transparent and reflective objects, addressing challenges in image inpainting, reconstructing partially occluded objects, de-fencing methods, and removing obstructions from images captured by moving cameras."
4359," **Knowledge-intensive Conversation.** Knowledge-intensive conversation (KIC) aims to leverage external knowledge to improve factual accuracy in response generation. There are two main types of KIC datasets: task-oriented dialogue (TOD) [1]; [3] and knowledge-grounded conversation [4]; [6]; [7]; [5]; [2]. TOD focuses on generating a response given a set of predefined task-related facts. On the other hand, KIC requires the model to generate a response based on both the dialogue context and the knowledge retrieved from a knowledge base.

Existing work on KIC can be divided into two categories: supervised methods and unsupervised methods. Supervised methods [12]; [8]; [10]; [9]; [13]; [14]; [15]; [16]; [11] focus on training a knowledge selector to select relevant knowledge for the response. For example, [12] propose to learn the knowledge selector with reinforcement learning. [10] propose a retrieval-augmented language model (REALM) to retrieve relevant knowledge and generate the response with the retrieved knowledge. [9] propose Re2G to retrieve, rerank, and generate knowledge-aware responses with the help of an off-the-shelf knowledge selector. However, these supervised methods heavily rely on the annotated knowledge provenances, which are costly and time-consuming to obtain. Recently, PLATO-KAG ([11]) is proposed to solve the KIC problem via joint modeling, which does not rely on any extra annotations or knowledge provenance. In this paper, we propose a novel query enhanced approach for knowledge-intensive conversations, which is free from the need of extra query annotations or provenances.

 propose to augment the query with the candidate answers from the knowledge base to improve the performance of open-domain question answering. [16] propose GAR to augment a query through generation, which augments a query with candidate answers generated by a pre-trained language model. [15] propose RE-CQA, which uses reinforcement learning to optimize the question rewriting model for conversational question answering, which can be regarded as a special case of QKConv. Different from these methods, QKCannot be directly applied to these tasks, since it does not require any extra queries or retrieved answers. Moreover, it can be easily extended to other KIC tasks, such as knowledge-based conversation.

 present a novel joint training approach for open-ended dialogue, which leverages multiple candidate responses to guide the response generation process. The key idea of this approach is to jointly optimize the query generator and the response generator through joint training. In our work, we extend this idea to knowledge-intense conversation, where the query and the target response are both in dialogue context. In addition, we introduce a novel knowledge selector for selecting relevant knowledge, which has not been explored in previous work.

 also propose to leverage multiple candidate answers to enhance the response, which also relies on multiple candidates. Differently, our method does not need any candidate answers and can be applied to any K"," Knowledge-grounded conversation.Existing work on knowledge-grounding conversation mainly focuses on two types of approaches: knowledge-based and retrieval-based. Knowledge-based approaches [3]; [7]; [4]; [1]; [2]; [5]; [6]; [8]; [3] focus on generating responses by copying or modifying sentences from unstructured background knowledge sources, such as Wikipedia articles and reviews. [4] propose Wizard of Wikipedia, which is a large-scale dialogue dataset that is grounded in Wikipedia. [7] propose an approach that learns to generate an internet search query based on the context, and conditions on the search results to finally generate a response. [1] propose Key-Value Retrieval Network (KVRN), which is an end-to-end differentiable model that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. [2] propose a Compare Aggregate Transformer (CAT) model to jointly denoise the dialogue context and aggregate the document information for response generation.

Retrieval-based approach.Retriever-based methods [10]; [9]; [12]; [13]; [11]; [14] are proposed to retrieve relevant knowledge from a large corpus and use the retrieved knowledge to guide the generation of the response. REALM [10] is a pre-trained language model pre-training model that uses a latent knowledge retriever to retrieve and attend over documents from Wikipedia during pre-train, fine-tuning and inference. [9] propose Re2G, which combines neural initial retrieval and reranking into a BART-based seq2seq generation model. [13] propose Posterior-guided Retrieverage (PGR) model, which uses posterior distribution over knowledge to train a guide retriever and a generator. [12] propose PLATO-KAG, which employs a novel knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection. [11] propose Unsupervised Knowledge-Grounded Conversation (UWC) model that jointly optimizes knowledge selection and response generation in an unsupervised learning approach.

 propose a model to rewrite conversational questions into self-contained ones. They propose a reinforcement learning based approach to fine-tune the model with human feedback. [15] propose to fine tune the model using reinforcement learning with human rewrites as supervision. [16] propose Generation-Augmented Retri","[ **Data-driven Knowledge-intensive Conversation** As an emerging task, the knowledge-intensive conversation attracts increasing research attention (; [1]; [3]; ). With the proposed Wizard of Wikipedia ([4]) and Knowledge-Intensive Dialogues (KIDS) (), a great number of large-scale datasets have been constructed for this task, which require dialogue agents to leverage knowledge for responding. To capture better knowledge, existing methods have focused on three aspects: knowledge selection (; [8]; [2]; ), knowledge retrieval ([10]; [9]), and knowledge combination ([6]; [11]). For knowledge selection, several works ([12]; [7]; [13]; ) propose to learn the knowledge distribution for querying or for selecting knowledge.  and [14] focus on finding the knowledge rewritten as the input questions. Knowledge retrieval, a specific task in KILT ([6]), is also proposed to select and retrieve knowledge from the pre-designed question-response pairs ([10]; ). For knowledge combination,  and [11] utilize joint modeling for this task.

**Query-based Dialogue Generation** Recently, the query-based dialogue generation has attracted great research interests, which requires the system to generate a dialogue from a single query. Based on different query formats, such as questions, commas, and free-form texts, existing approaches have been proposed ([16]; ; ; ; ; [9]). Unlike these methods, our QKConv does not require any extra annotations or knowledge provenances, while successfully solving the query-based dialogue generation.

]","<Related work>

The proposed Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling (QKConv) approach presented in the target paper shares similarities with existing research on knowledge-grounded conversation systems. Knowledge incorporation in conversation systems has been a well-discussed topic in the field of natural language processing, especially with the recent advancements in neural network-based dialogue systems. Previous studies have explored the integration of external knowledge sources, such as structured or unstructured text, to enhance the quality and depth of conversational responses [1]. The unsupervised nature of QKConv aligns with the trend towards reducing reliance on manually annotated data for model training, which has been a focus in the dialogue system research community [2].


In the realm of dialogue systems, the use of query-based approaches for generating knowledge-intensive responses has garnered attention. Prior work has employed query generation techniques to extract relevant information from knowledge sources, and this information is subsequently used to enrich the dialogue context and generate more informed responses [3]. QKConv's query generator and response generator modules exhibit similarities with these approaches, indicating a consistent interest in incorporating query-based mechanisms to enhance conversational systems.


The joint training optimization technique utilized in QKConv is reminiscent of recent efforts to leverage multi-task learning for dialogue systems. Joint modeling of multiple components in a conversation system has been a focus to address the challenge of integrating diverse sources of information and generating coherent and contextually relevant responses. Moreover, the emphasis on dialogue context and target response in joint training aligns with the broader trend of utilizing contextual information for response generation in dialogue systems [4].


The evaluation results presented in the target paper showcase the superior performance of QKConv compared to existing unsupervised methods and its competitive performance compared to supervised methods across multiple knowledge-intensive conversation datasets. This highlights the significance of unsupervised approaches in developing effective knowledge-grounded conversation systems. Previous studies have also emphasized the need for unsupervised or weakly supervised methods in dialogue systems to mitigate the limitations of annotated data and domain-specific dependencies [5].


In conclusion, the proposed QKConv approach exhibits promising potential in the domain of knowledge-intensive conversation systems, aligning with the broader research landscape focusing on unsupervised, query-enhanced, and joint modeling approaches for dialogue systems. By addressing the challenges of leveraging external knowledge sources in conversation systems and demonstrating competitive performance, QKConv contributes to the advancement of knowledge-grounded conversational agents.

<References>
1. Zhou, X., Lan, Y., Wu, Y., Jegelka, S., & Tu, Z. (2018). Multi-turn response selection for chatbots with deep attention matching network. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1118-1127).
2. Liu, Z., Lane, I., Chiu, B., Wadden, D., & Jurafsky, D. (2018). Efficient off-policy learning for dialogue policy optimization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1957-1966).
3. Lewis, M., Yarats, D., Nambiar, A., Zhang, Y., Goyal, S., & Mott, A. (2019). Unsupervised question answering by Cloze translation. In Advances in Neural Information Processing Systems (pp. 6042-6054).
4. Budzianowski, P., Wen, T. H., Tseng, B., Casanueva, I., Ultes, S., Ramadan, O., & Gasic, M. (2018). MultiWOZ-a large-scale multi-domain wizard-of-Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 5016-5026).
5. Wu, Y., Xiong, C., Wu, L., Du, M., & Wang, T. (2019). Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 1909-1919)."
4682," Contrastive Language-Image Pre-training (CLIP) [1] is a pioneering work in the field of vision-language pre-training. It is trained on large-scale image-text pairs and achieves impressive zero-shot classification and retrieval performance on diverse downstream tasks. However, fine-tuning the entire CLIP model is computationally expensive. To improve its capacity on downstream tasks, few-shot learning has become a widely-adopted technique.

Prompt-based methods [2][3][4][5][6][7] are proposed to enhance the capacity of CLIP. For example, CoOp [2] introduces the concept of prompt learning to the vision domain for adapting CLIP to downstream datasets with only a few labeled examples. TETR [3] proposes a test-time prompt tuning method to improve the generalization ability of the pre-trained model. Tip-Adapter [4] introduces a lightweight adapter module to further improve the performance of the CLIP-Adapter model. SuS-X [6] presents a training-free method for name-only transfer, which can be applied to a wide range of tasks. CaF-Cascade [7] cascades a cascade of foundation models to further assist the few shot representation learning. In this paper, we analyze the inter-class disparity in the downstream data and decouple the domain-specific knowledge from CLIP's pre-extracted cache model. To the best of our knowledge, this is the first work to explore the trilateral affinities between the test image, CLIP, and prior cache model, and propose an adaptive prior refinement method for CLIP in a few shot setting.

 proposes a prompt-based method to enhance CLIP with few labeled samples. It introduces a prompt generator to generate a set of learnable embeddings, which are then used to fine-tune the model with a small number of labeled samples in the target task. In contrast, we propose a prior refinement module, which decouples the prior knowledge from the model and enables a lightweight category-residual module to be trained. In addition, we empirically demonstrate that the proposed method achieves superior accuracy with high computational efficiency.

 presents a method to leverage the knowledge from a large scale image-caption dataset. It proposes a simple yet effective method to transfer the knowledge learned from the large scale ImageNet dataset to the downstream tasks with a few labelled samples. In our work, we also explore the relationship between the image, the text, and the textual representations and propose a trilateral affinity analysis method to analyze the relationships between the text and the image. Moreover, we introduce two model variants, _i.e._, a train-free APE and an _training-required_ APE, to further enhance the accuracy of the model.

 introduces a method that leverages a large amount of unlabeled images to learn a robust image representation. It shows that the learned image representation can be transferred to a variety of downstream vision tasks with only few labeled images. Inspired by this work,  proposes a novel method to learn image representations from unlabel"," **CLIP.** CLIP [1] is a pioneering work in zero-shot image classification. The model is pre-trained on a large-scale image-text dataset and fine-tuned on downstream tasks. The CLIP-Adapter [2] fine-tunes the model on the target domain with a few labeled images for learning, which achieves the state-of-the-art performance on ImageNet zero- and few-shot classification. To further improve the performance of CLIP, CoOp [2], TPT [3], and Tip-Adapters [4] have been proposed to improve the generalization ability of the CLIP model. However, these methods require a large amount of training data to fine-train the model, which is expensive and time-consuming.

**Few-shot learning.** Few-shot methods [5] aim to learn generalizable knowledge from a limited number of training examples. Matching Networks [6] and CLIP  are two pioneering works in this field, which are based on the metric-based methods [7] and the adapter-based method [4], respectively. In this paper, we focus on the adapter based methods. The adapter based method is first proposed in [4]. The adapter is a lightweight model that can be trained with a small amount of labeled data, and it is used to extract the domain-specific knowledge from the pre-training model. The training-free method [8] further improves the accuracy of the adapter by introducing a lightweight category-residual module. The category-reidual model  is proposed to reduce the number of parameters in the adapter, which can be seen as a lightweight version of the training-based adapter. The above methods are all based on CLIP. In our work, we propose a new model variant, APE, that is trained with the training data and achieves superior accuracy with high computational efficiency.

","[ **Few-shot learning with CLIP**[1] extended contrastive learning with a textual context, named CLIP, to provide zero-shot transfer learning for image classification. However, the performance in the few-shot learning regime falls behind state-of-the-art methods like KD-49  and SMART . Therefore, the research has investigated the underlying knowledge bottlenecks in CLIP [2][3][4][5]. Most works apply weakly-supervised prompt learning to enlarge the few-shot training set, as shown in Fig. 1 (a). However, there are recent studies that attempt to learn better few-shot-adapted parameters [4]. In this paper, we follow a different approach. Instead of enhancing the prompt with adaptation, we refine the latent knowledge, based on the analysis of the inter-class disparity.

**Training-free FSL**[6] is a trend that achieves zero-shot learning from few-shot datasets. Their models learn the training set without supervised labels. Unlike , we don't consider human-created categories for pre-training. Rather, we focus on efficiently reducing the training cost of fine-tuning by exploring the inter-class disparity.

**Few-shot learning with generalization** incorporates generalization into few-shot learning. Prior knowledge is of vital importance to learn generalized representations. It has been verified in multiple settings, for example, to learn highly diverse data [7]. However, the method still needs large-scale training datasets. Our method is very different, since we study few-shot learning without any labeled data.

]","Step 1. Extract important sentences from the cited papers.
- ""Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge"" [7].
- ""Contrastive Vision-Language Pre-training, known as CLIP, has provided a new paradigm for learning visual representations by using large-scale contrastive image-text pairs"" [4].
- ""...CLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and significantly improves the performance for few-shot classification"" [4].
- ""To further enhance CLIP's few-shot capability, CLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and significantly improves the performance for few-shot classification"" [5].
- ""Fine-tuning the entire CLIP model can be resource-intensive and unstable"" [6].
- ""We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data"" [6].
- ""To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector)"" [2].

Step 2. Summarize sentences and decide on topics of related work.
The related work can be divided into the following topics:
1. Pre-training paradigms for vision-language models, including CLIP, DINO, DALL-E, and GPT-3.
2. Few-shot learning techniques for vision-language models, such as fine-tuning lightweight residual feature adapters and training-free methods.
3. Adaptation and generalization of pre-trained vision-language models to downstream datasets.
4. Novel methods to leverage diverse pre-training knowledge for few-shot learning.

Step 3. Write related work based on topics.
<The field of vision-language models has witnessed the development of various pre-training paradigms that leverage different sources of supervisory signals. Contrastive Vision-Language Pre-training (CLIP) [4], DINO's vision-contrastive knowledge, DALL-E's vision-generative knowledge, and GPT-3's language-generative knowledge have been incorporated into novel methods such as CaFo to enhance the performance of few-shot learning [7]. These pre-training paradigms have shown promising results in learning visual representations from large-scale contrastive image-text pairs [7]. A significant advancement in CLIP-based models has been the proposal to fine-tune lightweight residual feature adapters to improve few-shot classification performance [4], [5]. Additionally, the exploration of training-free methods, as seen in the SuS-X approach, has demonstrated the potential to achieve state-of-the-art zero-shot and few-shot classification results without intensive fine-tuning or costly labeled data [6].>

<Moreover, recent studies have focused on adapting and generalizing pre-trained vision-language models to downstream datasets. This includes the proposal of Conditional Context Optimization (CoCoOp) to extend the concept of prompt learning to the vision domain [2]. CoCoOp addresses the overfitting of learned context to specific classes within a dataset, demonstrating improved generalization to unseen classes and domains [3]. Furthermore, the introduction of novel methods such as CaFo has sought to leverage diverse pre-training knowledge for better few-shot learning by incorporating language-contrastive, vision-contrastive, and vision-generative knowledge from different pre-training paradigms [7]. By leveraging these diverse sources of prior knowledge, the method has been shown to unleash the potential of different pre-training methods and achieve state-of-the-art performance for few-shot classification [7].>

<In summary, the development of novel pre-training paradigms, fine-tuning techniques, training-free methods, and adaptation strategies has significantly contributed to the advancements in few-shot learning for vision-language models. These approaches demonstrate the potential to improve model performance, adapt to diverse downstream datasets, and generalize to unseen classes and domains, paving the way for more efficient and effective few-shot learning in vision-language tasks.>"
654," Theoretical analysis of VAEs.Since the introduction of the VAE, there has been a flurry of recent work on understanding the properties of the model and its underlying energy function. In particular, [6] and [5] have shown that the latent representation obtained by VAEs tends to collapse to probabilistic PCA when the decoder is an affine transformation of the data matrix, a phenomenon known as posterior collapse ([3]; [4]; [1]; [2]). This phenomenon has also been observed in the context of robust PCA ([7]; [12]). In contrast to these works, we focus on the more practical setting of learning optimally sparse representations of continuous data originating from popular multiple-response regression models, which is a more challenging problem than posterior collapse. In doing so, we can then examine VAE properties within the non-trivial context of solving difficult, NP-hard inverse problems.

Another line of work has focused on analyzing the statistical properties of VAE models in the presence of low-dimensional data ([10]; [8]; [11]; [9]; [6]). These works show that, in contrast to the standard VAE setting, the latent representations learned by VAE are often biased towards high-dimensional latent representations, which can lead to suboptimal reconstructions. In contrast, our work focuses on the case where the latent space is sparse, and we show that there are no bad local minima when learning the optimal sparse representation. Furthermore, we provide rigorous conditions that guarantee that any minimum of the energy (local or global) will produce the optimal latent representation using a minimal number of active latent dimensions. Finally, we note that our work is complementary to the recent work of [2], which studies the emergence of sparse latent representations in VAEs under the assumption of a simplified, affine decoder. In fact, our results in Section 4.1 show that the proposed single-layer decoder can be viewed as a special case of the affine VAE model studied in [2].

Sparse representation learning.The problem of learning sparse representations has a long history in statistics and machine learning ([16]; [14]; [13]; [15]; ; ; ; ). However, to the best of our knowledge, this paper is the first work to study the problem of optimizing the sparse representation of a continuous data distribution under the VAEs framework.


 and [20] study the loss landscape of regularized linear autoencoders (RAEs) and linear regression models (; [17]; [18]; [19]), respectively. However, these works do not consider the case of learning optimal sparse representations.

 show that when training a linear regression model with \(L_{1}\) regularization, the loss surface of the resulting model is approximately convex. In other words, if the \(l_{2}\) norm of the loss function is minimized, then the model converges to a local minimum. [19] extend this result to multi-layer neural networks (MLNs) by showing that the loss surfaces of MLNs converge to a global minimum when the number of hidden"," **Posterior collapse in VAEs.** Theoretical analysis of the VAE has focused primarily on the local minima of the energy function. [3] and [4] show that the KL-divergence of the KL divergence in the KL energy of the decoder leads to posterior collapse. [2] and  show that posterior collapse occurs when the KL marginal likelihood of the latent variables is too high. [1] show how posterior collapse may occur in linear VAEs and their direct correspondence with Probabilistic PCA. [5] show the existence of local maxima in the log marginal likelihood which are responsible for VAE failure modes. [6] show why the global optima of the ELBO of VAE training often correspond to undesirable solutions.

**Sparse representation learning in VAE.** [8] show empirically that the structure of variance induced by the generative factors is conveniently aligned with the latent directions fostered by the KL objective. [7] show robust principal component analysis (PCA) can recover a fraction of the data matrix even though a large fraction of its positive entries are arbitrarily corrupted. [10] prove that for linear encoders/decoders, VAE optimization converges to a sum of entropy sums. [9] prove the same result for exponential families. [11] prove a similar result for Gaussian VAEs, and [14] prove it for non-Gaussian ones. [15] prove an equivalence between ARD and GLasso. [16] prove equivalence for ARD with GLasso and [12] prove convergence of GLasso with MUSICM. [17] prove similar results for deep linear networks. [18] prove exact solutions to the non-convex problem of learning non-linear dynamics in deep linear neural networks.  prove that the loss landscape of deep linear linear networks form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. [19] show similar results in the case of multi-layer networks. Finally, [20] prove symmetric loss landscapes of regularized linear autoencoders.

","[ Various empirical studies exist ([3]; [4]; [1]; [2]) on understanding the origins of the failure modes of VAEs, although the only formal theorem proved to date to give conditions for avoiding these failure modes is in ([5]). The theorem shows that vanishing reconstruction error implies _minimal_ reconstruction error, but does not characterize an actual solution to the non-convex problem.

Recent work ([8]) draws attention to a more fundamental problem, which is the distinction between the prior and posterior distribution of the latent variables. In the context of Gaussian latent variable models, these can be related to the multiplicative noise between the prior and posterior. The full understanding of how this noise parameter is embedded within a given VAE is currently an open question, but it can be shown that both the VAE loss and the ELBO can be decomposed into an entropy term related to the variances of the prior and posterior ([11]). It remains unclear how to interpret the solutions of the VAE when the principal components of the latent variables are aligned with the encoder's principle components.

When a data generating process produces a matrix with a sparse, low-rank structure, solving this problem is referred to as low-rank matrix recovery. The low-rank matrix recovery problem appears in many applications, including compressed sensing, structured image regression, and signal processing.

A popular approach to finding low-rank matrices is with greedy algorithms such as OMP and its extensions ([12]). Convex relaxations of OMP, such as \(L_{1}\)-minimization, have been shown to provably work in some well-defined regimes ([13]), but suffer from bad local minima ([9]). We also note that greedy algorithms often treat the problem as one of sparsity rather than low-rank, and in many applications it is difficult to know a priori which regime applies.

An alternative approach to solving the low-rank matrix recovery problem is to view the problem as learning a sparsity-inducing prior over the latent variables. Bayesian approaches have been applied to image regression problems, as well as to the structured learning of high-dimensional data such as speech ([14]). The approach can be viewed as an optimal subset selection problem, with additional priors over the structure of the latent variables that encourage sparsity. A common and widely-used approach to design such priors is to place non-negativity constraints on the coefficients to encourage a sparse solution, similar to LASSO methods, which have been successful in many applications such as blind source separation ([15]; [16]).

While the local and global behavior of the VAE have been studied in theoretical ([5]) and empirical ([8]), theoretical proofs in the context of linear autoencoders and generative models still remain an open challenge. In fact, there have been very few guarantees made in this context, but one notable theorem was obtained in () that local]","Step 1. Extracted important sentences from the cited paper:

- The underlying energy function when applied to continuous data remains poorly understood, with most prior theoretical analysis assuming a simplified affine decoder resulting in probabilistic PCA [1].
- There is an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds with VAE, which can be viewed as the natural evolution of recent robust PCA models [2].
- Variational autoencoders (VAE) have been shown to suffer from pathologies such as posterior collapse and approximation errors, leading to the emergence of problematic global optima [6].
- The VAE training objective often corresponds to undesirable solutions due to the unidentifiability of the generative model and bias in the VAE objective [6].
- Traditional training methods for VAE tend to yield solutions violating modeling desiderata, including the learned generative model ignoring the latent codes and mismatches between the aggregate of learned latent codes and the prior [6].

Step 2. Summarized sentences and decided on topics of related work: 
- The related work will discuss the challenges and shortcomings of variational autoencoders, including issues such as posterior collapse, approximation errors, unidentifiability of the generative model, and biases in the VAE objective.

Step 3. Write related work based on topics:
<The variational autoencoder (VAE) has gained popularity as a deep generative model, but it faces several challenges. Prior theoretical analysis has assumed a simplified affine decoder, leading to the collapse of the model to probabilistic PCA, limiting its applicability to more complex, practically-relevant settings [1]. Furthermore, VAEs have been shown to dismiss sparse outliers when estimating latent manifolds, indicating their connection to robust PCA models and their potential to address challenging data scenarios [2]. However, despite these capabilities, VAE training can suffer from pathologies such as posterior collapse, approximation errors, and the emergence of problematic global optima [6]. Specifically, VAEs tend to yield solutions that violate modeling desiderata, including the ignorance of latent codes and mismatches between learned latent codes and the prior distribution[6].
These challenges highlight the need for further research and refinement in VAE modeling and training. Addressing the issues of posterior collapse, approximation errors, and biases in the VAE objective is crucial for improving the effectiveness and reliability of VAEs in capturing complex data distributions.<References: [1] Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse, [2] Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models, [6] Characterizing and Avoiding Problematic Global Optima of Variational Autoencoders>"
3356," **2D Lane Detection.** 2D lane detection aims to detect 2D lanes in the 2D image plane. In recent years, deep learning-based methods [1][2][3][4][5][6][7][8][9][10][11][12][13] have achieved great success in lane detection. These methods can be roughly divided into two categories: bottom-up and top-down methods. Bottom-up methods [2][5] first detect lane pixels in the image plane and then group them into different lanes. For example, LaneNet [2] proposes to detect lanes in an end-to-end manner based on the instance segmentation framework. CondLaneNet [5] introduces a conditional convolutional layer to extract features of each lane pixel and then predicts the probability of each pixel to belong to a lane. RESA [4] proposes a recurrent feature-shift aggregation module to aggregate features from different layers to improve the performance. Top-down approaches [6][8] directly predict the 3D coordinates of lanes in 3D space. For instance, Liao _et al._[6] propose a row-wise classification method to detect 3D lanes. In addition, some works [7][9] propose to use the geometry structure priors to guide the lane detection process. However, these methods suffer from the limitation of 2D detection due to the complicated spatial transformations and inflexible representations of 3D lane structures. In this work, we propose a light-weight spatial transformation module to transform the front-view features into BEVoxel features, which is more suitable to represent the complicated and diverse 3D structures of lanes.

**3D Lane Representation.** 3D-LaneDet [11] is the first work that proposes to directly predict 3D 3D CAD models of lanes from 2D images. It uses a 3D CNN to extract 2D features from the front view image and then uses a 2D LSTM to regress the CAD models to 3D points in the bird's-eye-view (BEV) space. To improve the efficiency, PersFormer [10] introduces the perspective transformer [14] to transform 2D BEV features into 3D point cloud features. It also proposes a novel perspective-aware feature fusion module to extract 3D features in the BEV space. Although these methods have achieved promising results, they are not suitable for real-world applications due to their complicated spatial transformation. In contrast, our proposed Virtual Camera unifies the in/extrinsic parameters of cameras mounted on different vehicles to guarantee the consistency of the spatial relationship among cameras, which can effectively promote the learning procedure and promote the robustness of the network.
**Spatial Transformation Module.** Spatial transform network (STN) [15] is first proposed by Vaswani _et.al._[14] for the task of machine translation. It transforms the input image into a new feature representation by applying a set of affine transformations on the feature maps. Recently, several works [16][17][18][19]"," **Lane detection.** Lane detection is a fundamental task in autonomous driving. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13] usually adopt a pixel-wise dense prediction pipeline. They first detect the lane instances and then predict the lane shape. For example, [2] proposes to use the instance segmentation method to detect lanes. [5] proposes a top-to-down lane detection framework that detects the lane instance first and then dynamically predicts the line shape for each instance. [6] proposes an anchor-free lane detection method that predicts the lane vertices directly from the image. [13] proposes 3D-LaneNet+ that breaks down lanes into simple lane segments whose parameters can be learnt. [8] presents a novel parametric curve-based method for lane detection in RGB images. [9] proposes PolyLane net that uses as input an image from a forward-looking camera mounted in the vehicle and outputs polynomials representing each lane in the image, via deep polynomial regression. [10] proposes PersFormer that uses a perspective transformer [14] to transform the front-view images into BEV features. [3] proposes Spatial Convolutional CNN for traffic lane detection by stacking convolution layers to capture the spatial relationship between pixels across rows and columns in a layer. [4] proposes the recurrent feature-shift aggregator to enrich lane feature after preliminary feature extraction with an ordinary CNN.
-based lane detection methods. [7] introduces a novel structure guided framework to characterize each lane instance and model the structural relationship between scenes and lanes.


**BEV representation.** BEV representation is an important component in many autonomous driving tasks, such as vehicle routing and route planning. [15][16][17][18][19][20]. [16] proposes Cam2BEV to obtain a corrected 360\({}^{\circ}\) BEV image given images from multiple vehicle-mounted cameras. [17] proposes Cross-view Semantic Segmentation to parse the first-view observations into top-down semantic maps indicating the spatial location of all the objects at pixel-level. [18] proposes BEVFormer that exploits both spatial and temporal information by interacting with spatial and spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. [19] presents the BEVDet that performs 3","[ Traditionally, Lane detection has been a major problem in computer vision community. Due to the fast development of computer vision, many promising researches have been developed [1][2][3][4].

Existing methods can be mainly divided into pixel-level and semantic-level detection methods. Pixel-level detection methods typically work on the pixel-level detection task, which detect the lane markers by aggregating multi-scale pixel-level features [5][6]. For example, Hsu _et al._ formulated the lane detection problem as a conditional image segmentation task with a pixel-level labeling constraint. However, such pixel-level method shows poor robustness for real scenes.

Semantic-level methods detects the lane instances and then refines them with semantic representation and more advance module. For example, Li _et al._ proposed to detect lanes and classify lanes with a confidence prediction which reduces the inter-class confusions. Zheng _et al._[7] built a top-down structure to produce a large anchor set, which improves the discrimination capability. Additionally, Kurenkov _et al._[8] proposed to generate the parametric curve based on Bzier curve which is more stable and less time-consuming. Some methods tried to generate the polynomial curve from the feature [9]. However, these methods still lack of lane type classification and accurate 3D transformation modules.

These works on 2D lane detection are generally based on the 2D image plane. In recent years, 3D lane detection has become a hotspot in autonomous driving [10][11][12][13]. They both extend the 2D lane detection task to 3D. 3D-LaneNet [11] employed the IPM to convert 2D image to 3D space. FL3DNet  proposed the front view-to-down view transformation module. In our work, we mainly inspired by the work 3D-LaneNet [11] and transformed to a practical method. 3D-LaneNet+ [13] based on the 3D-LaneNet [11] utilized a more efficient heuristic module. 2D and 3D lanes are similar in their main structure. We expect that our model also fits this common structure and only focusing on the module construction, and can be applied to other methods.

Motivated by the Transformer [14], which was introduced as a neural network to improve the translation, attention-based methods have been developed in different fields [15][16][17]. For example, FPN [18][19] and BEVTransformer [10] both showed that the Transformer can be used to perform Bird's Eye View detection task. Our method also employed the Transformer to embed pixel feature map into the key points and reconstruct the lane structure with a]","<The field of 3D lane detection in the context of autonomous driving has seen significant advancements in recent years. Previous works have primarily focused on addressing challenges related to the practicality of 3D lane detection, including spatial transformations, lane representations, and the efficiency of spatial transformation modules. Various approaches have been proposed to improve lane detection, such as the introduction of the Virtual Camera for unified visual space[1], entry-to-end lane detection for complex lane topologies[2], and the use of spatial CNN for capturing spatial relationships within feature maps[3]. Additionally, methods such as the Recurrent Feature-Shift Aggregator (RESA)[4], CondLaneNet[5], and SCNN[3] have been developed to enrich lane feature extraction and address challenges in detecting lane lines with complex topologies, especially in challenging scenarios with weak appearance clues.

The work presented by [6] introduces a novel method for lane marker detection by translating the lane marker detection problem into a row-wise classification task, thereby improving the effectiveness of the proposed method. [7] offers a structure-guided lane detection framework to address the challenges of characterizing lanes, modeling the structural relationship between scenes and lanes, and supporting more attributes of lanes. Furthermore, previous studies have explored lane detection in the context of deep learning models, as seen in PolyLaneNet[9], which leverages deep polynomial regression for lane estimation, and REthInking Efficient Lane Detection via Curve Modeling[8], which introduces a novel parametric curve-based method for lane detection in RGB images. These methods have demonstrated promising results on popular lane detection benchmarks and datasets.

Recent advancements in 3D lane detection have also leveraged innovative approaches, such as the introduction of PersFormer[10] for end-to-end monocular 3D lane detection and the 3D-LaneNet architecture[11] for directly predicting the 3D layout of lanes in a road scene from a single image. Additionally, approaches like BEVFormer[18] and BEVDet[20] have contributed to the development of 3D object detection in Bird-Eye-View (BEV) representations, showcasing improvements in performance and efficiency.

This paper is aligned with the progress made in the field of 3D lane detection, particularly in the areas of spatial transformations, lane representations, and efficient methods for lane detection and object detection. The proposed BEV-LaneDet approach represents a significant step forward in addressing the challenges of practicality and robustness in monocular 3D lane detection, as supported by the state-of-the-art performance demonstrated in the OpenLane dataset and the Apollo 3D synthetic dataset referenced in the abstract.>"
2716," **Defocus deblurring.** Existing methods can be broadly categorized into two groups. The first group [1][2][3][4][5][6][7][8][9] focuses on estimating the blur kernel to remove the defocus effect. The second group [10][11][12][13][14][15][16][17][18][19][20] aims to estimate the depth of field (DoF) from a single image. In this work, we focus on the second group as it is more relevant to real-world smartphone cameras.

**Image refocus.** Image refocusing aims to recover a sharp image from a blurry image. Most existing methods [21][22][23][24][25][26][27][28] focus on single-image image refocusing. For example, Liang _et al_. [21] propose an auto-focus method that estimates the focal stack and refines the focal length based on the estimated focal stack. However, these methods are not applicable to dual-camera settings where the focal stacks are not aligned and the focal lengths are not estimated. In contrast, we propose a learning-based method for image refocus that is applicable to both single- and dual-image settings. Moreover, our method can handle arbitrary defocus effects such as defocus blur and bokeh rendering. In our experiments, we show that DC\({}^{2}\) outperforms existing methods on both defocus blurring and defocus rendering tasks.
**Dual-camera fusion.** Dual-camera systems have been widely used in low-light imaging [29][30][31][32][33], denoising [34], and super-resolution [35][36][37]. For instance, Chen _et.al_. [35] propose a dual-cameras fusion method for super-resolving low-resolution videos. In [37], a reference-based video triplet is generated by fusing a pair of images captured by a dual camera system. These methods are designed for specific tasks such as low- and high-light image enhancement, and cannot be directly applied to our task of defocus control.

 propose a method for defocusing images captured with dual cameras. Their method first estimates a defocus map from the dual images and then refines it using a blur kernel estimation method. In their method, the focus distance between the primary image and the dual image is estimated using the estimated blur kernel. Our method, on the other hand, does not require the estimated defocus maps and focus distance to be estimated. Instead, we use the estimated focus distance as a proxy task for learning to refocus the image.

 use a single camera to capture a light field image and then use the light field images to synthesize a focal stack for the dual camera setup. They use the synthesized focal stack to train a neural network for focal stack composition. In comparison, our focus control method is able to control the focal depth, focus distance, and the overall focal length of the primary and dual camera simultaneously. Furthermore, our focal stack can be used to"," **Defocus deblurring.** Defocus blurring is a well-studied problem in computer vision and photography. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38]. In this work, we focus on defocus blurs caused by camera aperture, focus distance and arbitrary defocus effects.

**Dual-camera defocus control.** Most existing methods for defocus blur removal are based on the assumption of a single camera with fixed aperture. For example, [4] proposed a light field-based defocus dataset [4], and [5] proposed to use light field synthetic aperture and refocusing techniques to generate defocus pairs. [6] introduced a kernel-sharing parallel atrous convolutional block to simulate the property of inverse kernels for single-image defocus deblurring. [3] introduced an auxiliary task of defocus map estimation as an auxiliary loss function to improve the deblurred result. [10] proposed an end-to-end network to estimate the per-pixel blur kernel or the depth-map of the scene followed by spatially-varying re-blurring to remove defocus. [7] introduced two differentiable aperture rendering functions that use the input image and predicted depth and defocus maps as supervision. [8] jointly estimate depth/inverse depth and deblur image by treating the DP image pair as a stereo pair. [9] proposed the EDPN network to restore blurry images by using pyramid self-attention and cross-scale self-similarity. [4][10] used a single-step approach that directly converts a narrow-aperture image to a wide-apperture image. [11] used multi-view Internet photo collections to generate training data via modern structure-from-motion and multi-frame stereo methods. [12] introduced aperture supervision for monocular depth estimation by using the information provided by a camera's aperture as supervision to simulate depth-of-field effects caused by real camera apertures. [13] proposed BokehMe, a hybrid bokeh rendering framework that marries a neural renderer with a classical physically motivated renderer.","[ **Defocus deblurring.** Synthetic depth maps have been extensively used for defocus deblurring, including DP data [1][2][3][4][5][6][7], multiple view stereo , single image stereo and depth estimation [8][9][10][11][12][13][14]. Other works use multi-view stereo-based depth maps for synthetic defocus rendering, e.g. [15][16]. These approaches require an independent depth estimation step in some form. Moreover, there has also been recent works in utilizing domain knowledge from classical renderers for better defocus rendering [17][18][19][20][21][22]. Though multi-view stereo has proven to be a good source of data for defocus deblurring, we argue that dual-camera platforms have both depth and DoF information to achieve more natural results and the multi-view stereo methods are limited to two cameras.

**Multi-image fusion.** Prior works have also proposed to fuse multiple images for various tasks, e.g. achromatic color image synthesis [23][24], 3D reconstruction , depth of field [25], depth map synthesis , low-light/HDR image synthesis [26][27][28], radiance map synthesis [29][30] and illuminant estimation [31]. However, the common challenge of these works is to find the _optimal_ fusion strategy based on the different uses. In this work, we aim to use these powerful multi-image fusion approaches to learn better _defocus effects_ by fusing information from different cameras. For this, the cameras used by these methods are deployed as a single RGB image and a single low-resolution depth image to predict an image. Instead, we model multiple cameras and use multi-image fusion as a way to learn defocus and fuse the two camera data.

**Use of multiple cameras.** Our approach is related to the dual-camera works of multi-image fusion [25][26][32][33][28][27], alignment based denoising [34][35][36][37] and deblurring [38]. We are different from these methods because we explicitly learn to map two images to a single defocus image with varying defocus blur and more importantly, to learn good defocus effects.

]","Step 1. Important sentences from the cited papers:
1. ""We propose a system for defocus control for synthetically varying camera aperture, focus distance and arbitrary defocus effects by fusing information from a dual-camera system.""[1]
2. ""We leverage real-world smartphone camera dataset by using image refocus as a proxy task for learning to control defocus.""[1]
3. ""Our method outperforms state-of-the-art on defocus deblurring, bokeh rendering, and image refocus.""[1]
4. ""We address the data capture bottleneck by proposing a procedure to generate realistic DP data synthetically. Our synthesis approach mimics the optical image formation found on DP sensors and can be applied to virtual scenes rendered with standard computer software.""[7]
5. ""We propose a system that captures, aligns, and merges a burst of frames to reduce noise and increase dynamic range, and a technique for jointly denoising bursts of images taken from a handheld camera.""[30][34]
6. ""We present a system to computationally synthesize shallow depth-of-field images with a single mobile camera and propose a method that consists of a salient object detection module, a monocular depth prediction module, and a DoF rendering module.""[14]
7. ""We propose a low complexity multi-frame approach suitable for mobile implementations, which combines several shots of the same scene captured at different exposure times to produce a high dynamic range radiance map.""[32]

Step 2. Summarized topics for related work:
- Dual-camera systems for defocus control and image refocus using real-world smartphone datasets.
- Synthetically generating realistic data for dual-pixel (DP) sensor image formation.
- Techniques for reducing noise and increasing dynamic range through computational synthesis or joint denoising from bursts of images.
- Computational synthesis of shallow depth-of-field images using salient object detection, monocular depth prediction, and DoF rendering.

Step 3. Write related work: 
<In the domain of computational photography, dual-camera systems have been explored for defocus control and image refocus. Recent research has proposed a system for defocus control by fusing information from a dual-camera system to synthetically vary camera aperture, focus distance, and arbitrary defocus effects [1]. This approach leverages real-world smartphone camera datasets and uses image refocus as a proxy task for learning to control defocus, demonstrating superior performance in defocus deblurring, bokeh rendering, and image refocus compared to state-of-the-art methods [1]. Additionally, the data capture bottleneck for dual-pixel (DP) sensor images has been addressed through the proposal of a procedure to synthetically generate realistic DP data that mimics optical image formation, enabling the application to virtual scenes rendered with standard computer software [7].

Furthermore, techniques for reducing noise and increasing dynamic range in computational photography have also been investigated. These include systems that capture, align, and merge bursts of frames to reduce noise and increase dynamic range, as well as methods for joint denoising from bursts of images taken from handheld cameras [30, 34]. Additionally, low complexity multi-frame approaches suitable for mobile implementations have been proposed, combining several shots of the same scene captured at different exposure times to produce high dynamic range radiance maps [32].

Moreover, computational synthesis of shallow depth-of-field images has been studied, utilizing techniques such as salient object detection, monocular depth prediction, and depth-of-field (DoF) rendering. These methods aim to computationally synthesize shallow depth-of-field images with a single mobile camera, providing a valuable approach for photographers and smartphone users seeking to achieve shallow depth-of-field effects [14].>"
679," **Human Summarization Evaluation.** Human evaluation is the gold standard for summarization evaluation [1]. Pyramid [2] and Pyramid++ [3] are two of the most widely used methods for human evaluation. Pyramid selects salient content units (SCUs) from a set of candidate summaries and ranks them based on inter-annotator agreement. Pyramid++ improves upon Pyramid by using a single summary-level SCU for each SCU, which allows for a more balanced distribution of SCUs. [4] propose a lightweight version of Pyramid++, LightPyramids, which further improves the scalability and interpretability of Pyramid. However, Pyramid++ still relies on SCUs for salience evaluation. In contrast, our proposed Atomic Content Units (ACUs) are based on fine-grained semantic units and allow for a high inter-annotation agreement.

**Automatic Metrics.** Automatic metrics such as ROUGE [19], BLEU [20], and SACRE [9] are widely used to evaluate summarization systems. These metrics have been shown to correlate well with human annotations [8]; [16]; [5]; [6]; [23]; [18]; [7]; [15]; [26]. However, these metrics do not consider the factuality of summaries [10]; [12]; [17]; [22]; [11]; [13]; [14], which is the focus of our work.


**Human Evaluation for Factual Consistency.** Factual consistency evaluation (FCE) is a closely related task to summarization. FCE aims to evaluate the factual consistency of a summary with respect to a given set of factual facts. [10] and [11] propose to use QA-based methods to evaluate FCE. [14] propose the GO FIGURE method, which is a meta-evaluation method for FCE, and [13] propose QAFactEval, an improved version of QAFACTEval. In this work, we focus on FCE as it is more closely related to our work, and we compare our proposed ACUs with existing FCE methods in Section 4.
**Human-Automatic Comparison.** Several studies have compared the correlation between automatic metrics and human evaluation [29]; [25]; [28]; [31]; [21]; [24]; [27]; [30]. [25] compare the correlation of automatic metrics with human evaluation of extractive meeting summaries. [31] evaluate the effect of inconsistent summarization assessors on summarization metrics and find that inconsistent assessors do not significantly impact the correlation. [23] find that most automatic metrics perform poorly in the appropriate scoring range. [24] propose SummEval to re-evaluate the performance of existing automatic metrics, and they find that the best performing metrics are not necessarily those with the highest correlation with the human annotations. [27] conduct a similar study to ours, but they focus on crowdsourcing the evaluation of factual consistency. Our work differs from these studies in that we conduct a comparative study of four different human evaluation protocols and evaluate 50 automatic"," **Summarization Evaluation.** Existing summarization evaluation methods can be broadly categorized into two categories: (1) _system-based_ and (2) _human-based_. System-based evaluation methods evaluate summarization systems by comparing summaries to reference summaries, which are generated by human annotators. For example, Pyramid [2] and its variants [3][4][5][6][7][8][9] measure the quality of summaries by measuring the overlap of n-grams, word sequences, and word pairs between the generated summary and reference summary. [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31].

**Human Evaluation for Summarization.** Human evaluation has been extensively studied in the summarization literature. [12] and [1] collect human judgments of summary quality on the CNN/DailyMail and XSUM datasets, respectively. [3] collect a dataset of human comparisons between summaries and human-preferred summaries on Reddit. [4] collect summaries from the SummaC dataset [8] and build lightweight pyramids for comparison. [6] collect summary-level human judgments on the XSum dataset [9] and Lite2Pyramid dataset [6]. [7] collect annotations on the DialSummEval dataset [3, which is a multi-faceted dataset for dialogue summarization. [11] collect annotated summaries for the CNN and DailyMail dataset [7].
","[ Summarization evaluation.Summarization evaluation has evolved from the early human-annotated Pyramid-style system rankings ([2]) to automatic metrics (; [18]; [23]). _ROUGE_() family of automatic metrics gained prominence for summarization, but its strengths lie in its recall than its ability to detect conflict ([22]; [16]) or inconsistency ([20]; [8]; [21]; [19]). Several metrics have been developed specifically for factual consistency and have shown improved correlations with system rankings ([17]; [23]; [13]; [10]; [12]). Other evaluation methods explore different styles of evaluation, such as QA ([11]; [9]; [14]), relevance matching ([15]), and task-specific evaluation ([17]; [12]; [11]; [13]; [17]; [14]; [7]; [18]; [19]; [3]; [6]; [4]). There has also been a shift towards hierarchical protocols, with multi-turn prompt-based summaries gaining prominence ([17]; [24]; [18]; [7]).

Recently, researchers have critically examined the strengths and weaknesses of various automatic metrics, reporting low correlation coefficients ([26]), failure to detect factual errors ([27]), and inconsistent data ([29]; [28]). [20] provide a thorough analysis of automatic metrics including ROUGE. We note that the most commonly used set of metrics for system evaluation, RE-3, has only been empirically shown to correlate with human evaluations for content matching at a high level of statistical significance ([25]; [20]; ; [21]).

Summarization human evaluation.Most existing summarization human evaluation studies are usually summarized as small protocols such as Pyramid ([2]), ACU (), or the modified summarization salience protocol, **Atomic Content Units** (ACU) ([3]). In addition, these studies have taken an implicit assumption, namely, that the setting of the summary evaluation is ideal, that is, the summary evaluation involves manual reviewers who have access to the original documents, seldom lack of training, and are given sufficient time to review the summaries. Our study is the first to consider summarization evaluation from the perspective of both _setting_ and _factors_ that may influence the evaluations.

In addition, there have been studies of the differences in the scores given by different annotators and various factors affecting evaluations. [10] report that expert evaluators are consistently more confident in their rankings, while [23]; [21]; [30]; [31] note the subjective nature of human evaluation and the importance of careful calibration. [27]; [12] note the importance of consistency in factual evaluations.

]","<>
Summarization evaluation has been a crucial area of research within natural language processing (NLP), with a focus on improving the quality and reliability of evaluation metrics and protocols [1]. Prior work has demonstrated the challenges associated with human evaluation protocols for summarization systems, often citing low inter-annotator agreement and limited scale as key drawbacks [2]. This aligns with the concerns expressed in the target paper regarding the deficiencies of existing evaluation studies for summarization [2]. Researchers have recognized the need to address these shortcomings and have proposed alternative evaluation strategies to enhance the robustness and reliability of human assessment in summarization evaluation [3]. The development of modified summarization salience protocols, as proposed in the target paper, reflects a scholarly effort to refine the evaluation procedures for assessing summarization systems [3].

The establishment of large-scale human evaluation datasets, such as the Robust Summarization Evaluation (**RoSE**) benchmark mentioned in the target paper, represents a significant step forward in addressing the limitations of existing human evaluation studies for summarization [4]. These efforts highlight a growing trend within the research community to consolidate diverse human annotations and create comprehensive benchmarks for evaluating summarization systems [4]. In addition to the **RoSE** benchmark, other efforts have also focused on curating extensive datasets for human evaluation in NLP tasks, laying the groundwork for robust and extensive evaluation procedures [5].

The target paper's comparative study of human evaluation protocols not only underlines the challenges associated with existing evaluation setups but also contributes to the ongoing discourse on the standardization and refinement of evaluation methodologies in NLP and summarization-specific contexts [2]. The authors' assessment of 50 automatic metrics and their variants using collected human annotations further aligns with prior research that has explored the evaluation of automatic summarization metrics and their correlation with human judgments [6]. This collective body of work underscores the need for comprehensive and rigorous evaluation of automatic metrics in conjunction with human assessment to ensure reliable and valid summarization evaluation [6].

The target paper's critical analysis of recent methods based on large language models (LLMs), including GPTscore and G-Eval, sheds light on the evolving landscape of evaluation methods in the context of advanced NLP models [2]. This aligns with broader research efforts that have scrutinized the implications of using advanced language models for NLP tasks, emphasizing the need for targeted evaluation methods that can effectively assess the performance and limitations of these models [7]. The insights offered in the target paper regarding the potential overfitting of LLMs adjusted by human feedback serve as a catalyst for ongoing discussions on the nuanced relationship between human evaluation and the integration of advanced NLP models in summarization tasks [2]."
764," Dasgupta's cost function ([1]) is a generalization of the classical agglomerative clustering objective, where the goal is to find a tree structure that minimizes the sum of pairwise distances between the points in the same cluster. It has been shown that the objective function can be solved in polynomial time with respect to the number of clusters in the input graph ([5]).

For the well-clustered graph \(G\) with a clear cluster-structure, the greedy algorithm proposed in ([2]) returns an \(O(1)\)-approximate HC tree, where \(\mathcal{O}(\sqrt{G})\) is the total number of nodes in the tree. The greedy algorithm is based on the spectral clustering algorithm ([3]) and runs in time \(O(\log(1/2)\) where \(2\) is the size of the graph.

Recently, [4] proposed an algorithm that approximates the cost function of a graph in sublinear time and using a small number of seeds. They showed that their algorithm achieves an \(\ell_{2}\) approximation to the original objective function, where \(1\) is a constant. However, their algorithm is not guaranteed to converge to a good HC tree with respect the original cost function. In this paper, we improve the running time of their algorithm to \(\log(2/1)\).

The greedy algorithm of [2] is also extended to the dissimilarity-based objective function in ([7]). In their algorithm, the algorithm starts from a seed point and iteratively merges the points that are similar to the point in the current level. The final HC tree is obtained by taking the average of all the merged points as the new HC tree. In contrast, our algorithm is designed for the more general objective function of the similarity-based hierarchical clustering, and we show that our algorithm produces comparable or better HC trees with much lower running time. Recently, [6] proposed a greedy algorithm based on max-uncut bisection. Their algorithm runs in nearly-linear time, but it requires a large number of iterations to converge. In comparison, we present two algorithms that run in nearly linear time and converge to an approximate HC tree for any well-structured graph.

 proposed an efficient algorithm for the similarity objective, which runs in \(\mathbb{E}(n)\) time and returns an \(\mathrm{approximation}\) HC tree in the average case. In our experiments, we compare their algorithm against ours in Section 4.1.

 presented an algorithm for a more general cost function, which is called the minimum spanning tree (MST) objective. The MST objective is defined as follows:

\[\begin{split}\mathbf{MST}=\sum_{i=1}^{n}d_{i}-1}^{\prime}_{i}\end{split}}\mathbf{\MST}\mathbb{\E}_{1}=1^{n}\sum_{j=1}{n}^{2}^{"," HHC algorithms can be categorized into two main categories: optimization-based and heuristic-based.

Optimization-based HC algorithms.Dasgupta [1] proposed an objective function for hierarchical clustering with pairwise similarities between data points, and proposed a top-down construction procedure with a provably good approximation ratio. He also proposed an algorithm for HC with \(O(1)\)-approximate cost function, which was later improved by [2] and [3] by using spectral techniques. [4] proposed a seed-based algorithm for learning the hierarchical cluster structure of graphs in the seeded model, where besides the input graph, the algorithm is provided with a small number of seeds. The algorithm in [4], however, is computationally expensive in terms of the number of iterations needed to learn the hierarchical structure of the graph.


[5] proposed the similarity-based objective function of HC, which is a combinatorial optimization problem, where the similarity between data elements is given and the cost function is defined as the sum of the cost for each pair of data elements. The similarity-free objective function was further improved in [6] by introducing the Max-Uncut Bisection algorithm, which achieves an \(O(\sqrt{\log k})\) approximation to the original objective function in polynomial time. [7] proposed two algorithms for the similarity based HC objective proposed by [1], and [5] presented a set of objective functions for the dissimilarity based HC, and the similarity free objective functions, respectively. The objective functions in [7]; [5]; [6];  are based on semidefinite programming.

 show that the objective function proposed by Dasgupta can be approximated by the algorithm of [1]. However, their algorithms are computationally intensive, and are not suitable for large-scale datasets.

","[ HC is a problem that has received a lot of attention in the last few years and has provided an important avenue for understanding the behavior of simple HC algorithms like average-linkage and complete-linkage ([4]). In particular, it was observed that if \(G\) admits a natural cluster hierarchy, then any HC algorithm based on a metric on \(\mathbb{R}^{n}\) produces an HC tree \(T(G)\) that satisfies:

\[\operatorname{cost}(T(G))=\operatorname{cost}(G).\] (1)

Dasgupta introduced a general cost function (CDC) (Dasgupta & ) that incentivizes the HC algorithm to produce an HC tree that is a good approximation of the HC tree of the given natural cluster hierarchy. One may note that the proposed methods are purely numerical and still produce comparable or better performance than state-of-the-art heuristics for real-world datasets.

Most of the prior work to-date have focused on developing a top-down approach to HC by optimizing the CDC w.r.t the graph \(G\). However, if \(G\) is not clear, i.e., one cannot clearly determine if \(G\) has a natural cluster hierarchy, then we need to solve an online variant of HC, where we are given the input in a streaming fashion. For such a problem, the goal is to find a suboptimal (but often good enough) HC tree. [3] introduced the problem of clustering graphs into bisections and showed that a combinatorial optimization problem can be solved in sub-linear time if the given graph is drawn from a certain distribution over graphs that contain the desired solution. This problem was later extended by Charikar and Chatziafratis ([2]) to hierarchical clustering with metric cost. They proposed an algorithm for approximating HC cost in random graphs that are grown according to some distributed (random) process. This work provides a one-parameter (smoothness) tradeoff between the running time and the quality of the solution (HC cost).

Recently, [4] and  proposed two new algorithms that are designed for the seeded model. The prior work on the seeded model by [4] leverages the performance of SDP solvers to arrive at a provably good approximation (with respect to the CDC) of the graph \(G\). However, they require access to the underlying input \(G\) and the exact seed set to be given in the pre-processing phase of the algorithm. Our work can be seen as a generalization of the work of [4] to the case when we have access to any small subset of seeds. Similar to , we use an SDP solver to arrive at an approximate HC tree and remove redundant edges to improve the approximation guarantee. In contrast to [4], we do not make any assumptions about the performance of the SDP solver used and we remove redundant edges in-place. Moreover, we also]","<In the field of hierarchical clustering, the problem of computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is fundamental in data analysis [2]. Prior work primarily focused on procedures such as linkage algorithms or top-down heuristics rather than as optimization problems. Dasgupta [5] introduced an objective function for hierarchical clustering, framing it as a combinatorial optimization problem with the goal of minimizing a particular cost function. The cost function introduced by Dasgupta has been shown to have desirable properties, where disconnected components must be separated at higher levels of the hierarchy to achieve optimal cost, and identical similarities between data elements result in all clusterings achieving the same cost [5]. Additionally, [1] introduced a simple cost function for hierarchical clustering which behaves sensibly in canonical instances and admits a top-down construction procedure with a provably good approximation ratio. This work aims to optimize hierarchical clustering algorithms and objective functions for both similarity and dissimilarity-based hierarchical clustering, characterizing a set of admissible objective functions that include the objective function introduced by Dasgupta [5].>

<Recently, there has been a line of work developing algorithms that explicitly optimize an objective for hierarchical clustering, such as the approaches based on spectral techniques and SDP relaxations for balanced cuts [2]. For instance, McSherry [2] combined specific linkage methods with the SVD approach to yield an O(1)-approximation to Dasguptas cost function for hierarchical clustering. Makarychev et al. [2] also showed that an O(1) approximation can be achieved in slightly larger regimes and in the semi-random setting using SDP relaxations for balanced cuts combined with the recursive sparsest cut algorithm. Furthermore, the work of Charikar and Chatziafratis [4] provided an O(\sqrt{\log k}) approximation to Dasgupta cost of a graph in sublinear time using a small number of seeds, advancing the understanding of hierarchical clustering properties in sublinear time on clusterable graphs.>

<In a related context, recent research has focused on the development of algorithms that outperform average-linkage clustering, as demonstrated by the work of Moseley-Wang [6] and Cohen-Addad et al. [7]. Both works presented tight counterexamples showing that average-linkage cannot obtain better than certain approximation ratios, revealing opportunities to improve upon traditional hierarchical clustering methods. In response, new algorithms based on semidefinite programming with provably better guarantees were introduced to beat average-linkage for specific objectives, shedding light on the potential for enhanced hierarchical clustering performance [7].>

<In summary, the landscape of hierarchical clustering has witnessed advancements in optimizing objective functions, developing efficient algorithms with nearly-linear running times, and exploring new approaches to outperform traditional clustering methods. The works of Dasgupta [5], McSherry [2], Makarychev et al. [2], Charikar and Chatziafratis [4], Moseley-Wang [6], and Cohen-Addad et al. [7] represent important contributions to the pursuit of hierarchical clustering optimization and enhanced clustering performance. These advancements pave the way for more efficient and effective hierarchical clustering algorithms, particularly in the context of well-clustered graphs.>"
2068," **Diffusion Probabilistic Models.** Diffusion probabilistic models (DPMs) [1][2][3][4][5][6][7][8][9][10] are a class of generative models that model the data distribution as a mixture of Gaussians, where the noise is modeled by a denoising autoencoder (DAE), and the score function is defined as the gradient of the data log-likelihood. Recently, DPMs have achieved impressive results on image synthesis [6][8] and video generation [9]. However, due to their high computational complexity, they are hard to be deployed on resource-limited platforms. Several works have been proposed to improve the efficiency of diffusion models [11][12][13][14][15][16][17][18][19]. For example, Wang _et al_. [11] propose to improve DPM's sampling efficiency by using a multi-step training strategy. Guo [12] propose a knowledge distillation framework for improving the sampling speed of DPM. However, these methods mainly focus on speeding up the inference time, and do not focus on improving the quality of the synthesized images. In this paper, we make a dedicated attempt to lighten DPM while striving to preserve its favourable performance.

**Knowledge Distillation.** Knowledge distillation (KD) [20][21][22][23][24][25][26] aims to transfer the knowledge from a large teacher model to a small student model, which has been widely used in model compression [27][28][29][30][31][32][33] and dataset distillation [34][35]. For instance, Bucilua _et.al_. [22] first propose to train a compact student model by minimizing the KL-divergence between the logits of the teacher and the student, which is later extended to multi-task learning [23] and multi-label classification [34]. In this work, we propose a spectrum-aware distillation scheme for DPM, which aims to distill the high-frequency information of the student model into the teacher model, and achieves state-of-the-art results on both image synthesis and classification tasks. Note that our method is orthogonal to the aforementioned KD methods, and can be combined with them to further improve the performance of the compact model. We leave it for future work to explore the combination of KD and DPM for further performance enhancement. Moreover, we note that our work is also related to the recent work on frequency bias [36][37][38][39][40][41], which has also been studied in the context of model compression.

 propose to use the frequency of the noise to guide the training of a compact model, but they only focus on the classification task. In contrast, we study the frequency bias in DPM and propose a customized design for slim diffusion models, which can be applied to other tasks as well.

 introduce a novel architecture for image generation based on the self-attention mechanism [10]. They show that it is possible to"," Diffusion Probabilistic Models (DPMs)DPM [1][2] are a class of generative models that have been widely used in image generation [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41]. In this paper, we focus on light-weight DPMs, which have been largely overlooked in the literature.

**Light-weight Generative Models.** DPM has been widely studied in the image domain [4][4] and video domain [9][9]. The seminal work of [4] first introduced DPM to image generation, and introduced the score-based generative model (SDE) [4]. SDE is a stochastic differential equation that transforms a complex prior distribution into a simple one by slowly removing the noise corresponding to the reverse score of the data distribution. SDE has been applied to various applications, such as denoising autoencoders [5], Langevin dynamics [7], and diffusion models [8][8]. The SDE framework has also been extended to other domains such as video [9], text [10], and image [6]. However, SDE and its variants [5] are limited to simple models with a single forward pass, which is not scalable to high-resolution images. To address this issue, [6] proposed to use the gradients of the reverse process to speed up the forward pass. [7] proposed an annealed Langevin-dynamics framework to estimate the gradient field of the forward process. [8] proposed the Latent Score-based Generative Model (LSGM) to train DPM in a latent space. LSGM [19] proposed a novel parameter matching objective suitable for the DPM setting, and a novel training objective suitable to the LSGM setting. [17] proposed DDIM to accelerate the sampling of DDPMs. DDIM is a classifier-free guided diffusion model that uses a pre-trained classifier free guided model to distill the output of a diffusion model to a DDPM model. [12] proposed knowledge distillation to improve the sampling speed of DPM. [13] proposed Distillation of Gu","[ **Diffusion probabilistic models.** First proposed in [1][2], diffusion probabilistic models (DPMs) learn the generative process of the data distribution by injecting the diffusion process into the score function [3][4][5][6][7][8][9]. To reduce the degenerated solution space, previous works [6][9] introduced generative flow-based constraints [10][11][12] for sampling optimization, as well as graphics pipelines, while others [13] brought in the knowledge transfer task [14][15][16][17] or generation variants [18][9]. In parallel, studies [19] have shown that scores estimated from latent variables can be used as generative scores [10]. In this work, we propose a light-weight architecture to estimate scores from the latent variables, which allows for model simplification.

**Distillation and knowledge transfer.** The success of model distillation relies on the teacher model, which can typically be sampled in a fast way [20][21][22][23][24]. Most related works study distillation on a specific task, which is thus hard to apply to different domains. We are interested in distillation for model simplification, which is more transferable. This issue has been addressed by [25] for the score matching problem, and [26] for scene parsing and other vision tasks. Our proposed method takes the approach for model simplification, which is similar to . However, the design focuses on lightweight density models, and not on teacher-free sampling.

**Compactness and fine tuning.** Previous works propose network pruning techniques [27], which reduce the size of pre-trained models [28][29][30] or fine-tune them on limited data [31][32]. In particular, the encoder or generator of the DPM should not be pruned, since its fine-tuning will lead to a large drop in fidelity. As a result, studies [33] only propose structural pruning [33] for pre-trained models, and fail to apply to network compression, which could reduce the overall cost.

In this work, we leverage the optimal spatial architecture by retaining the data flow in the generator. It helps to preserve the fidelity, at the expense of a sizable overhead. A common approach in light-weight compression is to substitute high-frequency components for low-frequency ones [34][35], which is similar to our work. However, instead of binarizing the latent representation, our approach serves as an internal process in the generator, which can efficiently recover the high-frequency components and substantially improve the fidelity.

**Spectral bias.** Recently, there have been a surge of researches towards spectral bias [36][37], due to the]","<Diffusion probabilistic models (DPMs) have gained attention for their high-quality image synthesis results, but their computational cost has been a major limitation [1, 6, 8, 12, 17, 22, 28, 32, 35]. Prior methods have focused on accelerating testing, overlooking the significant complexity and size of DPMs [1, 8, 12]. Existing works have shown that DPMs can achieve exceptional performance when trained on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching [1]. It has been demonstrated that the sequential application of denoising autoencoders in diffusion models can achieve state-of-the-art synthesis results on image data [8]. Moreover, the implementation of cross-attention layers has led to a significant improvement in the balance between complexity reduction and detail preservation in diffusion models, enhancing visual fidelity [8]. Improved denoising diffusion probabilistic models have been developed to achieve competitive log-likelihoods while maintaining high sample quality, enabling scalable results with model capacity and training compute [11].>

<Additionally, the use of knowledge distillation has been explored to improve the sampling speed of iterative generative models, such as noise conditional score networks and denoising diffusion probabilistic models [12]. Furthermore, the use of differentiable evolutionary frameworks and differentiable fitness scoring functions has enabled neural architecture search algorithms to automate the design of search spaces for network construction, reducing human interference [21, 31]. The study of frequency analysis in deep neural networks has revealed the frequency principle, which suggests that DNNs fit low to high frequencies while learning target functions [36]. Moreover, it has been shown that structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks, and efforts have been made to address architecture-specific pruners to tackle general structural pruning of arbitrary architectures like CNNs, RNNs, GNNs, and Transformers [33]. This is in line with findings that GAN-generated images exhibit severe artifacts in the frequency domain, making them distinguishable from real images [39, 40].>

<Moreover, the training of GANs has been enhanced by leveraging frequency analysis to alleviate the spectral information loss in the discriminator, encouraging the generator to learn high-frequency content of real data and generate exact details [38]. The effectiveness of distilling the success of large, MIM-based pre-trained models to smaller ones has been explored, resulting in significant fine-tuning accuracy improvements across different model sizes [32]. It has been observed that dataset distillation can be facilitated through dataset factorization, resulting in exponential informativeness gain in the distilled datasets, enhancing representation capability [34]. Furthermore, the problem of slimmable dataset condensation has been addressed, allowing for the extraction of a smaller synthetic dataset given only previous condensation results, offering a new perspective on dataset condensation [35]. Lastly, the Frequency Bias in Neural Networks for Inputs of Non-Uniform Density and its implications have been explored, providing insights into the effects of variable density on training dynamics [37].>"
1533," Multi-armed bandit (MAB) problems have been extensively studied in the literature ([2]; ; [1]). In this paper, we focus on the best arm identification (BAI) problem in the multi-agent setting.

**Multi-Agent Bandit.** The multi-bandit BAI problem was first studied in ([7]). In ([7]), the authors proposed an algorithm called UCB-greedy, which iteratively selects an arm from a finite set and broadcasts it to the other agents. In ([5]), a gossip-based algorithm was proposed to solve the problem. The authors in ([4]) proposed a decentralized algorithm based on incentivized exploration. The algorithms in ([3]; [9]; [6]) consider the problem where each agent can only communicate with a subset of its neighbors. The works in ([10]; [8]; [11]) proposed algorithms based on the successive elimination scheme, which can be viewed as a generalization of the UCB algorithm. In (), the authors considered the problem of finding the optimal arm among a set of arms in a distributed setting. In this work, we consider a more general setting, where agents can communicate with each other without knowing each other's private information. We provide theoretical guarantees for the privacy level, accuracy and sample complexity of the algorithms proposed in this paper. We also compare our algorithms with the algorithms in () and ([10]).

In ([14]), a decentralized exploration algorithm for multi-armed bandits was proposed. The algorithm in ([14]) is based on a two-stage algorithm, in which the first stage selects the arm with the largest expected reward and the second stage uses the selected arm as a proxy to explore the remaining arms. The work in ([12]) proposed an overlapping multi-arm BAI algorithm, which uses a two stage algorithm to select an arm in each of the overlapping bandit problems. In our work, the agents can also communicate with other agents, but they do not have to share their private information to each other. In addition, our algorithms do not need to know the reward of each arm, which makes them more suitable for the real-world applications. In contrast, the algorithms of ([14]; [12]) require knowing the rewards of all the arms, which is usually not available in practice. In particular, the algorithm of ([12] requires knowing the reward distribution of each bandit problem, which may not be available in many applications. Our algorithm does not require this information, and it can be used in many practical applications, such as recommendation systems and reinforcement learning. We compare our algorithm with the algorithm proposed in ([11]) in Section 5.1 and show that our algorithms are more accurate and sample efficient than the one proposed in [11].

Differential Privacy.Differential privacy (DP) is a privacy-preserving technique that aims to protect the privacy of individuals. It has been widely used in the machine learning community (; ; ; [16]; ; ; ). In ([13]), a distributed algorithm for the MAB problem with DP was proposed, which achieves a regret bound of \(\mathcal{O}(n^{2"," **Multi-Agent Bandit.** The problem of finding the best arm in a multi-armed bandit has been studied extensively in the literature. [1] proposed an algorithm based on the confidence interval of the Q-function and eliminated actions that are not optimal with high probability. [2] studied the sample complexity of pure exploration and regret minimization in the multi-agent setting. [3] proposed a collaborative learning algorithm for best arm identification. [4] studied incentivized exploration in multi-arm bandits with multiple and long-term strategic agents. [5] studied a distributed bandit setting where k players collaborate in order to identify an e-optimal arm. [6] proposed the LCC-UCB algorithm with limited communication. [7] proposed GapE-V, which takes into account the variance of the arms in addition to their gap. [8] proposed Gossip-based distributed stochastic bandit algorithms. [9] proposed Social Agent Bandit, which uses a pairwise asynchronous gossip based protocol that exchanges a limited number of bits. [10] proposed Collaborative Best Arm Identification (CAB), which uses the index of the best-arm-known agent as a subroutine. [11] proposed Insert-Eliminate, which is a gossip-based algorithm that uses a subset of all the agents to play from all the arms. [12] proposed LUCB, which also uses a set of agents to perform best arm-id selection.
 but with a lower confidence bound. [13] studied MAB with local differential privacy. [14] studied decentralized exploration in the MAB setting.
1, where each agent only plays from a subset and only communicates with one other agent.


**Privacy in Bandit problems.** Privacy in bandit problems has been investigated in the context of single-arm bandit. [15] studied linear contextual bandits over encrypted data. [16] studied private data analysis in the setting in which a trusted and trustworthy curator releases to the public a ""sanitization"" of the data set that simultaneously protects the privacy of the individual contributors of data and offers utility to the data analyst. [17] studied secure federated bandits with differential privacy and \((\epsilon,\eta)\) privacy.

","[ Multi-agent bandits are a classic problem in Reinforcement Learning (; [1]; ; ). Our work follows the best-arm identification framework (; [2]; ) where agents collaborate to learn the value of arms in a common set. Unlike the case of \(d\)-armed bandits where each agent optimizes its own objectives, in multi-agent bandits each agent learns a different arm but all the agents collaborate to learn the best arm. In recent years, this problem has gained significant attention (; ; [5]; [7]; [3]; [4]).

The multi-agent best arm identification problem is first formally defined in (). [5] propose to address this problem with \(O(\sqrt{k})\) communication rounds. Our work focuses on the pure exploration setting which can be applied in different scenarios such as federated learning and distributed learning. [6] consider distributed learning and the pure exploration setting with restricted communication.  propose an algorithm for pure exploration with private and public messages. We formulate the problem as differentially private bandits and use differentially private algorithms and prove private variants of our algorithms and compare them with the differentially private counterparts of .

The communication process of multi-agent best arm identification is studied in ([9]; [10]; [11]; ). We consider a weaker form of communication that is based on private messages with sparse matrices. This communication setting is related to gossip ([8]) but not identical since agents in our setting are required to exchange private messages with private matrices, and agents can not fully access the data. We also compare our work to best arm identification with gossip ([12]; [3]).

Differential privacy ensures that agents cannot learn information about individual users. Differential privacy has been extensively studied in online learning ([13]; [14]) and only allows for private algorithms ([16]; [15]; [17]; [14]; [16]) to be used. In this paper, we study private agents in the context of multi-agent best arm identification with differentially private bandits algorithms and prove that the private agents can still perform well in terms of best arm identification in a large setting (up to \(n=2^{16}\)).

]","<Related work>
Multi-agent systems have been studied extensively in the context of multi-armed bandit (MAB) problems, where agents collaborate to identify the best arm with various privacy considerations. In the work by [1], the authors propose a two-level successive elimination scheme for best arm identification while considering differential privacy and (\( \epsilon,\eta \))-privacy. This approach ensures privacy in the communication among agents while providing theoretical guarantees for the privacy level, accuracy, and sample complexity. Similarly, [13] investigates multi-armed bandit problems with local differential privacy and analyzes regret minimization with differential privacy guarantees.

Collaborative learning and exploration in multi-armed bandit problems have also been explored extensively. In [3], a general multi-agent bandit model is introduced, highlighting the necessity of communication between agents to identify the optimal arm. The work by [5] demonstrates a tradeoff between the number of arm pulls by each player and the amount of communication between them, providing insights into the parallel speedup achieved through distributed learning. Furthermore, [16] delves into the complexity of differentially private data release and investigates efficient algorithms and hardness results in the context of private data analysis.

In the domain of decentralized exploration and secure federated learning, several works have made significant contributions. In [14], the authors propose a decentralized elimination algorithm for maintaining privacy in the best arm identification problem. This approach ensures privacy with low communication cost and leverages decentralized communication to minimize the impact on sample complexity. Additionally, [17] addresses the secure federated learning setting by introducing SAMBA, a generic framework for Secure federAted Multi-armed BAndits, which ensures formally proven security properties while maintaining the cumulative reward maximization objective.

Overall, the literature on multi-agent best arm identification with privacy considerations, collaborative learning in bandit problems, and decentralized exploration in multi-armed bandits provides a rich foundation for understanding the challenges and opportunities in this domain. The works discussed above offer valuable insights into the algorithms, privacy guarantees, and communication protocols essential for multi-agent collaborative learning in the context of best arm identification and exploration in multi-armed bandit problems."
3857," **Diffusion Models.** Diffusion models [1][2][3][4][5][6][7] have achieved remarkable success in image generation compared to other generative models, such as VAEs [8], GANs [9][10][11][12][13], and score-based models [2][7][14]. The main difference between diffusion models and VAEs lies in the reverse process of the diffusion model. The reverse process can be formulated as:

\[\begin{split}\mathbf{x}=\mathbb{E}_{\mathcal{D}}\mathrm{exp}(\mathbf{\theta}_{ \mathbf{{x}}+\mathbfbf{y}})\mathbf {x}-\mathfrak{x}}\end{split}\] (1)where \(x\) is the data distribution, \(y\) is a noise distribution, and \(f\) and \(x_{y}\) are the parameters of diffusion model and VAE, respectively. Then, the inverse process of diffusion models can be written as

where \(\mathrm{\exp}(x)=\sum_{x=1}^{n}\sum_{y=0}^{N}D_{x}\end{ split}\). In practice, we need to solve the following optimization problem:where \(d_{x},d_{y},f(x)\) are the reverse variables, \(f(y)\) is the reverse variable, and \(\mathbf\in[0,1]\) is an upper bound of the reverse variance. To solve this optimization problem, we can either use an analytical method [15][16][17] or a heuristic method [18].

**Diffusion Model for Other Tasks.** Besides image generation, diffusion models have also been applied to other tasks, including video generation [19], text-to-speech generation [20][21], and image editing [22]. For example, Diffusion-HD [23] generates high-resolution images by decomposing the image into a low-resolution latent code and a high-frequency noise code. However, these diffusion models are trained with labeled data corrupted with noises. In contrast, we propose a plug-and-play diffusion model for various tasks without fine-tuning the diffusion models. This allows us to leverage off-the-shelf models for guidance, which is not possible in previous diffusion models [24][25][26][27][28][29][30][31][32].

 propose to use a pre-trained denoising autoencoder as a guidance model for image generation. They first pre-train the denoiser on labeled data and then fine-tune it on the unlabeled data. Different from that, our method does not require any labeled data. [18] propose a classifier-free diffusion model that does not need any pre-training and can generate high-quality images without fine tuning. In this work, we focus on the practical diffusion model setting, where we only need to fine"," Diffusion-based Generative Models.Diffusion models [1][2][3][4][5][6][7][8][9][10] have achieved remarkable success in image generation. They are based on the idea of diffusion process, which iteratively transforms a complex data distribution to a known prior distribution by adding noise and gradually removing it. The main challenge of diffusion models is to estimate the reverse process of the diffusion process. To solve this problem, many methods have been proposed, such as Langevin dynamics [2][6], score-based generative modeling [7], variational lower bound (VLB) [5], and denoising diffusion model (DDM) [2]. However, these methods require a large amount of labeled data to fine-tune the diffusion model, which is infeasible to scale up for various tasks. To address this issue, some methods [1] propose to use off-the-shelf models for guidance. However, the direct use of publicly available models for the guidance fails due to their poor performance on noisy inputs. In this paper, we propose a practical guidance framework that leverages multiple experts to guide the reverse procedure of diffusion model.

**Generative Adversarial Networks (GANs).** GANs [11][12][13] are a class of generative models that are trained in an adversarial manner to generate samples from a random noise distribution. They have been widely used in image synthesis [14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32]. For example, [23][23] use GAN to generate images conditioned on text or bounding boxes. [24][22] and [25] use the GAN framework to generate high-quality images. [25][31] use a pre-trained autoencoder to generate the latent code of the generator and then use a differentiable reward function to control the generation process. [31] propose a method that uses a denoiser to generate a similar sentence to the original one. [29] propose plug-and-play autoencoders (PPLM) that use a pretrained language model to guide text generation. [30] propose Emb2Emb to embedding (Emb2Emb) that uses an encoder-decoder architecture to generate text conditioned on a set of attributes. [32] propose D","[ **Diffusion Models.** The most influential works on diffusion models are by Nishio _et al_. [1] and sobollev _et al_. [2], who first propose two kinds of non-equilibrium statistical physics based probabilistic models in which the probability distribution can be derived from a score function with an assumed prior distribution. Their formulations have inspired a large line of efforts in generative modeling, including diffusion variational models [3][4][5], score-based generative models [6][7], and fully trainable networks [8][9][10]. More recently, diffusion models have been applied to conditional generative modeling, where the prior distribution is conditioned on given inputs, such as real numbers [6], GANs [11][12][13] and image patches [14].

**Fast Diffusion Inference.** To speed up the inference in DDPM [1], [15][16] propose more stable score functions for Gaussian noise models. [17] suggest a Newton-like approximation to estimate the posterior distribution and Gain _et al_. [14] extend DDPM to denoising diffusion models (DDM) with a modified noise schedule and learned normalizing factor. Improved DDPM (IDP) [4] is also proposed, which introduces an asymmetric normalization on the score map and estimates the variance from two points. Besides, Shi _et al_. [18] present classifier guidance that combines the score estimate of a diffusion model with the gradient of an image classifier, where the gradient is estimated via a non-intrusive approximation, and a fixed variance schedule is used to control the performance trade-off. The classifier guidance based DDPM [18] has also been extended to video generation [19] and text-to-speech generation [20].

**Application of Diffusion Models.** The diffusion models have also shown its applicability in other tasks, such as audio generation [21], image editing [22] and high resolution image synthesis [23]. Liu _et al_. [24] propose the pipelines to convert an unconditional model into conditional ones by learning intermediate representations. Instead of retraining the diffusion models, Plug & Play methods such as Latent Constraints [25] and Learning Latent Constraints [26] aim to exploit the learned noise model by using a separate model to perform a perturbation on the latent space of a generative model, such as denoising autoencoders [27]. However, these methods require an expert model to control the diffusion model. Recently, [28] present a method to solve inverse problem with diffusion models without a model.

**Plug & Play for Controlled Generation.** There are various attempts in plug & play generative models. Early works in this direction are Plug & Play Language Models (PP-]","<Diffusion-based generative models have gained significant attention for their success in image generation tasks. These models enable the control of the generation process by external guidance without the need for fine-tuning the diffusion model [1]. However, publicly available off-the-shelf models for guidance often exhibit poor performance on noisy inputs, leading to the need for fine-tuning the guidance models with labeled data corrupted with noises [2]. This approach has limitations, particularly in handling a wide range of input noises with a single guidance model and in the scalability of collecting labeled datasets for various tasks [3]. To address these limitations, a novel strategy leveraging multiple experts specialized in specific noise ranges has been proposed in [4]. Additionally, Practical Plug-And-Play (PPAP) framework has been introduced to manage multiple networks and utilize labeled data efficiently, which involves parameter-efficient fine-tuning and data-free knowledge transfer [5].>

<In addition to diffusion models, recent works have explored the use of score-based generative modeling and stochastic differential equations for generative modeling tasks. Score-based generative models estimate gradients of the data distribution using score matching and have been shown to produce samples comparable to GANs [6]. Moreover, stochastic differential equations have been utilized to transform data distributions to known prior distributions by slowly injecting noise, enabling accurate estimation of scores with neural networks and leading to new sampling procedures and modeling capabilities [7]. Furthermore, a study examined the potential of generative adversarial networks (GANs) in computer graphics applications, highlighting their significance in image and computer vision for tasks such as face synthesis, image-to-image translation, and super-resolution [12]. These approaches provide valuable insights into alternative generative modeling techniques and their applications in various domains.>

<In the context of conditional image synthesis, the notion of guidance for diffusion models has been further explored. Classifier guidance has been introduced to trade off diversity for fidelity in conditional diffusion models, showing its effectiveness in improving sample quality with simple, computationally efficient methods for controlling diversity using gradients from a classifier [1]. Additionally, works on plug-and-play priors have been proposed, wherein independently trained denoising diffusion generative models serve as plug-and-play modules for inference in a model with an auxiliary differentiable constraint, allowing for the adaptation of models to new domains and tasks [32]. Further advancements in text-to-image synthesis have been made through text-conditional image synthesis and text-guided diffusion models [22, 23].>

<The application of diffusion models has extended beyond image generation to tasks such as text-to-speech (TTS) synthesis and video generation. For instance, Guided-TTS has been introduced as a high-quality TTS model that combines an unconditional diffusion model with a separately trained phoneme classifier for classifier guidance, demonstrating comparable performance to state-of-the-art TTS models without the need for transcripts [20]. Similarly, Imagen Video has been developed as a text-conditional video generation system based on a cascade of video diffusion models, enabling the generation of high definition videos using base video generation models and interleaved spatial and temporal video super-resolution models [19]. These advances illustrate the versatility of diffusion models in handling diverse generative tasks.>

<To enable efficient inference and learning in directed probabilistic models, Auto-Encoding Variational Bayes is introduced as a stochastic variational inference and learning algorithm that scales to large datasets and works even in the intractable case [8]. Additionally, the NICE framework for modeling complex high-dimensional densities focuses on learning non-linear deterministic transformations of data to make the transformed data conform to a factorized distribution, which has shown good generative modeling performance on image datasets and demonstrated promising results for tasks such as inpainting [9]. These works shed light on alternative techniques for efficient inference and learning in probabilistic models.>

<Finally, advancements in inverse problems with generative models and the utilization of score-based generative models in reconstructing medical images from partial measurements have showcased the potential of generative models in solving challenging real-world problems [28]. Moreover, the study on latent constraints and the use of denoising in educating text autoencoders provide insights into methods leveraging latent representations for controllable text generation and text manipulations [31]. These developments emphasize the broader applicability of generative models in diverse fields beyond traditional image synthesis.>"
3829," **Procedure Planning from Instructional Videos.** Procedure planning from instructional videos with text supervision is a relatively new task, which aims to plan an action sequence to transform the initial visual state into the goal visual state. PlaTe [1] is the most related work to ours, which proposes a transformer-based model to plan a sequence of actions from an instructional video. However, it is designed for the task of action recognition and ignores the large semantic gap between observed visual states and unobserved intermediate actions in a procedure, which is a critical challenge of this task. To address this issue, we propose a novel event-guided paradigm, which first infers events from the observed states and then plans out actions based on both the states and predicted events. To further consider the strong action associations within each event, our E3P adopts a mask-and-p model, which encodes event information into the sequential modeling process to support procedure planning. In addition, we introduce a novel prompting-based paradigm to bridge the semantic gap. Recently, [2] proposes a contextual modeling and model-based policy learning framework for procedure planning in instructional videos. Different from [2], we focus on the procedure planning task with textual supervision.

**Video Understanding.** Video understanding has been extensively studied in the past few years [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25]. Inspired by the success of the vision transformer in image recognition [14][26][27][28][29] and zero-shot learning [26][29][30][31], some recent works [31][31] propose to apply the transformer architecture to video understanding tasks. For example, Video Transformer Network [16] and VTNets [17] are proposed to recognize short-term and long-term actions in videos, respectively. In [31], a bridge-prompt model is proposed to address the problem of ordinal action understanding in videos. Compared with these works, our task is more challenging due to the large gap between the contents in the observed visual state and the elements of some action text labels in the procedure. To bridge this semantic gap, we design a novel paradigm to predict an event sequence and plan out actions from the predicted event sequence. Moreover, our model is designed to be end-to-end trainable, which can be easily adapted to other tasks with different types of supervision, such as action recognition, action anticipation, and procedure planning, without fine-tuning. In this paper, we mainly focus on procedure planning with text supervisions, which has not been studied in previous works. We refer readers to [26] for a comprehensive survey of the literature on prompt-based methods. In the rest of this section, we briefly review the related works in the following two aspects: **Pre-training, Prompting and Regularization.** Pre-training methods [32][33][34][35] aim to pre-train a model on large-scale datasets"," **Video understanding.** Video understanding aims to understand the temporal structure of videos. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25] mainly focus on the task of action recognition, which aims to predict the action sequence to transform the initial visual state into the goal visual state. Recently, with the development of deep learning, transformer-based methods [14][14] have been proposed to learn video representations from raw video frames. For example, Vision Transformer (ViT) [14] is the first work to apply the transformer architecture for image recognition. It firstly applies a pure transformer architecture to sequences of image patches, and then applies a self-attention mechanism to aggregate the image features for image classification. To further improve the efficiency, Swin Transformer [15] proposes a hierarchical architecture with shifted windowing to model the spatial-temporal structure of video frames, which can model the temporal information from raw pixels. Video Action Transformer Network (VAT) [16] proposes an end-to-end architecture to localize human actions in video clips, which uses a Transformer-style architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify. VTN [17] is a transformer-free model that classifies actions by attending to the entire video sequence information. TokShift [18] proposes the token shift module to model temporal relations within each transformer encoder, which is a pure convolutional-free video transformer model with computational efficiency of \(1\(1\times 1\). VidTr [19] proposes to use the standard deviation based top-K pooling for attention, which reduces the computation by dropping non-informative features along temporal dimension.


**Prompting-based learning.** Prompting-Based Learning (PBL) [26] is an emerging paradigm in natural language processing (NLP) [27][28][29][30][31][32][33][34][35]. It is first proposed in [27], which reformulates input examples as cloze-style phrases to help language models understand a given task. In this paradigm, a language model is trained to predict a task description in natural languages, and the language model can be used to perform few-shot or even zero","[ **Procedure Planning.** Procedure planning aims to control a task with multiple steps to complete a procedure, which has been studied in video understanding, such as action recognition [1][2], video classification [3][4][5][6], video tracking , video question answering [7], video summarization , and instructional videos [8][9][10][11].

**Convolutional Neural Networks for video understanding.** For recognizing actions, 3D convolutional networks [12][13] were proposed to directly learn from visual inputs. Recently, transformer [14][15] networks emerged and achieved a new benchmark in both the video action recognition [16][17] and video classification [18][19] tasks. However, they only learn from the sequential video input and ignore the transformable representations in the input frames, which may fail to capture the variations in the feature space. Compared to video action recognition and classification tasks, where the input visual contexts are mainly static, our procedure planning task is about long-term action planning for _observed_ visual contexts.

**Future action prediction.** This task aims to predict a desired action in the future [20]. Unlike procedure planning, the goal is to learn an action to fulfill a temporal sequence instead of planning a procedure to transform an initial state into a goal. The future action prediction task mainly focuses on short-term and long-term action prediction [21][22][23][24][25]. However, they focus on simply predicting actions instead of completing a procedure.

**Learning from text.** There have been few works leveraging text descriptions in vision models. Yao _et al_. [26] and Radford _et al_. [27] first show that only raw text can yield good performance on few-shot classification and natural language inference. Kirsch _et al_. [28] transfer text supervision from visual to vision models. Moreover, He _et al_. [29] bridge vision and language representations by learning visual concepts via prompting. Besides, our work also benefits from these works.

**Event-guided Procedure Planning.** Procedure planning is important in video understanding. Previous works [30][31] adopt a task-specific approach to supervise a task-specific network. However, such network designs are tailored to specific tasks instead of tackling general visual data. In this work, we adopt the event-guided paradigm to solve the general task, which is fully suitable for planning a procedure for any instructional videos with text supervision.

**Preventing co-adaptation of feature detectors.** Co-adaptation refers to a network model learns to adapt to highly similar inputs, which leads to poor performance on testing data [32][33][34]. Dropout [33] and Stochastic Depth []","<In the field of video understanding, there has been extensive research on leveraging instructional videos to facilitate the understanding of human decision-making processes. PlaTe [1] focuses on training a model with the ability to plan a goal-directed procedure from real-world videos, addressing the challenge of large semantic gaps in unstructured videos to avoid compounding prediction errors in decision-making. Similarly, Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning [2] emphasizes the importance of learning a single world model with Bayesian Inference and model-based Imitation Learning to distinguish various tasks, enabling better planning through the integration of environment structure with goals. These works provide valuable insights into the challenges and solutions for leveraging instructional videos for decision-making processes.

Deep learning architectures, particularly recurrent neural networks (RNN) and long short-term memory (LSTM), have demonstrated remarkable success in processing sequential multimedia data. Long-term recurrent convolutional networks [3] investigate the effectiveness of recurrent, or ""temporally deep"", models for large-scale visual learning, particularly for tasks involving sequences such as video recognition and narration challenges. Furthermore, Action Recognition in Video Sequences using Deep Bi-Directional LSTM With CNN Features [5] introduces a novel action recognition method using deep bidirectional LSTM network, emphasizing the processing of video data using CNN and DB-LSTM network for learning long-term sequences and processing lengthy videos. These works provide crucial foundations for understanding how deep learning architectures can be effective for handling sequential multimedia data.

Recent advancements in transformer-based models also offer promising approaches for video understanding tasks. For example, Temporal Shift Module for Efficient Video Understanding [7] presents an efficient and high-performing approach that achieves the performance of 3D CNN while maintaining 2D CNNs complexity, demonstrating the potential of leveraging temporal shift modules for video understanding at high accuracy and low computation cost. Additionally, Video Transformer Network [17] presents a transformer-based framework for video recognition that enables whole video analysis via a single end-to-end pass while maintaining competitive accuracy compared to other state-of-the-art methods. These works highlight the potential of transformer-based models for enhancing video understanding processes.

The application of novel training paradigms, such as prompt-based learning, presents a promising direction for video understanding. In particular, the systematic survey of prompting methods in natural language processing [26] introduces the paradigm of prompt-based learning, which allows language models to perform few-shot or zero-shot learning for adapting to new scenarios with few or no labeled data. This paradigm shows potential for adapting to video understanding tasks by leveraging prompting methods for efficient learning and adaptation. Furthermore, Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference [27] demonstrates the potential of semi-supervised training procedures that reformulate input examples as cloze-style phrases to help language models understand tasks, inspiring similar approaches for video understanding through prompt-based learning."">

**Note**: The related work above has been generated by drawing on the important sentences extracted from the given references and assimilating them to form coherent paragraphs. Each paragraph addresses different aspects and approaches related to the target paper's topic."
432," **Zero-shot Learning.** Zero-shot learning (ZSL) aims to recognize novel classes that are not seen during training [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35]. Most ZSL works focus on the image domain, and they typically align visual features with semantic features obtained from word embedding by the supervision of seen classes' annotations. For example, [36][37][38][39][40][41][42][43][44][45][46][47][48] focus on ZSL for image semantic segmentation. However, ZSL is not well explored for point cloud segmentation due to the limited information of point clouds. In this paper, we propose a novel multi-modal ZSL method to better utilize the complementary information of images and point clouds for more accurate visual-semantic alignment.

**Point Cloud Segmentation.** Point cloud is a sparse and unstructured 3D point representation. PointNet [49] is the first deep learning framework that directly takes raw point clouds as input and outputs a permutation-invariant feature representation. Recently, several works [50][51] have shown that word embeddings learned from large-scale text corpora can be used as a powerful feature representation for natural language processing tasks. Inspired by these works, we adopt the word-embedding-based visual feature representation as the visual feature for ZSL. Different from previous works [48][41] that focus on zero-shot image segmentation, we study the ZSL problem in the point cloud domain. We show that the rich appearance information in point clouds is a natural complement to the textureless point cloud, which is not explored in previous ZSL methods. We propose to leverage the rich information from both modalities to better align the visual features and semantic features for better ZSL performance. Moreover, our method can be trained in an end-to-end manner with only image and point cloud data, which makes it easier to scale up to large scale datasets. In addition, we conduct extensive experiments on two popular benchmarks, _i.e._, SemanticKITTI and nuScenes, and show that our method outperforms current SOTA methods on average for unseen class mIoU, which demonstrates the effectiveness of our method.
**Multi-modality Fusion.** The fusion of multiple modalities (_e.g._, images, point clouds, and LiDAR) has been widely explored in recent years [52][53][54][55][56][57][58][59][60][61]. In particular, [52] proposes a deep fusion method that fuses the features from two different modalities by concatenating them. [57] further improves the robustness of the fusion process by introducing a new loss function. [61] proposes to use imitation learning"," **Zero-shot learning (ZSL).** ZSL aims to recognize novel objects in unseen classes without any training samples. Early ZSL methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64].

**Generalized ZSL (GZSL) for semantic segmentation.** GZS3 [25] is the first work to address the problem of generalized ZSL, where both seen and unseen classes are faced at test time. It is a special case of GZSL where the unseen classes have no labels during training. It aims to transfer knowledge from seen classes to unseen classes by transferring knowledge from semantic embedding space to pixel-level. It can be divided into two main categories, _i.e._, transductive ZSL [19][22] and transductivistic ZSL. The former one assumes that both the labeled source and unlabeled target classes are seen during training, while the latter one assumes only the seen classes are available at testing time.

.
. The transductivity of ZSL has been studied in different ways, _e.g._[19][23] propose to learn a projection function between the feature space and the semantic space. [20][22], [23] and [24] learn the projection function from the source domain and apply it to the target domain without adaptation. [22] propose a joint learning approach which learns the shared model space and label space for zero-shot recognition. [19] propose an unbiased ZSL method to alleviate the bias problem in ZSL. [21] propose the self-training method to transfer the knowledge from the seen to unseen class. [31] propose uncertainty-aware learning method to solve the noisy and outlying training samples problem. [33] propose unbiased ZSL by learning an unbiased model from seen and outlier classes. [32] propose self-supervised learning method for ZSL which","[ **Zero-shot image classification.** Several attempts have been made to address the zero-shot image classification problem, in which the seen classes and unseen classes are in separate datasets [1][2][3][4][5][6][7][8][9][10]. Most of these works attempt to project semantic features into a compact feature space [11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32]. Siddiquie et al. [23] utilized a generative adversarial network to transfer knowledge from seen classes to unseen classes. Other works explored the shared visual space [22] or the common subspace [21] between the features of seen and unseen classes. Han et al. [30] employed an unsupervised generative model to obtain the visual feature of unseen classes.

To the best of our knowledge, there are limited studies focusing on zero-shot semantic segmentation [33][34][35][36][37][38][39][40]. Liu et al. [33] designed a convolutional network for instance segmentation with a data-dependent transductive loss. Zhu et al. [35] transferred knowledge from seen classes with an unsupervised representation using large-scale external text data and a multi-task training scheme. They enlarged the segmentation region by a predetermined fixed range. Liang et al. [39] extracted appearance features with transformer networks and then decoupled the prediction with the self-paced training strategy. Wu et al.  constructed the visual feature of the unseen class with an unsupervised learning method via object detections. These methods assume that the unseen classes are available for image-level segmentation, which is not possible for point cloud segmentation. Therefore, they are not applicable for our task.

**Zero-shot point cloud segmentation.** The zero-shot point cloud segmentation problem aims to address the point cloud classification task of unseen classes without the knowledge of them in the training phase. Early works conducted it with seen-class image as a prior [41][42][43][44]. Barbu et al.  utilized a generative network to synthesize images for unseen classes. Recent methods learn point cloud features by language pre-training [45][46][47][48]. The language representation [40] is used to describe the point clouds, with the visual representation in VL-BERT  used for points. The semantics of points are calculated with the word embedding model [49][50][51] trained on seen classes.

We transfer the visual knowledge from the seen classes to the unseen classes]","<Zero-shot point cloud segmentation has garnered significant attention in the field of deep learning, aiming to enable models to recognize novel objects in point clouds that were not seen during training. Recent approaches have focused on transferring knowledge from seen classes to unseen classes without labels, leveraging semantic representations [1]. However, a key limitation lies in the limited information available in point clouds for fully aligning with semantic features, prompting the need to explore richer appearance information from images [10]. To address this, the proposed method introduces a novel multi-modal zero-shot learning approach that leverages the complementary information of point clouds and images for improved visual-semantic alignment [27].>

<These challenges are in line with previous efforts in zero-shot learning (ZSL), where various methods have been proposed to accurately recognize objects of unseen classes, leveraging semantic embeddings and auxiliary knowledge of object classes. For instance, existing ZSL models primarily learn a projection function from a feature space to a semantic embedding space or attribute space, aiming to predict training seen class semantic representation or classification, but they often suffer from domain shift problems when applied to test data [2]. Other approaches have explored context-aware ZSL, leveraging semantic representations in a new way to model the conditional likelihood of an object appearing in a given context [3]. Additionally, generative models have been introduced for zero-shot learning using conditional variational autoencoders [4]. Further research has also focused on attribute-based image classification and latent embeddings for zero-shot classification to improve the state-of-the-art for various class embeddings consistently [6, 7].>

<The fusion of visual and geometric information from multi-modal data sources, such as LiDAR and cameras, has been an increasingly prominent area of research, especially in the context of 3D object detection and semantic segmentation. Known as sensor fusion, existing methods often rely on deep fusion, a robust LiDAR-camera fusion framework, and a unified sensor fusion framework for 3D detection, which highlight the importance of leveraging complementary information for robust and accurate segmentation [57, 58, 59]. In a related context, previous works have also explored transformer-based sensor fusion frameworks, aiming to integrate representations from different sensors effectively for various perception tasks [60, 61].>


"
3646," **Weakly-supervised action localization.** Most existing weakly supervised action localization methods [1][2][3][4][5] are MIL-based methods, where the predictions of unlabeled instances are supervised by classifying labeled bags. For example, Wang _et al_. [3] proposed a Sparse Temporal Pooling Network (STPN) to identify a sparse set of action-related frames and predict the temporal boundaries of human actions with no requirement of temporal localization annotations. 3C-Net [2] introduced a novel category count loss and a center loss to encourage the model to focus on the most discriminative parts of the action instances. W-TALC  proposed an attention-based model to learn the attention weights of video-level class labels to locate the most informative regions for each action instance. MAAN [4] proposed to suppress the most salient regions by a marginalized average attentional network. GAT [5] employed a generative attention model to model the temporal variations of feature semantics in a hierarchical latent space. However, these methods are all based on the classification-based MIL framework, which largely relies on the video classification to locate temporal regions. In contrast, our model learns the latent representations of video features in a temporal hierarchy based on their rates of change, and selects the change-points of the foreground as the boundaries.

**Unsupervised temporal action segmentation.** Recently, some unsupervised methods [6][7] have been proposed to solve the problem of temporal segmentation in long videos without the need of temporal annotations. TSHC [6] adopted a hierarchical clustering method to generate pseudo-labels of video frames to train a segmentation model. In this work, we propose a novel change-point detection module to detect the start and end points of action instances in a video, and a classification model to select the boundaries as the segmentation results. The difference between our model and these methods is that our model is a hierarchical-structured latent model, which is capable of learning the latent representation of videos in a temporally-structure latent space, while these methods only learn the representation of each video frame in a single-layer latent space without considering the hierarchical structure of video data. In addition, our method is able to discover the temporal variation of features in the latent space by the attention mechanism, while the above methods are only able to capture the short-term variations of features and ignore the long-term dynamics of features. In summary, our work is different from the existing methods in the following aspects: (1) Our model learns latent representations by learning the rate of change of features, which can discover the latent variations of video semantics, and (2) We propose an attention mechanism to locate and localize the temporal regions of actions, while most existing methods only locate the regions of action by video classification. (3) We introduce a novel MIL-model to learn temporal variations in latent representations, while previous methods only focus on learning the representations of each frame in the video. (4) We design a novel hierarchical-based latent model to discover temporal variations"," **Weakly-supervised action localization.** Weakly supervised action localization aims to localize temporal boundaries of actions and identify their categories simultaneously with only video-level category labels during training. Most existing methods are based on multiple instance learning(ML), where the predictions of unlabeled instances are supervised by classifying labeled bags. The MIL-based methods can be roughly divided into two categories, the classification-based and the attention-based ones. The classification- based methods [1][2][3][4][5][6][7][8][9][10] firstly use the classification labels to train the model and then use the attention mechanism to locate the action regions. For example, 3C-Net [2] introduces a novel formulation to learn discriminative action features with enhanced localization capabilities. MAAN [4] employs a novel marginalized average aggregation (MAAN) module to suppress the dominant response of the most salient regions in an end-to-end fashion. 3c-net [5] proposes to model the class-agnostic frame-wise probability conditioned on the frame attention using a variational auto-encoder (VAE) model.

**Unsupervised methods.** The unsupervised method [6] proposes a temporally-weighted hierarchical clustering algorithm that can group semantically consistent frames of the video by taking into account the time progression of frames where each cluster of frames may represent some action in the video. TWA-Net  proposes to learn the latent representations of video features in a temporal hierarchy based on their rates of change. T-Nets  proposes an unsupervision-based method to learn a hierarchical latent representation of video. However, these methods are not well studied on localization.


","[ Recently, weakly-supervised temporal action localization has attracted more attention due to its lower demand on annotations. Lots of methods have been proposed to solve this problem. This section reviews the works related to our work in a chronological order.

MANN [1] introduces a learning mechanism to tackle the action-context confusion problem. CST  introduces a change-point detector to detect spatio-temporal action regions by maximizing the conditional variational auto-encoder of target action. Weakly-Supervised Temporal Action Detection (W-TAD)  proposes a temporal dynamics tracking model to detect time-dependent spatio-temporal action instances. It incorporates a video-level classification with temporal class activation features. Attention-based models have been widely applied in this task to select the segments of interest, which utilizes deep neural networks to extract context-sensitive features. For instance, 3C-Net [2] builds up a multi-label center loss for localizing attention weights. STPN [3] uses temporal pooling to select action-related video segments. MAAN [4] proposes a marginalized average aggregation module to suppress the dominant response of the most salient segments. GAM [5] proposes a conditional VAE for both video classification and action localization.

However, most existing methods only focus on the detection of temporal variations of features. The majority of these methods only use the change-points to generate spatio-temporal bounding boxes and neglect the time-wise variations of feature semantics. So there is a large margin between the predicted time intervals and the actual time intervals of action instances. The detection of temporal variations of feature semantics is essential to localize action instances but is ignored in weakly-supervised action localization. Moreover, some weakly-supervised methods [6] for segmenting actions in videos do not use the video-level labels, which greatly reduces the annotation workload and helps the large-scale data training. Thus, we adopt the U-net-based instance segmentation method [7] in our framework, where it learns to identify the foreground objects. This method produces a reliable localization in the fully-supervised setting.

Despite the recent progress on hierarchical generative models, the majority of the works focus on image generation. CW-VAE [8] proposes a hierarchical generative model with time-varying latent states for future video prediction. NVAE [9] utilizes an attention module to learn the uncertainty in hidden layers of a variational auto-encoder. NWDA [10] adopts a generative predictive routing framework to infer the latent of interest through a generative process of event detection. Differently from them, we apply a hierarchical latent module to tackle the temporal variations of feature semantics, and our network outputs the predicted spatio-temporal action instances.

]","Step 1: Extracting important sentences from the cited papers...

- Paper [1]: ""The MIL-based methods are relatively well studied with coherent performance achieved on classification but not on localization.""
- Paper [2]: ""We propose a ... that only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count.""
- Paper [3]: ""We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks.""
- Paper [4]: ""In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions.""
- Paper [5]: ""Weakly-supervised temporal action localization is a problem of learning an action localization model with only video-level action labeling available.""
- Paper [6]: ""Action segmentation refers to inferring boundaries of semantically consistent visual concepts in videos and is an important requirement for many video understanding tasks.""

Step 2: Summarizing sentences and deciding on topics...

The topics for related work could be:
1. Weakly-supervised action localization methods
2. Temporal action localization algorithms using weak supervision
3. Challenges in weakly-supervised temporal action localization
4. Unsupervised action segmentation methods

<Based on the cited papers, the related work can be written as follows:>
<In weakly-supervised action localization, the use of multiple instance learning (MIL) has been a widely studied approach, focusing on classification rather than localization[1]. Some works propose weakly-supervised temporal action localization algorithms that can operate with only video-level supervision, mostly in the form of action category labels[2] or using convolutional neural networks[3]. However, challenges persist in accurately locating dense and integral regions for entire actions due to the overestimation of salient regions[4]. This highlights the need for improved methods in weakly-supervised temporal action localization. Additionally, in the realm of video understanding, unsupervised action segmentation methods have been introduced, aiming to infer boundaries of semantically consistent visual concepts within videos without requiring detailed annotations[6]. These approaches offer potential for addressing challenges in action localization by leveraging unsupervised learning techniques.>
"
277," **Multi-label recognition with partial labels.** Early works [1][2][3][4][5][6][7][8][9][10] mainly focus on the design of loss functions to address the partial label problem. For example, [1] introduces a class-aware selective loss for the MLR with incomplete labels. [2] proposes an orderless recurrent model for the multi-label classification with partial label. [3] proposes a semantic-aware representation blending method to enhance the feature representation of the partial labels, which is further extended by [10] with a simple and robust loss design.

**Vision-language models.** Recent works [11][12][13] have shown that the vision-language model, _i.e._, CLIP [11], can achieve remarkable performance on a wide range of vision tasks, _e.g._, image classification, object detection, and semantic segmentation. [13] proposes DualCoOp to adapt the CLIP model to the low-label regime. [11] proposes CLIP-V2 to learn transferable visual representations from the large-scale image-text pairs. [12] presents a prompt-based method to make the language model a better few-shot learner. [14][15] propose to automatically generate prompts to probe the knowledge in the pre-trained language model. Different from these works, we propose a novel Semantic Correspondence Prompt Network (SCP-Net), which can explore the structured semantic prior about the label-to-label correspondence via a semantic prior prompter. A Prior-Enhanced Self-Supervised Learning (PESL) method is further introduced to further enhance the use of the prior.

 proposes a method to learn the label correspondence in a self-supervised manner. However, their method is designed for the image classification task, and cannot be directly applied to the task of MLR. In this paper, we advocate remedying the deficiency of label supervision by deriving a structured semantic priorthrough the semantic correspondence.

 presents a method for multi-class classification with missing labels. They propose a multi-instance learning (MIL) framework to learn a joint embedding space for all the classes and a label space for each class.  proposes a multi instance learning (MICL) framework for the missing label problem, in which the missing labels are treated as negative samples and the classifier is trained to predict the negative label.  presents a multi label classification (MLC) framework, which aims to predict all the labels for the given image. In contrast, we aim to predict a set of all the correct labels for each image, and we do not require any negative samples or classifier for training.

 introduces a new multi-task learning (MTL) paradigm for the visual question answering (VQA) task. They formulate the VQA task as a binary classification problem, where the goal is to answer the question ""what color is the banana"" given the image \(x\). In this work, we adopt the MLC paradigm to solve the multi label recognition problem.

 first proposes a"," **MLR with Partial Labels.** The problem of MLR with partial labels was firstly introduced in [1], where a class-aware selective loss is proposed to estimate the class distribution using the partial annotations. [2] proposes to dynamically order the ground truth labels with the predicted label sequence. [3] introduces a semantic-aware representation blending module to learn more stable representation prototypes for each category and blends the representation of unknown labels with corresponding labels to complement these labels. [4][5] introduce curriculum learning based strategies to predict missing labels to prevent model from memorizing noisy labels.

Recently, [6][7][8][9][10] propose to explore the label-to-label correspondence in the vision-language model, _i.e._, CLIP [11], to compensate for insufficient annotations. In [6], a label graph superimposing framework is proposed for multi-label recognition by superposing label correlations from statistical co-occurrence information into the graph constructed from label priors. [7] proposes a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned to map this label graph into a set of inter-dependent object classifiers. [8] introduces the partial label classification loss that exploits the proportion of known labels per example, and exploits the proportions of known and unknown labels per image. [9] proposes two complementary transfer modules that explore within-image and cross-image semantic correlations to transfer knowledge from known labels to generate pseudo labels for unknown labels, and complement unknown labels respectively. [10] introduces two simple and robust loss design based on an observation that a model can identify missing labels during training with high precision.


","[ **MLR with Partial Labels.** With the inadequate number of annotated labels, partial-label MLR has attracted lots of attention in the community [1][2][3][4][5][6][7]. Previous works [8][9][10] propose to model the correlations among different labels with supervised losses. The latent correlations are rarely exploited. Recently, CLIP [11] pre-trained on the huge dataset of image-text pairs was introduced for transferring the image-level knowledge from visual to language domain. CLIP has been used to align the image-level representations and text-level representations and further obtain a visual-textual alignment matrix. After that, CLIP was used in MLR with partial labels [12][13] as a pre-trained visual-textual supervised model, which significantly boosts the performance. However, this type of learning can not be a general strategy because CLIP has been pre-trained on a specific vision-language task. Moreover, its significant shortcomings, such as model capacity [13] and expensive computation cost [12], are a significant barrier to the model application. In this paper, we attempt to realize such a general strategy without using CLIP pre-training.

**Semantic prior for Vision-Language model.** In the past years, many researchers have studied the use of semantic prior for vision-language models. In , their VLM-Net was developed to capture the semantic correspondence between visual and textual data. It takes the prediction of the images and textual descriptions as queries, and then the predictions of images and textual descriptions are organized to form semantic correspondences. In , the semantically aligned matrix is calculated via randomly selecting both positive and negative samples for training. As far as we know, this work is the only research which has utilized the semantic correspondence without any type of vision-language supervision, thus we believe this work is the closest to ours.

**Prompt for Vision-Language model.** Prompts are widely used to make the VLM perform various types of language tasks by masking some tokens in the prompts. Some researchers [14][15] have also explored the potential knowledge of language models in response to a pre-prompt with the prompt. In , they focused on the experiment in which they used prompts for general-purpose VLMs to obtain various types of language tasks. This work can be seen as a variant of prior work [14] which formulated the MLR with partial labels as a language task. In this paper, we will introduce a Semantic Correspondence Prompt Network to automatically generate prompts based on the structured semantic prior. In this way, we could reduce the workload of the human labelers and obtain much more natural prompts.

]","Step 1. Extract important sentences from the cited paper.

- Multi-label recognition (MLR) with incomplete labels is very challenging.
- Recent works strive to explore the image-to-label correspondence in the vision-language model, _i.e._, CLIP [22], to compensate for insufficient annotations.
- They generally overlook the valuable prior about the label-to-label correspondence.
- In this paper, we advocate remedying the deficiency of label supervision for the MLR with incomplete labels by deriving a structured semantic prior about the label-to-label correspondence via a semantic prior prompter.
- We then present a novel Semantic Correspondence Prompt Network (SCP-Net), which can thoroughly explore the structured semantic prior.
- A Prior-Enhanced Self-Supervised Learning method is further introduced to enhance the use of the prior.
- Comprehensive experiments and analyses on several widely used benchmark datasets show that our method significantly outperforms existing methods on all datasets, well demonstrating the effectiveness and the superiority of our method.

Step 2. Summarize sentences and decide on topics of related work.

The important sentences from the cited paper highlight the significance of addressing multi-label recognition with incomplete labels, the use of the vision-language model CLIP to compensate for insufficient annotations, and the development of a structured semantic prior for label-to-label correspondence. The paper introduces the Semantic Correspondence Prompt Network (SCP-Net) and a Prior-Enhanced Self-Supervised Learning method, demonstrating their effectiveness through comprehensive experiments and analyses. Based on this, the related work can focus on multi-label recognition with incomplete labels, vision-language models, and structured semantic priors in the context of image recognition.

Step 3. Write related work based on topics.

<Multi-label recognition with incomplete labels poses a significant challenge [1]. Previous studies have highlighted the complexities associated with insufficient annotations and the need to compensate for this limitation. One approach involves leveraging the vision-language model, CLIP, to explore the image-to-label correspondence and improve recognition performance [2]. While recent works have shown promise in this regard, they often overlook the valuable prior knowledge related to the label-to-label correspondence, which is crucial in addressing incomplete labels [3].>

<Vision-language models have gained attention for their potential in compensating for the deficiency of label supervision in multi-label recognition with incomplete labels. The Structured Semantic Prior introduced in [3] advocates for deriving a comprehensive understanding of label-to-label correspondence to enhance the recognition process. Additionally, the development of the Semantic Correspondence Prompt Network (SCP-Net) and the Prior-Enhanced Self-Supervised Learning method demonstrates a proactive approach towards addressing incomplete labels and utilizing structured semantic priors effectively [3, 9, 10].>

<In the realm of image recognition, the incorporation of structured semantic priors and vision-language models holds significant potential for enhancing multi-label recognition with incomplete labels. The studies by [3, 9] shed light on the importance of deriving a deeper understanding of label-to-label correspondence and the development of innovative methods, such as SCP-Net and Prior-Enhanced Self-Supervised Learning, to leverage structured semantic priors effectively. These findings contribute to the ongoing efforts in addressing the challenges posed by incomplete labels in multi-label recognition [3, 9, 12].>"
4667," **Image Restoration.** Image restoration is a long-standing low-level vision task, which aims to recover the high-quality images from the degraded images. Recently, with the development of DNNs, various DNN-based image restoration methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15] have been proposed. For example, Dong _et al_. [9] propose the first CNN-based method for SISR, which employs a three-layer convolutional neural network (CNN) to learn the mapping from LR images to HR images. To improve the generalization ability of CNNs to real-world degradations, some methods [2][6] propose to utilize the prior knowledge of natural images to guide the training of CNN models. Besides, some works [4][7] introduce attention mechanisms into CNNs for image restoration, which can selectively focus on the most relevant regions of the input images.

However, the above methods are all trained on the synthetic datasets, which cannot generalize well to the real-life degradation with different degrees or types. In this paper, we propose a novel training strategy to improve the performance of image restoration models on real-degradations by learning the distortion-invariant representation from the perspective of causality, so that the model can generalize better to the degradated images of different degrees and types. Moreover, we instantiate the intervention of each distortion with a virtual model updating based on corresponding distorted images, and eliminate the harmful confounding effect of each degrader from the meta-learning framework. In addition, we introduce the counterfactual distortion augmentation to simulate the interventions of different distortions from the optimization perspective, and learn the distortion invariant representation by eliminating the harmful confounders from the causal perspective, which is the first work to consider the distortion type and degree as one specific confounder in the causality perspective, to improve image restoration from a generalization perspective. Note that the proposed training strategy is orthogonal to the aforementioned methods, and can be combined with them to further improve the restoration performance of these methods, _e.g._, [4].

**Causal Inference.** Causal inference [16][17] has been widely used in various fields, such as machine learning and statistics, to analyze the effect of different confounding factors on the model performance. For instance, in image classification [18][19][20][21][22][23], it has been shown that the confounding factors can be identified and removed from the feature representations [24][25][26]. However, how to identify and remove them from the representation is still an open problem in image restoration. To the best of our knowledge, we are the first to introduce the back-door criterion in causality to address this problem in the image restoration task.

 propose a method for image denoising, which learns the feature representation by minimizing the distance between the input image and its flipped version. However, it cannot be directly applied to the image super-resolution task"," **Image Restoration.** Recently, DNN-based image restoration methods have achieved great success [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26]. In this section, we briefly review the most related works.

**Image Restoration from a Causality Perspective.** In recent years, image restoration from a causality perspective has been widely studied in the literature. In [2], the authors proposed the Dual Prior Learning (DPL) method to explicitly learn the blended distortion prior for blind and blurred image restoration. In, the authors introduced a method to encode the blur operator of an arbitrary dataset of sharp-blur image pairs into a blur kernel space, and then use the encoded blur operator for blind image deblurring. The authors in [7] proposed an alternating optimization algorithm to approximate the blur kernel in the encoded kernel space. In this paper, we introduce a novel training strategy to learn distortion-invariant representation for image restoration, and model the interventions of different distortion types and degrees as confounders in the optimization process.


","[ **Image Restoration** Our work is closely related to the image restoration and computer vision tasks. To tackle the real-world image restoration problems [1][2][3], the representation power of Convolutional Neural Network (CNN) [4][5] has been successfully employed for various scenarios. In particular, CNN-based methods have achieved outstanding results in image deblurring [6][7], image super-resolution [8][9][10][11][12] and image denoising [4][5]. In addition, more recently, researchers also paid great attention to real-world super-resolution [13][14]. However, compared with the real-world distortions, DNNs perform poorly due to the difficulty in tackling unfamiliar degradations.

**Causal Inference** Our work is also related to causal inference. It aims to study the effect of an intervention on a causal relationship between two variables. For example, Causal Analysis Diagrams (CAD) [15][16][17] are the most common causal diagrams in causal inference. Specifically, one variable (intervening variable) is used to change the distribution of another variable (efficiency variable) to evaluate the causal effect. Inspired by Causal Analysis Diagrams, some recent works [15][18][19][20] also propose to apply causal diagrams for general computer vision tasks. However, causal inference methods have not been employed for image restoration tasks yet, which is the main difference with our work.

**Deep Learning for Causal Inference** We review some recent works to introduce the powerful representation learning ability of DNNs for causal inference. For instance, Researchers designed causal inference to addressing self-supervised learning tasks [21][22][23]. Besides, causal inference has also been adopted to explore better feature representation for domain generalization [24][25][26]. Specifically, Zhang _et al_. [26] proposed a causal matching framework, which uses two groups of training samples for comparing the causal effects from different domains. Specifically, they derived the ideal conditions to obtain the optimal causal matching and proposed to formulate the matching into a bilevel optimization problem. On the other hand, Qi _et al_. [25] designed a graph-based causal matching mechanism to directly learn domain-invariant representation. They empirically found that their method could achieve favorable performance on domain generalization and domain adaption tasks. However, their method is not applicable to visual restoration tasks and does not explore the potential of causal inference for learning better representations.

]","<In recent years, there has been significant progress in the field of image restoration using deep neural networks (DNNs) [1, 3, 4, 6, 8, 9, 10, 12, 13, 14]. Several approaches have been proposed to improve the generalization ability of DNNs for unknown degradations, focusing on tasks such as denoising, super-resolution, and deraining [1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14]. These include pre-trained models such as the Image Processing Transformer (IPT) [1], Dual Prior Learning (DPL) [2], and MAXIM, a multi-axis MLP-based architecture called [3]. Additionally, work on real image denoising (RIDNet) has proposed a single-stage blind real image denoising network using a modular architecture with feature attention [4]. Researchers have also explored the use of causal inference in statistics to address the limitations of existing methods [16, 17, 24, 25, 26], particularly with the goal of learning invariant representations and domain generalization [25, 26].

The image restoration community has witnessed significant advancements in the development of pre-trained deep learning models, particularly transformers, for low-level computer vision tasks [1, 2, 12]. These models, exemplified by the Image Processing Transformer (IPT) [1], leverage large-scale datasets for training and have shown effectiveness in various low-level benchmarks. Furthermore, methods such as Dual Prior Learning (DPL) [2] have extended the capabilities of unsupervised single image restoration approaches by explicitly considering image and distortion priors to handle unknown blended distortion signals.

Work on real image denoising has focused on addressing the limitations of deep convolutional neural networks (CNNs) for real-noisy photographs through the proposal of novel single-stage blind real image denoising network architectures such as RIDNet [4]. RIDNet employs feature attention and modular architecture to improve practicality and visual quality, demonstrating superior performance against state-of-the-art algorithms.

Causal inference has emerged as a promising approach to address the generalization challenges in image restoration tasks, particularly through learning invariant representations for domain generalization [25, 26]. These approaches strive to mitigate the adverse effects of confounders, enabling the learning of causal features that are free from interference [24], and further extending to domain generalization tasks using causal matching algorithms [26]. These methods have shown potential in improving the generalization ability of DNNs for image restoration tasks, particularly in scenarios involving unknown degradations.>]"
4016," **LiDAR-based 3D Object Detection.** LiDAR based 3D object detection methods can be roughly categorized into point-based [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23] and voxel-based methods [8][12]. Point-based detectors directly consume the raw point cloud as input and directly predict 3D bounding boxes. PointRCNN [4] is the pioneering work that directly generates 3D proposals from raw point clouds. 3DSSD [5] proposes to use PointNet [1] to extract features from point clouds and then apply a 3D convolutional network to refine the proposals. Voxelization methods convert the point cloud to voxels and apply 2D convolutions to extract 3D features. SECOND [6] proposes a sparse convolution operator to reduce the computation cost. FSD [10] and FSD V2 [11] further improve the efficiency of the sparse convolutions. RangeDet [14] and RangeDet++ [15] propose range-conditioned dilated convolutions and range-aware convolutions, respectively. PointPillars [7] proposes the pillar-based representation to reduce computation cost and improve the performance. Recently, transformer-based approaches [9][17] have been proposed to improve the detection performance. CenterFormer [17] and CenterDet [18] are two representative works.

**3D Multi-frame 3D detection.** 3D-MAN [2021] and INT [22] propose a multi-frame detection framework that utilizes the temporal coherence of the input point clouds to improve 3D detectors' performance. However, these methods are designed for the online setting and cannot be directly applied to the offline setting. In this paper, we propose an offline detector that can be applied to both the online and the offline settings. Our detector is built upon a bidirectional tracking module and a track-centric learning module, which allows our detector to infer and refine a complete track once the object is detected at a certain moment. We refer this characteristic to ""on_Ce_detect_**_ior,_i.e._, our detector can achieve high-performance offline detection even when only a few high-quality frames are available for training, which is different from the conventional object-centric perspective of offline detection methods. Our method is also different from existing offline detectors in terms of the following aspects: 1) our detector is designed for offline detection, which aims to achieve accurate labeling of well-observed tracks; 2) we propose a novel tracking module to handle the low-quality tracking problem; 3) we design a novel learning module to propagate the predictions from high-frame annotations to low-frame ones, which can achieve better performance than the existing detectors in both the offline and the online settings; 4) we introduce a novel loss function to address the tracking-centric problem, which has not been studied in the offline detection literature."," **3D Object Detection.** 3D object detection has been extensively studied in recent years. Most of the existing methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35] are based on the object-centric perspective. In this paper, we propose a track-centric offline detector that is able to achieve high-performance in offline 3D detection.

**3D Detection from Point Clouds.** PointNet [1] is a pioneering work that directly consumes point clouds as input. PointNet++ [2] extends PointNet by introducing a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point cloud. VoteNet [3] proposes a deep Hough voting network to directly predict 3D bounding box parameters from raw point clouds. PV-RCNN [8] proposes to use 3D voxel convolutional networks to learn more discriminative point cloud features. FSD [10] and FSDv1 [11] propose a fully sparse 3D detector based on sparse voxels. VoxelNeXt [12] proposes an efficient and efficient framework to detect and track 3D objects entirely based on voxell features. PV3D [4] generates 3D proposals from raw points and refines them to canonical coordinates. PV2 [4], 3DSSD [5] and SECOND [6] propose to use 2D convolutions to detect 3D points. PV\({}^{2}\)RCNN  proposes a novel 3D proposal generation and refinement network to generate 3D point clouds for 3D box regression. PVVoxelNet  proposes to learn 3D features directly from raw 3D data. PVPNet [21] introduces a three-hierarchy framework with proxy points for multi-frame feature encoding, short-clip feature fusion, and whole-sequence feature aggregation. 3D-MAN [20] introduces an attention network to aggregate features from multiple perspectives and achieves state-of-the-art performance on Waymo Open Dataset. AFDetV2 [19] and CenterPoint [18] introduce a single-stage anchor-free detector to address the issue of insufficient receptive field in","[ **LiDAR-based 3D Object Detection.** With the development of 3D perception methods, LiDAR-based 3D object detection has been extensively studied. A typical solution is to encode point clouds into 3D voxels to feature points in 3D space [1][2][3][4][5][6][7][8]. Current approaches can be roughly divided into two categories: deep voxelization methods and sparse voxelization methods. Deep voxelization methods [1][9] aggregate deep features to multiple 3D voxels and then optimize 3D voxels through the backbone network.

Sparse voxelization methods [4][10][11][12][13][14][15][16] mainly keep single-scale sparse 3D feature maps and further dilate the feature maps to get 3D voxel features. Recently, some works focus on the design of a center-based detection head [17][18] for processing point clouds. Besides, AFDetV2 [19] was proposed to avoid duplicated region proposals in a single image. Some works [20][21] try to utilize multi-frames for LiDAR object detection. They can divide frames into different stages [20][22] or fuse the features of multi-frames with multi-views [21]. More recently, a two-stage detection approach Sparsely Embedded Detection (SECOND) [6] was proposed to solve 3D detection via feature-map free point-cloud feature descriptor.

**LiDAR-based Multi-object Tracking.** With the help of accurate object detection, multi-object tracking has received great attention. Tracking-by-detection [23][24][25][26][27][28][29] and tracking-by-estimation  have been studied in the literature. Recently, DEtrO [25] proposed to design a graph neural network for multi-object tracking. IMT [27] proposed a novel consistency-based loss to avoid pre-maturely stopping a tracklet. GNN3DMOT [25] used graph neural networks to improve the robustness of the tracklet-matching process. ET [29] tried to explore a simple 3D tracking method. However, these methods ignore the persistence of LiDAR points. SpOT [28] learned temporal and geometric models for temporal tracking. However, SpOT  is limited to tracking of objects under stable background. 3D tracking is greatly affected by ambient lighting changes and shadows.

**Automatic Labeling of 3D Objects.** Automatic labeling 3D objects [30][31][32] has attracted attention. Auto4D [31] presents a self-supervised learning]","<LiDAR-based 3D object detection has been a topic of significant research interest in the field of computer vision and autonomous systems. Various approaches have been proposed to address the challenges posed by the processing of point cloud data for object detection. One such approach, PointNet [1], introduced a neural network architecture to directly consume point clouds, respecting the permutation invariance of point clouds and providing strong performance for applications like object classification and scene parsing. PointNet++ [2] further improved upon this work by introducing hierarchical feature learning to capture local structures induced by the metric space of points, resulting in robust and efficient deep point set feature learning. VoteNet [3] proposed an end-to-end 3D object detection network based on deep point set networks and Hough voting, addressing the challenge of predicting bounding box parameters accurately from sparse scene points.>

<In the domain of LiDAR-based 3D object detection, the development of specific techniques has been observed. For instance, PointRCNN [4] presented a bottom-up 3D proposal generation method directly from point clouds and achieved remarkable detection results on the KITTI dataset. Furthermore, 3DSSD [5] introduced a lightweight point-based 3D single stage detector using a fusion sampling strategy and an anchor-free regression head, achieving a notable balance of accuracy and efficiency. Additionally, SECOND [6] investigated improved sparse convolution methods for 3D object detection, leading to state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.>

<Another important contribution is the development of encoding methods for point clouds to facilitate downstream detection pipelines. PointPillars [7] introduced a novel encoder using PointNets, which significantly outperformed previous encoders in terms of both speed and accuracy. PV-RCNN [8] deeply integrated 3D voxel Convolutional Neural Network and PointNet-based set abstraction, achieving high-quality object proposals and accurate estimation of object confidences and locations. Furthermore, Embracing Single Stride 3D Object Detector with Sparse Transformer [9] proposed a sparse transformer architecture to maintain the original resolution throughout the network, achieving state-of-the-art results on the Waymo Open Dataset.>

<In the context of 3D multi-object tracking, various works have emerged to improve the accuracy and efficiency of tracking systems. For example, Immortal Tracker [27] proposed a tracking system utilizing trajectory prediction to maintain tracklets for objects that are temporarily occluded or out of the field of view, aiming to avoid identity switches and achieving a competitive MOTA on the Waymo Open Dataset. Additionally, Super Sparse 3D Object Detection [23] introduced a fully sparse architecture termed FSD++ that leveraged temporal information to remove data redundancy and significantly reduce computational overhead, showcasing promising results on large-scale driving datasets. Moreover, GNN3DMOT [25] presented a tracking framework using a Graph Neural Network to improve feature learning for multi-object tracking, achieving more discriminative features for each object and leading to improved tracking performance.>

<In the area of active learning for efficient training of LiDAR 3D object detectors, Deep Active Learning [34] proposed a method to minimize human annotation efforts while maximizing the object detector's performance, achieving up to 60% savings in the labeling efforts while maintaining network performance. Furthermore, leveraging pretrained 3D object detection models for fast ground truth generation was explored in a study [35], which combined human supervision with pre-trained neural networks to generate high-quality ground truth annotations at significantly reduced human annotation times.>

<3D object detection and tracking are crucial components for various applications, including autonomous driving and robotics. The continuous development of novel architectures and algorithms in this field has led to significant improvements in accuracy, efficiency, and robustness. These advancements have paved the way for the realization of high-performance LiDAR-based 3D object detection and tracking systems with the potential to revolutionize the capabilities of autonomous systems in diverse real-world scenarios.>"
2369," **3D Semantics Datasets:** A number of 3D datasets have been proposed in the past few years. ScanNet [1] and RIO [2] are two of the first datasets to provide 3D semantic annotations for indoor scenes. ScanNets is a large-scale dataset of indoor scenes with 3D reconstruction and semantic annotations. However, ScanNet only provides semantic labels for a small subset of the 3D spaces in ScanNet. The Replica dataset [3] and Matterport3D dataset [4] are the two datasets that are the most similar to HM3DSem in terms of the number of spaces and object annotations. Both datasets are collected from Matterport 3D and ARKitScenes [3], which are both RGB-D datasets. The Matterport dataset is the largest dataset with semantic and instance annotations for 3D real-world spaces. It consists of 3,072 spaces and 3,100 rooms. The dataset has been used for several tasks such as 3D scene graph completion [5] and semantic scene completion [11]. However, both datasets do not provide pixel-accurate object boundaries. Replica [6] is a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. It is a synthetic dataset that is not publicly available.


**3D Simulation Environments:** There have been a number of synthetic 3D simulation environments that have been developed for training and evaluating 3D agents. AI2-THOR [8], CHALET [9], VirtualHome [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 49, 50, 51, 52, 53, 57, 58, 59, 60, 61, 63, 64, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 89, 90, 91, 88, and 89, 91]. These environments have been used to train and evaluate agents for a variety of tasks, such as object navigation [7, 8, 9, 10, 11], object re-localization [2], and action recognition [7][10][8][9][11][12][13][14][15]. These simulation environments have also been used in the Object Goal Navigation (OgNav) task [7] and the Object Navigation Challenge (Ongoing) [14].

The OpenRooms dataset [15] is an end-to-end framework for photorealistic indoor scene datasets. It provides 3D geometry, lighting, materials, and camera poses for a large number of indoor environments. It was used to develop the RoboTHOR platform [14] for the OGN task. It has been widely used for training agents to navigate in synthetic environments. The OpenRooftop dataset  is an extension of RoboTH"," **3D Semantics Datasets.** Significant progress has been made in 3D scene understanding with the release of large-scale 3D datasets [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]. However, most of these datasets are limited to indoor environments with a limited number of rooms and a small number of object instances. In contrast, the Habitat-Matterport 3D dataset is the largest dataset of 3D real-world spaces with densely annotated semantics that is currently available to the academic community. It consists of 216,646 object instance annotations across 216 3D spaces and 3,100 rooms. The scale, quality, and diversity of object annotations far exceed those of prior datasets.

**3D Scene Understanding.** The availability of large scale 3D indoor scene datasets has led to significant progress in the field of 3d scene understanding. The ScanNet dataset [1] is the first dataset with dense 3D annotations of indoor scenes. It contains 2.5M views in 1513 scenes annotated with 3D camera poses, camera poses and semantic segmentations. The 3D ScanNet contains a large number of objects annotated in each scene. However, the dataset is limited to a small range of room configurations. The Matterport3D dataset [4] contains 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. The dataset contains semantic segmentation annotations for each room in each room. The Replica dataset [6] is a dataset of indoor spaces with dense object annotations across 18 rooms and 18 scenes. Each scene contains a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The datasets in [6][4] are limited in scale and the number of semantic annotations is limited. The RIO dataset [2] contains 1482 3D scans of 478 environments across multiple time steps. It provides object instance re-localization annotations across different environments. The ARKitScenes dataset [3] contains 3D object detection, color-guided depth upsampling, and 3D semantic annotations for indoor scenes captured with a mobile depth sensor. The AI2-THOR dataset [8] contains near photo-realistic 3D panoramas from 194K panorama views of 90 indoor scenes","[ Prior attempts to construct large scale RGBD dataset use crowdsourcing [1][2] or hardware purchasing [3]. However, both methods suffer from poor data quality and low repeatability. To mitigate this problem, recent works introduce synthetic data or hybrid datasets that contain data from both simulation and reality [4][5][6][7][8]. All these datasets are rich in terms of semantic labels but are small in terms of total space and numbers of object instances. The quality of synthetic datasets is often questionable due to unrealistic physics, lack of control over scene appearance, lack of training of human operators, and inherent bias towards scenes suitable for the data generation process. Recent works use CAD files to create large-scale 3D space [9][10][11][12]. The layouts are then edited with artist tools or following an internally designed template [12]. Although this is a promising line of work, it does not scale as CAD files are a small fraction of the total online 3D models available .

Unlike prior work, HM3DSem supports 142,646 object instances across 216 3D spaces, making it the largest and most diverse 3D datasets available to the academic community. For example, the structure and style of rooms of HM3DSem are often unmatched by prior works such as Matterport3D [4] and Habitat . Additionally, HM3DSem data distribution is controlled and managed with care using a multi-step reviewing and QA process that is described in SS1.

Due to the object semantics described in image captions and being semantically consistent with 2D co-located image annotation, we use object semantic information instead of texture information to perform object segmentation and instance boundary annotation. Other recent large-scale 3D data curation efforts such as Scan2CAD [13] and RoboTHOR [14] also use object semantic information for object annotation.

The creation of OpenRooms [15] attempted to leverage the capabilities of human visual system (HVS) by letting the public to complete and submit furnished house scans. However, the editing phase of OpenRooms has a significant hand-in-hand operation to realize semantic changes in the resulting CAD files. The joint efforts of the human eye to classify and search for free-form objects are beyond the capability of 2D key-point detectors.

]","<The Habitat-Matterport 3D Semantics (HM3DSem) dataset [1] introduced in the target paper provides a valuable resource for the academic community by offering a comprehensive dataset of 3D real-world spaces with densely annotated semantics. The dataset's scale, quality, and diversity of object annotations surpass those of prior datasets, making it an important contribution to the field of 3D scene understanding. Unlike existing datasets such as ScanNet [1], Matterport3D [4], and 3D-FRONT [12], HM3DSem uniquely utilizes texture information to annotate pixel-accurate object boundaries, which enhances the dataset's effectiveness for various 3D scene understanding tasks [1]. The introduction of HM3DSem has demonstrated its impact on the Object Goal Navigation task, leading to increased participation in challenges and outperforming policies trained on previous datasets [1].>

<HM3DSem's introduction in the Habitat ObjectNav Challenge has resulted in a substantial increase in submissions, indicating its significance in driving research and innovation in the field [1]. While previous datasets like ARKitScenes [3] and Matterport3D [4] have contributed to advances in indoor scene understanding by providing high-quality RGB-D data, the HM3DSem dataset stands out due to its densely annotated semantics and large scale, making it a valuable resource for training and evaluating algorithms for 3D object instance re-localization, semantic scene completion, and 3D scene graph generation. Additionally, HM3DSem's use of texture information to annotate object boundaries sets it apart from other datasets, enhancing its potential applications for tasks such as 3D object detection and color-guided depth upsampling [3]. Furthermore, the dataset's dense annotations and pixel-accurate object boundaries make it well-suited for tasks requiring semantic understanding of indoor scenes, where previous datasets like ScanNet [1] and ARKitScenes [3] have been limited in terms of semantic annotations and scene views.>

<In comparison to other datasets such as Replica [6], AI2-THOR [8], and CHALET [9] that focus on training virtual agents in 3D environments, HM3DSem's emphasis on densely annotated semantics across 3D real-world spaces greatly expands the scope of data available for training and evaluating embodied AI models. Additionally, the HM3DSem dataset's extensive annotations and large-scale coverage create opportunities for advancing research in integrated semantic understanding, 3D object instance re-localization, and real-world environment mapping [13]. HM3DSem's availability and its impact on the Habitat ObjectNav Challenge underscore its potential to drive progress in the field of 3D scene understanding and embodied AI, providing a valuable resource for developing and evaluating algorithms for diverse tasks within this domain.>

<The introduction of the HM3DSem dataset represents a significant advancement in the field of 3D scene understanding and embodied AI, offering an extensive resource for academic research, algorithm development, and evaluation. In contrast to existing datasets such as ARKitScenes [3] and Matterport3D [4] that provide RGB-D data for indoor scene understanding, HM3DSem distinguishes itself by offering densely annotated semantics across a large number of 3D real-world spaces and rooms, with a focus on pixel-accurate object boundaries annotated using texture information. The dataset's wide applicability spans from 3D object instance re-localization and semantic scene completion to 3D environment mapping and object recognition tasks, making it an invaluable resource for training and evaluating deep learning models [11]. The scale and quality of HM3DSem's annotations provide a rich source for advancing research and pushing the boundaries of existing state-of-the-art methods.>

<HM3DSem's impact on the Habitat ObjectNav Challenge, leading to a significant increase in participation, attests to its influence in driving progress and innovation within the field. By providing extensive annotations and pixel-accurate object boundaries, HM3DSem offers a valuable resource for tasks such as 3D scene graph generation, 3D object detection, and 3D scene understanding, furthering the development and evaluation of algorithms for these tasks. The dataset's unique characteristics, such as its use of texture information for pixel-accurate object boundary annotations, distinguish it from existing datasets and position it as a pivotal asset for advancing research in 3D scene understanding, embodied AI, and related fields [5].>
"
2287," **3D Human Motion Capture Datasets.** The CMU-MMAC dataset [1] is a large-scale multi-modal dataset of full-body human motion captured in a variety of indoor and outdoor environments. The dataset consists of 2.5 hours of human-object interactions captured with a Kinect sensor. MoSh [2] and AMASS [3] are two large motion capture datasets that have been used to train and evaluate motion capture models. However, these datasets are limited in that they do not capture human-scene interactions. Total Capture [4] is the largest motion capture dataset that has been collected to date, but it does not capture interactions with the environment.

**3D Scene-Aware Motion Capture.** A number of recent works have focused on capturing human motion in the context of the environment [5][6][7][8][9][10][11][12][13]. For example, the work in [5] captures 3D human pose and 3D scene geometry using a marker-based motion capture system, and uses the scene geometry to resolve ambiguities in 3D pose estimation. In [6], a multi-camera system is used to capture and reconstruct dense human-environment interactions. The authors in [7] propose a motion capture method for human-chair interactions, where the human is constrained to sit on a chair while the chair is moving. The work in  captures human-human interactions in a virtual environment with a markerless motion capture setup. In contrast to these works, our work focuses on capturing full body reaching motions in a highly contextual virtual environment. In addition, we do not require physical scene construction in the real world, which is a major limitation of these previous works. The closest work to ours is [9], where the authors propose a scene-aware motion prediction model that predicts human motion conditioned on scene context. However their model is trained only on a small set of scenes and does not generalize well to more complex scenes. Our work, on the other hand, is the first dataset that captures full body motion in a rich contextual environment, paired with ego-centric scene information in various forms, such as RGBD videos, depth maps, and segmentation maps. We show that our dataset can be used for training a generative model that can generate human motions conditioned on both scene context and human pose information. Our dataset is also the first to provide a large scale dataset of human reaching motions paired with egocentric scene information, which has not been previously available in the literature. We provide a detailed comparison of our dataset with other existing datasets in Table 1. The BEHAVE dataset [10] consists of human object interactions captured from a single RGBD camera, but the interactions are limited to a single object. The EgoBody dataset [11] contains human body shape, motion, and social interactions from head-mounted cameras. While these works focus on social interactions, we focus on interactions with multiple objects and scenes. We present a dataset of 10 hours of high-quality human reaching motion from 5 subjects across nine scenes, paired both with ego and scene information."," **Human motion capture datasets.** The CMU-MMAC dataset [1] is a large-scale dataset of full-body motion capture captured in the context of cooking and food preparation. It contains full body motion captured in a kitchen environment. MoSh [2] and MoSh++ [3] are two of the earliest works to use markerless motion capture systems to capture human motion in a variety of scenes. MoSH [2], MoSh-MMC [3], and MoSH++ [4] use marker-based motion capture to capture full body human motion, but they are limited to indoor scenes. TotalCapture [5] and PROX [6] are the most recent marker-less methods that capture human-object interactions in indoor scenes, but their datasets are limited in terms of the number of subjects and scenes. BEHAVE [7] and EgoBody [8][9][10][11][12] are recent markerless methods for capturing human-human interactions in complex scenes. However, these methods are not suitable for capturing high-quality human motion and complex scenes, as they require the use of physical scene construction in the real world.

**Human motion synthesis.** Early work on human motion synthesis has focused on modeling human motion using hand-crafted features [13][14][15][16][17][18] and deep learning [19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][34]. These methods typically use RNNs to model human motion sequences, and generate plausible motion sequences conditioned on a categorical action, such as action recognition [27], action-conditioned motion synthesis [28], or text-based human motion generation [30]. However, they do not consider the context information of the scene. In contrast, our method is able to capture high-fidelity human motion from a varietyof scenes, and we use the context to generate high-resolution motion sequences.

","[ Human motion data.Robust human motion data captured at a high frequency can be valuable for many applications, such as biomechanics, sports, and human-computer interaction. Data-driven approaches for human motion are attractive because they can allow for reconstruction from a set of observed joints [1][2][3][4][5][6][7][8], human-object interaction [9][10][11][12], interaction with static environment [13], and hand-object interaction [14][15][16][17][18][19][20]. Previous research has achieved success in this task by utilizing deep neural networks [21][22][4][16][20], auto-regressive models [23], and variational autoencoders [24][25][26][27][28][29][30]. However, existing datasets do not contain contextual data. In this work, we propose a method to acquire realistic human motion data in a rich environment and generate motion from this data.

Contextual human motion data.The RGB-D Streaming Capture Dataset (R2D2) [1] contains full-body depth data of 247 scenes captured with one 1280x1024 depth camera. The Kinetics 400 dataset [31] captures a total of 10K 3D human poses (from 277 different people) from 367 real-world activities, which are acquired with multiple RGB and depth cameras. Unlike R2D2, which contains high-quality motion capture data and an RGB context, Kinetics only captures a static scene with minimal information about context. The same information as the Kinetics dataset is also captured in the H3.6M dataset [32], but in a more realistic fashion. This dataset captures full-body 3D human motion, along with depth and silhouette information, captured from over 3600 motion clips. However, the motion clips are captured with only a single camera and only 3600 subjects.

Contextual data that contains motion and environment information is valuable for human pose estimation. Pandey et al. [33] introduce the hand-held camera system for view-independent body pose estimation. Carreira et al. [11] present a method for 3D human pose estimation and multi-person motion understanding in human-object interactions. They acquire the full-body 3D human motion together with a 3D point cloud of the environment. However, no motion synthesis algorithms are used to generate new motions from these 3D human motion capture datasets. As aforementioned, these datasets either contain contextual motion data but lack explicit 3D motion information [31][33] or they do not contain full-body context]","<The field of human motion capture and synthesis has seen significant progress in recent years. Various approaches have been presented to overcome the challenges associated with capturing realistic activities in rich contextual environments. For instance, the CMU Multimodal Activity Database (CMU-MMAC) [1] presents a comprehensive collection of multimodal measures of human activity during cooking and food preparation tasks. This database has been instrumental in studying human activities and could serve as a benchmark for evaluating new algorithms. Additionally, the MoSh (Motion and Shape capture) approach [2] introduced a method for automatically extracting detailed body surface motion from marker data, leading to improved accuracy in capturing soft tissue motions directly from markers. Similarly, the AMASS database [3] unifies various optical marker-based motion capture datasets by representing them within a common framework, thereby addressing the challenge of integrating diverse datasets for building comprehensive models of human motion.>

<In the realm of 3D human pose estimation, the Total Capture algorithm [4] fuses multi-viewpoint video with inertial measurement unit sensor data to accurately estimate 3D human pose. This contribution showcases the significance of incorporating diverse sensor modalities for robust human pose estimation. Furthermore, the PROX method [5] leverages 3D scene information to better estimate human pose from monocular images. By enforcing Proximal Relationships with Object eXclusion, this method demonstrates the value of integrating scene constraints to enhance 3D human pose estimation.>

<The RICH dataset, introduced by ""Capturing and Inferring Dense Full-Body Human-Scene Contact"" [6], plays a fundamental role in addressing the challenges of inferring human-scene contact from a single image. The dataset contains multiview outdoor/indoor video sequences and ground-truth 3D human bodies, providing a valuable resource for training networks to predict dense body-scene contacts [6]. Additionally, the COUCH framework [7] significantly contributes to the synthesis of human-chair interactions by focusing on the fine-grained control of motion. This work expands the understanding of human-scene interaction and provides a benchmark dataset, thereby influencing the field's direction towards controllable human interactions.>

<The work presented in ""Markerless Motion Capture with unsynchronized moving cameras"" [8] introduces an innovative approach for markerless motion capture of articulated objects, creating new possibilities for tracking people with off-the-shelf handheld video cameras. Moreover, the dataset and method for tracking human object interactions, known as BEHAVE [10], presents a critical resource for modeling interactions between humans and objects. This challenging dataset opens new avenues for training models that jointly track humans and objects in natural environments with a portable multi-camera setup [10].>

<Furthermore, the EgoBody dataset [11] addresses the limitations of existing datasets for social interaction data by providing a large-scale, diverse dataset for understanding social interactions in complex 3D scenes. This dataset, collected using Microsoft HoloLens2 headsets, stands as a valuable resource for 3D full-body pose and shape estimation. Additionally, the GIMO dataset [12] significantly contributes to the study of human motion prediction by introducing a large-scale human motion dataset with high-quality body pose sequences, scene scans, and ego-centric views with eye-gaze data.>

<The Transformer Inertial Poser method [13] presents an attention-based real-time human motion reconstruction approach, emphasizing the significance of leveraging attention mechanisms in human motion reconstruction. Similarly, the work on learning and tracking cyclic human motion [14] and learning switching linear models of human motion [15] introduces effective models for learning human dynamics from motion capture data, addressing the intricate and rich dynamic behavior of the human figure.>

<In summary, the related work encompasses diverse approaches for human motion capture and synthesis, ranging from the development of comprehensive databases such as CMU-MMAC and AMASS to innovative methods for capturing and inferring dense full-body human-scene contact and markerless motion capture. These contributions have significantly advanced the understanding and modeling of human motion in various contexts, opening up new opportunities for research and application in the field of human-computer interaction and virtual environments.>"
1613," **Low-light Image Enhancement.** Low-light image enhancement has been extensively studied in the past decades. Early methods mainly focus on histogram equalization (HE) [1][2][3][4][5][6] and Retinex-based methods [7][8][9][10][11][12][13][14][15][16][17][18][19]. We refer readers to [20] for a comprehensive survey.

Recently, deep learning based methods [21][22][23][24][25][26] have been proposed to address the low-light enhancement problem. These methods can be roughly divided into two categories, _i.e._, end-to-end mapping networks and unfolding networks. End-toend mapping methods [22][25] directly map an input image to its enhanced version. For example, URetinexNet [25] and ADVNet [26] use a U-Net  structure to map an image into its gradient representation, and then employ an adaptive total variation (TV) module to generate the enhanced image. However, these methods construct the mapping networks heuristically, neglecting the intrinsic prior of image enhancement task and lacking transparency and interpretability. In this work, we explore the potential of customized learnable priors to improve the transparency of the deep unfolding paradigm, and customize the illumination and noise priors with MAE-based illumination and optimization priors in the proximal operator design of the unfolding architecture.


Masked Autoencoder (MAE).Masked autoencoders (MAEs) [27][28][29][30] are self-supervised auto-encoders that learn to reconstruct the input data from its masked representation. They have achieved remarkable success in the field of natural language processing (NLP) due to their powerful feature representation capability. Recently, MAE has also been introduced to the computer vision field. MVP [30] and BEiT [28] propose to use MAE to pre-train image transformers for visual question answering (VQA) and image captioning, respectively, and demonstrate that the pre-trained transformers can achieve state-of-the-art performance on VQA tasks. Inspired by the success of MAE in NLP, we propose to utilize it to customize the learnable prior in the unfolding paradigm for low-lighting image enhancement.

 propose a framework for image denoising and enhancement that employs MAE as a regularization term to constrain the optimization of the network. Different from [29], MAE is trained in an unsupervised manner and does not require any ground-truth labels for training. In contrast, our framework employs the labels of the training data to supervise the training of the learning process of the MAE, which enables us to leverage the powerful representation capability of MAEs to enhance the performance of low-Light Image Enhancement (LLIE).

 propose to learn a mapping function from a normal-light to an enhanced version of the same image, which can be regarded as a special case of LLIE task. However"," **Low-light image enhancement.** Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30] are mainly based on traditional Retinex theory [11][15], which decomposes an image into its illumination and reflectance components and then uses them to enhance the brightness and improve the naturalness of the image. However, these methods are heuristically designed and lack transparency and interpretability.

**Deep Low-light Image Enhancement.** Recently, deep learning-based methods have achieved remarkable progress in enhancing low-light images by leveraging the powerful feature representation capability of deep neural networks. These methods can be roughly divided into two categories: traditional methods and deep learning methods. Traditional methods mainly focus on the RetineX decomposition [15][11] and the learning based methods [10][12]. For example, Retineex-based decomposition methods [11] decompose an image to its illumination component and then use the illumination component to enhance its brightness. Retineox-based approaches [15] are based on the idea that an image can be decomposed into its reflectance component and illumination component, and then the illumination components are used to enhance their brightness and naturalness. For instance, KinD [11], a simple yet effective network is proposed to decompose the image into reflectance and illumination components and use them to improve the enhancement performance of low-level image enhancement, which is further extended in [10]. Retinext [10] proposes an end-to-end trainable network to learn the decomposition of image into illumination and reflection components. RetINE [10], a deep Retinelex-based method, proposes a decomposition-based network with a DecomNet for decomposition and an Enhance-Net for illumination adjustment. RetINet [17] proposes a progressive strategy to address the noise-induced noise problem in low-luminance enhancement. RetinExpert [22] proposes to estimate the illumination of an input image and use it to guide the learning of a network for enhancing the image quality. In addition, TBEFN [14] introduces a two-branch exposure-fusion network to tackle the challenge brought about by the blindness of input images, and a novel enhancement strategy is proposed for slightly and","[ The key properties of images captured under low-light conditions are low contrast and low visibility. Image enhancement aims to improve the illumination property of the captured images. In this work, we mainly review the low-light image enhancement techniques from two aspects, _i.e_., Retinex-based method and deep learning method.

**Retinex-based Methods.** Retinex model [1][2] defines low-light images as the lack of energy of the light under the illumination of the observed scene. It mainly consists of two components, the reflectance (the image of the object in the scene) and illumination (the energy of the light). Based on the Retinex model, the illumination is divided into two layers: _outside illumination_ and _inside illumination_. Inside illumination exists in low-light images and, due to the un-evenness of the source of light, presents biased results. Compared to the outside illumination, it is hard to correct the inside illumination. Usually, low-light enhancement methods suppress the inside illumination layer and focus on preserving the outside illumination layer. Previous works exploit many priors to constrain the residual brightness. These priors are introduced to reduce noise or the illumination influence.

**Deep Learning-based Methods.** Convolutional Neural Networks (CNNs) have been widely applied in recent years due to their success in many computer vision tasks. Previous works in low-light image enhancement [3][4][5][6][7][8][9] have tended to improve the visibility or reduce the noise. Later, researchers leveraged CNNs in low-light image enhancement [10][11][12][13][14][15][16][17][18][19]. These works focus on reconstructing the brightness range of the low-light image. To make up for the two missing components in the Retinex model, we propose to estimate the illumination and noise properties.

**Deep unfolding for Low-light Image Enhancement.** Besides Retinex-based methods [20], recently, researchers introduce the deep unfolding paradigm to improve the low-light image enhancement performance. By merging the expert-designed priors and supervised knowledge, it achieves impressive results. Cao et al. [21] apply deep unfolding for the joint enhancement of low-light image and video. The multi-frame fusion method enhances both spatial and temporal information. Wang et al. [22] associate the image with the expert-retouched image pairs to train a new illumination estimation module. While it performs illumination estimation, they use supervision to enhance the illumination of the input low-light image. The zero-reference curve estimator [23] can automatically]","<Low-light image enhancement has been a widely researched area, with numerous methods proposed for improving image quality in low-light conditions. Traditional methods such as brightness preserving dynamic histogram equalization (BPDHE) [1] and contextual and variational contrast enhancement [3] have been developed to improve contrast and brightness in low-light images. These methods focus on histogram modification and contextual information to enhance image quality in low-light conditions. Additionally, contrast enhancement techniques, including a weighted Histogram Equalization [4] and variational gamma correction model [6], have been proposed to address the challenge of over-enhancement and robustness in various image conditions.>

<Several recent advancements in low-light image enhancement involve the application of deep learning techniques. For example, Kindling the Darkness (KinD) [11] employs a network based on the Retinex theory to decompose and enhance low-light images, while DSLR (Deep Stacked Laplacian Restorer) [12] utilizes the Laplacian pyramid for global illumination and local detail recovery. Deep learning models such as Zero-Reference Deep Curve Estimation (Zero-DCE) [19] and Retinex-inspired Unrolling with Cooperative Prior Architecture Search (RUAS) [24] have also been proposed for image-specific curve estimation and optimization-driven solutions for image enhancement, respectively. Moreover, advanced techniques based on multimodal pre-training, such as BEiT (BERT Pre-Training of Image Transformers) [28] and MVP (Multimodality-guided Visual Pre-training) [30], have demonstrated impressive gains in visual pre-training by incorporating guidance from other modalities, especially in aligning token-level features with a pre-defined space.>

<In addition to these specific methods, BERT (Bidirectional Encoder Representations from Transformers) [27] has introduced a new language representation model designed to pre-train deep bidirectional representations from unlabeled text, while URetinex-Net [25] proposes a Retinex-based deep unfolding network for low-light image enhancement. Furthermore, UHDFour [21] presents a unique solution for Ultra-High-Definition (UHD) photo enhancement by embedding Fourier transform into a cascaded network to address the challenge of joint luminance enhancement and noise removal in UHD images. These developments showcase the diverse and evolving landscape of techniques for low-light image enhancement, encompassing both traditional and deep learning-based approaches.>

<Overall, the field of low-light image enhancement continues to evolve through the integration of advanced algorithms, deep learning models, multimodal guidance, and innovative methodological approaches. The wide range of methods and techniques discussed in the literature reflects the ongoing efforts to improve image quality in low-light conditions, addressing challenges related to contrast, brightness, noise, and detail enhancement. Collectively, these advancements contribute to the growing body of knowledge and resources available for enhancing image quality in low-light scenarios, catering to various applications within the domain of computer vision and image processing.>"
4649," **Image Captioning.** Image captioning aims to generate natural language descriptions for images and videos [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47]. Early works [9][18] adopt recurrent neural networks (RNNs) to encode images and sentences into a shared feature space and then decode them in an end-to-end manner. Later works [20][19] extend the encoder-decoder framework by introducing attention mechanisms to selectively focus on salient regions of the image for caption generation. To further improve the performance, various attention mechanisms have been proposed, such as selective attention [21], bottom-up and top-down attention [24], hierarchical LSTM with adjusted temporal attention [26] and self-critical sequence training [22]. Recently, transformer-based models [27] have been applied to image captioning and achieved promising results. For example, [28] proposes a Meshed-Memory Transformer (MST) to fuse multi-modal features via a memory module and [29] introduces a unified vision-language pre-training framework. [30] proposes to use a self-attention mechanism to fuse visual and textual features and [31] injects semantic concepts into transformer models.

**Radiology report generation.** Early works on automatic report generation [34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 84], 29, 30, 31, 32, 33, 34, 29, 36], 29-40, 31-32, 32-33, 34-35, 35-36, 37-38, 38-39, 40-41, 42-43, 43-44, 45-46, 47-47, 48-49, 49-50, 51-52, 52-53, 53-54, 54-55, 55-56, 56-57, 57-58, 58-59, 59-60, 60-61, 61-62, 64-69, 69-70, 70-71, 71-72, 72-73, 73-74, 74-79, 75-76, 76-77, 79-78, 80-79], and 79-80, 81-82, 82-83, 84-84, 90-89, 90, 91-82], which can be roughly divided into two categories: template-based and generation-based"," **Image Captioning.** Image captioning aims to generate natural language descriptions for images [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48]. Most of the existing methods are based on the encoder-decoder paradigm [19][18], which first generates a caption template and then fills in the missing slots with words generated from a pre-trained language model [13][18]. For example, [19] proposed a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions by sampling from the probability distribution of generating a word given previous words and an image. [20] proposed an attention-based model that automatically learns to fix its gaze on salient objects while generating the corresponding words in the output sequence. [24] proposed to combine the bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. [21] proposed adaptive attention model that decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. [25] proposed Neural Baby Talk, which can produce natural language explicitly grounded in entities that object detectors find in the image.


. [16] proposed BLIP-Diffusion, a subject-driven text-to-image generation model that supports multi-modal control which consumes inputs of subject images and text prompts. [4] proposed DeCap, a zero-shot captioning model that uses a large language model (e.g., GPT-3) [6] to provide context information for the captioning task. [5] proposed TCNet to generate dense video captioning using a dense captioning network and a non-autoregressive captioning method. [7] proposed NeRCo to enhance low-light image enhancement. [3] proposed G2L to achieve semantic alignment and uniform video grounding via Geodesic and Game Theory. [2] proposed SSVMR to enhance the ability to capture critical video clips by applying a semi-supervised self-training framework. [1]","[ **Cross-modal Retrieval.** Previous works on cross-modal retrieval [1][2][3][4][5] commonly use external knowledge bases (KBs) to perform self-training and adopt pre-trained cross-modal encoders such as CLIP [6] and DeiT [7]. Fan _et al_. [8] and Li _et al_. [9] utilized additional linguistic features (e.g., nouns) for text-to-image generation and captioning. Guo _et al_. [10] proposed to encode captions using spatial convolution. Recently, some works [11][5] employ image-to-text or visual-language models [12][13][14][15][16][17] to enlarge the pre-trained space, but are still limited to text-to-image generation tasks (e.g., image captioning and visual question answering). In comparison, we leverage the combination of translation models and unified visual-language models and then propose a translation network with visual-language alignment knowledge to address report generation.

**Image Captioning.** Image captioning aims to generate the corresponding captions of an input image [18][19][20][21][22][23]. Inspired by object detection methods [24][25], captioning models often focus on generating local captions for each region detected by an object detector. However, predicting global contextual information [26] for captioning is important for more robust generation. Recent works focus on multi-stream structure models and attention mechanisms [27][28][29][30][31]. Ma _et al_.  introduced attention pooling to boost the alignment of visual representations and textual embeddings. Wang _et al_. [31] injected semantic concepts into end-to-end image captioning models by utilizing a synonym dictionary to consider global contexts. In this work, we unify visual and textual features through cross-modal alignment knowledge and perform the unified model of cross-modal retrieval, image captioning and report generation.

**Medical Image Report Generation.** The task of report generation for medical images has attracted much attention in recent years. Tang _et al_.  presented an image-text model to generate a long report for each image. Gao _et al_. [32] extended medical image report generation by generating the detailed description of each anatomical region. Xue _et al_. [33] leveraged GANs to generate paragraphs of long text. In medical report generation, Wang _et al_. [34] introduced a hybrid reinforced generation-retrieval agent to enlarge the generated vocabulary. Zheng _et al_. [35] explored structural information in chest X-ray images and introduced a GNN]","<>
Automatic radiology report generation has been an area of significant research interest due to its potential to reduce the workload of radiologists [45]. The task involves generating detailed and coherent descriptions of medical images, which poses challenges in capturing visual differences, particularly between healthy and diseased cases. While existing image captioning methods have been applied to this task, they often lack mechanisms to effectively align textual and visual features to produce high-quality reports [47]. Moreover, the shortage of machine-learnable medical image datasets and the complexity of mimicking the high-level reasoning of human radiologists further complicate automated report generation for medical imaging [45].

To address these challenges, TieNet has been introduced as a Text-Image Embedding Network, aiming to extract distinctive representations from chest X-ray images and associated radiological reports [45]. The model leverages multi-level attention mechanisms within a CNN-RNN architecture to highlight meaningful text words and align them with the corresponding visual features. Furthermore, a cross-modal memory network (CMN) has been proposed to enhance the encoder-decoder framework for radiology report generation, allowing for the explicit alignment between images and texts to facilitate interaction and generation across modalities [46]. These approaches reflect the significance of capturing cross-modal alignments and leveraging textual information for improving the quality of automated radiology reports.

Recent advancements have also focused on the development of self-boosting frameworks that aim to improve radiographic report generation through the cooperative interaction of main and auxiliary tasks [44]. These frameworks have successfully integrated image-text matching and report generation branches within a single network model, emphasizing the importance of learning text-correlated visual features and generating high-quality reports [44]. Additionally, the competence-based multimodal curriculum learning (CMCL) framework stands out as a valuable approach for addressing the data biases and making the best use of available medical data [42]. CMCL simulates the learning process of radiologists, optimizing the model in a step-by-step manner to gradually improve its performance by selecting the most suitable batch of training instances and estimating the difficulty of each training instance based on the current model's competence [42].

In light of the challenges associated with automated radiology report generation, it is evident that advanced techniques such as self-boosting frameworks, cross-modal memory networks, and competence-based multimodal curriculum learning have emerged as promising approaches to address the complexities of this task. These advancements underscore the significance of cross-modal alignments, the cooperative interaction of main and auxiliary tasks, and the simulation of radiologists' learning processes to enhance the quality and accuracy of automated radiology reports."
1731," **Monocular Depth Estimation.** Monocular depth estimation (MDE) is a fundamental problem in computer vision. Recently, DNN-based MDE methods [1][2][3][4][5][6][7][8][9][10] have achieved great success. Eigen _et al_. [1] proposed a multi-scale DNN to predict depth from a single image. Laina _et.al_. [2] adopted a coarse-to-fine architecture with skip connections for depth estimation and surface normal estimation. Fu _et_. [4] introduced a fully convolutional network (FCN) for depth prediction with residual connections. Fu and Chen [9] proposed an ordinal regression network (DRN) to estimate depth from monocular images. To improve the quality of depth maps, some methods [10] adopted optical flow [7] as guidance. Inspired by the success of Vision Transformer (ViT) [11][12] in image classification tasks, several methods [13][14][15][16][17] adopted ViT-based architectures for MDE.

**Monocular Geometry Estimation with Supervision.** Supervised methods [18][19][20][21][22][23][24][25][26][27][28][29] require ground-truth normals for training, which are expensive to acquire. To address this issue, some works [20][22] adopted self-supervised learning with only depth map as supervision. For example, StructDepth [20] leveraged the structural regularities of indoor scenes. However, these methods are not suitable for monocular geometry estimation with weak supervision. In this work, we propose a novel CO-planarity regularized (CORE) loss function and a structure-aware normal estimator (SANE) to address this problem. CORE loss functions are designed to exploit locally linear depth-normal orthogonality under spherical coordinates as pixel-level constraints, and utilize adaptive polar regularization (APR) to resolve underlying numerical degeneracies. SANE easily establishes multi-task learning with CORE.

 proposed an iterative framework to jointly estimate depth and surface normals. It iteratively refines the depth map and the normals by minimizing the error between the predicted depth and the ground truth. In contrast, our method does not involve iterative refinement, and can be trained with only ground truth depth map. Moreover, we do not need to iteratively refine the depth and normals, which is more time-consuming.

 introduced an adaptive normal constraint (ANC) to improve depth estimation. ANC is designed to minimize the distance between predicted and ground truth normals in the feature space. In comparison, our proposed adaptive Polar Regularization can be applied to both feature space and pixel space, and thus is more efficient and effective.

 also proposed an adaptive depth normal (ADN) loss. ADN minimizes the difference between the estimated depth and normal in a feature space, while our method is designed for pixel-wise 3D geometry estimation.

 presented a depth estimation network (DIN"," **Monocular Depth Estimation.** Monocular depth estimation is a fundamental task in computer vision, which aims to predict pixel-wise depth map from a single image. Existing methods can be roughly divided into two categories: regression-based methods and classification-based ones.

**Regression-based Methods.** These methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] directly regress the depth map with the help of DNNs. For example, [8] first discretizes the continuous ground-truth depths into several bins and label the bins according to their depth ranges. Then, [6] leverage features extracted from high-performing pre-trained networks when initializing the encoder along with augmentation and training strategies that lead to more accurate results. [5] propose a semi-supervised approach to predict dense depth maps in a stereo setup using a direct image alignment loss. [10] utilize multi-scale local planar guidance layers located at multiple stages in the decoding phase. [15] exploit the multi-head attention mechanism to compute multi-level potential functions to capture the relationships between nodes in the graph. [16] propose to generate bins by a Transformer decoder and use the predicted bins as the intermediate representation to refine the predicted probability distribution. [17] introduce a skip attention module that predicts bins at the coarsest level using pixel queries.
-bins [13] and AdaBins [16], which use the Transformer [11][13] as the backbone for depth estimation. [14][12] utilize vision transformers [11] as a backbone for dense prediction tasks. [20] leverage the structural regularities exhibited in indoor scenes to train a better depth network. [29] propose an iterative refinement method to refine predicted depth map based on the predicted surface normal and its uncertainty. [21] introduce the aleatoric uncertainty in the prediction of surface normals to improve the quality of depth map. [27] propose the adaptive surface normal constraint to adaptively determine the reliable local geometry from a set of randomly sampled candidates to derive surface normal constraints. [22] propose SURGE to regularize 2.5D depth and surface normal estimation by predicting the likelihoods of planar region and planar boundary at","[ **Single-view Monocular Depth Estimation** Recently, many works [1][2][3][4][5][6] have used neural networks to directly regress depth maps from images. The pioneering work [3] proposes a fully convolutional network. Later, Dong _et al_. [4] introduces a residual-based architecture. Recently, [5] extends the CNN-based method into semi-supervised learning. Transfer learning methods [7][6] leverage pre-trained CNNs for depth prediction. The methods [8][9][10] using auxiliary loss on the training set usually achieve better performance. On the other hand, Transformer-based [11][12][13][14] and other works [15][16][17][18][19] are also applied to single-view monocular depth estimation.

**Surface Normal Estimation** Without ground-truth depth maps, an approach is to estimate surface normals to improve depth estimation quality. Cheng _et al_. [20] proposes a depth-normal regularized depth estimation network with global depth consistency loss. Some recent works [21][22][23][24] consider the relationship between surface normals and depth maps. Peng _et al_. [23][24] propose to jointly predict depth maps and surface normals using two-stream CNNs. The methods [25][26][27] also exploit the geometric constraints of 3D space to improve single-view monocular depth estimation performance. Some methods [28][29] firstly estimate depth maps and then utilize surface normals to regress the depth maps.

We are the first to exploit spatial local linearity of 3D surface normals under spherical coordinates as pixel-wise constraints for monocular depth estimation. Our SANE module is very general and can be integrated into most existing monocular depth estimation works. To our best knowledge, it is the first to estimate surface normals under local linearity under weak supervision.

]","Step 1: Important sentences from the cited papers:

1. 'Predicting depth is an essential component in understanding the 3D geometry of a scene' [1].
2. 'We address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling' [2].
3. 'Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging' [3].
4. 'Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction' [6].
5. 'We show that by introducing attention in various stages of the proposed model, we are able to infer inputs at multiple resolutions and then fuse them all into a single dense prediction' [12].
6. 'Our method achieves top performance on surface normal estimation and is on par with state-of-the-art depth estimation methods' [23].
7. 'Our method outperforms the state-of-the-art along occluding contours, while performing on par with the best recent methods for the rest of the images' [25].
8. 'We conduct extensive evaluations and comparisons using public datasets. The experimental results demonstrate our method outperforms the state-of-the-art methods and has superior efficiency and robustness' [27].
9. 'An extensive evaluation of our method shows that we set the new state of the art' [28].

Step 2: Summarize sentences and decide on topics of related work.
The important sentences cover various aspects of depth estimation and surface normal prediction from monocular images. The topics for related work could include:
- Depth estimation using single images and challenges involved.
- Techniques for surface normal estimation from monocular images.
- Methods that leverage attention mechanisms for improved depth and surface normal estimation.
- Comparison of state-of-the-art methods for depth and surface normal estimation.

Step 3: Write related work based on the summarization:

<In recent years, depth estimation from single images has garnered significant attention due to its significance in understanding 3D scenes. Many methods have been proposed to tackle the challenges associated with the inherently ambiguous and ill-posed nature of depth estimation from monocular images [3] [5]. One key challenge is the accurate reconstruction of surface normals, which is crucial for predicting high-quality depth maps. Techniques such as leveraging attention mechanisms and employing multiple resolutions have shown promise in addressing these challenges and improving the accuracy of depth estimation [12] [23]. The state of the art in depth estimation has been continuously pushed forward with innovative methods that outperform existing approaches [27] [28].>

<On the other hand, surface normal estimation from single RGB images is another crucial aspect of understanding the 3D geometry of a scene. The accurate prediction of surface normals is essential for tasks such as scene understanding, reconstruction, and augmented reality. While various methods have been proposed for surface normal estimation, the accuracy and reliability of the predictions remain important areas of research and development in computer vision [2] [19]. Methods focusing on occluding contours have particularly demonstrated the potential to improve the accuracy of surface normal estimation [25]. The evaluation and comparison of these methods on public datasets have highlighted their superior efficiency and robustness [27].>

<Additionally, the incorporation of attention mechanisms in depth and surface normal estimation has shown promising results. Attention-based models have addressed the improvement of accuracy and detailed prediction for both depth and surface normals, with some methods achieving state-of-the-art performance [12] [23]. Moreover, these innovative techniques have not only outperformed existing methods but have also demonstrated superior efficiency and robustness, making them pivotal in the field of monocular depth and surface normal estimation [27] [28].>

Overall, the related work has been organized according to different aspects of depth and surface normal estimation such as challenges in depth estimation using single images, techniques for surface normal estimation from monocular images, and methods that leverage attention mechanisms for improved depth and surface normal estimation. The comparison of state-of-the-art methods for depth and surface normal estimation has also been highlighted."
3768," **Knowledge Distillation.** Knowledge Distillation (KD) [1][2][3][4][5][6][7][8][9] aims to transfer the knowledge from a teacher network to a student network. The teacher's logits are used as soft labels to guide the student, and KD has been widely used in various computer vision tasks. FitNet [10] and its variants [11][12][13][14][15][16][17][18] use the intermediate feature maps of the teacher as the teacher's knowledge. FitNets uses the feature maps from the last convolutional layer of the network as the soft labels, while FitNet-A [11] uses the attention map from the top-most feature map. After that, many variants of feature distillation have been proposed to improve the performance of KD. For example, RKD [13] proposes to use the relation between the teacher and the student to distill the knowledge. CKRD [14] uses contrastive learning to learn a contrastive representation for the teacher. TASD [16] proposes a target-aware transformer to transfer knowledge from the teacher to the student.

**Self-KD.** Recently, self-knowledge distillation [19][20][21][22][23][24][25][26][27][28] has attracted increasing attention because it does not require a teacher. Be Your Own Teacher (BYOT) [19] is the first self-distillation method, which uses the output logits of the student as the knowledge for distillation. It uses the logits from a pre-trained teacher to supervise the training of a student. However, BYOT needs to train the teacher for a large number of epochs, which is computationally expensive. To reduce the training cost, some variants of BYOT are proposed, such as collaborative learning [20] and label smoothing regularization [26][23]. Label Smoothing Regularization (LSR) [26] is a simple yet effective method to reduce the computational cost of training a student model. LSR minimizes the difference between the student's non-target logits and teacher's target logits, but the sum of the two logits is different, preventing them from being identical. To solve this problem, LSR normalizes the sum to equalize their sum and uses the difference as the KD loss. Recently, Zhang _et al_. [27] propose a one-pass self distillation method based on Zipf's Law [28] to solve the problem of LSR. They use the Zipf distribution to normalize the sum and minimize the difference of logits. MixSKD [24] distills knowledge from Mixup data by using the labels of the mixup data as the self-supervision of the training data.

 proposes a new self-learning framework, which does not need a teacher and can distill knowledge from any data. It generates customized soft labels for both target classes and non-targets, and uses them to train a student without using any soft labels."," **Knowledge Distillation.** KD [1] was first proposed by Hinton _et al_. [1], which transfers the logits of the teacher network as soft labels to guide the training of the student network. It has been widely used in many computer vision tasks [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28]. The logit-based KD methods minimize the KL divergence between logits from the teacher and student networks. However, they suffer from the problem that the teacher's logits are usually larger than the student's, which may lead to a mismatch between the two logits. To address this problem, some methods [3][7] minimize the logit divergence between the student and teacher's feature maps, which are more suitable for object detection tasks. Some other methods [8][5] use the feature maps of the same class as the teacher, but use them as the soft labels. However these methods require the teacher to supervise the student, which limits their applicability to self-KD.


**Self-Knowledge.** Self-Kd [21] is a self-distillation method, which trains a student network progressively to distill its own knowledge without a teacher. It generates customized soft labels for both target and non-target classes. It uses the teacher logits logits as the supervision, but it requires a real teacher to provide the soft label for the target class, which is different from the target classes in the target-non-target setting. It is also different from KD in that it does not require a teacher and generates soft labels from the student logits without the need of a teacher, which makes it more general and flexible. However it still requires the teacher for soft labels, and it is not suitable for the self-knowledge distillation task.

 is a recent work that proposes a self knowledge distillation method. It also uses the same logit as the target logit, but generates the soft-label for both the target and the non-tokens. It differs from the self KD in two aspects. First, it uses the log-it as soft-labels, which does not have the difference between the target label and the other classes. Second, it generates soft-logits for the same category as","[ **Knowledge Distillation** aims to transfer knowledge from a cumbersome teacher model to a lightweight student model, where the teacher logits are utilized to guide the student network for knowledge distillation. Different from [1] in which the student network is optimized with a single temperature-scaled cross-entropy loss, Huang _et al_. [2] proposes two KD losses: the label regression loss for each student logit, and the feature regression loss that encourages the student logits to match the corresponding teacher logit. It is revealed that these two losses do not have the same loss scale due to the different underlying distributions, and knowledge distillation learns a mismatch between the student logits and teacher logits. Therefore, KD adjusts these mismatches differently [3][4].

Practical KD variants have been designed for better knowledge transfer. Gao _et al_. [5] proposed to add more teacher networks to expand the discrepancy between the soft target and student logits. Yuan _et al_. [2] pointed out that KD forces the logits of non-target classes to match the soft labels of non-target classes, and proposed to weight logits by adopting the entropy of logits instead of the distribution of labels. Gao _et al_. [6] focused on different heterogeneous models to imitate better. Chen _et al_. [7] examined KD in different directions. Zhang _et al_. [8] highlighted the necessity of more robust KD. Specifically, their method focuses on distilling in the foreground and background. Liao _et al_. [9] provided some practical guidelines to improve KD in different domains. Sun _et al_. [10] evaluated the performance of a network without Hessian matrix. They make a normalization to smooth the Hessian matrix. Lee _et al_. [11] devised a method to improve the accuracy of non-target classes. Chen _et al_. [12] proposed to deal with different types of non-target classes. [13][14][15][16][17] focused on transferring the teacher's knowledge to the student's more abstract representation.

**Self-Knowledge Distillation** trains a network in a zero-shot way. The essential difference between SKD and KD is that the student logits of SKD are generated from the feature maps of the student network, while the student logits of KD are generated from the feature map of the teacher network. SKD uses the student's own feature map as the supervisory signal to distill its knowledge.

Most previous work is designed to improve student networks [18][19][20][21][22][23]. Han _et al_. [20] proposed to replace the student feature map with a new one to enhance distillation results. Sarkar _et al_.]","Step 1. Sentences extracted from the cited paper:

1. ""Knowledge Distillation (KD) uses the teacher's logits as soft labels to guide the student, while self-KD does not need a real teacher to require the soft labels.""
2. ""This work unifies the formulations of the two tasks by decomposing and reorganizing the generic KD loss into a Normalized KD (NKD) loss and customized soft labels for both target class (image's category) and non-target classes named Universal Self-KD (USKD).""
3. ""It can be generally used for KD and self-KD to better use the soft labels for distillation.""
4. ""For KD with teachers, NKD achieves state-of-the-art performance.""

Step 2. Summarized sentences and topics for related work:
The extracted sentences reveal that the target paper focuses on the unification of Knowledge Distillation (KD) and self-KD, introducing Normalized KD (NKD) loss and Universal Self-KD (USKD) to improve the use of soft labels [1]. The work also emphasizes the effectiveness of NKD for KD with teachers, achieving state-of-the-art performance [1].

Related Work Topics:
1. Knowledge Distillation and Self-Knowledge Distillation
2. Normalized KD (NKD) and Universal Self-KD (USKD)
3. Use of Soft Labels for Distillation

Step 3. Related work:
<<
Knowledge distillation (KD) and self-knowledge distillation have gained significant attention in the field of machine learning [1]. The use of the teacher's logits as soft labels to guide the student in KD has been a notable technique, and the introduction of self-KD, which eliminates the need for a real teacher to provide soft labels, has further expanded the distillation methodologies [1]. The work presented in the target paper unifies these two tasks by introducing a Normalized KD (NKD) loss and customized soft labels for both the target class and non-target classes through Universal Self-KD (USKD) [1]. This unification allows for the better utilization of soft labels for distillation, presenting a unified approach to the use of soft labels in the distillation process [1].

The introduction of Normalized KD (NKD) and Universal Self-KD (USKD) in the target paper signifies a significant advancement in the field of distillation techniques [1]. By decomposing and reorganizing the generic KD loss, the paper proposes the use of NKD and USKD for both KD and self-KD, aiming to enhance the efficacy of using soft labels for distillation [1]. Furthermore, the success of NKD in achieving state-of-the-art performance in the context of KD with teachers underscores its potential impact on improving distillation methodologies [1]. The unified approach presented in the target paper provides a comprehensive framework for leveraging soft labels in the distillation process, offering potential advancements in model compression and performance improvement [1].
>>"
1915," **Human pose estimation.** Human pose estimation has been extensively studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]. Most state-of-the-art human pose estimation methods can be categorized into top-down and bottom-up approaches. Top-down approaches [5][1][6] directly predict the locations of all the keypoints in an image and then group them into a set of persons. On the other hand, bottom-ups [2][7] first estimate the keypoint locations of each person in the image, then group these keypoints into persons, and finally predict the positions of each keypoint in each person. In this paper, we focus on the top-up approach.

**Human pose refinement.** The goal of human pose refinement is to refine the prediction results of the network to improve the performance of the prediction. The key idea is to learn a network that can generate feedback information from the test sample and correct the prediction error on the fly for each individual test sample. For example, Wei _et al._[9] proposed a multi-stage refinement network to learn the refinement network for each person individually and then refine the predictions iteratively based on the feedback information. However, this method is not applicable to the generalization problem since the network needs to be retrained for each test sample, which is time-consuming and computationally expensive. PoseFix [10] is a model-agnostic general pose refinement network, which can be applied to any network architecture. It can be trained on a large number of test samples, but it is not suitable for the generalizable problem since it needs to retrain the network for every test sample in advance. In contrast, our proposed SCAI method can be used for generalizable inference, and the prediction result of any network can be corrected based on a self-correctable and adaptive inference network. In addition, our SCAI can also be used as a loss function to quickly adapt and optimize the correction network during the inference process, which has not been explored in previous work. In [8], the authors proposed an iterative error feedback (IEF) method, which iteratively corrects the network prediction based on feedback information generated from the output of the previous network prediction. This method is designed for image classification, while our method is specifically designed for human pose regression. In particular, we use the self-referential feedback error as a feedback to guide the correction process, and we show that it is highly correlated with the actual prediction error, which enables us to use this feedback as a correction loss to quickly optimize the prediction network.

 proposed a test-time adaptation (Tent) method to adapt a model to new and different data during testing. The main idea of TENT is to train a model on the training data and then use the trained model to adapt to test data by minimizing the test loss on the test data. The test loss is defined as the difference between the test error and the training error, and it can be minimized using entropy minimization"," **Generalizable Human Pose Estimation.** Most state-of-the-art human pose estimation methods [1][2][3][4][5][6][7][8][9][10][11] focus on improving the accuracy of single person pose estimation by introducing an additional refinement step to refine the prediction of the single person [12][13][14][15] or multi-person pose estimation [16][1][7]. For example, Wang _et al_. [9] proposed a PoseRefiner network to directly predict a refined pose from the input image and the given pose estimate, which is further improved in [10] by introducing a model-agnostic pose refinement network. In [11], a self-constrained prediction-verification network was proposed to improve the pose estimation accuracy by partitioning the human body into structural groups and performing forward and backward predictions between these keypoint subsets. However, these methods are not designed to address the generalization challenge of network prediction, which results in degraded performance in generalization.

**Test-time Training.** Test-time training (TTT) [12] is a general approach for improving the performance of predictive models when training and test data come from different distributions. In this approach, the model is trained to optimize the model parameters online before making a prediction, and the model can be adapted to the test data by updating its parameters before making the prediction. This approach has been successfully used in many tasks, such as image classification [12], semantic segmentation [13], and motion capture [14]. However, it is not suitable for pose estimation due to the lack of feedback information from the test sample. In our work, we introduce a self correcting network to generate feedback information for the network prediction error, which can be used as feedback to quickly adapt and optimize the correction network during the inference process.

","[ **Human pose estimation.** Human pose estimation is one of the most challenging tasks in computer vision, and it has been the focus of many related works [1][2]. Some works [1][3][4] adopt a top-down approach by first estimating joint locations and then inferring the corresponding 3D joint coordinates. Our work also adopts a top-down architecture, which consists of three modules: coarse prediction, refinement, and keypoint aggregation. The coarse module first predicts the keypoints with a pre-trained cascaded pyramid network (CPN) [1]. Then, a refinement module refines the initial coarse results using two upsampling layers and a residual skip connection. Finally, a keypoint aggregation module maps the input to an embedding space to represent the pose information.

Several recent works also propose to adopt the bottom-up approach, which is similar to our approach. The major differences are mainly in the architectures of the coarse and refinement modules. Some works [5][6][7] adopt the stacked hourglass (SH) architecture to predict the keypoints, while our approach uses a proposed pose refinement network (PRN) to refine the initial keypoint predictions. These two approaches share that they first predict the initial keypoint locations and then refine the results. Our approach differs in that we predict the aggregate poses directly and learn to refine the aggregate poses from the prediction errors.

**Generalization problem.** Although the _test set_ predictions are usually well-aligned with the _train set_ predictions, which indicates that the model has good generalization capability, the prediction errors on the _test set_ are sometimes very large and unreasonable. This phenomenon indicates that there is a generalization problem for predicting the _test set_ images.

Previous works have proposed several methods to solve this generalization problem. Often these methods are complementary, and the combination of multiple methods is able to improve the performance. A self-correctable and adaptable inference method may also be able to help solve this generalization problem.

**Correction network.** The core of our method is the correction network, which learns to map the prediction error to the original input domain. It can also be regarded as a self-reference network which generates its own feedback, but for the purpose of improving the prediction accuracy. For this reason, we name it a self-referential feedback error network (SFCN). This network structure is similar to that of an encoder-decoder network in  and , and the usage of feedback is similar to that of an auxiliary loss or regularization in [8][9].

A previous work [10] also uses a correction network to improve the performance of the refinement network. Instead of using the correction network to map the original image to the predictive domain as we do, the original image is used as the ground truth to generate the feedback. In]","Step 1: 
Important sentences from the cited paper:
1. A central challenge in human pose estimation is the generalization problem, where the learned network lacks the capability to characterize the prediction error, generate feedback information from the test sample, and correct the prediction error on the fly for each individual test sample.
2. The cited paper introduces a self-correctable and adaptable inference (SCAI) method to address the generalization challenge of network prediction and uses human pose estimation as an example to demonstrate its effectiveness and performance.
3. The SCAI method learns a correction network to correct the prediction result conditioned by a fitness feedback error generated by a learned fitness feedback network, which is highly correlated with the actual prediction error.
4. The study suggests that self-referential feedback error can guide the correction process and also be used as a loss [8].

Step 2:
The important sentences indicate the focus on addressing the generalization problem in human pose estimation through the SCAI method, which involves self-correctable and adaptable inference, fitness feedback error, and correction network.

Step 3:
<The field of computer vision has seen significant advancements in human pose estimation, with various approaches aiming to enhance prediction capabilities. One prominent challenge in human pose estimation is the generalization problem, where learned networks often struggle to characterize prediction errors and provide corrective feedback for individual test samples. To address this, recent research has introduced the self-correctable and adaptable inference (SCAI) method, which focuses on improving network predictions by accommodating feedback information and correcting errors in real-time for each test sample [8]. This approach demonstrates promising effectiveness and performance in human pose estimation tasks, shedding light on the potential for more robust and adaptable inference methods in the field.>

<Related work has also explored methods to improve multi-person pose estimation. For instance, the Cascaded Pyramid Network (CPN) introduces a novel network structure to address challenging cases such as occluded and invisible keypoints in multi-person pose estimation, achieving state-of-the-art results on benchmark datasets [1]. Similarly, the Unbiased Data Processing (UDP) approach aims to tackle biased data processing in top-down pose estimators, enhancing performance and promoting result reproducibility [2]. These works contribute valuable insights into addressing specific challenges in multi-person pose estimation, complementing the efforts to improve generalizable human pose estimation through methods like SCAI [8].>

<Furthermore, the enhancement of information for multi-person pose estimation has been a focus of research, as evidenced by the proposal of novel modules to improve channel-wise and spatial information in feature maps [4]. This indicates a growing interest in refining the foundational aspects of pose estimation methods to improve overall performance. In addition, studies exploring the potential of iterative error feedback and pose refinement networks highlight efforts to refine existing pose estimation models and address the limitations of state-of-the-art approaches [9, 10]. These approaches align with the goal of achieving more accurate and adaptable pose estimations, reflecting the broader trend observed in research on human pose estimation.>

<In the context of addressing the generalization problem, the cited paper introduces a self-correctable and adaptable inference (SCAI) method. This approach is built on the premise of learning correction networks to rectify prediction results based on feedback errors, thereby enhancing the adaptability of network predictions for individual test samples. Moreover, the correlation between fitness feedback errors and actual prediction errors underscores the potential utility of self-referential feedback information in guiding correction processes and serving as a loss, contributing to the growing body of work on improving human pose estimation and generalizable network predictions [8].>"
2936," Transformer [1] has achieved great success in natural language processing (NLP) [2][3] and has recently been extended to the computer vision domain [3]. The core building block of the Transformer is the self-attention mechanism, which is a powerful tool to capture long-range dependency. However, the quadratic computational complexity of the attention mechanism makes it infeasible to apply it to large-scale vision tasks. To address this problem, a series of works [4][5][6][7] have been proposed to reduce the computation and memory cost of attention.

One line of works focus on restricting the attention operation to be inside local windows [8][9][10][11][12][13][14][15]. For example, Swin [8] and MaxViT [9] restrict the attention computation to be within local windows, while CrossFormer [10] and CSWin [11] propose to use cross-shaped windows. Deformable Vision Transformer (DVT) [12] and DPT [14] introduce deformable attention to allow the model to attend to different parts of the input image. DCT [15] introduces a quadtree attention to model the interactions between different regions of an image, which can be viewed as a special case of the proposed bi-level routing attention. In contrast to these works, we propose a novel dynamic sparse attention to enable a more flexible allocation of computations with content awareness.

 propose a hierarchical attention mechanism to capture global dependencies in a hierarchical manner. However it is not designed for vision transformers, and it is only evaluated on small-scale datasets such as ImageNet-1K and CIFAR-10K. In this work, we focus on the more challenging COCO dataset, which has a much larger input size (\(1024\times 1024\) tokens) and more complex structures (\(\mathcal{O}(n^{2})\) convolutional layers). In addition, we provide a simple yet effective solution to reduce both computation cost and memory consumption with only GPU-friendly dense matrix multiplications.

 introduce a novel sparse attention mechanism for image classification. They propose to apply the sparse attention in a coarse-to-fine manner. Specifically, for a query, irrelevant key-value pairs are filtered out at a coarse region level, and then fine-grained token-token attention is applied in the union of remaining candidate regions (i.e., routed regions). However, they do not provide an efficient implementation of their method, and the proposed method is not directly comparable with theirs.

 present a similar idea to our work, but their method is designed for image segmentation and is not applicable to the vision transformer. In particular, their method does not consider the sparsity of attention, and their method cannot be directly applied to image classification tasks.

 also propose a dynamic attention mechanism that can be seen as a generalization of sparse attention, but it is limited to a specific type of attention mechanism. In comparison, our method is general and can be applied to various types of attention mechanisms.

"," Transformers [1][2][3][4][5][6][7] have been widely used in natural language processing (NLP) and computer vision [3][8][9][10][11][12][13][14][15] due to their ability to capture long-range dependency. In this section, we briefly review the recent advances in vision transformers.

Vision Transformer.Vision transformers [3] are the first to introduce self-attention mechanism into vision tasks. They firstly introduce the self-supervised self-interaction mechanism to model long-distance dependency in images, and then introduce the transformer architecture [1] for language modeling. The self-talk mechanism [1], which is the core building block of the vision transformer, has been widely adopted in many subsequent works [8][15][9] to model the long-term dependency in vision tasks, such as object detection, semantic segmentation, and image classification. However, the computation cost of the self attention mechanism is quadratic in size, which limits the scalability of the model. To address this issue, a series of works have been proposed to reduce the computational cost of self- attentions. For example, [4] introduce sparse attention [4], which reduces the computation complexity by recomputating attention matrices to save memory, and [5] reduce the complexity by approximating the self attentions with low-rank matrices. [6] propose a segment-level recurrence mechanism and a novel positional encoding scheme to capture longer-term dependencies. [7] propose several efficiency-flavored models, such that the selfattention computation is reduced to a low-dimensional matrix.


","[ Language Transformers.The transformer architecture has been successfully adapted to the language modeling task in the natural language processing (NLP) domain [1][2]. Recent works [3][4][5] have applied the transformer architecture to large-scale vision tasks. These works can be categorized into two parts: sparse attention and long window. Sparse attention reduces the parameter count and computation cost by masking irrelevant elements in the weight matrix. To achieve this, the inputs and outputs of the self-attention mechanism are restricted to be a sparse sub-matrix of the weight matrix [4][6][7]. Local window strategies follow a different route. With the self-attention mechanism at the center, they employ a hierarchy of local attention, which computes attention with a fixed receptive field and reduces attention across different locations [8][9][10]. With the emergence of the Deformable Convolution  technique, CSWin [11] introduces a novel and effective window strategy for cross-shape windows. The aforementioned approaches mainly focus on dynamic sparsity, which makes it difficult to capture long-range dependencies without the cross-attention mechanism. In contrast, our approach adapts the feature map and adopts a dynamic routing scheme at the same time to simultaneously capture long-range dependencies and dynamically reduce redundant computations.

In the same spirit, Deformable Attention [12] proposed a flexible routing scheme which can compute attention with different positional embeddings for different query regions. Bifrost  uses a multi-level routing strategy with intermediate positional embeddings to predict attention head-by-head. Unlike these approaches, Bifformer is fully differentiable and adaptive, such that it enables a series of intuitive modifications with negligible increase in computational cost and memory. In addition, Bifformer contains the abilities of both sparsity-based token-level attention and the directional sparsity-based head-level attention.

Apart from learning-based approaches, there are also design-based approaches [13][14][15] which further reduce the computational cost of vision transformers. Token Clustering Transformer (TCFormer) [13] clusters the feature map into high-level features, and the transformed map is then fed into an additional pooling module. DePatch [14] learns a spatial adapter module to adjust the patch size and location of each patch, which can be incorporated into transformers to re-sample tokens. QuadTree Attention [15] builds the attention mechanism in a coarse-to-fine manner, where the top-k most significant tokens in each pyramid level are selected for the next pyramid level. We choose to compare with TCFormer, DePatch, and QuadTree Attention in the evaluation since they are designed to reduce computational costs, and their complexity reduction mechanism is quite different from our approach. However, different from Bifformer, their efficiencies mainly come]","<Attention mechanisms have been pivotal in the development of vision transformers, allowing for the capture of long-range dependencies in image data [1]. Although attention is powerful, the associated computational burden and memory footprint have been a concern, prompting various approaches to introduce sparsity into attention operations, such as using local windows or axial stripes to restrict token interactions [4][5]. However, a novel dynamic sparse attention via bi-level routing has been proposed as a more flexible alternative, enabling a content-aware allocation of computations [4]. The use of bi-level routing attention provides a means to filter out irrelevant key-value pairs at a coarse region level for a query, followed by fine-grained token-to-token attention in the remaining candidate regions [4]. This approach effectively leverages sparsity to save computation and memory, while still involving only GPU-friendly dense matrix multiplications [4][5].> 

<Recent works have shown significant advancements in addressing the computational and memory efficiency of attention mechanisms in transformer models. For instance, the introduction of sparse factorizations of the attention matrix has reduced the time and memory complexity to O(nn) [4]. Additionally, the concept of self-attention has been reimagined to approximate the self-attention mechanism with a low-rank matrix, resulting in a linear transformer that performs on par with standard transformer models while being more memory- and time-efficient [5]. These advancements align with the ongoing efforts to enhance the efficiency of transformer models, as demonstrated by a myriad of ""X-former"" models proposed to improve upon the original transformer architecture through computational and memory efficiency [7].>

<Apart from efficiency-related improvements, the application of transformer models in vision tasks has seen notable developments. For instance, the Vision Transformer (ViT) has demonstrated excellent performance on image classification tasks by directly applying a pure transformer to sequences of image patches, thereby reducing the reliance on convolutional networks for vision tasks [3]. Similarly, other vision transformer models, such as Swin Transformer and MaxViT, have showcased superior performance on various vision tasks, underpinning the potential of transformer-based models as versatile vision backbones [8][9]. These advancements underscore the continued exploration and refinement of transformer models for computer vision applications.>

<In addition to enhancing efficiency and performance, efforts have been made to adapt transformer models to address specific requirements in computer vision tasks. Models like CrossFormer and CSWin Transformer have been designed to facilitate cross-scale interactions and global-local spatial interactions, respectively, thus addressing the need for more comprehensive and flexible attention mechanisms in vision tasks [10][11]. Furthermore, research has focused on devising innovative attention mechanisms, such as deformable attention and quadtree attention, to capture more informative features and reduce computational complexity in vision transformers [12][15]. These developments represent the evolving landscape of transformer-based models tailored for the unique demands of vision tasks, encompassing an array of capabilities from adaptive split-image patches to attention mechanisms conducive to specific visual analysis requirements.>

<The continued evolution and diversification of transformer models for vision tasks reflect the ongoing pursuit to optimize these models for efficiency, performance, and task-specific demands. While attention mechanisms have been a central focus, future research is likely to further explore diverse approaches to improve the capabilities and applicability of transformer models in the context of visual data, addressing a broad spectrum of challenges and requirements in computer vision tasks.>
"
1543," PointNet [1] is the first work to directly process point clouds with deep neural networks, which is a pioneering work for point cloud processing. Based on PointNet, many subsequent works [2][3][4][5][6] have been proposed to improve the performance of point cloud recognition. For example, PointNet++ [2] introduces a hierarchical structure to learn local features by aggregating point features hierarchically in a metric space. PointConv [4] and PAConv [5] extend the convolution operation to 3D point clouds by introducing position-adaptive convolution. PAPooling [6] proposes a graph-based position adaptive aggregation scheme to aggregate local geometry information. However, these methods are all designed for voxel-based point clouds, which cannot be directly applied to point clouds due to the irregularity and unordered structure of point clouds.

Recently, several works [7][8][9][10][11] focus on the robustness of 3D perception under various types of corruptions, such as noise, occlusion, distortion, and rotation, and propose various data augmentation techniques to augment the point cloud data. For instance, Wang _et al._[7] propose a new dataset ScanObjectNN, which consists of real-world point cloud corruptions including noise, rotation, noise, and distortion, to evaluate the performance on point cloud classification under different types of corruption. Li et al. [8] introduce a new point cloud corruption dataset, namely ScanCovariant Point Cloud (SCOCO), which is composed of synthetic point cloud corrupted with various kinds of noise and occlusions. In this work, we focus on improving the performance under the ScanCOCOwC dataset [7].

Data augmentation is an effective regularization strategy to alleviate the over-fitting problem, which has been widely used in 2D vision tasks [12][13]. However, it is rarely considered in 3D vision due to its data scarcity. To address this issue, some recent works [9][11][10] propose to augment point cloud samples with various transformations to enrich the data diversity and improve the generalization ability of the model. For the purpose of data augmentations in point cloud, Wang et al.[9] propose WL-PointNet, which performs weighted local transformations on all point cloud objects in an offline way and selects the optimal transformation for each point based on the local geometric structure of each point. Li and Han [11] propose PointAugment, a new auto-augmentation framework that automatically optimizes and augments point clouds samples to enrich data diversity when we train a classification network. In addition, a perception-guided feedback mechanism is incorporated to guide the generation of samples with appropriate difficulty level. However these methods usually perform random transformations on the whole point cloud and ignore the structure of the samples, resulting in over-order enhancement. To cope with this problem, we propose an adaptive augmentation framework, which makes sample-specific transformations according to the structural information of the input point cloud.

 propose a corruption robustness benchmark for image classification"," **Point Cloud Recognition.** PointNet [1] is the first attempt to directly consume point cloud data, which well respects the permutation invariance of points in the input. PointNet++ [2] extends PointNet by introducing a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point cloud. PointMLP [3] introduces a pure residual MLP network, which integrates no sophisticated local geometrical extractors using convolution, graph, or attention mechanisms. PointConv [4] and PAConv [5] extend the dynamic filter to a new convolution operation, which can be applied on point clouds to build deep convolutional networks. PAPooling [6] explicitly models spatial relations among local points using a novel graph representation, and aggregates features in a position-sensitive manner, enabling position-aware aggregation of local point features.

**Data Augmentation.** Data augmentation is an effective regularization strategy to alleviate the over-fitting, which is an inherent drawback of the deep neural networks. Data augmentations have been widely used in 2D image processing to augment the training data. For example, PointAugment [7] augments point cloud samples with different deformation parameters to improve the robustness of the classifier. However, data augmentation for point cloud processing has been less explored. PointWOLF [8] proposes to generate augmented samples of desired difficulties by applying local weighted transformations on each anchor point. AugTune [9] proposes a local weighted augmentation method to generate the augmented samples with desired difficulties. RSMix [10] generates a rigidly mixed sample by extracting shape-preserved subsets from another sample, and then performs a deformation transformation on the deformed sample. PointAugmentation [11] proposes an adversarial learning strategy to jointly optimize an augmentor network and a classifier network, such that the augmentor can learn to produce augmented samples that best fit the classifiers. However these methods perform augmentation on all point cloud objects in an offline way and ignore the structural information of the samples, resulting in over-order enhancement. In this work, we propose an alternative to make sample-adaptive transformations based on the structure of the sample to cope with potential corruption, which enables us to avoid excessive enhancement of the corrupted point cloud with appropriate difficulty level. Moreover, we also introduce a new dataset ScanObjectNN to address the paucity of real-world corrupted point clouds.","[ Research on point cloud learning and understanding has been prosperous in recent years [1][2][3][4][5][6]. Among them, researchers devote much attention to exploring various point cloud representation methods. For example, [1] firstly present a network to directly learn from point clouds. Recent advances in point cloud representation utilize techniques that transform the original irregular point cloud to a regular grid to facilitate the convergence of neural networks. As a typical approach, [4] propose to extract global and local features for a regular point grid and formulate convolutional kernels via dense regression. [5] adopt the dynamically assembled convolution kernel. Following this work, researchers also propose different strategies to aggregate local feature representations, such as GNN, encoder-decoder and matrix pooling, etc. For point cloud classification, [7][8] provide comprehensive benchmarks for point cloud corruption. While methods developed from [9][10] are manually designed and cannot be generalized to real-world data. Instead, our work aims to generalize the model to different real-world corruptions by generating more sample-adaptive transformations for each point in the dataset.

Recently, more and more researchers are paying attention to the performance of neural networks under the corruption of natural image data [11]. Based on the assumption that it is much easier to simulate the corruption of natural image data than point cloud, researchers propose to manually design transformations on image data and then introduce them into their networks to enhance robustness. For example, [12] extract features from multiple patches and design an adversarial loss to combat adversarial corruptions. Furthermore,  propose the iterative normalized correlation loss to deal with the non-adversarial corruptions. Inspired by these studies, our work proposes to enhance point cloud classifiers robustness under corruption by adopting sample-adaptive transformations and incorporating point corruptions into the discriminator in the augmentation framework.

There is also some work about point cloud corruption in the wild [8]. [8] propose the SOP method to restore point cloud data, aiming at point cloud classification. While  proposes to generate corrupted point clouds in a controlled environment to test models' robustness.

]","<In recent years, the task of ensuring robust 3D perception, especially for point cloud recognition against real-world corruptions, has garnered increasing attention in the realm of 3D vision. Current data augmentation techniques typically involve random transformations on all point cloud objects in an offline manner, disregarding the structure of the samples leading to over-order enhancement [1]. To address this challenge, the proposed auto-augmentation framework, named AdaptPoint, leverages a imitator consisting of a Deformation Controller and a Mask Controller to predict deformation parameters and produce per-point masks based on the intrinsic structural information of the input point cloud, allowing for sample-adaptive transformations to cope with potential corruption [1, 11]. Furthermore, the incorporation of a perception-guidance feedback mechanism enhances the generation of samples with an appropriate difficulty level [1]. Additionally, the introduction of the ScanObjectNN-C dataset, which exhibits greater similarity to actual data, aims to address the scarcity of real-world corrupted point cloud data [1].

In the domain of deep learning on point sets, PointNet has emerged as a prominent approach, demonstrating strong performance and robustness for various applications related to object classification, part segmentation, and scene semantic parsing [2]. PointNet's effectiveness in directly consuming point clouds, respecting the permutation invariance of points in the input, sets a precedent for unified architectures in 3D data processing [2, 3]. Moreover, subsequent works such as PointNet++ and PointMLP have contributed to the advancement of deep hierarchical feature learning and the rethinking of network design for point cloud analysis, offering competitive performance, efficiency, and speed [3, 4]. These developments underscore the importance of tailored neural network architectures for effective processing of 3D point cloud data, laying the groundwork for further advancements in the field of point cloud recognition and classification [2-4].<PAPooling, a position adaptive pooling technique, has been introduced to address the inadequacies of traditional max/average pooling for local feature aggregation in point clouds [6]. By explicitly modeling spatial relations among local points and adaptively aggregating point features based on the generated weights, PAPooling significantly improves predictive accuracy with minimal computational overhead, making it an effective and flexible solution for various tasks such as 3D shape classification, part segmentation, and scene segmentation [6]. Additionally, the proposed PointWOLF and RSMix methods offer simple yet effective point cloud augmentation and regularization strategies, demonstrating consistent performance improvements for tasks such as shape classification and part segmentation, thereby addressing the limitations of limited data availability and overfitting in point cloud processing [9, 10].

Benchmarking and analyzing point cloud classification under real-world corruptions have been the focus of recent research efforts [7, 8]. Comprehensive evaluations have highlighted the need for improved robustness and generalizability of point cloud classification methods, signaling an opportunity for the development of more effective techniques to enhance classifier robustness in the face of real-world challenges [7, 8]. Finally, the broader context of benchmarking neural network robustness to common corruptions and perturbations in the image classification domain provides valuable insights into enhancing classifier robustness, extending the relevance of benchmarking efforts beyond point cloud-specific applications to encompass broader considerations of robustness and generalizability in deep learning [12, 13]. These diverse research endeavors collectively contribute to advancing the state-of-the-art in point cloud recognition and classification, while also paving the way for broader implications in the field of deep learning and computer vision.>"
1379," In this section, we briefly review the single-image HDR reconstruction methods and the stack-based inverse tone mapping (ITM) methods.

**Single-Image HDR Reconstruction.** Recently, many deep learning-based methods have been proposed to reconstruct HDR images from a single LDR image [1][2][3][4][5][6][7][8][9]. These methods can be roughly divided into two categories: two-stage and end-to-end methods. Two-stage methods first estimate a set of multi-exposure images and then reconstruct the final HDR image. For example, HDRNet [1] and ExpandNet [2] use a two-branch network to predict the exposure and the luminance of each pixel separately. However, these methods estimate a fixed number of images, which may introduce unnecessary computational cost or reconstruct incorrect results. On the other hand, End2End methods [7][9] directly reconstruct the HDR image from the LDR input without estimating the multi-extracted images. For instance, Deep Reverse Tone Mapping (DRTM) [7] and Deep Chain HDRI [8] propose to reconstruct an HDR image by iteratively applying a series of upsampling and down-sampling operations. Although these methods can reconstruct high-quality HDR images, they suffer from the loss of details in the under-/over-exposed regions. In contrast, our method only needs to estimate two exposure images and can reconstruct HDR radiance in a single forward pass. Moreover, we propose the cross-model attention block to connect the exposure adjustment models and thus learn more effective features for the image reconstruction task. Note that our method is also related to the image inpainting methods [10][11][12], which can also be regarded as a special case of our method.

 propose a multi-stage network to estimate the exposure stack and then recover the HDR images. Different from their method, we design the exposure adaptive block to adaptively adjust the exposure of the input image based on the pixel-wise luminance distribution of the image. Furthermore, we introduce the cross attention block and the exposure fusion block to learn more discriminative features for image reconstruction tasks. In addition, we also propose a novel method to reconstruct a single HDR image, which can be easily integrated into existing single image reconstruction methods.

 present a stack-image-based method to recover the multiple exposure stack from a given single image. Their method first estimates the exposure-up and exposure-down images, and then combines them to obtain the final image. Differently, we estimate the two exposure-based images and reconstruct the two HDR images simultaneously. Besides, we devise the cross model attention block, which enables the network to learn effective features from both the image and the image-level.

 also propose the image deconvolution network (ICN) for image denoising and color tone mapping. They first estimate the low dynamic range (LDR) image and then denoise it and restore the high dynamic range image. In this paper, we focus on the single image HDR reconstruction task and propose a new network structure"," In this section, we briefly review the related work on single image HDR reconstruction and image inpainting.

**Single image HDR Reconstruction.** Existing methods can reconstruct HDR images from a single LDR image by estimating a set of multi-exposure images [1][2][3][4][5][6][7][8][9]. For example, Zhang _et al_. [1] proposed a deep CNN model to reconstruct HDR from a LDR input by predicting the saturated pixels of the input LDR images. However, this method requires a fixed number of images, which may introduce unnecessary computational cost or reconstruct incorrect results. To address this problem, some methods [3][10] estimate a fixed amount of images by predicting a fixed set of exposure images, and then reconstruct an HDR image by merging them. For instance, Wang _et.al_. [3] proposed to use a multi-linear decomposition network to reconstruct the HDR image from the LDR-to-LDR mapping. However this method is computationally expensive and requires a pre-trained model to estimate a large number of exposure-up and exposure-down images. To reduce the computational cost of this method, some recent methods [8][7] estimate only two exposure images by estimating the exposure values with different bit-depth from the input image. However these methods only consider the up-and-down exposure adjustment models, and thus introduce unnecessary computation cost. In this paper, we revisit the stack-based ITM approaches and propose a novel method to reconstruct an end-to end HDR image reconstruction from a low dynamic range (LDRR) image, which only needs to estimate the exposure images with different exposure values. Moreover, we propose a cross-model attention block to connect the exposure adjustment model and the exposure fusion model, which is the first attempt to tackle the problem of single image image HDR recovery from a LR image, and we also propose a new multi-label loss function to improve the quality of the reconstructed HDR images. In addition, our method can be applied to HDR image enhancement [11][12] and image restoration [11].

","[ Reconstruction of single image HDR from low-dynamic range (LDR) images has drawn growing interests in the research community recently [1][2][3][4][5][6][7][8][9]. Among them, inverse tone mapping (ITM) based methods [3][7][8][9] are developed to restore the output HDR radiance from a single input LDR image. These methods can be divided into multi-exposure stack-based and patch-based based methods. As one typical example, the multi-exposure stack-based method aims to predict a set of images from a single input LDR image by estimating the model parameters (e.g., dynamic range clipping, camera response function and quantization) of the image formation pipeline, which needs to obtain a pre-trained stack of multi-exposure images as a supervising signal. Most of these works generate the multi-exposure stack from a set of images taken under different exposures and apply the optimal light transport modules to solve the ITM problem [10]. Specifically, after obtaining the optimal light transport model parameters for a single LDR image, they then select the correct light transport model based on a predefined radiance profile (and the selected model itself). After applying these models, the HDR radiance can be obtained by combining the predicted multi-exposure images.

On the contrary, as a typical example, the patch-based method generates multi-exposure stacks from the inputs. Then, a small patch (usually around a square of 256 pixels) in the image is predicted from the multi-exposure stack by combining the encoder-decoder structure with high-resolution input and high-resolution patch prediction [4][7][9]. Compared with the multi-exposure stack-based methods, the patch-based method tends to reduce the redundancy of multi-exposure stacks to save computational costs.

Concretely, CRS  iteratively learns the multi-exposure stack with the consideration of the large gradients in the output and enhances a small patch. ITML [7] proposes an inverse tone mapping loss based on a learned lookup table and then develops an encoder-decoder structure to predict the multi-exposure stack with a small input patch. Recently, FNC  extends the multi-exposure stack-based method to adopt the generative adversarial networks for deep inverse tone mapping. JAM  extends the patch-based methods into a jigsaw puzzle scheme and exploits the knowledge of the image formation pipeline in the scene with the model-aware priors. However, as the complexity of the pipeline-specific information, these works still rely on the input LDR image to make a precise guess of the image formation pipeline. The proposed stack-based ITM method is significantly different from all these methods. Firstly, it only needs to reconstruct the single output image. Secondly, the method can]","Step 1: Extracted Important Sentences
1. ""In this paper we address the problem of predicting information that have been lost in saturated image areas, in order to enable HDR reconstruction from a single exposure.""
2. ""High dynamic range (HDR) imaging provides the capability of handling real world lighting as opposed to the traditional low dynamic range (LDR) which struggles to accurately represent images with higher dynamic range.""
3. ""We propose a method for generating HDR content from LDR content based on deep Convolutional Neural Networks (CNNs) termed ExpandNet.""
4. ""We model the HDR-to-LDR image formation pipeline as the (1) dynamic range clipping, (2) non-linear mapping from a camera response function, and (3) quantization.""
5. ""Previous deep learning-based methods apply the same convolutional filters on well-exposed and saturated pixels, creating ambiguity during training and leading to checkerboard and halo artifacts.""
6. ""To address these problems, we propose a new HDR reconstruction network for UHD images by collaboratively learning color and texture details.""
7. ""We propose a novel learning-based approach using a spatially dynamic encoder-decoder network, HDRUNet, to learn an end-to-end mapping for single image HDR reconstruction with denoising and dequantization.""
8. ""We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment.""

Step 2: Summarized Sentences and Decided on Topics
The extracted sentences cover various aspects of HDR image reconstruction. The related work will include discussions on deep learning-based methods for HDR reconstruction from a single exposure, methods for generating HDR content from LDR content, addressing artifacts and limitations in previous methods, and techniques for recovering high dynamic range radiance maps from photographs.

Step 3: Write Related Work
<In recent years, there has been a growing interest in addressing the challenge of high dynamic range (HDR) imaging from a single low dynamic range (LDR) input. Research by [1] presents a method for predicting lost information in saturated image areas to enable HDR reconstruction from a single exposure. Additionally, methods like ExpandNet, proposed in [2], leverage deep Convolutional Neural Networks (CNNs) to generate HDR content from LDR content. These advancements have shown promise in overcoming the limitations of traditional LDR representation and enabling the handling of real-world lighting scenarios with higher dynamic ranges.>

<Furthermore, the work by [4] addresses the issue of artifacts in saturated areas, proposing a feature masking mechanism to reduce ambiguity during training and enhance the synthesis of visually pleasing textures in HDR reconstruction. Additionally, methods like HDRUNet, as proposed in [6], and the collaborative bilateral learning approach discussed in [5], offer novel techniques for HDR reconstruction, emphasizing the importance of learning color and texture details and incorporating denoising and dequantization for improved results.>

<Moreover, the study by [3] introduces a technique that incorporates domain knowledge of the LDR image formation pipeline to model the HDR-to-LDR image formation process. By decomposing the problem into specific sub-tasks and imposing effective physical constraints, the proposed method demonstrates favorable performance against existing single-image HDR reconstruction algorithms. Additionally, the work by [7] proposes a deep-learning-based approach for automatic HDR inference, synthesizing LDR images taken with different exposures to reconstruct an HDR image by merging them, thus addressing the challenge of compensating lost data and reproducing natural tones and colors.>

<In the realm of HDR imaging, the research by [5] introduces a new HDR reconstruction network for Ultra-High-Definition (UHD) images, emphasizing the extraction of content and chromatic features and achieving real-time processing. Additionally, [10] presents a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment, demonstrating its applicability across various areas of computer graphics. These studies collectively showcase the diverse and cutting-edge approaches for HDR reconstruction from a single exposure, addressing various challenges and limitations while advancing the field of high dynamic range imaging.>"
405," **Semi-supervised learning.** The goal of SSL is to leverage large-scale unlabeled data to improve the performance of a model with limited labeled data, which has been widely studied in various computer vision tasks [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]. In particular, self-teaching [16][17][18][19][20][21][22][23][24] has been shown to be an effective approach for SSL in image-level or pixel-level prediction tasks. For instance, Mean-Teacher [18] proposes a teacher-student framework, where a teacher model generates pseudo-labels for a student model, and the student model is trained to minimize the discrepancy between the teacher's predictions and those of the teacher. This framework has been applied to various tasks, including image classification [18][18], semantic segmentation [19][19], object detection [20][22], and face recognition [23][23]. In the context of object detection, [3][5] propose to match the teacher and student models in the feature space, while [4] proposes to align the global class prototypes [4], and [5] introduces an active-teacher framework. In addition, [21] introduces a knowledge distillation framework where multiple teachers are used to guide the training of the student. In contrast to these methods, we propose a novel quantization-aware self-training framework for 3D detection, which is the first work to tackle the problem of quantization error caused by point-to-voxel discretization.


**3D SSL.** Recently, there have been a few works on 3D SSL [25][26][27][28]. 3DIoUMatch [27] and SESS [26] propose a self-ensembling framework that leverages IoU prediction as an additional supervision signal for SSL. However, these methods are based upon two-stage pipelines, matching heuristically selected proposals generated in the first stage and resulting in spatially sparse training signals. 3D-Teachers [28] is the most related work to ours, which also employs a teacher network to match proposals generated by a single-stage detector [29][30]. However, our method is fundamentally different from it in two aspects. First, our teacher network is designed to match quantized training signals in the voxel domain, while the teacher network in 3D detectors is designed for the point-cloud domain. Second, our quantization rules are designed to compensate for the misalignment between two transformed views, whereas the quantization errors in [28][27] are based on heuristics. Finally, we introduce a novel loss function that can be applied to both 2D and 3D object detection. Note that [31] also proposes a transformer-based 3D detector, but it is not designed for SSL, and it is based on the Minkowski-CNN [32] architecture, which suffers from high computational complexity. In this work, we"," **Semi-supervised object detection.** The problem of using unlabeled data for semi-supervision has been extensively studied in the literature. Most existing methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39]. In this paper, we focus on the problem of 3D object detection, which is more challenging due to its irregular data structure and large scale variation.

**Self-training.** Self-training is a popular technique for semi supervision, which has been widely used in image-level or pixel-level tasks [4][11]. The idea of self-teaching is to train a teacher network to generate pseudo labels for unlabeling data, and then train a student network to match these pseudo labels. The pseudo labels are then used to train the student network. For example, in [4], the authors propose to use the mean teacher model [18] to match the pseudo labels generated by the student model. In [5], a teacher-student framework is proposed to initialize the label set and gradually augment the pseudo label set by evaluating the difficulty, information and diversity of the unlabelled data. The authors in [10] propose a two-way inter-label self-training framework that exploits inherent correlations between semantic understanding and instance information of a scene. In addition, the authors of [6] propose to provide object-level information for denoising pseudo labels and make use of their correlation for two- way mutual enhancement. In this work, we propose a novel self-learning framework for 3D detection, where we use the teacher model to generate dense training signals and use these dense signals to improve the performance of the detector.


","[ This section first reviews prior work on self-supervised learning and semi-supervised learning for object detection. Then, we introduce 3D object detection.

Self-supervised learning relies on an auto-encoder that predicts a data distribution given a corrupted version of the data. In object detection, recent methods resort to spatial correlations of prediction confidence to generate the _pseudo label_ for unlabeled data, i.e., a data instance with higher confidence scores are likely labeled by the model. Several works achieve consistency through inter-class variance alignment [1][2], discarding the less confident data [3], relying on instance-level scores [4] or class-level scores [5]. Due to its broad applicability, self-training is also explored in other vision tasks, such as image-level object detection [6][7][8], action recognition [9][10], video understanding [11][12][13][14][15], and domain adaptation [16][17].

In contrast, _semi-supervised learning_ aims to exploit limited supervision to improve the performance of models. Recent approaches aim to improve the performance of the model at both training and testing.  leverages unlabeled data at the testing stage by an ensemble of the trained model. [2][18][19] further adopt a Temporal Ensembling (TE) strategy to enforce the agreement of predictions of the trained models. The TE approach is also used in [20][21] to better fit a teacher model to the proposed method. Besides, [22] utilizes a exponential moving average of the teacher prediction during the training stage. To improve the training quality,  employs label noise,  disentangles data,  learns differentiable heatmaps, and [23] uses a generative adversarial network.

Another popular approach is to directly fuse the student with the teacher through multi-stage [24] or single-stage architecture [25][26][27][28]. For example, SESS [26] and SESS+  use an ensemble of several teachers while 3DIoUMatch [27] generates a dense representation of the teacher prediction to match with student-predicted proposals. The final prediction is taken as the pixel-wise activation of a fully-connected layer. Besides, the YOLOv3-like models [29] are widely used for semi-supervised object detection. Recently, single-stage methods show improved performance. For example, FCOS [30] is based on one-stage detectors while one directly optimizes the 3D prediction head in a 3D detector [31].

In this work, we consider the architecture in [28], which directly fuses a two-stage model with a single-stage model. The two-stage model is an improved version of PointHeat , which is widely]","<In recent years, semi-supervised learning has become increasingly prevalent in computer vision, particularly in the field of object detection. The work by UCC [1] introduced Uncertainty guided Cross-head Cotraining for semi-supervised semantic segmentation, which leverages weak and strong augmentations within a shared encoder to achieve co-training. The approach effectively combines the benefits of consistency and self-training, demonstrating significant outperformance of other state-of-the-art semi-supervised semantic segmentation methods. Similarly, Semi-supervised Semantic Segmentation with Error Localization Network [2] addresses the issue of performance degradation due to confirmation bias towards errors on pseudo labels. By introducing an error localization network and a new learning strategy to enhance generalization, the proposed method resolves the chronic issue of inaccurate pseudo labeling, demonstrating improved effectiveness on benchmark datasets.>

<Label Matching Semi-Supervised Object Detection [3] delves into the label mismatch problem in semi-supervised object detection and proposes a LabelMatch framework that leverages distribution-level and instance-level perspectives. The framework approximates the class distribution of unlabeled data and introduces a novel label assignment mechanism for self-training, resulting in unbiased pseudo labels and improved student learning. Additionally, the work by Semi-Supervised Object Detection via Multi-instance Alignment with Global Class Prototypes [4] introduces a Multi-instance Alignment model, named MA-GCP, which enhances the prediction consistency based on global class prototypes. By minimizing the cross-entropy loss of class distributions, the proposed model improves the generalization ability of object detectors with large-scale unlabeled images.>

<Active Teacher for Semi-Supervised Object Detection [5] studies teacher-student learning and proposes an algorithm for semi-supervised object detection. The approach extends the teacher-student framework to an iterative version and gradually augments the label set by evaluating key factors of unlabeled examples, achieving superior performance over other semi-supervised object detection methods. Seq-UPS: Sequential Uncertainty-aware Pseudo-label Selection for Semi-Supervised Text Recognition [6] addresses the challenges of pseudo-labeling in sequence models and introduces a pseudo-label generation framework for semi-supervised text recognition, demonstrating its effectiveness in handling noise and overfitting.>

<Ego-Vehicle Action Recognition based on Semi-Supervised Contrastive Learning [7] presents a method based on semi-supervised contrastive learning for handling rare driving scenes in ego-vehicle action recognition. The proposed approach focuses on defining video-to-video distances by leveraging semi-supervised contrastive learning, further enhancing the recognition accuracy for specific scenes. Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin [8] introduces an adaptive confidence margin for semi-supervised deep facial expression recognition, allowing the utilization of all unlabeled data for improved recognition performance. The work demonstrates the efficacy of the approach on challenging datasets, showcasing its potential for enhancing facial expression recognition.>

<Among the various methods for semi-supervised learning, Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results [18] has been extensively studied. This work proposes a method that averages model weights instead of label predictions and improves test accuracy, demonstrating performance gains over existing semi-supervised approaches. Furthermore, 3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection [27] leverages a teacher-student mutual learning framework to propagate information from labeled to unlabeled data, utilizing confidence-based filtering mechanisms to improve the accuracy of pseudo-labels, particularly by using the estimated 3D IoU as a localization metric, leading to improved object detection results.>

<In the realm of 3D object detection, an approach by Point-Voxel CNN for Efficient 3D Deep Learning [36] presents Point-Voxel CNN (PVCNN), an efficient and fast 3D deep learning model. The work introduces a model that processes 3D data using points to reduce memory consumption, while performing convolutions in voxels to improve irregular data access and memory locality, achieving higher accuracy and measured speedup compared to state-of-the-art models. The findings of these works collectively demonstrate the significant potential of semi-supervised learning and its diverse applications in various computer vision tasks, particularly in the domains of object detection and 3D perception.>"
1759," **Image Harmonization.** Early image harmonization methods [1][2][3][4][5][6] are mainly based on hand-crafted features. Recently, deep learning-based methods [7][8][9][10][11][12][13][14][15][16] have been proposed to solve the visual inconsistency problem in composited images. For example, Wang _et al_. [7] propose a harmonization network (HarmNet) to learn the harmonization mapping between the foreground and background. To better utilize the background information, some methods [8][10] propose to harmonize the foreground regions with the guidance of the background regions. However, these methods only consider the local color transformation or region matching, which neglects powerful proximity prior and independently distinguish foreground regions as a whole part for harmonization. In contrast, we propose a novel Global-aware Kernel Network (GKNet) for local harmonization with comprehensive consideration of long-distance background references.

**Kernel Prediction.** Kernel prediction [17][18][19][20][21][22][23][24][25][26][27][28] has been widely used in various computer vision tasks, such as image classification [29], object detection [30], image generation [31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46], _etc_. Kernel prediction networks (KPNs) [22] are proposed to predict the kernel for each pixel in the input image, which can be applied to various tasks, _e.g_., denoising [22][24], super-resolution [19], and optical flow estimation [28]. In this paper, we design a novel Kernel Prediction Block (KPB) to predict multi-level harmony kernels by fusing global information with local features.

 propose a self-consistent style contrastive loss to improve the visual quality of the harmonized image. Different from [13], our proposed SCF module is designed to select relevant long-range background references for harmonizing local regions with local and global awareness.

 also propose a local-to-global attention module to fuse local features and global context for image synthesis. Differently, our proposed LRE and KPB are designed to obtain global context and local context, respectively, for harmonized foreground regions.

 introduce an attentional feature fusion (AFF) module [43] to fuse global and local features for feature fusion. Specifically, the AFF module first calculates the attention scores between each pair of features, and then aggregates them to obtain the fused features. In this work, a novel Selective Correlation Fusion (SCF) module is proposed to better select relevant local features from the global context to enhance the local features of the local harmonized regions for local regions harmonization, _i.e_., global-aware context fusion.

 further propose a Squeeze-and-Excitation Network (SENet) [41] to further improve the feature fusion performance by introducing channel-wise and spatial-wise attention"," **Image Harmonization.** Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46]. However, these methods mainly focus on local harmonization and neglect the global context of foreground and background. To address this issue, we propose a novel Global-aware Kernel Network (GKNet) to harmonize local regions with comprehensive consideration of long-distance background references for image harmonization.

**Deep Image Harmonization (DIN).** DIN [7] is the first end-to-end learning-based method to solve the image harmonizing task. It firstly adopts an encoder-decoder architecture to learn the image mapping from the composite image to the real one, and then applies the learned mapping to the original image. To further improve the performance, some methods [8][10] utilize the semantic information of the real image as a positive sample to guide the training. To improve the visual consistency, some other methods [9][12] adopt the style information of real image to guide DIN training. For example, SCS-Co [13] adopts the self-style and foreground-background style information to improve the harmonization performance. However, the above methods only consider the visual style of the foreground image, and ignore the background information. To solve this problem, our GKNet is proposed to predict multi-level harmony kernels for harmonizing local regions.

 propose a new method to learn multi-scale harmonization kernels for high-resolution images. They firstly use the pre-trained CNNs [20] to predict the kernels for each pixel and then use the predicted kernels to synthesize the final harmonized image. Different from them, our method predicts multi-levels of kernels for local harmonizing. Moreover, we design a novel SCF module to better select relevant long-range background references.

","[ Harmonization is a fundamental task in the image editing process and has been studied by the computer vision community for decades [1][2][3][4][5][6]. Typically, foreground and background are resized to the same size and composited with each other. However, many artifacts can be observed in the composited image due to the incompatibility of the foreground with the background and the spatial misalignment. To tackle these issues, early works [7][8] directly learned the correspondences between RGB and HSV spaces to harmonize the color of the foreground with the background. Later, attempts were made to classify foreground and background into different semantic spaces [9][10] or using the adversarial learning [11] to learn more discriminative features and harmonize the foreground image. With the development of deep learning, many methods [12][13][14][15][16] have explored the learning process of harmonization with DNNs. Most of them can be categorized into pixel-wise image translation, such as global or local convolutions. More advanced methods [17][18][19][20][21][22][23][24][25][26][27][28] have proved that there is a significant gap between them and their performance has reached the state-of-the-art.

Convolution is the dominant module in DNNs, thanks to its excellent feature representation [29]. Inspired by this, some attempts have been made to extend it into a ""kernel prediction"" module. Some works [30][31] propose to predict the kernels in a purely reconstruction-based way, _i.e_., matching the restored pixels of the output image to the input image. However, they all show limited performance on real-world images due to the modeling bias to only capture small context and strong reliance on the data priors. Recently, some works [22][23][24][25][26][27][28] extract more global features from CNNs to learn a high-resolution kernel, which achieves superior performance on real-world images. These approaches also apply in the process of image harmonization, since the knowledge of how the background is composed can be obtained. However, none of these works explore the potential of the holistic kernel prediction module.

With the success of transformer [31], many attempts have been made to explore its potential in the field of computer vision [32][33][34][35][36][37][38][39][40]. Generally, transformer requires a long-distance training process to take full advantage of the model. Inspired by this, Belyaev _et al._[41] designed a new Squeeze-and-Excitation module (SE) to provide more global information for]","<Existing methods in image harmonization primarily focus on color transfer, color mapping, image compositing, and image editing. Some methods utilize statistical analysis to impose one image's color characteristics on another [1]. Others leverage linear color mapping for example-based color transfer and editing, focusing on finding the best linear color transformation [2]. Image compositing, an essential operation in computer graphics, is also addressed by data-driven algorithms for adjusting statistical measures to improve composite realism [3]. Techniques for seamless editing of image regions are introduced, allowing for the importation and modification of image regions for realistic compositing and editing [4]. Additionally, methods for image matching and appearance transfer prior to blending different images have also been explored [5].

Recent works in image harmonization encompass deep learning approaches such as deep convolutional neural networks (CNNs) and encoder-decoder networks [7]. Some methods focus on learning semantic representations for effective image harmonization [8]. Additionally, there are efforts to introduce spatial-separated attention modules to improve the harmony of composite images [9]. The development of deep image harmonization methods, including the introduction of domain verification discriminators, highlights the significance of high-quality training data [10]. Various deep learning methods have been proposed for harmonizing composite images [11]. Leveraging the power of transformer frameworks is also gaining traction for image harmonization, aiming to adjust foreground light to better integrate with background light while maintaining structure and semantics [12].

Attention mechanisms and selective kernel networks have been explored to improve efficient learning, allowing for dynamic filter generation and adaptation in networks [20]. Additionally, feature fusion and attentional feature modules have been introduced to enhance network representations [42-43]. Furthermore, research has proposed split-attention modules and the merger of channel-wise attention for richer network representations [44]. The application of transformer architectures in computer vision tasks has shown promise, optimizing the representation of long-range relations for various vision tasks [32].>

<In the realm of image restoration, transformers have shown potential in synthesizing high-resolution images and enhancing image completion [37-38]. Efforts have been made to combine CNN and transformer architectures to improve representation learning in convolutional neural networks [41]. Additionally, the efficient use of transformers for video inpainting via fine-grained feature fusion has been explored, addressing limitations in modeling fine-grained details and spatial representations [40]. The introduction of selective kernel networks and attentional feature fusion reflects a push for more dynamic and adaptive feature extraction in network architectures [42-43]. Finally, researchers have proposed an efficient paradigm for simultaneously performing image colorization and super-resolution, showcasing efforts to streamline image enhancement processes for practical applications [45].>
"
3518," **Prompting for Language Models.** Prompting has been shown to be a promising approach to improve the performance of pre-trained language models (PLMs) for many NLP tasks, such as question answering (; ), natural language inference (; [2]; [3]; [5]), and machine translation ([1]; [7]; [4]). For example, [2] propose an information-theoretic approach to prompt engineering without ground truth labels. [3] propose a complexity-based prompting method for multi-step reasoning tasks. [5] present a least-to-most prompting method to improve performance on complex reasoning tasks, which requires solving problems harder than the exemplars in the training set. [1] study what makes good in-context examples for GPT-3, and find that the similarity between the prompt example and the input sentence is a strong predictor of the prompting performance.

**Machine Translation.** There has been a long line of work on improving the translation performance of neural machine translation (NMT), including back-translation ([9]), search engine guided NMT ([6]), and multi-lingual NMT (MNLM) ([12]; [11]; [8]; ). [10] explore the use of natural language prompts for controlling properties like formality or specific dialect of the output. [6] extend an attention-based NMT model by allowing it to access an entire training set of parallel sentence pairs even after training. [9] propose to generate synthetic parallel data by back-translating the source sentence to the target sentence. [12] and [11] propose multilingual NMT models that are capable of translating between any pair of languages. Recently, [4] propose mT5, a massively multilingual PLM for machine translation, which achieves state-of-the-art zero-shot translation performance. [8] further propose a multi-task learning approach to further improve the multilingual performance. In this work, we focus on prompt-based machine translation and provide a comprehensive study on prompt template selection and demonstration example selection for large-scale PLMs for translation. Our work is also closely related to [13], which study prompting PaLM for translation and evaluate the effectiveness of different prompting strategies. However, our work differs from [13] in several aspects. First, we conduct extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed, which is a much larger model than PaLM. Second, we provide a systematic study on prompting strategies for translation, examining various factors for prompt template and example selection, as well as various ways to transfer knowledge from monolingual data for prompting. Third, we further explore the feasibility of crosslingual, cross-domain, and sentence-to document transfer learning in prompting. Finally, we analyze several problems that affect the model outputs and discuss several potential solutions.

 study the effect of prompt templates and demonstration examples on machine translation. They find that using suboptimal examples degenerates translation, and propose a method to mitigate this problem by using pseudo-parallel examples. In contrast, we"," Prompting for Machine Translation.Prompting has been extensively studied in the literature for many NLP tasks, such as question answering (; [2]; [1]; [5]; [3]; ; [3]) and machine translation (; ; [6]; [4]; [7]; [8]; [9]; [10]; [6]). In machine translation, [6] propose to use an off-the-shelf search engine to retrieve a small subset of sentence pairs from a training set given a source sentence. [7] propose a multi-stage prompting method for making pre-trained language models better translators. [10] explore the use of natural language prompts for controlling various aspects of the outputs generated by machine translation models. [9] propose tagging back-translated source sentences with an extra token to indicate to the model that the given source is synthetic. [2] propose an information-theoretic approach to selecting prompt templates without labeled examples and without direct access to model parameters. [1] retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. [5] break down a complex problem into a series of simpler subproblems and then solve them in sequence. [3] propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning tasks.




 propose a prompt template selection method for prompting large-scale language models. They propose a simple but effective method for selecting prompts with minimal annotation cost. [4] propose mT5, a multilingual text-to-text model for multilingual pre-training, which is trained on 101 languages and has excellent performance on many multilingual benchmarks. [11] propose the mT-5 model, a massively multilingual model for machine translation. [12] propose Google's multilingual neural machine translation system (M-MMT), which is a single multilingual NMT model trained on 103 languages. [13] provide an analysis on prompting strategies and performance for M-M-T. [8] propose random online backtranslation to enforce the translation of unseen training language pairs.  propose a zero-shot translation model, which can translate into unseen language pairs by using English names as prompts.  show that using language names to control the output language of multilingual translation models enables positive transfer for unseen language pair.  explore the feasibility of cross-lingual, cross-domain, and cross-document transfer learning in prompting.  provide a comprehensive","[ **Prompting.** Pre-trained language models (PLMs) have shown impressive performance on a wide range of NLP tasks, yet they were not specifically trained on the downstream tasks. Prompt engineering is therefore an active research area that allows us to customize the prompts to guide PLMs to learn the desired task. Examples include [12, 13, 14] for text classification,  for text generation,  for zero-shot classification,  for cloze-style tasks, and  for task-specific pre-training.

Recent studies extend prompting to machine translation.  select the example by checking whether its input and output tokens are close. [3] formulate the prompting problem as an information theory task, minimizing the mutual information between the input and output. In contrast, the focus of [2] is to find the prompt with maximum mutual information in the absence of training examples, by minimizing the entropy of an oracle. Prompt engineering for machine translation is further investigated in  and .  improve prompting via reading comprehension.  and [5] develop a method to prompt PLMs to solve problems harder than those seen in the prompts. [4] propose Least-to-Most Prompting, which uses simple reasoning to gradually build the reasoning capabilities of PLMs to solve more complex problems. [1] demonstrate that using semantic similarity as a feature could predict the performance of examples better.

Prompt engineering could be categorized into two types, i.e., _fine-tuning_ and _zero-shot_ prompting. We focus on zero-shot prompting in this study. Different from zero-shot prompting, fine-tuning prompting aims to help PLMs learn a task after training with enough data of the task. Inspired by prompting, some other techniques have been proposed to help PLMs learn the translation of unseen language pairs. These techniques include Beam Search (), Search Engine-Guided NMT ([6]), re-implementation of back-translation ([9]), best practice ([10]), cross-lingual zero-shot transfer ([11]), and fine-tuning ([12]; [10]; ; ). Prompting may complement or replace these techniques.

**Translation.** As most PLMs are trained in an unsupervised manner, it is often challenging to transfer the learned knowledge of language to a new domain. In machine translation, such a challenge is well addressed by multilingual (zero-shot) translation. Early studies such as ([12]; [8]; [11]; [10]) have shown promising results on the English-German and English-Romanian language pairs, which provide the motivation for multilingual translation research. To better address the problem, back-translation is developed in (), and zero-shot decoding is further investigated in [10] and [10]. Prompting is]","Step 1: Important sentences from cited references.
- Reference [1]: ""Prompting has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities.""
- Reference [2]: ""We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model.""
- Reference [3]: ""We propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning.""
- Reference [4]: ""The recent Text-to-Text Transfer Transformer (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks.""
- Reference [5]: ""Least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts.""
- Reference [6]: ""The proposed approach consists of two stages. In the first stage retrieval stage, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence.""
- Reference [7]: ""Multi-Stage Prompting divides the translation process via pre-trained language models into three separate stages: the encoding stage, the re-encoding stage, and the decoding stage.""
- Reference [8]: ""We explore ways to improve massively multilingual models for neural machine translation (NMT) and deliver poor zero-shot translations.""
- Reference [9]: ""We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token.""
- Reference [10]: ""We demonstrate that natural language prompts allow us to influence properties like formality or specific dialect of the output.""
- Reference [11]: ""We introduce our efforts towards building a universal neural machine translation (NMT) system capable of translating between any language pair.""
- Reference [12]: ""Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language.""
- Reference [13]: ""We probe the abilities of the pathways language model (PaLM) to translate between languages and investigate various strategies for choosing translation examples for few-shot prompting.""

Step 2: Summarize sentences and decide on topics of related work.
- Prompting has shown superior performance across various NLP tasks.
- Methods for prompt template selection without labeled examples or direct model access have been introduced.
- Complexity-based prompting and least-to-most prompting have been proposed for effective example selection.
- Text-to-Text Transfer Transformer has achieved state-of-the-art results for English-language NLP tasks.
- Multi-stage prompting and ways to improve massively multilingual NMT models have been explored.
- Simplified back-translation techniques using tagging have been proposed.
- Natural language prompts can influence specific properties of output.
- Efforts have been made towards building a universal NMT system.

Step 3: Write related work based on topics.
<Recent advancements in natural language processing (NLP) have highlighted the superior performance of prompting across a wide range of tasks, particularly with its in-context learning abilities [1]. The introduction of new methods for prompt template selection without labeled examples or direct model access signifies a shift towards more efficient and accessible prompting strategies [2]. Additionally, the proposal of complexity-based prompting and least-to-most prompting for effective example selection demonstrates ongoing efforts to refine prompting techniques for multi-step reasoning and complex problem-solving [3, 5]. Meanwhile, the success of the Text-to-Text Transfer Transformer (T5) in achieving state-of-the-art results for English-language NLP tasks showcases the potential of text-to-text format and scale in advancing machine translation [4].>

<Further exploration into multi-stage prompting and strategies to enhance massively multilingual neural machine translation (NMT) models reflects a commitment to optimizing translation processes and improving performance across various languages and tasks [7, 8, 11, 12]. The proposal of simplified back-translation techniques using tagging not only presents a streamlined alternative but also contributes to refining the quality and effectiveness of synthetic parallel data in NMT [9]. Moreover, the demonstration of natural language prompts' influence on specific properties of output underscores the potential for targeted control and customization within machine translation systems [10]. These collective endeavors signify a concerted push towards establishing more effective, efficient, and versatile NMT methods and models.>

<In conclusion, the ongoing advancements in prompting strategies and the quest for universal NMT systems demonstrate the evolving landscape of machine translation and the persistent drive to enhance its capabilities and performance [11, 12, 13]. As research continues to uncover new methods and refine existing techniques, the potential for significant strides in NMT efficiency, accuracy, and adaptability remains a promising prospect for the future of language translation technologies.>"
3009," **Instance Segmentation.** Most of the existing instance segmentation methods [1][2][3][4][5][6][7][8][9] are based on fully-supervised learning. Mask R-CNN [4] is one of the most representative methods, which employs a two-stage pipeline to first generate a coarse mask and then refine it to obtain the final segmentation results. To improve the quality of the predicted masks, Mask Scoring R-RCNN [2] introduces a mask scoring branch to predict the confidence score of each predicted mask. In addition, some methods [6][1][7] combine the top-down and bottom-up approaches to obtain more accurate instance masks. For example, BlendMask [6] combines the predictions from the two stages to generate more accurate masks. Recently, Mask Transformer [8] and Masked-Attention Transformer (MAT) [9] have been proposed to address the problem of semantic segmentation with transformers.

**Weakly-Supervised Instance Learning.** Recently, weakly supervised instance learning methods [10][11][12][13][14][15] have attracted much attention due to the cheaper cost of obtaining instance masks compared with fully-labelled images. BBAM [12] proposes a bounding box attribution map (B-BAM) to highlight the most discriminative regions for each instance. BoxInst [13] extends BBAM by adding a mask prediction branch to generate the instance mask. Bounding box tightness prior (BBTP) [10] is proposed to estimate the tightness of the bounding boxes for each object and then generate instance masks based on the estimated tightness. SIS [11] proposes an instance-agnostic framework to learn instance masks with only image-level labels. Inter-Pixel Relations (IPR) [14] leverages the inter-pixel relations between pixels to generate instance-aware segmentation masks. WISIS [15] proposes to learn the instance masks by aggregating the instance-specific semantic information. Different from the above methods, our proposed method does not rely on any extra annotations and can be trained with only box-level annotations. Moreover, we propose a novel prototype-based semantic-aware instance mask generation paradigm to handle the occlusions between objects, which has not been explored in the existing methods. Note that our method is different from the methods that use object proposals [16][17] to generate object masks [18][15]. The object proposals in these methods are usually generated by a pre-trained object detector, which is time-consuming and labor-intensive. In contrast, our method directly generates instance masks from the low-level image features without the need of any object proposals or object proposals, which can achieve much faster inference speed and achieves better performance compared with the methods based on object proposals. Note also that our approach can be easily integrated into the aforementioned methods to further improve their performance.
**Prototype-based Methods.** The concept of class activation maps (CAMs) [19] has been widely used in the weakly-super"," **Instance Segmentation.** Instance segmentation is a fundamental task in computer vision and has been extensively studied in recent years. Mask-based instance segmentation methods [1][2][3][4][5][6][7][8][9] have been widely used in the literature. Mask R-CNN [4] is the pioneering work in this field, which adds a mask prediction branch to the backbone of a fully-supervised object detection network (FoDNet) to generate instance masks for each object instance. Mask Scoring R-RCNN [2] introduces a mask scoring strategy to calibrate the misalignment between mask quality and classification score. PANet [3] proposes a path aggregation network to enhance the information propagation between different levels of the feature pyramid. Mask2Former [9] introduces the masked attention module to learn per-pixel position-sensitive instance features with only one convolution layer, thus achieving state-of-the-art performance on the PASCAL VOC  and MS COCO  datasets. RefineMask [1] incorporates fine-grained features during the instance-wise segmenting process in a multi-stage manner to refine high-quality masks. BlendMask [6] combines instance-level information with semantic information with lower-level fine-granularity to improve the performance of instance segmentations. Recently, PANet  proposes a novel path aggregation module to enhance feature propagation between feature pyramid and bottom-up-down feature maps.

3 [3], SOLOv2 [7] and k-meanswers [8] propose to learn the mask head of the segmenter conditioned on the location of the instance mask. Different from the above methods, we propose a novel box-supervision based instance mask generation paradigm to handle the occlusions between objects and improve the semantic-aware mask generation ability. Our work is also closely related to the weakly-supervisory methods [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28].

**Weakly-Supervised Instance Segmentations.** Weakly supervised semantic segmentation (WSIS) is a more challenging task than weakly supervisory instance (WSSS) due to the lack of instance labels. Most of the existing WSIS methods [11][13] use the bounding box annotations as extra supervision.","[ As a core step of the most Mask R-CNN approaches [1][2][3][4], the segmentation branch aims to assign a mask for each object. We only review the recent state-of-the-art methods and include a brief summary of the previous approaches.

With the rise of a fully convolutional architecture in fully-supervised instance segmentation, there are many networks [5][6][7] for capturing the context information. For a less-annotated training data, Mask2Former [8][9] has successfully tackled the mask prediction problem without any additional annotation from label-free synthetic datasets. Unlike such methods which only focus on object boundary and occupancy, our work is geared towards obtaining category-level masks, in which the foreground objects need to be segmented while distinguishing from the nearby background pixels.

Weakly-supervised instance segmentation methods can be classified into two categories: 1) based on bounding box-level annotations, and 2) based on the image-level semantic labels [10][11][12][13][14][15]. Bounding box-supervised instance segmentation methods usually employ region-proposal based architecture such as [16][17][18], where the object instances are roughly detected in the first place and then these regions will be refined [14][18] or propagated [11] to the mask-level prediction. On the other hand, image-level labels [10][15] can generate instance-level attention maps with a dedicated region-proposal generator [19] and the classifiers can be trained in a multi-task manner to assign class labels to the discovered regions [20][21][22][23]. However, our method belongs to the former category, since it only requires simple bounding box annotations and hence has a good practical value. The core of the recent methods [11][13] is to address the figure-ground separation, in which objects are located in the superpixel segmented masks and the backgrounds are computed in the rest of the space.

In the training process, the instance segmentation model takes the generated pseudo labels for the region-proposal process and then these instances are used for generating pseudo labels of mask predictions. The methods that employ the box-supervised strategy are expected to outperform the image-level methods since the latter usually suffer from high false positive rates due to the inaccurate proposed bounding boxes. To handle the high false positive rates and enhance the boundary information, Wang _et al_.  proposed to focus on the inner regions of the object instances. BoYang _et al_. [11] built an instance segmentation pipeline to obtain accurate pseudo labels from the detected bounding boxes. Based on an image-level background model, IG _et al_. [24] uses such pseudo labels to alleviate the issues of domain g]","<Instance segmentation is a crucial task in computer vision, and weakly supervised instance segmentation using bounding box annotations has garnered much attention recently. Several state-of-the-art methods have been proposed to address challenges in weakly supervised instance segmentation. For instance, RefineMask [1] introduces a method for high-quality instance segmentation by incorporating fine-grained features during the instance-wise segmenting process. Mask Scoring R-CNN [2] addresses the problem of misalignment between mask quality and mask score in instance segmentation by proposing a network block to learn the quality of predicted instance masks. Path Aggregation Network (PANet) [3] enhances the information flow in proposal-based instance segmentation frameworks by boosting feature hierarchies and introducing adaptive feature pooling.

Another notable method is BoxInst [13], which achieves mask-level instance segmentation using only bounding-box annotations for training, demonstrating a significant improvement in performance compared to previous methods. Moreover, Weakly Supervised Learning of Instance Segmentation with Inter-Pixel Relations [14] presents a novel approach for learning instance segmentation with image-level class labels as supervision, where pseudo instance segmentation labels are generated and used to train a fully supervised model. Furthermore, GrabCut [16] and Multiscale Combinatorial Grouping (MCG) [17] offer insights into efficient, interactive foreground/background segmentation and hierarchical image segmentation, respectively.

In the realm of weakly supervised semantic segmentation, Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation [25] proposes a method to address the issue of noisy pseudo labels in self-training by directly exploring the pixel distributions of the target domain data. Self-Erasing Network for Integral Object Attention [21] introduces a method for improving object attention accuracy by prohibiting attentions from spreading to unexpected background regions. Additionally, Rethinking Semantic Segmentation: A Prototype View [27] presents a nonparametric framework for semantic segmentation, offering compelling results over various datasets, and demonstrating performance in the large-vocabulary situation.

Overall, recent advancements in weakly supervised instance and semantic segmentation demonstrate the potential for achieving high-quality results with limited supervision. These methods have introduced innovative approaches to handle challenges such as noisy annotations, misalignment between mask quality and score, and information flow in proposal-based instance segmentation frameworks, paving the way for improved performance in weakly supervised instance and semantic segmentation tasks.>"
1856," **Adversarial Attacks.** Szegedy _et al_. [1] first discovered the vulnerability of deep neural networks to adversarial examples, which can be crafted by adding small perturbations to the original input image. Goodfellow and LeCun [2] proposed the Fast Gradient Sign Method (FGSM) to generate adversarial samples, which is one of the most widely used attacks. After that, many variants of FGSM have been proposed, such as Basic Iterative Method (BIM) [3], Projected Gradient Descent (PGD) [4], DeepFool [5], Carlini and Wagner Attack (C&W) [6], and many others [7][8][9][10][11][12][13].

**Robust Training.** Adversarial training [2][3][4][14][15][16][17][18][19] has been shown to be the most effective defense against adversarial attacks. In adversarial training, a model is first trained on adversarially generated examples, and then the model is fine-tuned on the original data to achieve the desired robustness. In this way, the robustness of the model can be improved without compromising its accuracy. However, the adversarial example generated during training may not be the optimal one for the model to be robust. To address this issue, several works have proposed to add regularization terms to the loss function. For example, Madry _et.al_. [14] proposed a trade-off between adversarial robustness and accuracy, and proposed a method called TRADES, which minimizes the cross-entropy loss between the original model and its adversarial version, while maintaining its classification accuracy. Xie _et_. [15] found that the misclassified examples in the training data can be used to improve the robust performance of the models. They proposed a regularization term to penalize the difference between the predicted and the ground-truth labels, and showed that the resulting models are more robust.

Frequency Bias in Vision Transformers.Frequency bias has been widely studied in the field of vision transformers (ViTs) [20][21]. In [20], Fourier Domain Analysis (FDA) was used to analyze the anti-moversoothing effect of ViTs. In [21], the authors found that multi-head self-attentions (MSAs) in ViTs are sensitive to the low-frequency components of the input image, and that the high-frequency parts of the image are more discriminative for the task at hand. In, the authors studied the effect of the frequency bias in the context of the ViT architecture. They found that MSAs tend to focus more on the frequency components of input images, while the low frequency components are more important for the tasks at hand, and the high frequency components tend to be less important for these tasks. In addition, the authors showed that ViTs tend to memorize the frequency features of the original images, which leads to poor generalization to new datasets with distinct frequency features."," **Adversarial training.** Adversarial examples were first discovered by Szegedy _et al_. [1] and Szeged _et.al_. [2] in the context of machine learning. Since then, adversarial training [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22] has been widely studied as a way to improve the robustness of DNNs against adversarial examples. In this section, we briefly review the most relevant works in this area.

**Adversarially-trained models.** The adversarially trained models have been shown to be more robust to adversarial perturbations than the non-adversarial ones [3]. However, the computational cost of training a DNN is prohibitively high, making it difficult to deploy these models in real-world applications. To address this issue, several works have been proposed to improve model robustness. For example, [3] proposed to use adversarial learning [3], which trains the model on adversarial samples generated by maximizing the loss of the adversarial example. [4] proposed a method called Residual Gradient Descent (RGD) to train the model with adversarial attacks. [6] proposed three different adversarial attack methods based on different distance metrics. [13] proposed an adaptive boundary attack method to find the minimal perturbation necessary to change the class of a given input image. [5] proposed the DeepFool algorithm to efficiently compute perturbed images by optimizing the gradient of the loss function. [8] proposed Boundary attack to attack black-box models with limited information. [7] introduced a decision-based attack that solely relies on the final model decision. [10] proposed Input Diversity Attack to generate more transferable examples by applying random transformations to the input images at each iteration. [9] presented a translation-invariant attack method that optimizes a perturbed image over an ensemble of translated images, which is less sensitive to the white-box model being attacked and has better transferability. [11] presented the CKA metric to measure the similarities between the internal representations of different networks. [14] proposed TRADES to trade adversarial robustness off against accuracy. [15] proposed MART to explicitly distinguish the misclassified and correctly classified examples during the training process. [16] proposed","[ **Adversarial attacks and robust models** The ability of neural networks to classify natural images correctly has been well established in a plethora of benchmark datasets. However, it was shown by Goodfellow et al. [1] that a small perturbation can induce significant changes to the classification result. This phenomenon is well known as the adversarial example problem, which has brought great concerns to many fields. Today, there are a variety of methods to generate adversarial examples, e.g., FGSM [2], I-FGSM , DF [3], PGD [4], and DeepFool [5]. With the rapid development of robust learning approaches, a diverse set of attacks and corresponding models have been developed to showcase their performance [6][7][8][9]. Recently, Wang et al.  argued that the ability to classify adversarial examples has no significant correlation with the generalization of natural images. The main conclusions and discussions can be found in Section 3.

**Transferability of adversarial attacks** Beyond the capabilities to generate adversarial examples, there are studies on the evaluation metrics and evaluation techniques of adversarial attacks. Fawzi et al. [10] proposed an ensemble of diverse attacks to evaluate the transferability of adversarial attacks. Wei et al. [11] focused on the agreement of intermediate representations learned by different neural networks. Although they are very useful to study transferability and the robustness of deep neural networks, they cannot be used to directly compare different attack methods. Furthermore, among the few transferability evaluation tools, attack and defense evaluation benchmarks are of fundamental importance. One of the most popular transferability evaluation benchmarks is PI [12], which evaluates the transferability of PGD adversarial attacks through the neural network architecture and adversarial parameters. Xiao et al. [13] proposed a set of transferability evaluation benchmarks to test the quality of fast boundary attacks. Aiming at identifying the attack that is best at inducing transferability to a new model, the evaluation strategies have been developing in this direction. Among the available attacks and methods, there is still a gap between the behavior of high-frequency features of natural and adversarial examples, which has been neglected in most evaluation methods.

**Robustness and accuracy of adversarial models** There have been many studies that analyzed the correlation between robustness and accuracy of adversarial models. Schuster et al. [14] explored the trade-off between accuracy and robustness, and concluded that the accuracy decreases gradually when the robustness increases. Guo et al. [15] empirically demonstrated that the misclassified examples by adversarial examples will be correctly classified with high confidence. Wang et al. [16] provided a strong correlation between the flatness of weight loss landscape and robust generalization gap. Additionally, there are works like [17] that addressed the issue of ineffective weight perturbation and explained how the frequency of weight loss contributes to]","Step 1: Extracting important sentences from the cited papers: 

1. \[1\] states the vulnerability of deep neural networks to adversarial samples and reports properties such as the lack of distinction between individual high-level units and random linear combinations of high-level units. 
2. While \[2\] argues that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature, it also provides a new quantitative method for generating adversarial examples.
3. \[3\] discusses how adversarial training can be applied to large models and datasets, highlighting the recommendation for scaling adversarial training and the observation that adversarial training confers robustness to single-step attack methods.
4. In \[4\], the authors study the adversarial robustness of neural networks through the lens of robust optimization.
5. \[5\] introduces the DeepFool algorithm to efficiently compute perturbations that fool deep networks and reliably quantify the robustness of these classifiers. 
6. In \[9\], a method is proposed to generate more transferable adversarial examples against defense models, and it is shown to be generally applicable to any gradient-based attack method.
7. \[16\] identifies a correlation between the flatness of weight loss landscape and robust generalization gap and proposes Adversarial Weight Perturbation (AWP) to explicitly regulate the flatness of the weight loss landscape.

Step 2: Summarizing the sentences and deciding on topics for related work: 
The important sentences extracted from the cited papers suggest the following related work topics:
1. Understanding the vulnerability of deep neural networks to adversarial attacks, and the properties of adversarial examples.
2. Methods for generating adversarial examples and computing their perturbations.
3. Adversarial training and robust optimization techniques for neural networks.
4. Approaches for improving the transferability and generalization of adversarial examples.
5. Studies on the weight loss landscape and its correlation with robust generalization.

Step 3: Writing related work based on the chosen topics:
<>
The vulnerability of deep neural networks to adversarial samples and the lack of distinction between individual high-level units and random linear combinations of high-level units [1] underscore the need for understanding the properties and root causes of adversarial attacks. This has led to the development of methods for generating adversarial examples and computing their perturbations, as seen in the DeepFool algorithm [5] and the proposed Adversarial Weight Perturbation approach [16]. Additionally, the application of adversarial training to large models and datasets has been a focus, with advancements in scaling adversarial training and conferring robustness to single-step attack methods [3].

Understanding the vulnerability of neural networks to adversarial attacks has sparked interest in adversarial training and robust optimization techniques. Prior works have explored robust optimization through the lens of adversarial robustness, providing a broad and unifying view, and identifying methods for training and attacking neural networks [4]. Furthermore, the development of methods to generate more transferable adversarial examples against defense models has been a crucial aspect of improving the robustness of neural networks [9].

In addition to understanding the properties of adversarial examples, recent research has also delved into studying the weight loss landscape and its correlation with robust generalization. This has led to the proposal of the Adversarial Weight Perturbation approach, which explicitly regulates the flatness of the weight loss landscape, demonstrating a clear connection between the flatness of weight loss landscape and robust generalization [16]. These developments highlight the multifaceted efforts to comprehend and address the challenges posed by adversarial attacks, paving the way for more robust and resilient neural network models."
3219," **Single-Source Domain Generalization (SDA).** SDA aims to generalize a model trained on multiple source domains to a distribution unseen during training. To mitigate this issue, a predetermined distribution shift has been addressed independently [1][2][3][4][5][6][7][8]. For example, Zhang _et al_. [1] proposed an adversarial data augmentation method to augment the training data with adversarial perturbations. Li _et.al_. [2] proposed a meta-learning framework to learn domain-invariant representations. Wang _et_. [3] introduced a maximum-entropy adversarial training method to improve the generalization of SDA models. Zhang and Liu [4] proposed to adaptively learn the normalization parameters for each domain. Li and Zhang [7] proposed meta convolutional neural networks (MCNNs) to learn a shared feature space for multiple domains. Wang and Li [6] proposed learning to diversify (L2D) for SDA. L2D learns a feature extractor and a domain-specific classifier for each source domain, which are then combined to generate diverse samples for the target domain.

**Unbiased Learning.** Unbiased learning [9][10][11][12][13][14][15][16][17] aims to mitigate the impact of dataset bias on the model's performance. For instance, Zhang and Li  proposed a bias-contrastive and bias-balanced learning (BCL) method to learn unbiased representations by minimizing the distance between the biased features and the unbiased features. Wang [17] proposed unsupervised learning of debiased representations with pseudo-attributes. Li [16] proposed the conservative approach for unbiased learning on unknown biases. However, these methods assume the target distribution is known at training, which limits their applicability in real-world applications. In contrast, our method allows adaptive inference once we know the target, which is accessible only at testing.

 proposed a conservative approach to robustness on diverse sampling errors, which can be viewed as a special case of our proposed method. The main difference between our method and theirs is that our method adaptively adjusts the feature space at every prediction, while theirs only allows instance-wise adaptive inference.

 introduced a conservative framework for robustness to diverse sampling error. They proposed a regularization term to encourage the model to focus on the most discriminative parts of the input data. Our method differs from theirs in two aspects. First, we propose a unified framework for generalization on diverse distributions. Second, we focus on a more challenging problem, i.e., SDA, while their method focuses on SDA and de-biasing.

 presented a general framework for learning a robust model on diverse datasets. Their method is based on the idea that the model should be robust to the worst-case distribution shift between the training and test distributions. However they did not consider the case where the distribution mismatch is unknown at training. Therefore, their method cannot be directly applied to our problem.

 provided a theoretical analysis of the robustness of CNN"," **Single Domain Generalization.** Single domain generalization (SDA) aims to generalize a model trained on multiple source domains to a distributionally different target domain [1][2][3][4][5][6][7][8]. Most of the SDA methods are based on adversarial data augmentation [3][1][8], which augments the training data with examples from a fictitious target domain that is ""hard"" under the current model. For example, [1] proposed an iterative procedure that augments training data from a single source domain with examples of a fictitious domain that are ""hard"". [2] proposed a meta-learning scheme to create ""fictitious"" yet ""challenging"" populations, from which a model can learn to improve it's generalization with theoretical guarantees. [3] proposed to generate ""hard adversarial perturbations"" that are largely different from the source distribution. [4] proposed the adversarial domain augmentation (ADA) method, which generates adversarial examples from the target domain with a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). [5] proposed progressive domain expansion network (PDEN), which progressively generates multiple domains in order to simulate various photometric and geometric transforms in unseen domains. [6] synthesizes images from diverse distributions that are complementary to the source ones by maximizing the mutual information between the generated and source samples. [7] proposed meta convolutional neural network (MCNN) to decompose the convolution feature maps into meta features and reformulate them as a composition of related meta features. [8] proposed attention consistency loss to ensure that class activation maps across original and corrupted versions of the same training sample are aligned.


**De-biasing.** The de-biased representation learning (DRL) [9][10][11][12][13][14][15][16][17] aims to learn unbiased representations by removing the bias information from the model's representation. [13] proposed Bias-Contrastive (BiasCon) loss, which leverages the knowledge of bias labels. [15] proposed bias-balanced regression to train a model with balanced target distribution and bias-bias regression to learn a model that is biased toward the bias. [10] proposed representation learning with statistical independence (SIN) loss to learn features that have maximum discriminative power with respect to the task","[ **Distribution shifts**: We have to consider distribution shifts when data samples are collected on a limited domain and used for training and testing. Existing approaches such as domain adversarial learning ([11]; ; ; [2]; [4]; [6]; [5]) and adversarial data augmentation ([10]; [9]; [3]) try to learn representations independent of any specific domain. Recently, [7] and [8] present the benefits of learning invariant representations via domain-invariant classification layers. Similarly,  and  propose to define prototypical representations instead of domain-invariant representations. Nevertheless, these methods ignore the unreliable prediction due to unknown distribution mismatch among input images.

**Debiasing**: A preliminary work on debiasing is proposed by  and [12], but these techniques are limited in that a class is over- or under-learned at inference, thus leading to deterioration of generalization on the test domain. As a more reliable approach, several methods employ disentangled representations with the bias ([14]; [15]; [16]; [17]). One notable strategy is to re-train the model with diverse samples ([13]; ). However, the unsupervised bias learning methods need a self-training procedure, which is undesirable in practice.

Our method is not based on these existing approaches for mitigating bias. We propose an adaptive method to make predictions while referring to multi-domain distributions, so that we do not rely on a single domain data for training.

]","Step 1. Extract important sentences from the cited paper.

1. ""We are concerned with learning models that generalize well to different \emph{unseen} domains.""
2. ""We propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is 'hard' under the current model.""
3. ""We are concerned with a worst-case scenario in model generalization, in the sense that a model aims to perform well on many unseen domains while there is only one single domain available for training.""
4. ""We propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works.""
5. ""A promising solution is to learn cross-domain invariant representations by expanding the coverage of the training domain.""
6. ""To tackle this problem, we propose a style-complement module to enhance the generalization power of the model by synthesizing images from diverse distributions that are complementary to the source ones.""
7. ""We propose a new model, termed meta convolutional neural network, to solve the single domain generalization problem in image recognition.""
8. ""In this work, we propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased.""
9. ""Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible.""
10. ""Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift.""
11. ""We propose a conservative framework that employs this internal information for unbiased learning.""
12. ""Dataset bias is a critical challenge in machine learning since it often leads to a negative impact on a model due to the unintended decision rules captured by spurious correlations.""

Step 2. Summarize sentences and decide on topics of related work.

The important sentences cover a wide range of methods to address biases and robustness in machine learning models. Topics for related work could include: 
1. Adversarial data augmentation for domain generalization
2. Single domain generalization and domain expansion
3. Normalization methods and their impact on domain generalization
4. De-biasing techniques in deep neural networks
5. Unsupervised learning of debiased representations
6. Addressing dataset bias and superficial statistics in deep neural networks.

Step 3. Write related work based on topics.

<Adversarial data augmentation has been proposed as a promising method to improve model generalization to unseen domains [1]. The technique involves a worst-case formulation over data distributions and an iterative procedure for augmenting the dataset with examples from a fictitious target domain that challenges the current model [1]. Similarly, a style-complement module has been suggested to enhance model generalization by synthesizing images from diverse distributions complementary to the source domain [6]. These methods aim to address the distribution shift and improve the robustness of machine learning models by considering adversarial examples and diverse distributions.]

<In the context of single domain generalization, meta convolutional neural networks have been proposed as a new model to solve the problem of training with data from only one domain and achieving good performance on unseen domains [7]. Additionally, progressive domain expansion networks have been introduced to learn cross-domain invariant representations and have shown promise in improving generalization performance across multiple benchmark datasets [5]. These approaches focus on training models with limited domain data and aim to improve their generalization capabilities to diverse unseen domains.]

<Normalization methods have been investigated for their impact on domain generalization, particularly in the context of single domain generalization. The adaptive standardization and rescaling normalization approach have been proposed as a generic normalization method to improve model generalization performance across domains, especially on target domains with large discrepancies from the source domain [4]. These approaches highlight the importance of normalization techniques in enhancing model generalization to unseen domains.]

<De-biasing techniques in deep neural networks have been explored to address biases and superficial statistics that can impact model generalization. Novel regularization algorithms have been proposed to train deep neural networks in the presence of severely biased training data, aiming to mitigate the impact of biases on model performance [9, 16]. Unsupervised learning methods have also been suggested for debiasing representations by leveraging pseudo-attributes identified through clustering in the feature embedding space, demonstrating the efficacy of the approach on multiple standard benchmarks [17]. These methods aim to mitigate the negative impact of dataset bias and superficial statistics on model generalization.]

<In conclusion, the methods and techniques discussed above demonstrate a diverse range of approaches to address biases and improve robustness in machine learning models. These approaches offer valuable insights into mitigating the impact of biases and improving model generalization to diverse and unseen domains, contributing to the advancement of robust machine learning algorithms.>

Please note that the related work above is a simulated example and the content is generated based on the provided sentences from the cited documents."
823," **Argument Maps.** There is a large body of work on argument maps ; [3]; [4]; [6]; [2]; [5]; ; ; ;. Most of them focus on identifying argument components and their relations. [1] use contextualized word embeddings for argument classification and clustering. However, they do not consider the unidirectional relation between argument nodes.

**Sentence Embeddings.** Sentence embedding models [11]; [7]; [10]; [8]; [12]; [13] have been shown to be effective in many NLP tasks. Most of these models are based on the Siamese architecture [9]. They learn a similarity function between two sentences by minimizing the distance between them and maximizing that between dissimilar sentences. In this work, we use the BERT embedding model [11] as our baseline model. We show that encoding a node into different embedding vectors for each of the parent and child cases improves the performance of our task. We also show the effectiveness of our approach in the few-shot setting. To the best of our knowledge, this is the first work to study the problem of node placement in argument maps, and we show that it is challenging for both humans and models, especially in high and low-resource scenarios. We conduct experiments with various models and training strategies, and establish an upper-bound of human performance on the task of node-placement. We further show that our approach can be easily adapted to few- and zero-shot learning scenarios. In addition, we conduct an annotation study to highlight the ambiguity of the task, and show that the task is more challenging for humans than for models, and that we can achieve better performance than humans on this task in low- and high-resource settings. We leave further investigation of the relation between our task and other sentence embedding tasks for future work.
**Few-shot Learning.** Our work is also related to the few shot learning literature [20]; [18]; [19]; [16]; [14]; [15]; [17]. These works show that large pre-trained language models can be fine-tuned in a few steps for a variety of tasks, and can achieve state-of-the-art performance on a wide range of tasks. Our work differs from these works in that we focus on the case where only a small number of arguments are available for each node in the argument map, and the goal is to place new arguments under suitable parents for the node.

 propose a similar task to ours, but they only consider binary relations between arguments. In contrast, our task considers both binary and non-binary relations between nodes in the map.

 also propose a task similar to ours in the context of argument retrieval. Their task is to retrieve arguments that are most relevant to a given topic from a large corpus, and to rank the arguments based on their relevance to the given topic. They use a two-step approach, where they first retrieve arguments from the corpus and then rank them based on similarity to the retrieved arguments. Our task differs from theirs in two ways."," Argument Maps.Argument maps have been used for many NLP tasks, such as argument search [1], argument parsing [3], argument retrieval [2], argument mining [4], and argument polarity prediction [5], among others. In this work, we focus on the task of node placement in argument maps.

Sentence embeddings.Sentence-pair regression tasks such as semantic textual similarity (STS) [7]; [8]; [10]; [9] and sentence embedding transfer [12]; [13]; [12] have been extensively studied in recent years. BERT [11] and its variants [11]; [1]; [7] have achieved state-of-the-art results on STS tasks. However, BERT-based models suffer from representation collapse [16]; [17]; [15]; [18]; [14]; [16], which means that they cannot perform well in the few-shot setting. To address this issue, many works have focused on improving the performance of BERT models in the low-resource setting. For example, [14] propose to replace BERT's verbalizers with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factor of 5 and 100, respectively. [17] propose a prompt-free framework that uses BERT as a language model to generate task prompts and convert them into positive and negative pairs. [15] propose T-Few, a parameter-efficient method that scales activations by learning a small set of parameters for each input sentence. [12], on the other than BERT, propose to use contrastive learning to fine-tune BERT in an unsupervised and self-supervised manner.


","[ **Argument mining:** Argument mining [4]; [6]; [3]; [2] aims to extract argumentative content from natural language text. Typically, this is done by decomposing discourse into its components, such as arguments, counterarguments and rebuttals. Recent work has addressed the task of finding the best reply to a post [2], using fine-tuned LMs [1]; [5] or enabling task-specific negative samples . We contribute to this line of work by arguing for the problem of automatic placement of nodes in argument maps.

**Learning sentence embeddings:** The best way to measure the relevance of two sentences is by computing their similarity [9]; [5]. Many works have shown that the BERT architecture [11] reaches good results on the STS benchmark  and other sentence similarity tasks [10]; ; [7]; [8]. Recent work suggests that adding or taking away knowledge from BERT affects its performance [12]; ; . We focus on a variant of STS where relevance is a matter of a parent-child relation.

**Learning with less data:** Recent research in NLP aims at generalizing pre-trained models to new datasets, allowing for small supervised dataset sizes . Exploring unlabeled data has also become a mainstream approach to enhance the performance of pre-trained LMs . This body of work includes few-shot fine-tuning [13]; [18]; [14]; [15], in-context learning [16]; [17], or generation [20]; [19]. In this work, we aim to generalize a model across a task to a larger domain by handling missing relations, i.e., ignoring arguments in the training data and placing them during the inference.

]","<Related work>
Argumentation and discourse mapping have been active areas of research in the field of natural language processing and information retrieval. Several studies have focused on structuring and analyzing discourse through argument maps, which organize the relationships between different arguments into a hierarchical tree structure [1]. These maps serve as a visual representation of the interconnectedness of arguments, making the discourse more comprehensible and less redundant. However, as highlighted in the target paper, the challenge lies in maintaining and expanding these maps, especially in large-scale, high-resource scenarios such as online discussions. The task of node placement, as introduced in the target paper, aligns with the broader efforts to support users in structuring and navigating complex argument maps.

Prior work in this domain has explored various aspects of argumentation and discourse modeling, often with the aim of enhancing user interactions and comprehension in online discourse platforms. For example, studies have investigated techniques for automating argument mapping and visualization to support users in organizing their arguments and counterarguments effectively [2]. Additionally, research has addressed the challenge of identifying the relationships between different nodes in argument maps, which is crucial for tasks such as node placement and suggestion of candidate parents for new contributions [3]. These efforts are closely related to the task proposed in the target paper, as they converge on the overarching goal of facilitating meaningful interactions and decision-making in discourse environments.

Furthermore, the target paper's focus on modeling unidirectional relations in argument maps aligns with broader research on modeling semantic and relational structures in natural language processing. Prior work has explored the use of embeddings and graph-based representations to capture the complex relationships between different elements in textual data, including arguments and their connections [4]. The application of such techniques to the specific task of node placement in argument maps, as presented in the target paper, contributes to the growing body of research on leveraging advanced language and knowledge representations for discourse modeling and analysis.

In addition, the target paper's emphasis on the challenges posed by large-scale, heterogeneous argument maps is situated within the context of efforts to address scalability and diversity in discourse analysis and decision support systems. Research in this area has investigated scalable methods for processing and analyzing large volumes of textual data, with a focus on maintaining the coherence and structure of complex discourse representations [5]. The target paper's experimental findings on the few-shot effectiveness of the proposed approach also resonate with the broader interest in developing robust and adaptable models for analyzing and interpreting textual data in diverse domains and applications.

Overall, the task of node placement in argument maps, as addressed in the target paper, intersects with a rich body of related work in argumentation theory, discourse modeling, natural language processing, and knowledge representation. The proposed modeling techniques, experimental insights, and practical implications contribute to the ongoing research efforts aimed at enhancing the accessibility, usability, and effectiveness of discourse analysis and decision support systems in various domains."
1105," **Audio-driven 2D Facial Animation.** Audio-driven talking head generation methods [1][2][3][4][5] aim to generate a video of a target actor based on a short reference video. These methods can be divided into two categories: one-shot methods and two-stage methods. The methods in the first category generate talking head videos from a single input video. For example, MakeItTalk [2] generates expressive talking heads from a facial image with the help of a pre-trained 3D Morphable Model (3DMM) [6]. Audio2Head [7] and LipSync3D [8] synthesize talking heads with natural head motion by warping the input video to the target actor's 3DMM. However, these methods do not consider the identity-specific speaking style and facial idiosyncrasies, thus, resulting in unrealistic and inaccurate lip movements. To address this, we introduce a novel loss function based on detected bilabial consonants to ensure plausible lip closures and consequently improve the realism of the generated expressions.

The second category of methods [9][10][11] synthesizes 3D facial animation based on the input audio. For instance, Neural Radiance Fields (NeRF)  have been used to synthesize novel facial expressions from 2D facial images. For these methods, the input is a 2D image and the output is a 3D face mesh. For the methods in this category, the 3D geometry of the target face is first deformed to match the audio. Then, the synthesized facial expressions are decoded from the deformed 3D mesh. In this paper, we focus on the second category. We propose a method that learns a prior from a large facial expression dataset, which is used as a prior for audio-driven facial animation. We then utilize this prior to optimize for the personalized speaking style of a given target actor. To the best of our knowledge, we are the first to propose a speech-driven face animation method that considers the identity specific speaking style, facial idiosyncratics, and the facial topology. We show that our method outperforms the state-of-the-art methods in terms of both audio fidelity and visual quality. Moreover, our method is able to generate high-quality facial animations for virtual reality and telepresence systems, which have not been explored by previous methods. We provide a detailed comparison of our method with existing methods in Tab. 1.
**Speech-Driven 3D Face Animation Methods.** Early methods [12][13][14][15][16][17][18][19][20] use hand-crafted features to drive the animation of 3D faces based on speech. For examples, MikeTalk [12] and JALI [14] use morphing visemes, which are a set of low-frequency components extracted from the mouth region. These components are then used to drive a sequence of high-frequency morphemes to animate a target face. Recently, deep learning-based methods have been proposed to drive facial animations based on audio [17][20][19]. For example"," Speech-driven 3D Facial Animation.Speech-driven facial animation has been widely explored in the past few years. Early methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] are mainly based on 2D facial landmarks or 3D face models. For example, You said that? [1] and MakeItTalk [2] use a joint embedding of the target face and audio to generate talking face video frames. Audio2Head [7] uses a keypoint-based dense motion field representation to model the head pose and lip motion. Audio-2Head generates photo-realistic talking head videos with natural head motion. However, it requires a large amount of training data to learn the head motion model, which limits its applicability to new speakers. Audio3D [8] uses 3DMM [6] to model head pose, expression, and lighting to synthesize high-quality talking face videos. AudioRNN [5] uses audio and a short video of the source person to learn personalized head pose. AudioFlow [4] generates high-resolution videos with arbitrary expression and lighting by using a flow-guided video generator to synthesise high-definition videos. However these methods only consider the identity-specific speaking style and facial idiosyncrasies, and do not consider the lip movements of the actor. In contrast, we propose a method that learns identity-aware details from a short reference video and produces personalized facial expressions.
-driven speech-driven video generation methods [8][10] are based on the idea of audio-driven face synthesis, which synthesizes personalized facial attributes such as eye blink and lip movements.

Audio-driven talking head generation.Audio2Head and AudioFlow use audio to model personalized attributes like eye blink, lip movements, and mouth movements. Audio1Head [1], Audio2head [7], AudioFlow  and Audio2Face [8], all use audio as input to generate personalized attributes. AudioE3D  uses audio to learn a personalized 3D head pose model and a 2D texture atlas to model facial geometry and texture, and uses the 3D geometry to model face pose and expression. AudioNeRF [10] and AD-NeRF[10] use audio and audio-guided neural radiance fields to synthesized talking head video. AudioRadiance [9] and D","[ Audio-driven face animation is a well-studied problem. Since the pioneering work of 3D morphable model of the human face [1], the main focus of this field has been on producing natural talking head videos with expressive speaking styles. In the early works, several 3D face models [2][3][4][5][6][7] were designed to deform the neutral face model with the speech signal. However, the effects of the high-level features like facial expressions and poses were often neglected. 3D audio-visual datasets have also been used to leverage the correlation between audio and facial animation [8][9][10][11][12]. However, these methods do not take into account the identity-specific characteristics and depend on the target facial model to generate the animation. In contrast, our method uses a learned identity-specific prior to optimize the audio-driven speaking style and head pose.

A line of work has used viseme-level features as input to generate facial animation. A viseme is a sequence of still frames acquired by observing a face when a specific audio is played [13][14][15]. When played with a speech signal, they animate the face accordingly and produce satisfying speech animation. Due to the disjoint nature of audio and visemes, different works [13][16] have attempted to learn a mapping from a temporal phonetic sequence to the corresponding viseme animation to produce realistic and synchronized facial animations. These methods used unsupervised neural networks [17] or reinforcement learning (RL)  to learn mappings. However, they assume a neutral face with no personalized identity-specific characteristics. They are also capable of producing only static lips and head motion.

To better capture the dynamics of the face and lip, a line of work has instead learned a mapping from the full audio signal to the face animation. This is especially important for generating a face model based on the input audio, which can be a limiting factor for the expressive facial animation [18][19]. Recently, two concurrent works [18][19] have proposed end-to-end approaches that use 3D facial models (e.g., FaceShop , FaceShop3D , FaceFormer [20]) as the target space. FaceFormer  is one of the best performing methods for speech-driven facial animation. It uses a deep neural network with an LSTM architecture and a generative adversarial network (GAN) framework for input audio. To learn realistic speech animation, FaceFormer [20] uses a large-scale 3D audio-visual dataset  as training data, and has good lip motion generation and synchronization capability. However, it does not consider facial idiosyncrasies. This is because it does not require the video data of the target speaker to train the model. Our proposed method explicitly exploits a facial animation dataset to learn]","Step 1. Extracted important sentences from the cited papers:

1. ""We present a method for generating a video of a talking face."" [1]
2. ""We present a method that generates expressive talking heads from a single facial image with audio as the only input."" [2]
3. ""We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photo-realistic video."" [3]
4. ""One-shot talking face generation should synthesize high visual quality facial videos with reasonable animations of expression and head pose, and just utilize arbitrary driving audio and arbitrary single face image as the source."" [4]
5. ""Real-world talking faces often accompany with natural head movement."" [5]
6. ""In this article, we provide a detailed survey of 3D Morphable Face Models over the 20 years since they were first proposed."" [6]
7. ""We propose an audio-driven talking-head method to generate photo-realistic talking-head videos from a single reference image."" [7]
8. ""In this paper, we present a video-based learning framework for animating personalized 3D talking faces from audio."" [8]
9. ""We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face."" [9]
10. ""Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output."" [10]
11. ""We observe that the input audio is highly correlated to lip motion while less correlated to other personalized attributes (e.g., head movements)."" [11]
12. ""We present MikeTalk, a text-to-audiovisual speech synthesizer which converts input text into an audiovisual speech stream."" [12]
13. ""The approach follows a kind of bootstrap procedure."" [13]
14. ""We present a system that, given an input audio soundtrack and speech transcript, automatically generates expressive lip-synchronized facial animation that is amenable to further artistic refinement, and that is comparable with both performance capture and professional animator output."" [14]
15. ""An incoming speech signal is time aligned using a speech recognizer to generate a phonetic sequence which is then converted to the corresponding viseme sequence to be animated."" [15]

Step 2. The topics of the related work:
- Methods for generating talking face videos from audio and image inputs
- One-shot talking face generation
- Audio-driven talking-head generation with personalized head movement
- Survey of 3D Morphable Face Models
- Dynamic neural radiance fields for facial animation
- Speech-driven facial animation and lip synchronization
- Viseme model for speech-driven facial animation

<Related work>
Methods for generating talking face videos from audio and image inputs have been extensively explored [1][2][3]. These methods aim to generate expressive talking heads from either a single facial image with audio as the only input, or from a sequence of audio to synthesize a photorealistic video. Other approaches focus on one-shot talking face generation, aiming to synthesize high visual quality facial videos from arbitrary driving audio and single face image as the source [4]. Additionally, there are methods that tackle the challenge of natural head movement accompanying real-world talking faces, which involves personalized head pose, expression, and lip synchronization [5].

The development and survey of 3D Morphable Face Models have been extensively described over the past two decades [6]. This research has led to various advancements in the field of 3D facial animation, including the proposal of dynamic neural radiance fields for modeling the appearance and dynamics of a human face [9], and the introduction of an audio-driven neural radiance field for talking head synthesis [10].

Efforts have also been made to personalize talking head generation by disentangling face attributes such as lip movement and head poses from input audio signals, which has shown promising results in achieving high-quality and personalized talking head generation [11].

Furthermore, the utilization of viseme models for speech-driven facial animation has been a subject of interest, focusing on accurately synchronized mouth movements during speech generation [12][15].

Overall, the diverse approaches and techniques in generating speech-driven facial animation have significantly advanced the state of the art in creating natural, expressive, and personalized talking head videos."
1788," **Multi-label Image Recognition.** Early multi-label image recognition methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18] are based on hand-crafted features. For example, Wang _et al_. [7] propose to exploit the correlations among labels to improve the recognition performance. Zhang _et.al_. [2] propose a joint class-aware map disentangling and label correlation embedding method to learn label-specific features. Li _et_. [3] introduce a graph convolutional network (GCN) to model the label correlations. In [10], a CNN-RNN framework is proposed to jointly learn the image features and label dependencies.

**Prompt Tuning.** Prompt tuning [19][20][21][22][23][24][25][26] is an efficient way to adapt large vision-language pre-trained models (_e.g_. CLIP) to various downstream tasks in data-limited or label-limited settings. Specifically, CLIP [20] proposes a visual prompt tuning (VPLT) framework to adapt CLIP to various vision-and-language tasks. In VPLT, visual data (_i.e_. images) is by default prerequisite for learning prompts in existing methods. In contrast, we propose to treat texts as alternatives to images for prompt tuning and introduce Tal prompting. In addition, we further propose TaI-DPT to extract both coarse- and fine-grained embeddings for enhancing the recognition accuracy.

 propose a prompt distribution learning (PDL) method to adapt a CLIP model to address the limited annotations problem. In PDL, image-text contrastive learning is employed to align the two modalities for training CLIP. In this work, we advocate that the effectiveness of contrastive loss in aligning images and texts further makes it feasible to train CLIP with Tal prompting, where sentences in the wild serve as alternatives for images.

 introduce a prompt learning framework (CPL) for adapting a VL model to the zero-shot setting. In CPL, the model is first fine-tuned on the test set and then applied to the downstream tasks. Different from our work, CPL does not consider the multi-instance learning (MIL) problem, which is more challenging than multi-class classification. Moreover, CPT [21] introduces a colorful prompt tuning framework (CPT-L) to adapt VL models to the few-shot learning task. Differently, our proposed Tal prompting does not require any training data and can be applied to both zero- and few-label settings.

 present a multi-task learning (MTL) framework (MTL-Net) to learn multi-modal representations from a large-scale unlabeled image dataset. In MTL-net, a multihead attention network (MAN) is introduced to extract multi-level features from the image and text, which are then fed into a classifier to predict the corresponding labels."," **Multi-label image recognition.** Multi-label recognition has been extensively studied in the past decades [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26]. In this work, we focus on the multi-label classification task, which is more challenging due to the presence of multiple labels in an image. Early works [7][4] mainly focus on exploring the label dependencies among different labels. For example, Zhang _et al_. [7] proposed a regional latent semantic dependency (RLSD) model to capture label dependencies at the local and global level. In [4], a two-stream framework was proposed to recognize multi-class objects from global image to local regions, similar to how human beings perceive objects. In, a multi-task learning framework was presented to learn the label dependency among different images. In addition, Zhang [2] proposed to disentangle class-specific maps corresponding to discriminative category-wise information and then evaluate the label co-occurrence of these maps.

**Prompt tuning.** Prompt tuning has been employed as an efficient way to adapt large vision-language pre-trained models (_e.g_. CLIP) to various downstream tasks in data-limited or label-limited settings [20][20]. In [20], a small amount of model parameters are frozen in the input space while keeping the model backbone frozen. In contrast, VPT [20] introduced the idea of prompt tuning for large-scale pre-training models (_i.e_., CLIP, VLMs) and proposed Visual Prompt Tuning (VPT) to adapt CLIP to image classification tasks. In VPT, a small portion of model weights are frozen while the rest of the model is fine-tuned. In CPT [21], a cross-modal prompt tuning method was proposed for VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap between the training and testing datasets. In UDA [19], a prompt learning paradigm was introduced to align the feature spaces of source and target domains for domain adaptation [19].

In contrast to VPT and UDA, we propose to treat texts as images for","[ Multi-label Image RecognitionMulti-label image recognition is a fundamental problem with practical importance because real-world images generally contain multiple labels [1]. Multi-label image recognition suffers from both sparsely labeled images and limited or no human-labeled labels, which has been addressed in the literature by employing weak or weakly supervised learning [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18]. As a typical method, weakly supervised learning  aims to obtain class labels with some known ground-truth, and early-stage learning methods [2][3] generate human-independent proposals to alleviate the need of precise annotations for the final recognition. Specifically, by simultaneously extracting visual features and class representations of cropped image patches, these early-stage learning methods can reduce the annotation burden and also improve the recognition accuracy.

Moreover, multiple-instance learning (MIL) is another paradigm for learning with sparse labels. Early MIL methods [16][17][18] focus on mapping each image patch into a class-independent representation and then generating new samples for every label category through cluster combination. Recent MIL methods [3][4][5][6][9] attempt to integrate multiple categories into one representation space to perform binary classification. Generally speaking, early MIL methods obtain accuracy in a consistent range, while the performance of recent MIL methods is usually unsatisfactory. More importantly, all these methods assume that the labels are available, while multi-label image recognition with partial annotations has also received increasing attention. For instance, by introducing the idea of pseudo-labeling and learning pseudo labels with additional constraints, there are some methods [12][13][18] that take advantage of partial labels to alleviate the label complexity and improve the model performance.

Prompt TuningPrompt tuning is originally proposed by the CLIP team to adapt vision-language pre-trained models to different downstream tasks, and has received considerable attention in recent years. The main idea is to replace the parameters of backbone layers with prompts to achieve knowledge transfer [19][20][21][22][23][24]. More concretely, the CLIP paper [19] shows that the prompt-tuned model can be viewed as a new model with learned parameters optimized on downstream datasets, while DPR [20] introduces a simple training procedure. The work of CPT [21] learns image-text co-descriptions for early prompts to be improved for downstream tasks. Several approaches [22][23][24] further extend prompt tuning to multi-task learning. However, only a few methods [25][26] focus on multi-label image recognition. We are inspired by this fact and provide an alternative and easy way to]","<The task of multi-label image recognition has garnered significant attention due to its practical importance in various computer vision applications. Previous works have focused on different aspects to improve the performance of multi-label image recognition models. Asymmetric Loss (ASL) has been introduced as a novel loss function to address the positive-negative label imbalance problem in multi-label classification [1]. The proposed ASL dynamically down-weights easy negative samples and discards mislabeled samples, leading to improved mean Average Precision (mAP) scores. Additionally, methods like Multi-Label Image Recognition with Graph Convolutional Networks (GCN) have been proposed to capture label dependencies by building a directed graph over object labels and applying GCN to map this label graph into a set of inter-dependent object classifiers [3]. These classifiers, when applied to image descriptors, resulted in improved multi-label image recognition performance.>

<Furthermore, recent advancements have explored the use of reinforcement learning for multi-label image classification. Reinforced Multi-Label Image Classification by Exploring Curriculum introduces a reinforcement learning approach for sequentially predicting labels, inspired by curriculum learning mechanisms observed in human learning processes [5]. This approach exploits image features and previous label predictions to maximize long-term reward and has demonstrated its effectiveness in multi-label image classification tasks. Moreover, methods such as CNN-RNN: A Unified Framework for Multi-label Image Classification incorporate recurrent neural networks to predict multiple labels within a query by observing a few supporting examples [10]. The model learns a joint image-label embedding to characterize semantic label dependencies and has shown improved performance compared to traditional multi-label classification models.>

<Several works have also addressed the challenge of adapting pre-trained vision-language models to downstream tasks. For instance, Visual Prompt Tuning (VPT) has been proposed as an efficient alternative to full fine-tuning for large-scale Transformer models in vision, achieving significant performance gains compared to other parameter-efficient tuning protocols [20]. Additionally, Prompt Distribution Learning has been introduced to learn low-bias prompts from a few samples and capture the distribution of diverse prompts to handle varying visual representations effectively [25]. These approaches have demonstrated their effectiveness in efficiently adapting pre-trained vision-language models to downstream recognition tasks.>

<Moreover, recent developments have focused on leveraging partial labels for multi-label image recognition. Learning a Deep ConvNet for Multi-Label Classification With Partial Labels addresses the challenge of reducing annotation costs by training models with only partial labels, and introduces a new classification loss that exploits the proportion of known labels per example [16]. Similarly, Structured Semantic Transfer for Multi-Label Recognition with Partial Labels proposes a structured semantic transfer framework that enables training multi-label recognition models with partial labels by leveraging within-image and cross-image semantic correlations to transfer knowledge of known labels to generate pseudo-labels for unknown labels [17]. These approaches offer valuable insights into training multi-label recognition models with partial annotations and have shown promising performance in overcoming data annotation challenges.>

<In addition to these advancements, recent works have introduced creative solutions to improve multi-label few-shot learning. LaSO: Label-Set Operations Networks for Multi-Label Few-Shot Learning addresses the scenario of multi-label few-shot classification by synthesizing samples with multiple labels, enabling augmentation for examples of novel categories and facilitating multi-label few-shot classifier learning [14]. Furthermore, Meta-Learning for Multi-Label Few-Shot Classification extends several solutions specifically designed for single-label few-shot learning to work in the multi-label regime, demonstrating the benefits of the proposed label-propagation algorithm in conjunction with the neural label count module for multi-label few-shot learning [15]. These methods offer innovative approaches to multi-label few-shot learning, showcasing the ongoing efforts to address the challenges of learning from limited examples in a multi-label setting.>"
341," **Contrastive Learning.** Recently, contrastive-based methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] have achieved great success in the field of self-supervised representation learning. The core idea of contrastive learning is to pull the representations of positive pairs closer and push those of negative pairs apart. The positive pairs are usually generated by different augmentations of the same image. MoCo [2] and SimCLR [1] use a momentum encoder to maintain a queue of negative samples, while SimSiam [6] and BYOL [5] do not use negative samples at all. CPC [3] and SwAV [4] propose to learn representations by clustering the samples into a set of prototypes, where each prototype is treated as a separate class. In this paper, we adopt the contrastive loss in [2][1] to learn the representation of each superclass.

**Supervised Contrastive Learning (SCL).** SCL [10] is proposed to replace the cross-entropy loss in the supervised setting with a supervised contrastive objective. It uses the labeled data to generate positive pairs and pulls the representations from the same class closer and pushes those from different classes apart. TCL [11] extends SCL to the long-tailed classification problem and proposes a target-aware loss to improve the performance of long-tail classes. In [12], TCL is applied to the natural language processing task and achieves state-of-the-art performance. Supervised contrastive replay (SCR) [13[13] proposes to use the nearest class mean classifier in online class-incremental continual learning. However, all these methods are designed for classification tasks and cannot be directly applied to superclass scenarios. In contrast, our SCL is designed for superclass learning and can be easily applied to other classification tasks. In addition, we provide theoretical analysis on the generalization error bound of SCL and show that it can be bounded by the number of superclasses. Besides, SCLRE can be applied to various classification tasks, such as image classification, object detection, semantic segmentation, and image retrieval, which is beyond the scope of this paper. In order, we also provide a detailed comparison with other contrastive methods in Sec. 4.2.2 and show the superiority of our method over SCL on CIFAR-100 and four high-resolution datasets. In the following, we briefly review the related works in the fields of natural language and computer vision, and then introduce the self-attention technique.
**Self-Attention.** Transformer [15] is first proposed by Vaswani _et al._[15] for the task of machine translation and has become the de facto standard in NLP. BERT [16] and its variants [17][18][19] are pre-trained on large-scale unlabeled corpora and achieve great success on various NLP tasks. Inspired by BERT, ViT [20] and MA"," **Contrastive Learning.** Recently, contrastive learning methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] have achieved great success in self-supervised learning. These contrastive-based methods [2][1][4] learn representations by pulling together samples from the same class and pushing apart samples from different classes. SimSiam [6] and SimSio [6], which are based on instance discrimination, use a batch contrastive loss [10] and a stop-gradient operation [13] to minimize the distance between positive samples and push apart samples of different classes, respectively. SimCSE [9] and BYOL [5] are two recent works that use the instance discrimination loss and a momentum encoder [2] to learn representations. However, these contrastive methods suffer from the problem of class collision problem. To address this problem, some works [14][11] propose to use the supervised contrastive method [10][12] to improve the performance of contrastive approaches. For example, [12] proposes a token-level supervised method that aims at maximizing the distance of representation of different punctuation marks in the embedding space. [11] proposes to use a targeted contrastive approach to learn the representation of long-tailed classes. [13][13] propose a supervised method to leverage the Nearest Class Mean Class-Mean (NCM) classifier to reduce the recency bias in online class-incremental continual learning.


However, these methods are designed for classification tasks, which are not suitable for superclass scenarios. In this paper, we propose a superclass learning framework, which is more suitable for the superclass scenario.

 is a recent work that proposes to learn superclass-aware decision boundary by leveraging the self-attention technique. Theoretically, we prove that by leveraging attention techniques the generalization error of SCLRE can be bounded under super class scenarios.

","[ **Self-supervised Learning.** To boost the representation power of a network without additional annotation cost, contrastive learning techniques are often applied for image recognition. To better align with this trend, we use the Siamese network ([37; 38]) as the core architecture in our approach. First proposed in , contrastive learning is firstly based on the key idea of discriminative learning. Thus, it suffers from heavy over-fitting on data and collapse with the limited diversity of positive samples. To alleviate these issues, large batch ([2]; [5]; [4]) and momentum pre-training ([6]) are proposed to reduce the risk of mode collapse. After that, [1] propose to learn better embedding by adaptive instance weight in large batches. Following this, graph contrastive learning ([7]) is firstly proposed to train image models for multi-modal data. To apply the contrastive learning framework in speech recognition, [8] propose a self-supervised approach by generating images from text description. [9] formulate a simple contrastive learning framework for sentence embedding, then,  extend the self-supervised learning to audio domain.

To handle these problem,  transforms multiple views of an image and applies a ProtoNet architecture to recognize the generated transformation of original images. [10] combine the linear projection and nearest neighbor to recognize images in contrastive learning. [11] introduce supervised contrastive learning in a long-tail recognition scenario. [12] apply supervised contrastive learning to token level to mitigate the influences of class imbalance.

**Superclass Scenarios.** A series of superclass-related methods are proposed to handle this class of problems, such as incremental learning (; ), supervised contrastive learning (), weakly supervised contrastive learning ([14]), and replay training (; [13]).  propose an attention-based model to address the problem of noisy superclasses and support the existence of label errors in the raw categories. [15] propose to compare feature representations in each epoch and perform fine-tuning by solving the class collision issue in contrastive learning.

**Self-supervised Learning in Natural Language Processing (NLP).** Natural language pretraining techniques have shown to improve many NLP performance metrics, such as modeling large amounts of data, reducing model training time and resulting in competitive performance ([16]; [19]; [17]; [18]). Most of these methods depend on the massive amount of raw text and build complicated architectures, such as BERT ([16]), ALBERT ([17]), RoBERTa ([19]), GPT-3 ([18]) and etc.

Since transformer ([20]; [15]) architecture is a prevailing and powerful framework to use as backbone in natural language processing, the representations extracted by transformer and self-supervised learning are applied to various problems, such as video recognition (; ), image]","<The problem of superclass learning, where data is organized into artificial super categories, has garnered significant attention due to its real-world applicability [1]. Existing classification techniques struggle to recognize super categories without raw class labels, leading to performance degradation or requiring extensive annotation costs. To address this gap, the SuperClass Learning with Representation Enhancement (SCLRE) framework was proposed in the target paper [2]. SCLRE leverages enhanced representation to collapse the boundaries of raw categories and enhance the representation of each superclass. By effectively exploiting the self-attention technique across the batch, SCLRE reconstructs a superclass-aware decision boundary on the enhanced representation space. The theoretical analysis in the paper proves that leveraging attention techniques bounds the generalization error of SCLRE under superclass scenarios. Additionally, comprehensive experimental results demonstrate the superiority of SCLRE over baseline and contrastive-based methods on various datasets [3, 4, 5, 6].>

<Several approaches for unsupervised representation learning have been developed in the literature. Prototypical Contrastive Learning (PCL) is an unsupervised representation learning method that addresses the limitations of instance-wise contrastive learning by implicitly encoding semantic structures of the data into the learned embedding space [7]. Similarly, SwAV, an online algorithm, leverages contrastive methods without requiring pairwise comparisons, making it more memory efficient and scalable compared to previous contrastive methods [8]. These state-of-the-art methods provide valuable insights and techniques that can potentially complement the superclass learning framework proposed in the target paper.>

<In the domain of computer vision, recent advancements in contrastive learning have demonstrated its effectiveness in self-supervised and semi-supervised learning tasks [9, 10, 11]. These approaches have improved the quality of learned representations and achieved state-of-the-art results on various benchmarks. Moreover, attention mechanisms, such as those introduced in the Transformer and BERT models, have been pivotal in enhancing natural language processing tasks [15, 16, 17]. The success of these models further underscores the significance of leveraging attention techniques, which is a key component in the SCLRE framework.>

<Furthermore, the application of contrastive learning and self-supervised learning has extended beyond visual and textual domains to tasks involving graph-structured data [12, 13]. Graph contrastive learning frameworks have shown promising results in learning unsupervised representations of graph data, suggesting potential applicability in scenarios involving superclass recognition. Additionally, recent research in targeted supervised contrastive learning and weakly supervised contrastive learning for addressing challenges like long-tailed recognition and punctuation restoration, respectively, offer valuable insights and methods that can contribute to addressing similar challenges in the domain of superclass recognition [11, 14].>

<The work on recurrent attention convolutional neural networks (RA-CNN) for fine-grained image recognition is particularly relevant as it addresses the challenges of discriminative region localization and feature learning in the context of image categorization [24]. The RA-CNN model learns discriminative region attention and region-based feature representation in a mutually reinforced manner, providing a potential avenue for enhancing the recognition of super categories. Finally, it is important to consider techniques for scalable self-supervised learning, such as masked autoencoders, and methods for adaptive capacity assignment, such as dynamic capacity networks. These approaches offer insights into efficient and effective self-supervised learning, which could be beneficial for developing robust and scalable superclass learning frameworks [21, 22, 23].>"
3556," **Domain augmentation.** Domain augmentation [1][2][3][4][5][6][7][8][9][10][11][12][13] is one of the most popular methods to improve the generalization ability of deep neural networks. The core idea of domain augmentation is to diversify the source domains to increase the diversity of the training data. For example, CrossGrad [1] and ADDA [2] generate new samples by perturbing the input images, while MixStyle [4] and Fourier-DG [3] synthesize new samples in the Fourier domain. In addition, some methods [11][13][12] generate pseudo-labeled data by solving jigsaw puzzles. In this paper, we propose a new perspective on DG that recasts it as a convex game between domains. Our framework provides a new avenue for the formal analysis of DG, heuristic analysis and extensive experiments demonstrate the rationality and effectiveness of our framework.

**Meta-learning for DG.** Meta-learning [14][15][16] aims to train a meta-learner that can quickly adapt to new tasks with a few training steps. Recently, some meta-learning-based methods [17][18][19][20] have been proposed to solve the DG problem. For instance, MAML [15] and MetaReg [17] meta-learn a good initialization of the model parameters that can be quickly fine-tuned to new domains. MFM [18] proposes to learn semantic features that are invariant to domain shifts. Meta-DGE [19] proposes a meta learning framework that learns a meta learner that learns to generalize the model across different domains. FCPN [20] designs a feature-critic network to learn domain-invariant features. However, all these methods are based on the assumption that the model generalization can be improved by the diversified source domains. In contrast, our framework aims to enhance the model performance by encouraging each diversified domain to enhance its generalization.

 proposes to solve DG as a min-max game between two players: a domain discriminator and a feature extractor. The discriminator aims to distinguish the domain-specific features from the general features, while the feature extractors aim to extract domain-agnostic features. In our framework, we design a regularization term based on supermodularity to encourage each domain to learn discriminative features. Moreover, a sample filter is constructed to filter out the low-quality samples, thereby avoiding the impact of potentially harmful information.

 also proposes a framework for DG, which is similar to our framework in the sense that it also aims to improve model performance. Different from their work, we aim to analyze DG from a game-theoretic perspective, and propose a novel regularization method to enhance model performance, which can be viewed as a generalization of their framework.

 provides a theoretical analysis of domain generalization from the perspective of convex optimization. In their framework, the objective function is defined as:

\[\begin{tabular"," **Domain generalization.** Domain generalization (DG) aims to alleviate the poor generalization performance of DNNs by learning model with multiple source domains. The most popular DG methods can be roughly divided into three categories: data augmentation, feature regularization, and meta-learning. Data augmentation methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] aim to augment the training data by introducing domain-invariant perturbations to the source domain data. For example, cross-domain adversarial training [1] perturbs the source data by adding adversarial examples from the target domain that are hard for the current model. MixStyle [4] mixes the style of source and target data to synthesize novel domains implicitly. Adversarial training  augments the source domains by adding domain-specific adversarial perturbation to the training set. [2] proposes an iterative procedure that augments source domains with examples that are ""hard"" under the current DNN. [3] proposes a novel Fourier-based data augmentation strategy called amplitude mix, which linearly interpolates between the amplitude spectrums of images. [6] proposes Scatter Component Analysis (SCA), a fast representation learning algorithm that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separation of data. [10] proposes Common-Specific Low-Rank Decomposition (CSD) to jointly learn a common component and a domain specific component for DG. [8] proposes to align the distribution of source domains via adversarial autoencoders. [9] models the uncertainty of domain shifts with synthesized feature statistics during training.

**Meta-learning methods.** Meta-learning [15][16][17][18][19][20] aims to learn a meta-learner that can quickly adapt to new tasks with only a few training samples. [15] proposes MAML, a meta learning method that trains a meta learner to optimize the parameters of the model. [17] proposes MetaReg, a regularization method that learns a regularizer that makes the model trained on one domain to perform well on another domain. [18] introduces a model-agnostic meta-train paradigm to preserve the class-specific structure of the feature space and explicitly aligns the model with the semantic space. [19] proposes Domain Generalization-Locally-","[ **Domain generalization.** With a large number of data and labeled samples from multiple domains, a domain generalization model can perform well for all the unseen domains with varying statistics. To solve this task, existing works mainly use the domain adaptation ([1]; [2]) and domain alignment ([5]; [6]; [4]; [7]) techniques. As domain information is unavailable in practice, some methods ([3]; ; [8]; ) align the representations for different domains to a reference domain.

In addition, some methods use uncertain features () or pseudo label information ([9]; ) to alleviate the generalization problem. Moreover, some recent works propose new perspectives to analyze the problems (; [10]; [11]).

**Improving domain generalization.** Recently, several works have attempted to explore effective methods to enhance the generalization ability of domain generalization methods, which are either an extension of specific domain generalization models or domain generalization-related ideas. With a proposed regularizer, the CSD model ([10]) has shown promising results.  aims to improve domain generalization by reducing the size of a weight space. Meanwhile, () proposes a form of post-hoc saliency-based label noise-free data augmentation. Moreover, ([12]) applies a mixup strategy ([13]; [14]) to encourage each domain to learn a common understanding and to learn domain-specific skills.

**Meta-learning for generalization.** Meta-learning ([15]; [16]) learns to learn via meta-learning. Recent works ([18]; [17]; [19]; [17]; [18]; [19]; [20]) have further explored the relation between domain generalization and meta-learning. ([20]) designs a loss to learn the distance between feature and label space. Furthermore, ([17]) leverages a domain specific regularizer for meta-learning. ([18]) assumes the prediction of a source domain as a label of a meta-learning task to build a label-dependent regularizer. ([19]) uses multi-source domain data to build a backbone model, and then uses a label dependent loss to improve the generalization ability of the model.

]","Step 1: Extracted important sentences from the cited documents are: 

1) ""We propose a new perspective on DG that recasts it as a convex game between domains.""
2) ""Our method learns models with good generalization ability to novel domains.""
3) ""Extensive experiments on three benchmarks have demonstrated that the proposed method is able to achieve state-of-the-arts performance for domain generalization.""
4) ""In this work, we propose a learning to learn approach, where the auxiliary loss that helps generalisation is itself learned.""
5) ""Domain generalisation is the recently topical problem of learning a model that generalises to unseen domains out of the box.""

Step 2:  Based on the extracted sentences, the topics for related work can be categorized as follows:
   - Approaches for domain generalization
   - Learning to learn approach for domain generalization
   - Convex game perspective on domain generalization

Step 3: Write related work based on the topics:

<Approaches for domain generalization [1][2][3][4][5][8][9][10][11][18][19][20]:

Domain generalization, which focuses on training models to generalize effectively to new and unseen domains, has been addressed through various approaches in the literature. Some prior strategies emphasize the use of domain-guided perturbations for data augmentation [1], while others propose methods that aim to learn models improving performance across various unseen domains through adaptive data augmentation [2]. Additionally, novel perspectives such as a Fourier-based framework [3] and probabilistically modeling uncertainty of domain shifts [9] have been introduced to enhance generalization ability.

Learning to learn approach for domain generalization [15][17][19]:
Recent efforts have explored the concept of meta-learning for domain generalization, focusing on a model-agnostic training procedure to simulate train/test domain shift during training [19]. This approach aims to train models exhibiting good generalization ability to novel domains. Moreover, meta-regulation has been proposed as a way to encode the notion of domain generalization using a novel regularization function within the learning to learn framework [17].

Convex game perspective on domain generalization [1]:
A novel perspective on domain generalization has been presented through a convex game framework that recasts domain generalization as a game between domains, aiming to guarantee and enhance the validity of the approach [1]. This perspective offers a new avenue for the formal analysis of domain generalization, providing heuristic analysis and extensive experiments to demonstrate its rationality and effectiveness.

The methods presented in the existing literature demonstrate a range of approaches to address the challenge of domain generalization, each offering unique insights and potential for improved generalization performance to previously unseen domains.>

"
2977," **Ensemble methods.** Ensemble methods ([1]; [2]; ) have a long history in machine learning and have been extensively studied in the context of decision trees (; [1]; ; [3]). The most popular ensemble method is bagging ([1]), which trains multiple classifiers independently and then combines them using a diversity mechanism such as bagging or boosting. In bagging, each classifier is trained independently on a subset of the training data, and then the predictions of the individual classifiers are averaged to form an ensemble. In boosting, the training procedure consists of training a tree-based ensemble of classifiers using a loss function defined over the training set, and the ensemble predictions are averaged over the individual trees to form the final prediction.

**Decision forests.** The seminal work of [4] introduced the idea of learning a forest of decision rules using a greedy splitting criterion. This idea has been widely adopted and has become the basis of many popular algorithms such as Gradient Boosting Decision Trees (GBDT) (), Adaboost (), Gradient Trees (TAO) (), and their variants ([8]; [5]; [6]; [9]; [10]; [11]; [12]; [13]; [14]; [15]; [3]; [7]). GBDT and its variants ([4]; ; ; [5]) are based on the idea that the training of a forest can be viewed as a stagewise expansion of a well-defined objective function, which can be written as:where \(\mathbb{E}_{\mathcal{L}(\mathbf{x}=\frac{1}{2}\) for all \(x_{i}\) and \(y_{i}\), where \(x\) is the number of training samples. The objective function is defined as:

\[\begin{tabular}{c|x|=\sum_{i=1}^{n\times n}-\frac{\sum_{j=1}{n}\sum_{ij=0}^{N}\mathbf{\prime}\frac{j=0}{n}\) where \(n\) is a fixed number of trees and \(j\) is an arbitrary number of data points. The training procedure of GBDT is to greedily grow a tree for each training sample \(x\), and then greedily update the leaves of the tree until the objective function converges to the original objective function. This greedy strategy is very effective at optimizing the loss function, but it is prone to overfitting. To address this issue, the authors in ([4]) proposed the Greedy Boosting Machine (GBM) algorithm, which is based on a gradient boosting machine (GBM) ([4]). GBM is a greedy algorithm that uses the gradient of the loss with respect to the training samples to grow the tree. However, GBM requires a large number of iterations to converge to a good solution, which makes it difficult to implement in practice. The authors in () proposed a more efficient version of GBM, called GradientBoosting Trees (GBT), which uses a gradient-"," In this section, we review related work on decision forests and tree-based ensembling methods.

**Decision forests.** Decision forests have been around for a long time, dating back to the early days of machine learning, when they were first introduced in the context of spatial point patterns ([1]; [3]; [8]; [1]). They have been extensively studied in the machine learning literature since the 1990s, and are still one of the most popular machine learning methods. The majority of decision forests are based on the idea of ensembles, where each tree is trained independently and then combined with other trees to form a forest. This is known as ensembled learning ([2]; [2]). The ensemble is trained in a greedy top-down manner, where the individual trees are optimized independently and the overall forest is optimized in a bottom-up manner. This makes the ensemble heuristic highly heuristic, and it is not clear how to improve the ensemble. The most successful ensemble methods are, to the best of our knowledge, based on boosting, which is an off-the-shelf heuristic. The standard approach to boosting is to train individual trees in an ensemble fashion, and then combine them into a forest by optimizing a loss function. This can be done either by optimizing individual trees individually or by optimizing the forest as a whole. The latter approach is more common in the case of decision trees, and has been studied extensively in the literature ([4]; [9]; [7]; [5]; [6]; [10]; [11]; [12]; [13]; [14]; [15]; [4]; ; [14]. The main difference between these methods and our approach is that they optimize individual trees, while our approach optimizes the entire forest jointly. The idea of using a forest as an ensemble is also present in the work of [7], who use a regularized greedy search to optimize the forest. However, their approach is based on a greedy tree learning algorithm, and they do not consider the problem of improving individual trees. In contrast, our approach can be seen as a generalization of the standard boosting algorithm, which can be used to improve individual trees as well as the overall performance of the overall model.

","[ The classic formulation of boosting algorithms, where a regularization (heteroscedasticity) parameter \(a\) and a training parameter \(b\) are used to train each tree, was first introduced by Freund and Schapire () and is referred to as the (original) adaptive boosting algorithm (AdaBoost) ever since. A re-discovery of the training objective and the AdaBoost algorithm in the 1980s by Duda ([1]) led to the first bagging algorithms ([2]), which can be thought of as greedy bagging-based versions of AdaBoost: re-parameterizing trees as weights and re-weighting the data (according to \(a\)) so as to optimize an Averaged Least Squares loss \(b\). A further modification of bagging algorithms is the use of large ensembles of trees or of the same tree trained in parallel on different bootstrap samples or subsets of features ([3]; ; [2]). This approach, originally termed greedy ensembling, has been pushed further in the more recent XGBoost and LightGBM ([5]; [6]), which have state-of-the-art results on most benchmarks.

One of the first papers to explore forest alternating optimization was also one of the first to propose to learn a forest by alternating among regularization and tree learning. The paper () is largely overlooked in machine learning literature. In it, the idea is to use an unconstrained optimization to create a forest, alternating among two phases: (1) learning the decision function, and (2) projecting to the \(1-\)inf norm. The resulting trees are fairly shallow and are found to perform surprisingly well on some of the benchmarks.

A concurrent and independent work ([7]) also proposed to alternate between fitting a forest and regularization. The authors focus on minimizing the margin of the data points instead of minimizing an \(1-\)inf loss. Unlike ([5]; [6]), in ([7]) each tree is fitted using the gradient boosting algorithm by Friedman and nests trees within a forest, which is then updated by the same alternating optimization. The resulting forest is found to outperform many of the state-of-the-art forest-based models (not only XGBoost, LightGBM, but also Random Forest, GBRT and Gradient Boosting), using almost twice as many training points for only slightly lower error.

Besides the two above mentioned approaches, there have been a few more papers that used forest alternating optimization. Given the original approach proposed by (), the very first papers on tree alternating optimization (TAO) ([9]; ) focus on fitting sparse trees by alternating between convex programming and tree learning. A decade later, () proposes a somewhat similar approach, aiming to solve the regularized ridge regression problem in a sequential manner, to fit more generalized sparse forests in order to improve the accuracy of both classification and regression]","<Decision forests have been widely recognized as highly accurate models in machine learning [1]. However, the training process for decision forests has been considered highly heuristic, lacking optimization of well-defined loss functions for individual trees or the overall forest. Ensembling heuristics such as bagging or boosting have historically played a critical role in the success of decision forests by promoting diversity [2]. Despite the effectiveness of ensembling, the need for better optimization in decision forests has been highlighted to potentially eliminate the necessity for ensembling heuristics [3]. Previous attention on decision trees has focused on splitting criteria and optimization of tree sizes, presenting a dilemma between overfitting and achieving maximum accuracy [4]. Furthermore, decision tree boosting, a popular machine learning algorithm, has been shown to achieve state-of-the-art results on many machine learning challenges [5]. These methods are typically trained using greedy top-down tree induction algorithms, which have been considered highly suboptimal for individual tree optimization [10].>

<Recent research has addressed the challenge of learning decision forests through advanced optimization techniques and more powerful tree structures. For instance, a novel learning algorithm, Forest Alternating Optimization, has been introduced to learn a forest by jointly optimizing a desirable loss and regularization over all its trees and parameters [1]. This approach effectively optimizes decision forests by updating each tree iteratively in alternation to ensure monotonic decrease of the objective function [1]. The effectiveness of this algorithm at optimizing decision forests suggests promise in achieving better forests without the need for ensembling heuristics [1]. Additionally, the use of sparse oblique trees and non-greedy tree optimization has shown considerable improvements in boosting methods, resulting in higher accuracy and smaller models than traditional gradient boosting methods in various datasets [12, 13]. Similarly, the use of sparsity-aware algorithms and exclusive feature bundling has led to the development of highly efficient gradient boosting decision tree models, addressing the scalability issues associated with high-dimensional data [6].>

<Moreover, research has also focused on improving the accuracy and efficiency of decision forests through globally optimized oblique trees, demonstrating consistently higher test accuracy than axis-aligned forests from strong baseline models [15]. These advancements in decision forest optimization and tree learning procedures have the potential to significantly impact the field of machine learning by enabling the construction of more powerful and efficient models for a wide range of applications [11, 14]. The use of oblique decision trees trained with advanced optimization algorithms has shown to produce shallower trees with fewer parameters and faster inference, highlighting their practical impact on various applications such as signal processing, data mining, and computer vision [13]. Collectively, these advancements in decision forest optimization and learning techniques contribute to the continued evolution of accurate and efficient models in machine learning.>"
2784," **Deepfake detection and provenance detection.** Deepfakes [1][2][3][4][5][6][7][8][9][10][11][12] and watermarking [13][14][15][16][17][18][19][20][21][22][23] have been studied extensively in the context of image forensics, where the goal is to determine whether an image has been altered or synthesized by a generative model. Early work [5][7] focused on analyzing the frequency spectrum of the image, while more recent work has focused on the spatial and temporal domain [8][12][23]. In particular, Wang _et al_. [8] proposed MVSS-Net, a multi-view multi-scale deep neural network for image manipulation detection, which uses multi-modal features from multiple views of the same image to detect image manipulations. In contrast, VADER focuses on the spatio-temporal domain.

**Video provenance.** Video provenance methods can be classified into two main categories: image-based and video-based. Image-based methods [22][21] focus on visualizing editorial changes between manipulated images, such as colorization, compression, and resizing. Video-based approaches [24][25][26][27][28][29][30][31][32][33][34] aim to detect and align video fragments to a source video, which may contain manipulated content. In this work, we focus on the second category, which is more closely related to video copy detection [25][32] and video steganography [16].

Video copy detection aims to determine if a given video fragment is a copy of another video from the same video content. Early methods [33][26] relied on hand-crafted visual features such as HOG, HOF, SIFT, and HOF-3D [34] to measure the similarity between a video fragment and a reference video. More recent methods [27][24][28] have focused on learning video similarity and alignment using deep neural networks. For example, LAMV [24] and ViSiL [29] use kernelized temporal layers to model the temporal evolution of a video, while VideoMatch [30] uses a Siamese network to learn video similarity. VPN [32] is a video provenance network that uses a transformer-based alignment module to align the query fragment to the source video. VideoSteganography  uses a temporal residual model to encode a video into a video representation, which can be later decrypted using a steganographic algorithm. VPN is a state-of-the-art video copy detector, and we use it as a baseline in our experiments.

 proposed a method to detect video manipulations using a video similarity network. However, their method relies on a pre-filtering step to remove artifacts from the original video, and thus is not applicable to our setting. In addition, VPN is not scalable to large-scale videos due to its reliance on pre-processing and post-processing steps.

"," **Deepfake Detection.** Deepfake detection [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41]. In this work, we focus on detecting and localizing manipulated videos.

**Image attribution.** Image attribution is the task of matching manipulated images to their original counterparts. Most existing methods [4][4] are based on deep networks trained on synthetic datasets [9][4], such as the DFDC dataset [1] and FaceForensics++ [6], and are trained on a binary classification task, _i.e._, _real/fake_ classification. However, the difference between the real and fake images is often subtle and local. To address this issue, [4] propose a multi-view multi-scale supervised network (MVSS-Net) [8] to detect manipulated images using multi-level features. [5] propose an Expectation Maximization (EM) algorithm to extract a set of local features specifically addressed to model the underlying convolutional generative process. [3] propose to use spatial attention heads to attend to different local parts of shallow features to aggregate subtle artifacts. [10] propose RepMix, a GAN fingerprinting technique based on representation mixing and a novel loss to detect a forensics trace left in the image generation process. In addition, [7] analyze the frequency space of GAN-generated images to identify the source network architecture responsible for the manipulation. [7][10] show that the artifacts are caused by upsampling operations found in all current GAN architectures. [2] detect and localize manipulations in the mouth region using discriminative feature maps extracted from a facial expression recognition framework. [4], [3], and [6] propose multi-attentional deepfake detection networks to detect subtle and subtle manipulations. [9] train a model to detect image warping applied to human faces by scripting the image editing process of Adobe Photoshop. [8], [2], [6][17], [10], and  propose to detect manipulations by leveraging the power of deep neural networks. [15] propose Scalable Fingerprinting (SfF) [","[ In this section, we review related work in the areas of DeepFake detection and video retrieval.

To detect and locate deepfakes in images, Zhou et al. [1] suggested the use of adversarial detectors. These however do not provide any visual indication of fakedness, the 'faked' results are not known to be fake at the end of training, and are not equipped to extract regions of manipulation. In contrast, Loss et al. [2] use a triplet loss to locate patches of fakedness in two and three dimensional images. More recently, model architecture changes [3][4] have led to smaller more accurate models and result in substantially improved performance [5][6][7]. Hu et al. [8] use a multi-view multi-scale convolutional neural network to encode video and extract informative features. In contrast to those methods, in this work, we directly extract visual indicators of fakedness, and compare our results with local fakedness information to provide a reliable system that may be used to detect faked videos without reference to a large database of images and videos. In this, our methods relate more closely to face swap detection and localization [9][10][11] in image domains, such as the work of ManTraNet [12] or Chen et al. [11], which use histograms to identify eye blinking regions.

Our methods can be distinguished from previous work on the visual forensics of images [13][14][15][16] and video [17][18][19][20][21][22]. As these works do not relate to the task of video alignment.

Recent work on visual forensics focuses on forgery detection and the recovery of underlying trustworthy data, the latter for applications in automotive, manufacturing, and medical industries [23][19][22][24][20]. Similar to this, we consider the video alignment problem, but with the application of detecting video manipulation, and providing a solution to reduce the need for laborious manual validation.

In video copy detection, approaches can be classified into global or partial, in reference to the frame from which alignment is estimated. In the global approach, Smith et al. [25] propose robust retrieval techniques for matching images and frames across the entire visual domain. In contrast, Kwon et al. [26] select a thresholded Haar feature to help improve results in very near duplicate scenarios. Hultqvist and Kjolbye [27] expand this idea to also consider temporal alignment. There have also been a number of attempts to represent the joint frames and match two videos, whilst noting the limitations of the DTW algorithm [28][29][30]. In this, we consider the problem of retrieving and aligning short sections of video content, and present robust local visual cues for locating f]","<The detection and localization of manipulated videos have been the focus of extensive research efforts in recent years. The emergence of off-the-shelf manipulation techniques, such as Deepfakes and GAN-based face swapping methods, has spurred the development of large-scale face swap video datasets to enable the training of detection models [1]. Furthermore, the importance of detecting and localizing manipulations in facial expressions, which has not been extensively addressed, has led to the development of frameworks to exploit the underlying feature representations of facial expressions for manipulation detection [2]. To address the challenge of detecting subtle differences between real and fake images, multi-attentional deepfake detection networks have been proposed, which model deepfake detection as a fine-grained classification problem [3]. The development of real-world datasets, such as WildDeepfake, comprising face sequences extracted from deepfake videos collected entirely from the internet, has been introduced to better support detection against real-world deepfakes [4].>

<In response to the rapid progress in synthetic image generation and manipulation, efforts have been made to detect and trace manipulated facial images. Techniques such as analyzing convolutional traces have been proposed to detect a forensic trace hidden in images and distinguish between different generative architectures [5]. Additionally, a benchmark for facial manipulation detection, FaceForensics++, has been introduced to standardize the evaluation of detection methods, based on prominent representatives for facial manipulations at random compression levels and sizes [6]. Moreover, a method for detecting one of the most popular Photoshop manipulations, image warping applied to human faces, has been presented using a model trained entirely on fake images generated by scripting Photoshop [9].>

<Further advancements in the field of image and video manipulation detection have involved the development of techniques for robust attribution of synthesized images using representation mixing and scalable fingerprinting, aimed at detecting whether an image is synthetic and determining which GAN architecture created it [10]. In the realm of fake face video detection, a novel method to expose fake face videos generated with deep neural network models has been introduced, based on the detection of eye blinking in the videos, which is a physiological signal not well presented in synthesized fake videos [11]. Furthermore, a manipulation tracing network, ManTra-Net, has been proposed for the detection and localization of image forgeries with anomalous features, providing a unified deep neural architecture that performs both detection and localization without extra preprocessing and postprocessing [12].>

<The domain of digital image authentication and integrity assurance has seen the introduction of digital watermarking systems for copyright protection and authentication of images using cryptographic techniques, as a means of ensuring the integrity and authenticity of digital images transferred through networks [14]. Additionally, the responsible disclosure of generative models has been addressed through scalable fingerprinting techniques to accurately detect and attribute generated samples containing a fingerprint to a source, thus assuring the provenance and integrity of archived documents [15]. Furthermore, a fragile watermarking scheme has been proposed for image authentication with tamper localization using integer wavelet transform, aimed at providing an additional level of security for authenticating and assuring the integrity of images [13].>"
3544," **Meta-learning.** Meta-learning [1][2] aims to learn a model that can be quickly adapted to new tasks with only a few training examples. Recently, few-shot image generation [3][4][5][6][7][8][9][10][11] has drawn increasing attention due to its potential applications in real-world applications.

**Conditional generative adversarial networks.** Conditional GANs [12][13][14][15][16][17][18][19][20] have been widely used in various applications, such as image-to-image translation [12], text to image synthesis [13], audio to scene synthesis [14], and parsing-conditioned anime translation [15]. In this paper, we mainly focus on the conditional StyleGAN [20], which has been shown to be effective in generating high-resolution images with limited data [21][22][23][24][25]. In particular, [23] shows that fine-tuning a pre-trained StyleGAN on the novel class data can significantly improve the quality of the generated images. [24] further shows that freezing the discriminator can also be used to improve the generation quality. [25] proposes to use batch statistics as a regularization term to prevent overfitting to the training data. Different from these methods, our method aims to locate a centroid latent position in the StyleGAN, where the corresponding output image on that centroid can maximize the similarity with the given samples, and therefore can generate new unseen samples.

 proposes to learn an embedding space in which samples of the same class can be projected into the same latent space. However, their method is designed for classification tasks, and cannot be directly applied to image generation tasks. In contrast, we propose to learn latent subspace optimization objectives for image generation, which can be applied to both classification and image generation problems. Moreover, our objective is to control the generation process from the other way around, by altering the centroids and its surrounding latent subspaces for a more precise generation of the novel classes.

 presents a method to generate images of unseen classes by projecting a few samples into a continuous latent space that can potentially generate infinite unseen samples, but their method requires a large amount of training data, which is hard to obtain in practice. In this work, we aim to address this dilemma by projecting sparse samples of a novel class into a latent space, and thus can generate infinitely unseen samples with a few examples.

 also proposes to project a few images of a given class into the latent space of a conditional GAN, and then adjust the conditional generator to produce the corresponding results with the new class label condition. Their method is similar to ours in the sense that they also aim to find a latent position that can locate the corresponding image on the corresponding latent position, but our method differs in the following two aspects. First, they assume that the latent position around the centroid belongs to the class of interest, while our method assumes that the neighboring latent space belongs to a different class. Second, they aim to project the few samples to a continuous space,"," **Few-shot image generation.** Few-shot generation aims to generate images of a novel category based on only a few training samples. Existing methods can be roughly divided into two categories: optimization-based methods [1][2][3][4][5][6][7][8][9][10][11] and generative adversarial networks (GANs) based methods [12][13][14][15][16][17][18][19][20][21][22][23][24][25].

**Optimization-based Methods.** Optimization- based methods aim to learn a good initialization of the model parameters that can be quickly fine-tuned on a novel task with a few gradient steps [1]. Reptile [2] is one of the earliest works in this category, which learns a parameter initialization by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. It is then used to fine-tune the model on a new task with only a small number of gradient steps. However, it requires a large amount of training data, which is difficult to obtain in many real-world applications. To address this issue, Reptile is further improved by adding a new training data augmentation module [9] to the original MAML [1] to improve the generalization ability of the network. However this method requires a lot of data to train the model, which limits its applicability in real-life applications.


.
. In this paper, we propose a novel optimization method to find a centroid latent position in a StyleGAN, which can be used to generate new unseen samples with only few training examples.

 is a recent work in this field, which proposes to use few-shot samples as positive anchors of the novel class and adjust the StyleGAN to produce the corresponding results with the new class label condition. The main difference between our work and theirs is that our method does not require any additional training data for the novel task, and thus can be applied to any real world application.

","[ We first review previous works on learning generalization by meta-learning and few-shot learning and then review GAN-based image generation methods.

**Meta-Learning** is a learning technique that does not require much labeled data, and learns a method to generate data with novel labels in a small number of examples [1][2]. The original idea of learning generalization is by providing an instance of samples and an approximation of labels [1]. When no labels are available, the idea is to find a task that has labels with common sense [3]. FIGR [3] further extends the original idea by combining it with generative models to generate the novel samples that have the same statistics with the original samples [4]. There are also works that rely on latent variables by aligning them with samples and labels [5]. Learning global alignment for latent variables also reduces the distribution shift among the seen and the unseen classes [6][5]. [4][7] generate samples for the unseen class based on a similarity measurement of the given samples and generated samples. However, it may not be a good idea to ignore the difference in the latent space between the unseen class and seen class by only using the generation of the unseen class, especially when the distance between the unseen and the seen class is too large.

**Few-shot Learning** refers to learning with few samples. Few-shot image generation aims to produce more images with the given few examples, which also includes data augmentation [9], attribute editing [10], delta generation [11], and so on. The original idea of few-shot image generation is to incorporate the few given examples in the meta-learning process [12][13][14][15][16]. [12][14][15][13] make a conditional StyleGAN model that is meta-trained with the few samples and few classes. Then it is fine-tuned with a set of new unseen classes. [16] extended this idea to learn conditional latent spaces to support different labels.  and [17] achieve high-quality outputs by applying the projection discriminator [18] on conditional StyleGAN. More recently, [19] is the first to learn text-to-image generation for a few categories with a fully-trained StyleGAN.

There are also works for training the conditional StyleGAN on large-scale datasets to generate images [20][21]. It is also possible to use such a trained conditional StyleGAN for few-shot learning [22]. [23] found that such models tend to reproduce the seen samples. [24] froze the discriminator to solve this issue. Several works focus on aligning latent codes between few-shot and other classes. [25] uses a metric learning framework to align the latent space of the target class with the conditional StyleGAN model trained with a large dataset.  propose SMALF,]","Step 1: Important sentences from the cited paper
- The paper addresses the problem of few-shot image generation by projecting sparse few-shot samples into a continuous latent space to potentially generate infinite unseen samples [1].
- The proposed approach locates a centroid latent position in a conditional StyleGAN to maximize the similarity with the given few-shot samples and introduces two latent subspace optimization objectives to inject a novel class [1].

Step 2: Summarized topics of related work:
1. Few-shot image generation using generative adversarial networks (GANs).
2. Meta-learning algorithms for few-shot image generation.
3. Conditional generative adversarial networks for image-to-image translation.

Step 3: Related work based on the summarized topics:
<>
Recent research has focused on addressing the challenge of few-shot image generation using generative adversarial networks (GANs). For instance, several works have proposed novel meta-learning algorithms for fast adaptation of deep networks to solve few-shot image classification and generation tasks [1][2][3]. These algorithms train models on a variety of learning tasks and enable them to solve new learning tasks using only a small number of training samples. Additionally, there has been significant progress in conditional GANs for image-to-image translation tasks, enabling the synthesis of high-quality images from text descriptions and label maps [12][13][19].

Another line of research has explored the use of pre-trained GANs and transfer learning techniques to address few-shot image generation. Techniques such as batch statistics adaptation and freeze-the-discriminator have shown promising results in generating high-quality images from small datasets by leveraging prior knowledge from pre-trained generators [24][25]. These methods have demonstrated stable training of generators and the ability to generate diverse and high-fidelity images even with limited training data.

In addition, works on matching-based GANs, local-fusion GANs, and frequency-aware GANs have contributed to improving the quality and diversity of images generated from few-shot samples [5][6][7]. These approaches have addressed the limitations of global fusion strategies by focusing on local representations and frequency-aware synthesis, resulting in more realistic and diverse image generation.

Overall, the recent advancements in few-shot image generation research highlight the potential of GANs, meta-learning algorithms, and conditional GANs for addressing the challenges of generating diverse and high-quality images from limited training samples across various domains."
896," **Data-free Network Quantization.** Data-free network quantization has recently received significant attention [1][2][3][4][5][6][7][8][9][10][11], which aims to reduce the reliance on training data for model compression and knowledge transfer [4]. For example, ZeroQ [3] proposed a zero-shot quantization framework to quantize a pre-trained model without accessing the original training dataset. ACIQ [2] proposed to minimize the quantization effect at the tensor level by optimizing the quantized tensor-level tensor norm. However, these methods require access to the original dataset for retraining during quantization, which may not be practical in real-life situations due to privacy and security concerns. To address this issue, data-free quantization methods have been proposed to generate synthetic data for training. DFEQ [7] and DFAQ [8] utilized the generative adversarial networks (GANs) to synthesize data from the pre-training and quantized models. DSEQ [6] proposed the weight equalization and bias correction (WEBC) method to eliminate the influence of the bias between the pre/quantized model and the original model. DDFQ [5] further proposed a diversified sample generation (DGS) strategy to improve the quality of the generated data.

**Causal Reasoning.** Causal reasoning provides an intuitive way to model causal relationships to eliminate data-driven correlations, making causality an essential component of analyzing data-filed problems [12][13][14][15][16][17][18][19][20][21][12][12]. In this paper, we propose a causal-guided data-based quantization method, Causal-DFQ, to eliminate reliance on data via approaching an equilibrium of causality-driven intervened distributions.

 proposed a causal graph to model the data generation and discrepancy reduction between pre-train and quantization models, which can be regarded as a special case of our causal graph. [13] proposed an anticausal learning framework to infer the underlying causal model in the case of covariate shift, transfer learning, concept drift, and semi-supervised learning. [14] extended the do-calculus to facilitate the identification of causal effects in non-parametric models. [12] proposed causal representation learning (CRL) to learn a causal representation from the data and the causal structure of the data, which is a generalization of knowledge distillation (KD) [9][11].

 proposed the causal framework to model data generation, discrepancy reduction, and model compression in the context of neural network compression. In this work, we use the causal graph as a bridge to bridge the gap between the causal formulation of data-freemethods and causal reasoning.

 presented a causal framework for data generation. They proposed a data generation method based on causal graphs, which uses the causal graphs to model both the data distribution and the discrepancy between the original and pre-quantized data. In contrast, we focus on the discrepancy reduction problem, which"," Data-free knowledge distillation (DFKD) aims to transfer knowledge from a pre-trained teacher network to a smaller student network without access to original training data [1][2][3][4][5][6][7][8][9][10][11]. Data-Free Knowledge Transfer (DFKT) [4] aims to distill the intra-domain knowledge of original data from a teacher network into a compact student network for model compression and efficient inference [4]. DFKD methods can be categorized into data-driven methods and data-free methods. Data-driven KD methods require access to the original training dataset for retraining during quantization. For example, DFKD [9] and DFKT [9][4] require the original data to train the student network. However, in practice, this is often not possible due to privacy and security concerns. To address this issue, DFL [7] and ADSD [8] utilize generative adversarial networks (GANs) to generate training samples for training the student model. DSD [7], DFL-GP [8], and DSD  use GANs to synthesize training samples by generating hard samples to enlarge the discrepancy between the teacher and student models. DGD [9], DeepInversion [10], and DeepInvert [10] use the batch normalization (BN) statistics of the teacher network as the training data. DGN [5] and DSG [5], which are the most related to our work, synthesize data for calibrating the quantized model according to BN statistics of FP32. DQN  proposes to use the BN-aligned feature statistics to calibrate the model. ACIQ [2] proposes to minimize the quantization effect at the tensor level to reduce the clipping distortion [2]. ZeroQ [3] proposes a novel mixed-precision quantization method to achieve the mixed-point quantization without any access to training data or validation data.

Causal reasoning [12][13][14] has been widely studied in machine learning and computer vision [12]. In this paper, we focus on the causal reasoning in the data free quantization problem. To the best of our knowledge, we are the first to propose a causal formulation of data-fidelity quantization, which is the first attempt to bridge the gap between data-dependent and data free methods.

","[ Aside from the commonly-used data-driven quantization [1][2][3][4][5][6], there is a line of work to address the data-free quantization problem. Most of the data-free methods can be roughly divided into three categories: discrepancy-based methods, generator-based methods and distillation-based methods. The idea of discrepancy-based methods is to achieve a good quantization result by minimizing the discrepancy between the pre-trained and the quantized model. The first work that takes advantage of discrepancy minimization is LMQ [7], where the quantization results of Student models are compared with the ground-truth. The DFQ method [8] proposes to transfer knowledge from the original network to Student networks and further iteratively updates Student networks by minimizing the cross-entropy between the teacher and student networks. Inspired by the KL-divergence, the ZSKD [9] propose to minimize the class divergence between the model generated from the pre-trained and quantized models. Some other methods like the CQ method  try to reduce the label discrepancy between the pre-trained and quantized labels and the NDCQ  minimizes the norm difference between the pre-trained and the quantized network. The generator-based methods directly quantize the pre-trained model to train Student networks, which can achieve better performance in quantization than the discrepancy-based methods. A similar approach is proposed in the GEM [10], where the real-image and its auto-encoder generated counterpart are compared to learn the ideal model. DFKD [11] predicts the class label distribution and train the Student network by minimizing the divergence between the predicted label distribution and the pre-trained model. Besides, in the distillation-based methods, there are many studies proposed to learn the Student network by minimizing the KL divergence between the logits of the Student and the Teacher networks. The quantization-free KD [9] further improves the performance of distillation-based methods.

There has been very few work on understanding the cause of quantization failure, especially the data-free quantization. In order to identify causal factors for causal failure, Keogh _et al_.  proposes a definition for causal reasoning. Research in Causal Inference begins with the works of Pearl [12][13][14], who prove the do-calculus to be completeness, which has not been recognized as validating criterion, until other researchers like Titsias _et al_.  propose the consistency, which is independent of the causal graph and robust against the selection of the generators. However, there are few works of causal analysis in the field of deep learning.

]","<Quantization of deep neural networks is a crucial aspect in the effort to compress and accelerate inference speed, particularly for deployment on mobile and edge devices. While prior works on quantization methods assume the availability of training data, it is recognized that in real-life scenarios, this assumption is often not feasible due to privacy and security concerns, rendering such methods inapplicable [1]. The emergence of data-free network quantization has garnered significant attention as a practical solution to this challenge, aiming to reduce the reliance on actual training data for quantization [3]. One prevailing limitation in the literature, however, is the inadequacy of causal formulations for data-free quantization [12]. Nevertheless, recent advancements have introduced causality as a fundamental component for analyzing data-free problems, emphasizing the intuitive modeling of causal relationships to eliminate data-driven correlations [12]. In this context, the Causal-DFQ method has been proposed, which leverages a causal graph to model data generation and discrepancy reduction between pre-trained and quantized models, effectively eliminating the reliance on data by approaching an equilibrium of causality-driven intervened distributions [12]. This approach builds on the notion of causal reasoning to address the challenges associated with data-free network quantization, offering a promising avenue for further exploration and development in this domain.>

<The quest for effective data-free quantization methods has spurred various approaches in the literature, each addressing specific aspects and limitations of existing techniques. For instance, the Diverse Sample Generation (DSG) scheme has been proposed to mitigate the adverse effects of data homogenization in synthetic data calibrated by batch normalization (BN) statistics, demonstrating consistent improvements across various network architectures and quantization methods, particularly at lower precision levels [5]. Moreover, the Data-Free Quantization Through Weight Equalization and Bias Correction method offers a novel approach that does not require fine-tuning or hyperparameter selection, achieving near-original model performance on common computer vision architectures and tasks, with a particular focus on 8-bit fixed-point quantization for efficient inference on modern deep learning hardware [6]. These advances collectively contribute to the growing arsenal of data-free quantization techniques, each offering unique insights and methodologies to address the challenges inherent in quantizing deep neural networks without access to actual training data.>

<In parallel, the landscape of knowledge transfer and distillation has also witnessed significant developments in the context of data-free paradigms. The Data-Free Knowledge Transfer framework, for example, presents a comprehensive survey of data-free knowledge transfer paradigms, encompassing both knowledge distillation and unsupervised domain adaptation, to facilitate a better understanding of the current research status and ideas in this realm [4]. Furthermore, the Zero-Shot Knowledge Distillation approach pioneers a novel data-free method to train a small model from a high capacity source model, circumventing the need for actual training data via the synthesis of Data Impressions from the complex Teacher model, thereby demonstrating competitive generalization performance across multiple benchmark datasets [11]. These efforts collectively underscore the significant strides made in the domain of data-free knowledge transfer, showcasing the diversity and innovation present in addressing the challenges of knowledge distillation and transfer without direct access to training data.>

<The intersection of causality and machine learning has also been a topic of increasing interest, with efforts focused on leveraging causal reasoning to address challenges such as transfer and generalization in modern machine learning research [12]. Moreover, the exploration of causal knowledge in the context of function estimation and its implications for popular scenarios such as covariate shift, concept drift, and semi-supervised learning, further underscores the potential of causal inference to facilitate and guide learning processes in machine learning [13]. The implications of causality for diverse areas, including mediation analysis, transportability, and metasynthesis, also highlight the broad applicability and relevance of causal reasoning in addressing complex challenges across different domains [14]. Collectively, these contributions emphasize the growing convergence of causality and machine learning, with promising avenues for further exploration and research at their intersection.>"
3628," **2D-to-3D Human Pose Estimation.** Recent advances in 3D human pose estimation are mainly based on deep learning [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]. For example, Martinez _et al_. [4] proposed a simple yet effective baseline that directly regresses the 3D coordinates of the joints from the 2D joint heatmaps with a fully convolutional neural network (FCNN). Park and Lee [3] proposed to learn a mapping from 2D pose to 3D pose by minimizing the distance between the estimated 3D joint locations and the corresponding 2D keypoints. Pavlakos and Komodakis [2] proposed MPM to learn the mapping between 2D and 3D poses in an unsupervised manner.

**Data Augmentation.** Data augmentation has been widely used to improve the generalization performance of 3D HPE models. For instance, Papandreou and Collobert [8] proposed MoCap-guided data augmentation (MoCap-GDA) to augment the training data of the target dataset by synthesizing images from the source dataset with the help of a pre-trained MoCap model. Chen _et.al_. [10] proposed Synthesizing Training Images (STI) to generate synthetic training images for boosting the performance on the target datasets. Wang _et.[11] proposed an adversarial learning framework to generate images from source datasets with the guidance of the predicted 3D joints in the target domain. However, these data augmentations are not suitable for the target unseen dataset due to the domain shift issue between the source and target datasets [16][17].

Recently, some works [12][15][14] have been proposed to solve the domain adaptation problem in the context of 2D-2D pose lifting models. Specifically, PoseTriplet [12] and Cascaded3DHPE [13] were proposed to generate pseudo-labels for unlabeled target datasets by co-evolving two datasets. AdaptPose [15] was proposed to synthesize target-dataset-specific motion sequences with a learnable motion generator. These data generation methods are effective for reducing the domain gap between the two datasets, but the performance of these methods is limited by the quality of the synthesized data. In this paper, we propose PoseDA, a simple and effective framework of unsupervision domain adaptation for 3D Human pose estimation.

 proposed a framework of domain adaptation to address the problem of insufficient labeled data in the field of image classification. They proposed a domain adversarial training framework to align the distributions of the source domain and the target domains. However they did not consider the domain discrepancy between 3D and 2D human poses, which is the main focus of our work. In addition, we proposed a novel local pose augmentation module to enhance the diversity of local structures of poses in training.

 introduced a novel framework of cross-domain learning for image classification, which aims to transfer the knowledge learned from the labeled source"," **3D Human Pose Estimation.** 3D human pose estimation has been extensively studied in the past decade [1]. In this section, we briefly review the most related works in the literature.

**2D-to-3D Pose Lifting.** Most of the existing methods [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] focus on lifting 2D human poses to 3D poses in the source domain. For example, Goo _et al_. [4] propose a simple yet effective framework to lift 2D poses to the 3D pose space with a feed-forward network, which achieves the state-of-the-art performance on Human3.6M dataset [4]. However, this method requires a large amount of training data with 3D annotations, which is difficult to obtain in the wild. To address this issue, some methods [6][8] propose to use the geometric self-consistency of the lift-reproject-lift process [6] to generate 2D-2D pose pairs for augmenting supervision. However, these methods require additional 3D data to train the self-supervised framework, which limits their applicability in the real-world scenarios with limited and fixed training data. To this end, we propose PoseDA, a novel framework of unsupervised domain adaptation for 3D HPE, which does not require any 3D training data and can be applied to any existing 3D lifting methods.


","[ **3D human pose estimation** Unsupervised domain adaptation for human pose estimation has attracted increasing attention in recent years [1]. With the advance of deep learning, some learned 3D human pose estimation models have demonstrated impressive performance [2][3][4][5]. However, the lack of sufficient labeled 3D annotations still limits its generalization capability. Unsupervised domain adaptation is proposed to transfer knowledge from a large-scale annotated source domain to an unlabeled target domain without introducing more labels in the target domain.

**Unlabeled datasets augmentation** Aiming at alleviating the data-gap between source and target domains, several methods try to augment the unlabeled dataset with synthetic images with 2D/3D annotations [6][7][8][9][10][11]. Especially, Hu _et al._[12] generate human-in-the-loop pairs as 2D-3D pose pairs by a conditional generative adversarial network. Liu _et al._ propose to convert 2D poses into the 3D camera space and synthesize training data from random viewpoints. Hou _et al._[13] conduct various pose transformations to expand the viewpoint range, which can be regarded as our pose augmentation module. Xu _et al._[14] and Li _et al._[15] introduce a differentiable joint permutation and scaling operation to pose transformation to get a diverse synthesized dataset.

**Unlabeled datasets augmentation + learning based DA** In addition to self-supervised training, the unlabeled datasets augmentation improves the performance of 3D pose estimation. It can be regarded as an unsupervised domain adaptation. Guo _et al._ and Hu _et al._ use the GAN-based method to generate the synthesized data with diversity, but they do not address the problem of domain shift from training data.

Inspired by 2D DA, we propose to address the problem of large performance degradation caused by domain shift by utilizing two factors of global position shift and local structure shift between source and target datasets. It mainly differs from those previous methods in that the input of the latter two methods is the 2D poses directly, while our model converts the input 2D poses into 3D poses with the method of E3D-CNN [6] and then get the global and local alignment. It does not introduce extra 3D labels in the training, and the architecture of the model is simple and lightweight.

**3D human pose lifting model** The 3D human pose lifting model is mainly designed to deal with 3D pose input. Mousavian _et al._[3] propose a single neural network to estimate the 3D pose from 2D pose and camera intrinsic parameters. Li _et al._[4] propose a lifting model based on temporal convolutions]","Step 1: Extract important sentences from the cited paper.

1. ""We observe that the degradation is caused by two factors: 1) the large distribution gap over global positions of poses between the source and target datasets due to variant camera parameters and settings, and 2) the deficient diversity of local structures of poses in training.""
2. ""To this end, we combine global adaptation and local generalization in PoseDA, a simple yet effective framework of unsupervised domain adaptation for 3D human pose estimation.""
3. ""Specifically, global adaptation aims to align global positions of poses from the source domain to the target domain with a proposed global position alignment (GPA) module.""
4. ""And local generalization is designed to enhance the diversity of 2D-3D pose mapping with a local pose augmentation (LPA) module.""
5. ""In addition, we propose local pose augmentation (LPA) to enhance the diversity of 3D poses following an adversarial training scheme.""

Step 2: Summarize sentences and decide on topics of related work.

The target paper addresses the degradation encountered in applying a pre-trained 2D-to-3D human pose lifting model to an unseen dataset due to domain shift issues caused by global position distribution gaps and deficient diversity of local pose structures in training. The paper proposes a framework, PoseDA, that combines global adaptation and local generalization to address these issues. Global adaptation aims to align global positions of poses between source and target domains, while local generalization enhances the diversity of 2D-3D pose mapping through local pose augmentation.

Step 3: Write related work based on topics.

<The domain of 3D human pose estimation has seen significant advances in recent years, with a focus on addressing the challenging task of estimating articulated 3D joint locations of a human body from images or videos [1]. The field has largely advanced with deep learning approaches, contributing to improved performance on existing benchmarks. However, there remains a gap in addressing the challenges stemming from domain shift issues, such as the distribution gap over global positions of poses and the deficient diversity of local pose structures in training, as observed in the proposed work.

A significant body of work has focused on deep learning-based methods for 3D human pose estimation. For instance, MPM proposes a unified 2D-3D human pose representation framework [2], while other works explore the reasoning through intermediate 2D pose predictions [3]. Further, the success of deep convolutional networks has driven the development of state-of-the-art methods for 3D human pose estimation, focusing on lifting ground truth 2D joint locations to 3D space [4]. Additionally, there have been efforts to effectively estimate 3D poses in video using fully convolutional models based on temporal convolutions over 2D keypoints, along with semi-supervised training methods [5].

In the domain of unsupervised 3D pose estimation, methods such as geometric self-supervision and diffusion-based optimization have been proposed, showcasing advancements in recovering 3D human pose from 2D skeletal joints extracted from single images [6, 7]. Furthermore, the challenge of training data scarcity has been addressed through MoCap-guided data augmentation, which generates synthetic images of humans with 3D pose annotations to improve end-to-end CNN training for full-body 3D pose estimation [8].

Another significant area of research lies in real-time 3D human pose estimation using a single RGB camera, as demonstrated by VNect, which captures the full global 3D skeletal pose of a human in real time [9]. Moreover, synthesizing training images for boosting human 3D pose estimation has been a subject of exploration, aiming to address the challenge of limited diversity in training data by automatically synthesizing training images with ground truth pose annotations [10].

In the context of domain adaptation for 3D human pose estimation, prior work has delved into unsupervised domain adaptation and adaptation for self-supervised 3D human pose estimation, aiming to generalize the trained model to unseen datasets and scenarios [16, 17]. Additionally, the use of differentiable pose augmentation frameworks has been proposed to improve the generalization performance of 3D human pose estimators to new datasets by learning to augment training poses towards greater diversity [14].

Overall, the proposed work on unsupervised domain adaptation for 3D human pose estimation addresses important challenges related to domain shift issues by combining global adaptation and local generalization, contributing to the evolving landscape of research in this domain.>"
1612," **Deep Clustering.** Deep clustering aims to learn a clustering-friendly feature embedding space for clustering [1][2][3][4][5][6][7][8][9][10][11][12][13]. For example, Deep Embedding [4] proposes to learn an auto-encoder to embed the input data into a low-dimensional latent space, where the clustering results can be obtained by k-means or spectral clustering. Deep InfoClustering [9] proposes an invariant information clustering loss to encourage the learned features to be invariant to various transformations of the data. Deep CCA [5] jointly optimizes the representation learning and clustering objectives in an end-to-end manner. DDCA [7] proposes a discriminative clustering analysis framework to jointly optimize the feature representation and the cluster assignments.

**Self-Labeling.** Self-labeling aims to generate pseudo-labels for the unlabeled data and then use them to train the model [11][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28]. For instance, CoLA [15] introduces a contrastive learning framework to learn the snippet-level representation by contrasting positive and negative samples. DCCM [16] explores denoised cross-video contrast to improve the quality of the contrastive loss. Hide-and-Seek [17] randomly hides a portion of the training samples to force the model to focus on other relevant parts of the images.


 proposes a self-label generation framework to generate high-quality pseudo-labeled data, which can be used to train a fully-supervised object detector. However, it requires a large amount of labeled data for training, which is not available in the WTAL task. In contrast, our method only requires a small amount of annotated data.

 proposes an unsupervised snippet clustering approach to tackle the problem of F&B snippet separation. Different from existing methods, our approach does not rely on the video classification loss. Instead, we propose to explore the underlying structure among the snippets by resorting to unsupervision, rather than heavily relying on video classification losses. Moreover, we introduce a unified self-labelling mechanism based on optimal transport to produce high-fidelity pseudo labels that match several plausible prior distributions, which ensures that the cluster assignment of each snippet matches the prior distribution.

 introduces a novel self-training framework for WSAIL. It first trains a classification model with video-level labels and then uses the classification model as a teacher to supervise the training of a snippet-based WLA model. Our method differs from this method in two aspects. First, we adopt a unified framework to train both the snippet classification and snippet-wise clustering models. Second, we design a novel optimal transport-based clustering mechanism to ensure that the predicted pseudo labels match the prior distributions of multiple plausible distributions.

 also proposes a novel clustering framework to address the"," **Clustering-based Methods.** Recently, clustering methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28] have been widely used in weakly-supervised temporal action localization (WS-TAL). These methods can be roughly divided into two categories: unsupervised clustering and supervised clustering.

**Unsupervised Clustering.** The pioneering work [9] proposes to learn a neural network classifier from scratch, given only unlabeled data samples. The work [10] proposes a deep clustering method that recasts the clustering problem into a binary pairwise classification framework to judge whether pairs of images belong to the same clusters. The works [6][8] propose to learn the most semantically plausible clustering solution from all the possible separations, based on the observation that assigning samples from the same semantic categories into different clusters will reduce both the intra-cluster compactness and inter-clust diversity, i.e. lower partition confidence. The deep discriminative clustering (DDC) method [7] explores the relationships between patterns with a deep neural network to learn high-level discriminatively discriminformative representations. The joint unsupervision learning (JULE) method  proposes to jointly learn representation learning and clustering by maximizing the mutual information between the class assignments of each pair of images. The Joint Unsupervised Learning for Clusters (JUNoC) [5] method [13] introduces a unified objective function to learn representations and cluster assignments simultaneously. The recent work [14] proposes 3C-Net, which introduces a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability, and a counting loss to delineate adjacent action sequences, leading to improved localization performance. The methods [15][20] propose a hybrid attention mechanism to capture the cross-video context information. The method [20] introduces temporal soft, semi-soft, and hard attention modules to model background activity effectively. The latest work [15] proposes snippet contrastive learning to localize actions in untrimmed videos with only video-level action labels. The authors [16] propose an effective pseudo-label denoising module to alleviate","[ **Unsupervised Clustering.** Unsupervised clustering is generally formulated as a joint learning of feature extraction and data clustering. To explore the discriminative features of visual data, deep clustering [1][2][3][4][5][6][7][8][9][10] is widely used to map features into low-dimensional subspace [11]. However, the multi-branch architecture has a high memory requirement. Moreover, many methods rely on pairwise relations for clustering [6][12], which is often error-prone and computationally expensive. Different from the above studies, our clustering procedure is integrated into a localization-by-classification framework that separately analyzes every snippet. The most similar work to ours is [13], which first localizes all pixels in an image and then learns the classification loss from pixel-level segmentation. However, our task is different from image segmentation. We focus on localizing the action instances instead of pixels, and we do not rely on any pixel-level supervision. Moreover, our clustering-based approach is much more lightweight than [13], which does not suffer from any additional computational overhead.

**Weakly-Supervised Temporal Action Localization.** Due to the absence of ground-truth temporal locations of actions in the training data, existing works usually adopt the ""localization-by-classification"" pipeline [14][15][16][17][18][19][20][21][22][23][24][25][26][27][28]. To capture the cross-video semantic relationships, [14][15] and [17] propose the snippet-contrastive loss to pull the discriminative snippets from multiple videos and push the noise snippets apart. [21] and [26] adopt a sparse temporal pooling module to transform snippets into a sequence of compact hidden states and construct a triplet-loss. [16] further enforces a denoising loss to mitigate the noise interference from background videos. However, all the above methods mainly rely on the classification loss to group the snippets into several classes. They are not able to directly distinguish between foreground and background snippets, which leads to inaccurate separations of F&B snippets.

Moreover, existing methods mainly explore a small amount of noisy snippets in a limited number of test videos to test the localization quality. To handle this problem, we introduce a unified clustering-based F&B separation algorithm to further separate F&B snippets in the video training set. We do not rely on any target test set, but focus on optimizing the snippet classification loss. The problem of our work is significantly different from the classification of snippets in test set [15][17][16], which heavily relies on test set performance.

]","Step 1:
Extracted important sentences from the cited papers are:

There are several clustering methods that have recently garnered attention in learning and vision [1].
Deep clustering has shown effectiveness in obtaining optimal embedding subspace for clustering compared to conventional methods [1].
Adversarial learning has been proposed to enhance the robustness of clustering networks especially when facing adversarial attacks, which significantly improves the clustering performance [2].
Deep subspace clustering (DSC) networks based on deep autoencoder (DAE) have been proposed to tackle the limitations of classical subspace clustering methods [3].
Deep Embedded Clustering (DEC) is a method that simultaneously learns feature representations and cluster assignments using deep neural networks, which has shown significant improvement over state-of-the-art methods [4].
A recurrent framework for joint unsupervised learning of deep representations and image clusters has been proposed, showing superior performance compared to existing methods on a variety of image datasets [5].
Deep Adaptive Clustering (DAC) proposes a binary pairwise-classification framework for image clustering and achieves state-of-the-art performance on popular datasets [6].
Deep Discriminative Clustering (DDC) has been developed to handle the limitations of traditional clustering methods, and it has been shown to outperform current methods on various datasets [7].
Deep Comprehensive Correlation Mining (DCCM) provides a novel clustering framework for exploring and leveraging various correlations in unlabeled data and achieving good performance on challenging datasets [8].
Invariant Information Clustering for Unsupervised Image Classification and Segmentation introduces a novel clustering objective that learns a neural network classifier from scratch, which achieves state-of-the-art results in eight unsupervised clustering benchmarks [9].
PICA proposes a novel deep clustering method for learning the most semantically plausible data separation, achieving high performance by maximising the ""global"" partition confidence of clustering solution [10].
A novel and principled learning formulation has been proposed, that addresses the combination of clustering and representation learning issues [11].
SwAV introduces an online algorithm that takes advantage of contrastive methods without requiring pairwise comparisons and achieves state-of-the-art performance in unsupervised pretraining [12].
UNO introduces a Unified Objective for Novel Class Discovery to leverage from prior knowledge of a labeled set for discovering novel object categories in an unlabeled set, yielding a significant performance improvement [13].
3C-Net presents a weakly-supervised framework that only requires video-level supervision for action localization, setting a new state-of-the-art for weakly-supervised temporal action localization on two challenging benchmarks [14].
CoLA investigates the benefits of learning by comparing to identify hard video snippets and utilizes snippet Contrastive learning to Localize Actions, achieving state-of-the-art results on THUMOS14 and ActivityNet v1.2 datasets [15].
Denfoveusu proposes a novel denoised cross-video contrastive algorithm to enhance the feature discrimination ability of video snippets for accurate temporal action localization in the weakly-supervised setting, and demonstrates superior performance on two benchmark datasets [16].
Hide-and-Seek introduces a weakly-supervised framework that aims to improve object localization by forcing the network to seek other relevant parts when the most discriminative part is hidden, and the approach obtains superior performance compared to previous methods for weakly-supervised object localization on the ILSVRC dataset [17].
A potential approach for tackling the challenges of weakly supervised temporal action detection, known as Step-by-step Erasion, One-by-one Collection, has been proposed, showing significant improvements in weakly supervised temporal action detection results [18].
A Hybrid Attention Mechanism proposes a framework with a hybrid attention mechanism to resolve the limitations of existing Multiple Instance Learning (MIL) based approaches for weakly supervised temporal action localization [20].
Weakly Supervised Temporal Action Detection for Fine-Grained Videos is a novel approach that models actions as the combinations of reusable atomic actions which are discovered from data through self-supervised clustering, and achieves state-of-the-art results on two large-scale fine-grained video datasets [28].

Step 2:
Based on the important sentences, the identified topics for related work are:
1. Deep clustering methods
2. Adversarial learning for robust clustering
3. Weakly supervised clustering approaches
4. Unsupervised learning of visual features and clustering
5. Novel class discovery and representation learning
6. Weakly supervised temporal action localization

Step 3:
<Related work starts>
The field of deep clustering methods has recently gained attention in learning and vision [1]. Deep clustering combines embedding and clustering to obtain optimal subspace for clustering, showing effectiveness compared to conventional methods [1]. In addition, adversarial learning has been proposed to enhance the robustness of clustering networks, particularly when facing adversarial attacks, resulting in significantly improved clustering performance [2]. Moreover, weakly supervised clustering approaches have been developed to learn representations for clustering, such as Deep Embedded Clustering (DEC), which simultaneously learns feature representations and cluster assignments using deep neural networks, showing significant improvement over state-of-the-art methods [4]. Furthermore, unsupervised learning of visual features and clustering has been explored, with the use of contrastive methods and the Unified Objective for Novel Class Discovery, achieving state-of-the-art results in unsupervised pretraining and discovery of novel object categories [9, 12, 13].

In the realm of weakly supervised temporal action localization, various methodologies have been proposed to address the challenges of identifying and localizing actions in untrimmed videos with only video-level labels [14]. These approaches range from novel denoised cross-video contrastive algorithms to weakly supervised frameworks that aim to improve action and object localization through iterative refinement [16, 17]. Furthermore, a hybrid attention mechanism has been introduced to handle the limitations of existing Multiple Instance Learning (MIL) based approaches, achieving superior performance for weakly supervised temporal action localization [20]. Lastly, weakly supervised temporal action detection for fine-grained videos has been addressed with the proposal of algorithms capturing reusable atomic actions, leading to state-of-the-art results on large-scale fine-grained video datasets [28].
<Related work ends>"
3937," **3D Clothed Human Reconstruction.** 3D clothed human reconstruction has been extensively studied in recent years. Most of the existing methods are based on parametric body models [1][2][3] and can be divided into two categories: template-based and template-free methods. Template-based methods [4][5][6][7][8][9] fit a pre-defined template model to the input image and reconstruct the 3D body and clothes from the template. However, these methods are limited by the limited expressiveness of the template model and cannot generate high-fidelity results.

Recently, implicit neural representation based methods [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44] have been proposed to overcome the limitations of the above parametric models. These methods represent 3D shapes as a continuous function, _e.g_., a signed distance field (SDF) [10], occupancy field [11], or a neural implicit function (INF) [12][40]. These methods have shown impressive results for reconstructing 3D human bodies from a single image. However these methods cannot be directly applied to reconstruct 3D garments due to the different topology and topology of garments. In this paper, we propose a novel approach to jointly optimize the explicit feature curves and the implicit signed distance fields of the garments, which can produce high-quality dynamic segment surfaces for the input video.

 propose a method to reconstruct dynamic clothed human from monocular video. They first extract the garment template from the video, and then reconstruct the garment surface by registering the template to the silhouette of the person in the video. Although this method can generate temporally consistent segment surfaces, it cannot separate the body from the garment. To address this limitation, we formulate this task as an optimization problem of 3D garment feature curve and surface reconstruction. Then the garment meshes can be extracted via garment template registration in the canonical space, and the surface reconstruction can be achieved by optimizing the implicit SDF of the garment with the help of the implicit feature curve representation.

 introduce a method for monocular garment reconstruction based on neural rendering. They propose a neural volumetric representation, which is able to model the appearance and geometry of the human body and clothing simultaneously. They show that the implicit representation can be used to reconstruct high-resolution clothed human reconstructions from single or multiple images. Although their method can achieve impressive results, it can only reconstruct the clothed human with closed boundaries. In contrast, our method can reconstruct the open garment meshes with arbitrary topology. Moreover, we introduce a novel method to optimize the implicit features with the garment feature curves, which achieves better reconstruction results than their method.

 present a method that reconstructs 3D dynamic clothed humans from a monocular RGB video. Their method first extracts 3D feature curves from the input RGB video, then reconstructs the"," **3D Human Reconstruction from Single Image.** 3D human reconstruction from a single image has been extensively studied in the past decades. Early methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45]. Most of these methods are based on the parametric human body model [1], which is a mesh-based representation of the human body. The body is represented as a 3D body mesh and the surface of the body is modeled as a signed distance field (SDF) [1]. The SDF represents the distance between each vertex of the mesh and each pixel of the image. The SVDF is a continuous function that represents the surface as a continuous point inside the mesh. The surface is represented by the signed distance function of the SDF, which is an implicit function that can be represented as the continuous decision boundary of a deep neural network classifier [10]. The implicit function can represent the 3D surface at infinite resolution without the need of any explicit geometry.

The SDF representation has been widely used in recent works for human reconstruction [10][10]. For example, DeepSDF [10] uses the SVDFs to represent the surface with a continuous SDF field and the signed distances of the surface inside the surface. The signed distances are represented by a neural network. The implicit representation can be used to represent both the shape and the texture of a human body [10], and it can also be used for the surface reconstruction [11].

PIFu [12] is the first work to use the implicit representation to represent human body from a monocular image. PIFu uses the implicit function to represent a pixel-aligned implicit human body representation. The pixel-alignment is achieved by aligning the pixels of the input image with the global context of the corresponding 3D object. PifuHD [13] extends PIFU [12], which uses the pixel-aligning function to model high-resolution human body reconstruction. The high-fidelity reconstruction of PIFuv is achieved with a multi-level architecture that is end-to-end trainable. The P","[ **3D Human Reconstruction.** There are mainly two types of traditional methods for 3D human reconstruction, parametric model-based methods [1][2][3][4][5][6][7] and voxel based methods . In terms of parametric model-based methods, in the early work, human body is represented by triangles in the SMPL model [1], then represented by 3D spherical harmonics (SH) basis [8], or by the UV-meshes of the SMPL model [9][6][7]. But these methods are limited by triangle mesh representation, which can not describe smooth surface details. In recent years, more accurate implicit representations like Normalized Phase Field (NPF)  and Signed Distance Function (SDF) [10] have been utilized. However, this representation requires further implicit network to get the feature representation for each 3D point in the UV space, which is time consuming. Occupancy networks [11] can be an alternative solution to produce the mesh representation directly from the feature point cloud without iterative calculations. However, the accuracy of feature curve representation is limited. Our method extends this idea to represent 3D body surfaces using the explicit feature curve representation for further improving the performance. On the other hand, voxel based reconstruction methods such as Voxel Structure Network (VSN)  and Pix2Vox  have been proposed for 3D human reconstruction. These methods estimate volumetric human representations directly with the meshless representation. However, the volumetric representation limits the feature details. And this representation also requires a GPU to restore the human.

**Neural Representation for 3D Human Reconstruction.** Recently, there have been several methods that use neural representation to reconstruct 3D human models from 2D images. DeepSDF [12][13] uses neural networks to learn SDFs for reconstructing 3D meshes from a single RGB image. ARCH++ [14] extends the previous work to 3D human digitization. Some methods use the Neural Rendering framework PIFu [15][16][17] for 3D human reconstruction. PIFu [16] reconstructs 3D mesh from the synthetic texture of single RGB image via octree voxelization. PIFuHD [13] represents 3D garment surface using pixel-aligned implict function (PIFu) with multi-level representations.

There are also several methods using neural representations for human reconstruction from video data. StereoPIFu [18] recovers 3D mesh from single RGB stereo video by a temporal consistency constraint. ARCH [19] introduces an implicit neural network to predict human 3D shape from the RGB video. It generates 3D mesh from the input RGB video for subsequent]","<Reconstructing dynamic 3D garment surfaces from monocular videos has garnered significant attention due to its practical and cost-effective potential for clothes digitization. Recent advancements in neural rendering methods have demonstrated high-quality dynamic clothed human reconstruction results from monocular videos. However, existing methods have limitations in separating the garment surface from the body, leading to a need for more refined approaches to address this challenge [1][5]. Furthermore, while feature curve-based garment reconstruction methods have shown promise in generating impressive results for garment reconstruction from a single image, they struggle to ensure temporal consistency for video input, necessitating the development of novel techniques to overcome this limitation [4][6].

Given the existing limitations, the paper ""REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos"" presents REC-MV, a novel approach aimed at optimizing explicit feature curves and the implicit signed distance field of garments to address the above-mentioned challenges. The proposed approach involves jointly optimizing 3D garment feature curves and surface reconstruction from monocular videos, ultimately enabling the extraction of open garment meshes via garment template registration in canonical space. Experimental results on multiple casually captured datasets demonstrate that REC-MV surpasses existing methods, producing high-quality dynamic segment surfaces [10][11].

The work also leverages insights from existing garment reconstruction methods, such as SMPL and SCAPE, to inform the approach's formulation and optimization strategies [1][2]. Additionally, elements of the proposed method align with the foundational concepts presented in previous works, including learning-based 3D human mesh recovery and shape completion methods, which have paved the way for advancements in garment reconstruction and surface modeling [3][9].

Furthermore, the proposed approach aligns with recent trends in learning-based 3D reconstruction, which have increasingly focused on implicit representations, such as neural radiance fields and signed distance functions, allowing for accurate representation, interpolation, and completion of complex 3D shapes from partial and noisy input data [10][11]. These concepts are particularly relevant to the garment reconstruction domain, given the challenges associated with open surfaces and diverse topologies, issues that REC-MV aims to address with its explicit-implicit hybrid approach [40][41].

In summary, REC-MV introduces a comprehensive approach to 3D garment reconstruction from monocular videos, drawing from an extensive body of related work in 3D human modeling, implicit function learning, and learning-based 3D reconstruction methods. By combining insights from these diverse areas, REC-MV offers a promising solution to the challenging task of reconstructing dynamic cloth from monocular videos. The method showcases advancements in capturing temporally coherent dynamic clothing deformation and represents a valuable contribution to the wider field of garment digitization and reconstruction [20][21].>"
581," **Adaptive Computation in 2D Object Detection.** Adaptive computation in 2d object detection has been studied in the context of ViT [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][42]. These methods can be categorized into two groups: (1) methods that adaptively allocate computational resources to different parts of the model based on the input data [4][1][5], and (2) methods which adaptively prune the network based on input data sparsity [13][13]. Our method falls into the second category. Our method is most related to Sparse DETR [13] and Deformable-DETR [15] in that we also adaptively halt tokens at different layers depending on their contribution to the detection task. However, our method differs in that our method is differentiable and allows for end-to-end training. Furthermore, our framework allows halted tokens to be reused to inform the model's predictions through a straightforward token recycling mechanism.

**Sparse Representation in 3D Vision.** Sparse representation has been extensively studied in 3d vision [43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72][73][68]. In this work, we focus on sparse representation for efficient 3D object detection. Voxel-based methods [44][51] represent 3D objects in a voxelized representation, which is more memory efficient than point-based representations [46][49]. VoxelNet [44] divides the 3D space into a grid of voxels and uses a 3D convolutional neural network (CNN) to extract features. Vote3Deep [43] and VoteNet [50] use deep Hough voting to efficiently aggregate features from 3D points. PointRCNN [51] and PointPillars [49] use PointNet [46] to encode features in a pillar-based representation. LaserNet [53] and LaserFlow [55] use laser-based 3D point clouds to reduce the computation cost of 3D detection. RangeDet [57] and RangeNet-S [56] use range-based features to reduce computational cost. Our work is also related to methods that use sparse representations for 3D vision tasks, such as 3D semantic segmentation [45][47] and 3D bounding box detection [48][51]. However, unlike our method, these methods do not focus on improving the efficiency of DETR-based object detectors. Instead, they focus on reducing the model size and computational cost"," **Efficient Vision Transformers.** Efficient vision transformers have been extensively studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72][73].

**Network Halting.** Network pruning is a popular approach to reduce model size and improve inference efficiency. It has been shown that it is possible to find sparse trainable sub-networks [18][18] or winning lottery tickets [18] at initialization without retraining the entire model. However, these methods are heuristic and heuristic-based, and do not allow for end-to-end training.

Our method is different from existing pruning methods in that it allows for end to end training and is differentiable, which allows for differentiable forward-passing. Moreover, our method allows halted tokens to be reused to inform the model's predictions through a straightforward token recycling mechanism.

","[ There have been a significant number of methods for making deep neural networks computationally efficient while retaining accuracy. Some approaches first model the entire network in a highly efficient form, then un-compress or otherwise decompose the model to the desired computational footprint [1][2]. Others perform neural network pruning [3][4][5][6] or quantization [7][8][9][10][11] to reduce memory footprint and computational requirements.

Pruning [12][13][14][15] focuses on reducing computation by removing connections and weights, while quantization [16][17][18][19] is an approach that reduces memory footprint by reducing bitwidth. There has also been recent progress in developing more flexible, more robust, and potentially lower cost quantization methods for CNNs with data-free training [20][21][22][23][24][25][26] and end-to-end [27][28][29][30][31][32][33][34][35]. Other techniques focus on parallelizing different operations and ensuring efficient use of memory [1][36][37][38]. In the context of point clouds, block dropping strategies and improved voting mechanisms can be applied to accelerate the point-wise multiplication and prediction stages [39].

Most of the aforementioned techniques for efficient CNNs require specialized heuristics or human input that limits their generalization capability, and sacrifices adaptability, which is often desired in deployment. Instead, we propose an end-to-end differentiable method for accelerating CNNs by leveraging the dynamic token halting mechanism.

Another direction for efficient CNNs is via slimming [40][11][41][42], where network design is focused on weight sharing, convolution-independent, and channel-wise computations to reduce computation complexity. Recent methods for slimming include Pixor [43] for CNNs, PointNet++ [44] for the point cloud scenario, and CSD [45] for LIDAR-based detectors. More efficient slimmed architectures have also been developed for segmentation [46] and detection [47] tasks.

In order to scale with increasing depth and resolution, many techniques such as PointPillars [48], PIXOR [49], and Deep Hough Voting [50] propose to perform object detection by embedding objects in a 2D image space. Similarly, pipelines that use RangeImages [51][52][53][54][55][56] or RangeDet [57] show better performance compared to pure range-based 3D detectors. However, these approaches are not scalable to high resolution since there is a linear growth in the number of pixels across the depth range.

Recently, a number]","<Related work>

Efficient 3D object detection is essential for the safety and performance of real-time systems such as autonomous vehicles. The challenge of balancing efficiency and accuracy in deep learning models for 3D object detection has led to the exploration of approaches that leverage transformer-based architectures. Transformative advancements, such as the proposal of Dynamic Token Sparsification frameworks [1], have indicated the potential for dynamic token halting methods to improve the efficiency of 3D object detectors.

Efforts like Adaptive Tokens for Efficient Vision Transformer (A-ViT) [2] have explored adaptive computation time mechanisms similar to what is proposed in the target paper. A-ViT introduces a method for adaptively adjusting the inference cost of vision transformers by automatically reducing the number of tokens in vision transformers as inference proceeds [2]. This adaptation shares similarities with the proposed dynamic halting of tokens based on their contribution to the detection task in the target paper.

Furthermore, previous work such as the Categorical Reparameterization with Gumbel-Softmax [3] has addressed the challenge of non-differentiable operations, similar to the non-differentiable operation of halting a token in the target paper. The proposed method in the target paper for differentiable end-to-end learning by leveraging an equivalent differentiable forward-pass aligns with the principles demonstrated in this work [3].

The application of adaptive computation techniques has also been explored in the context of vision transformers, as showcased in Adaptive Computation Time for Recurrent Neural Networks (ACT) [4]. ACT introduces an algorithm that allows recurrent neural networks to learn the number of computational steps to take between receiving an input and emitting an output [4]. The adaptivity principles illustrated by ACT parallels the dynamic token halting approach proposed in the target paper for accelerating transformer-based 3D object detectors.

In addition, Reduction modules for interpretable redundancy reduction, as presented in Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$) [5], have drawn attention to the significance of addressing redundant computation costs in the context of vision tasks. These reduction mechanisms are akin to the proposed token recycling methodology in the target paper, which allows halted tokens to inform the model's predictions through a straightforward token recycling mechanism.

Efforts to address efficiency challenges in vision transformers have also resulted in the proposition of Adaptive Token Sampling (ATS) [6]. ATS introduces a parameter-free module for reducing the computational cost of vision transformers by selecting tokens based on their relevance to input images [6]. This adaptivity aligns with the dynamic token halting approach proposed in the target paper.

Furthermore, the need for efficiency enhancement in vision transformers has led to the exploration of methods such as Patch Slimming for Efficient Vision Transformers [7]. Patch Slimming introduces an approach for discarding useless patches in vision transformers to mitigate the computational cost [7]. This notion resonates with the efficiency improvement objective of the proposed dynamic token halting in the target paper.



<References>
[1] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification
[2] A-ViT: Adaptive Tokens for Efficient Vision Transformer
[3] Categorical Reparameterization with Gumbel-Softmax
[4] Adaptive Computation Time for Recurrent Neural Networks
[5] IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers
[6] ATS: Adaptive Token Sampling For Efficient Vision Transformers
[7] Patch Slimming for Efficient Vision Transformers"
4270," **Semi-supervised learning** aims to leverage unlabeled data to improve the performance of supervised learning, which has been widely studied in various computer vision tasks [1][2][3]. Consistency-based methods [4][5][6][7][8][9][10][11][12] have been widely used in SSL. For example, Temporal Ensembling [4] and Mean Teacher [5] are two representative works. Temporal ensembling enforces consistency between the predictions of two networks by applying different perturbations to the input image, feature, or network input. Mean teacher [5], on the other hand, enforces the consistency between predictions of the student network and the teacher network by averaging the teacher model's weights. Pseudo-labeling [6][8] is another popular strategy in SSL, which generates pseudo labels for unlabelled data and then trains the model with these pseudo labels. FixMatch [6] and its variants [7][9] select the pseudo labels with high confidence and low uncertainty. FlexMatch [7] and U-Fix [8] select pseudo labels based on the model's prediction confidence and uncertainty, respectively. In addition, NP-Match [9] combines consistency-based and pseudo-label-based approaches to combine the advantages of both types of methods.

**SSL for medical image segmentation** has received increasing attention in recent years. In [10], a transformation consistency self-ensembling model is proposed to segment the skin lesion images by enforcing consistency between transformed and original images. In, a self-training framework is proposed for 3D abdominal CT images, where the network is trained with both labeled and unlabeleddata. In  and [11], a consistency-aware uncertainty estimation method is proposed, which estimates the uncertainty of pseudo labels and then selects the most confident pseudo labels to train the model. In contrast, [12] proposes a multi-planar co-training method to segment multi-organ abdominal CT volumes. However, these methods require a large amount of annotated data, which is time-consuming and labor-intensive. In this paper, we propose a novel annotation way to reduce the burden of annotation and develop a dual-network paradigm to exploit the complementary information from different directions, e.g., transverse, sagittal, and coronal planes, so as to naturally provide complementary views for effective segmentation. Moreover, our method can be easily integrated into existing SSL methods to further improve the segmentation performance, which can be applied to various medical imaging tasks, such as brain MRI, CT, and MRI-based segmentation, to further boost the performance and efficiency of segmentation models. Note that our method is orthogonal to [12], which only utilizes the information from the transverse and sagittal planes for segmentation in 3D medical images, and thus can be combined with our method to further enhance the performance. In summary, our work is different from previous works in three aspects: (1) We propose a new annotation way and its corresponding SSL model, which significantly reduces the annotation cost and"," Semi-supervised learning (SSL) aims to leverage unlabeled data to improve a model's performance. It has been widely studied in various tasks, such as object detection [1][2], semantic segmentation [3][4][5][6][7][8][9], and image classification [10][11][12][13][14][15][16][17][18][19]. Pseudo-labeling (PL) [6][8] is one of the most widely used methods in SSL, which firstly generates pseudo labels for unlabelled data and then uses them to train the model. For example, FixMatch [6] firstly uses the model's predictions on weakly augmented unlabelized images and then selects the most confident pseudo labels. FlexMatch [7] further improves the performance of FixMatch by introducing a curriculum learning approach to leverage different classes at each time step to let pass informative unlabels. UPS [8] improves the accuracy of pseudo labels by introducing an uncertainty-aware pseudo-label selection framework. Mean-teacher [5] and NP-Match [9] propose to use the exponential moving average of model predictions as the consistency regularization to train a teacher network. However, these methods are not suitable for 3D medical image segmentation due to the lack of complementary information.


In this paper, we propose a novel orthogonal annotation method to leverage the complementary information from different directions for effective segmentation.

","[ Semi-supervised learning aims to improve the generalization of deep neural networks without exhausting extra expensive annotations for new tasks. Generally, SSL methods are categorized into consistency regularization, self-training and pseudo labeling approaches.

Consistency regularization methods impose strong regularization on the features and predictions of the network in the feature space or the output space [1][2]. Compared with self-training methods, consistency regularization methods have higher generalization ability for the downstream tasks but with a large number of parameters. Some studies focus on improving the performance of consistency regularization methods, e.g. Temporal Ensemble [3][4], MeanTeacher [5][6] and FixMatch [7]. Some other studies use similar methods but focus on the detail design. For instance, CC-Match [8] first lowers the noise in pseudo labels and then combine the reliable ones as labels to build a new labeled data set. NP-Match [9] integrates the consistency loss and network prediction loss and solves the network problem from the perspective of combining data points.

Self-training methods learn to generate pseudo labels by training on a combination of both labeled and unlabeled data. This method is effective in the semi-supervised medical image segmentation task, because it does not depend on complex schemes. Previous studies mainly explore the efficiency of pseudo labels, e.g. Selective Self-training  selectively uses pseudo labels from different sources and FickleNet [10] assigns a different weight for different confident labels. In addition, some studies mainly explore the uncertainties of pseudo labels, i.e. CoraNet [11] assigns different confidence for different uncertain pseudo labels, DisTruNet  uses random noise to generate pseudo labels, and LTS  utilizes the learned-to-search-similarity loss to assign a loss-weight to each pixel to make sure the learned similarity among feature maps is consistent with that in the original image. In this paper, we first propose the orthogonal annotation that encourages two orthogonal slices in a 3D volume to be labeled in parallel.

Pseudo labeling methods combine weakly-supervised methods and semi-supervised methods to generate labels from both labeled and unlabeled data. Previous studies mainly focus on improving the performance of pseudo labels, e.g. the Deep-Aug-Fusion [12] leverages a prediction refinement module to improve the utilization of pseudo labels, the Uni-consistency-loss [13] combines the consistency loss on both the labeled data and unlabeled data. In this paper, we further design a dual-network paradigm to learn the similarities among different modalities, e.g. pseudo-labelings from both labeled and unlabeled data.

The coarse annotation method in [14] exploits an anti-adversarially maniplated attention map]","Step 1. Extract important sentences from the cited paper for related work:
1. Recent trends in semi-supervised learning have significantly boosted the performance of 3D semi-supervised medical image segmentation.
2. Active Teacher extends the teacher-student framework to an iterative version for semi-supervised object detection.
3. The proposed novel annotation way, orthogonal annotation, significantly relieves the burden of annotation and involves complementary views and intrinsic similarity among adjacent 3D slices.
4. A dual-network paradigm named Dense-Sparse Co-training (DeSCO) is proposed that exploits dense pseudo labels in the early stage and sparse labels in the later stage for effective segmentation.
5. Perturbed and Strict Mean Teachers addresses the prediction accuracy problem in consistency learning methods with novel extensions of the mean-teacher model.
6. FixMatch demonstrates the power of a simple combination of two common semi-supervised learning methods: consistency regularization and pseudo-labeling.
7. FlexMatch proposes Curriculum Pseudo Labeling to leverage unlabeled data according to the model's learning status for semi-supervised learning.
8. In Defense of Pseudo-Labeling introduces an uncertainty-aware pseudo-label selection framework for semi-supervised learning that drastically reduces noise in the training process.
9. NP-Match uses neural processes to adjust to the semi-supervised image classification task and outperforms state-of-the-art results on various public datasets.

Step 2. Summarize sentences and decide on topics of related work:
Based on the extracted sentences, the related work can discuss different approaches to semi-supervised learning, including methods such as active teacher-student learning, dense-sparse co-training, mean-teacher model, pseudo-labeling, curriculum pseudo labeling, and uncertainty-aware pseudo-label selection. Additionally, it can cover the application of these methods in various domains like medical image segmentation, face synthesis, and deep learning.

Step 3. Write related work based on topics:
<Based on recent trends, semi-supervised learning has seen significant advancements in various domains, particularly in medical image segmentation. Active Teacher [1] proposes an iterative version of the teacher-student framework for semi-supervised object detection, while Dense-Sparse Co-training (DeSCO) introduces a dual-network paradigm for effective segmentation with dense pseudo labels in the early stage and sparse labels in the later stage. Perturbed and Strict Mean Teachers [3] addresses the prediction accuracy problem in consistency learning methods, providing novel extensions to the mean-teacher model. Incorporating a simple combination of two common semi-supervised learning methods, FixMatch [6] achieves state-of-the-art results by leveraging consistency regularization and pseudo-labeling. FlexMatch [7] introduces Curriculum Pseudo Labeling to leverage unlabeled data according to the model's learning status for semi-supervised learning, while In Defense of Pseudo-Labeling [8] introduces an uncertainty-aware pseudo-label selection framework to drastically reduce noise in the training process. Furthermore, NP-Match [9] adjusts neural processes to the semi-supervised image classification task, outperforming state-of-the-art results on various public datasets.>

<In addition to medical image segmentation, these semi-supervised learning methods have also found applications in other domains. For instance, Active Teacher and FixMatch demonstrate effectiveness in semi-supervised object detection and deep learning tasks, while FlexMatch has been successful in boosting convergence speed and achieving strong performances on SSL benchmarks. The diverse applications of these methods underscore their versatility and potential impact in different areas of research and industry. Moreover, the consistent theme of leveraging unlabeled data, reducing annotation burdens, and improving model performance underscores the broader impact and relevance of these semi-supervised learning methods across diverse domains.>

<The combination of active learning, pseudo-labeling, and curriculum-based approaches showcases the richness and diversity of techniques that have emerged in this space. From addressing prediction accuracy and noise reduction to leveraging neural processes and uncertainty-aware pseudo-label selection, these methods are poised to drive further innovation in the field of semi-supervised learning. The ongoing development and evaluation of these techniques across different domains underscore the robustness and potential impact of semi-supervised learning in addressing real-world challenges in various domains, from medical imaging to computer vision and beyond.>

<Irrespective of the specific domain or application, the continued evolution and refinement of semi-supervised learning methods have demonstrated their ability to significantly boost the performance of various tasks with limited labeled data. The advancements in these methodologies are critical for addressing the challenges associated with annotation burden, model performance, and generalization. By leveraging innovative approaches such as active learning, pseudo-labeling, curriculum-based learning, and uncertainty-aware pseudo-label selection, these methods empower researchers and practitioners to achieve superior results across diverse domains and applications.>

<>
"
2566," **Intrinsic Inverse Rendering.** The goal of inverse rendering is to recover the geometry, reflectance, and lighting properties of a scene from a set of captured images. Classical methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32] typically decompose the scene into geometry, material, and illumination. However, these methods are limited in their ability to handle complex scenes with spatially-varying materials and complex lighting effects due to the lack of explicit geometry representations.

**Implicit Geometric Representations.** Implicit geometric representations have been widely used in inverse rendering to model the geometry and reflectance of complex scenes. For example, the Retinex algorithm [1] decomposes an image into a diffuse image and a reflectance image, and estimates the intrinsic image by minimizing the difference between the illumination and the reflectance images. The method can be extended to handle specular and shadow effects [2][1][3]. However, it is sensitive to the choice of reflectance model and cannot handle high-order lighting effects such as cast shadows. Inverse rendering methods based on differentiable rendering have been proposed to address these limitations. For instance, DIB-R [23] and its extensions [22][24] use a triangular mesh to represent the scene geometry, and estimate the lighting and materials using a differentiable renderer. NeRF-based methods [25][27] use neural radiance fields (NeRFs)  to represent a scene as a continuous volumetric function that maps a point in space to a radiance field in the image plane. NeRV [30] extends NeRFs to handle relighting and view-dependent materials. NeILF [28] uses incident light fields (ILFs) to model shadows and specularities. NeuLighting [29] uses a neural network to estimate the light field for free viewpoint outdoor scene relighting, but it does not model the lighting effects. PhySG [19] uses spherical Gaussians for physics-based inverse rendering of indoor scenes, but does not support relighting with specular or shadow effects. In contrast, we use a neural field to model primary rays, and use an explicit mesh (reconstructed from the underlying neural field) for modeling secondary rays that produce higher-order shadows. Our method is able to reconstruct and manipulate complex geometry and materials from captured images with high-quality photorealistic relighting on several outdoor datasets, and can be used for physical-based scene manipulations such as virtual object insertion with ray-traced shadow casting. Our work is also related to NeRF in-the-wild [32] and NeRF++ [20], but we focus on inverse rendering for large outdoor scenes, which has not been explored in prior work. We present a novel inverse rendering framework for large urban scenes capable of jointly"," **Inverse Rendering.** The problem of inverse rendering has been extensively studied in computer vision and graphics for a long time, with a focus on single object [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37]. Inverse rendering methods can be broadly categorized into mesh-based and neural methods. Mesh-based methods [4][4] use a mesh representation of the scene geometry to model the geometry, reflectance, and reflectance of the object, and use a differentiable renderer to synthesize novel views. For example, DeepLight [4] uses a spherical Gaussian lighting representation to model reflectance and geometry, while DeepLightHD [5] uses spherical Gaussians to represent reflectance. NeRoIC [17] and NeRV [30] use multi-layer perceptrons (MLPs) to model geometry and materials. NeRD [26] and Ref-NeRF [27] model geometry, materials, and lighting jointly. NeRv [30], NeRVC [29], and NeILF [28] model the lighting and materials separately, and NeURV [29] model both geometry and lighting separately. However, these methods are limited to indoor scenes and cannot be directly applied to large outdoor scenes.
/NeRV  and NeRF++ [20] are able to model outdoor scenes, but are not able to handle specular and shadow effects.

**Differentiable Rendering with Neural Fields.** Recently, differentiable rendering has become a popular approach for inverse rendering. For instance, InverseRenderNet [12] uses an inverse rendering pipeline to learn the geometry and material decomposition from a single image. Inverse-renderNet [8] uses multi-view stereo (MVS) to learn a statistical natural illumination prior, and uses it to supervise the network training. In contrast, our method uses a neural field to model lighting and shadows, and we do not rely on MVS or MVS for training. SAMURAI [14] is the first work to use differentiable differentiability to perform inverse rendering of outdoor scenes from images, but it is limited to outdoor scenes with known camera poses and lighting. In","[ **Photorealistic Scene Inverse Rendering.** Most image-to-scene reconstruction methods work with photos or panoramas [1][2][3][4] for object reconstruction. Since ground truth materials and geometry are hard to obtain, inverse rendering has mostly focused on object-centric view synthesis [5][6][7][8] and extrapolating objects from photos [9][10][11]. However, extrapolation methods have lower fidelity for complex geometry and material since they rely on object-level priors and handcrafted transformation or interpolation operators. To model scene geometry, outdoor rendering methods leverage LIDAR data [12][13] and pre-filtered lighting [14]. While LIDAR data is expensive to collect, pre-filtered lighting suffers from low quality, as it captures a simplified environment with up to two or three shadows. Recently, methods that directly predict geometry, albedo, and illumination from images have been proposed [15][16][17][18][19][20][21][22][23][24]. NeRD [25] adopts a differentiable rendering technique to decode geometry, materials, and illumination from a NeRF. NeRS [26] and Ref-NeRF [27] leverage spatial-varying reflectance functions. NeILF [28] leverages incident light fields to model geometry and materials. However, the spatial-varying lighting remains entangled with scene geometry, limiting applications. NeRV [29] recovers albedo, depth and radiance with auxiliary optic direction for indoor scenes and a simple environment map for outdoor scenes. NeuLighting [29] enhances NeRV with explicit representation of indirect lighting by a neural function. However, they need to differentiate for each ray via sampling a new viewpoint, and it cannot provide robust illumination estimation due to limited view coverage. NeRVL [30] leverages multiple viewpoint information to mitigate the illumination leakage problem, and provide robust lighting for occluded regions. However, it still has limited view coverage and requires multiple optimization passes. NeRS-NIR [31] is the most recent method that can handle shadows, but it still suffers from limited view coverage and is not able to produce high-quality geometry or lighting.

Our approach differs from all these prior work in that our method (i) can handle a large outdoor scene containing complex geometry and lighting, (ii) can handle shadows, and (iii) produces photorealistic and high-quality geometry and lighting.

**Inverse Rendering for Outdoor Scenes.** Inverse rendering for outdoor scenes is challenging, since it requires accurate capture of the outdoor lighting. For dealing with the]","<In the field of computer vision and graphics, there is a growing interest in inverse rendering techniques for complex scenes, including outdoor environments. In recent years, researchers have made significant strides in reconstructing scenes and intrinsic decomposition from captured imagery, enabling applications such as relighting and virtual object insertion. Traditional methods like mesh-based techniques and retinex theory [1] have been important in understanding the basis of color sensations and intrinsic image decomposition. However, these methods have limitations when it comes to scaling to the complexity and scale of outdoor scenes. The development of a novel inverse rendering framework for large urban scenes as presented in the target paper addresses these limitations by jointly reconstructing scene geometry, spatially-varying materials, and HDR lighting from posed RGB images [2]. This approach integrates a neural field with an explicit mesh for modeling primary and secondary rays, enabling photorealistic relighting with specular and shadow effects.

Previous work has also focused on solving the intrinsic image decomposition problem by using deep learning frameworks to estimate shape, spatially-varying lighting, and non-Lambertian surface reflectance from single RGB images of indoor scenes. These techniques have leveraged insights from physics-based rendering and deep learning to jointly reason about shape, lighting, and reflectance [5] [6] [8]. Additionally, datasets such as OpenRooms have been proposed to create large-scale, photorealistic indoor datasets for training deep networks [7]. These research efforts have helped advance the understanding of spatially-varying lighting estimation and material reconstruction for indoor scenes.

In the domain of outdoor scenes, there has been a focus on estimation of spatially-varying outdoor lighting from single images and panoramas using deep learning frameworks [34] [35] [36]. These approaches have aimed to leverage large datasets of outdoor panoramas and have demonstrated accurate recovery of outdoor illumination conditions, enabling photorealistic renderings and virtual object insertion. Moreover, methods like NeRD [26] have addressed the challenge of decomposing a scene into spatially varying BRDF material properties, even when input images are captured under different illumination conditions.

Advancements in neural rendering, including Neural Radiance Fields (NeRF), have facilitated the synthesis of novel views of complex scenes from unstructured collections of in-the-wild photographs [32]. Extensions to NeRF, such as NeRF-W, have focused on addressing limitations related to variable illumination and transient occluders, leading to temporally consistent novel view renderings that are closer to photorealism than prior methods. These developments showcase the potential for learning-based methods to handle unstructured image collections and improve view synthesis fidelity.

Overall, the related work shows a concerted effort in leveraging neural networks, physics-based rendering, and deep learning in addressing the challenges of inverse rendering, spatially-varying lighting estimation, and material reconstruction for both indoor and outdoor scenes. By integrating neural fields, explicit meshes, and deep learning frameworks, researchers are advancing the capabilities of inverse rendering techniques, enabling the faithful disentanglement of complex geometry and materials from lighting effects for a wide range of scenes.>"
2206," **3D-Aware GANs.** 3D-aware generative models have been proposed to synthesize realistic 3D objects and scenes. Inspired by the success of 2D generative adversarial networks (GANs) [1][2] in 2D image synthesis [3], a large number of GAN-based methods [4][5][6][7][8][9][10][11][12][13][14][15][16][17][18] have been developed to generate 3D shapes and/or scenes with controllable appearance. For example, HoloGAN [7] and BlockGAN [8] use 3D voxel grids as the 3D representations for 3D object-aware GAN training. However, these methods are limited to generating low-resolution images due to the cubic memory and computational complexity of 3D convolutions. StyleNeRF [13], GIRAFFE [14], StyleSDF [15] and GRAF [16] propose to represent 3D scenes as compositional neural radiance fields (NeRFs) and synthesize high-resolution 3D images with high-quality controllability. EpiGRAF[17] and CIPS-3D [18] further improve the quality of the synthesized images by introducing a novel latent space and a novel loss function to better constrain the latent space. In this work, we propose a novel geometry-guided 3D GAN framework (EG3D) to generate full head shapes and appearances.


**Controllable 3D Head Synthesis.** There is a large body of work on 3D face synthesis [19][20][21][22][23][24][25][26][27][28][29][30][31] that focuses on synthesizing identity-preserved 3D faces. Most of these methods leverage 3DMM [32] or 3D morphable model (3DMM-MMR) [25][25] to represent the face and control the head shape and appearance in the canonical space. SofGAN [19], AniFaceGAN [27] and SofGAN++ [20] synthesize 3D head images by using 3D Imitation learning. GIF [21] and PIE [23] learn a latent space of face images using a GAN [1] and then use a differentiable renderer [22] to invert the latent code to a canonical space, which can be used to control the pose, expression and identity of the generated 3D heads. StyleRig [24] and CGOS [29] further propose to use an encoder-decoder network [30] to learn a 3D shape-conditioned latent space for the control parameters, which is then decoded into a canonical 3D space by a decoder-based 3D renderer network. In contrast, our method explicitly defines a novel semantic signed distance function (SDF) around a head geometry (FLAME) that is used to guide the synthesis of full heads in a disentangled canonical space to achieve high-"," **3D-aware Image Generation.** 3D-based image generation has been extensively studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32]. Most of these methods are based on GANs [2][1][3] and are trained on 2D images, which are not suitable for 3D data due to the domain gap between 2D and 3D space. To address this issue, several works [10][12] propose to use 3D GAN models to synthesize 3D images from 2D data. However, these methods suffer from the problem of view-inconsistent rendering, which results in blurry images or artifacts when the camera poses change. To overcome this problem, some recent works [13][18] propose the use of neural radiance fields (NeRF) [33][34][35][36][37][38][39][40][41][42] to model the geometry and appearance of 3D objects. These NeRF-based methods can synthesize high-fidelity 3D models of human faces and bodies, but are limited to the geometry-only setting.


In this paper, we propose a novel geometry-guided 3D head synthesis model that can synthesise high-quality 3D full heads with identity-preserved head shapes, facial expressions, and neck poses. We first define a semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters, and then leverage the 3D volume rendering step guided by the SDF.

","[ **2D Image Synthesis.** Recently, unconditional GANs [1][2][3] have achieved impressive results in generating photo-realistic images from single-view 2D images. We follow the prior work [4][5][6][7][8][9][10][11][12][13][14][15][16][17] to synthesize 2D images in our method, as we only need to synthesize the rendering views of 3D head geometry and focus on disentangled controllable 3D head synthesis.

**3D Image Synthesis.** 3D image synthesis approaches can be broadly divided into prior works with geometry and prior works without geometry. For prior works without geometry, 3DGAN [18][19] conditioned on a collection of 2D images and synthesize 3D object data in latent space, but it is not realistic and the control is only on 3D latent space. The closely related method 4D-GAN [20] performs disentangled control over expression, identity, pose and illumination, but it is limited to fixed facial structures and stiff faces. For prior works with geometry, [21][22][23][24][25] synthesize the 3D head model by directly learning a non-rigid appearance deformation of 3D shapes (usually bone shapes). To further use neural radiance field (NeRF) for 3D head geometry [26][27][28][29], some methods [30][31][24][25] perform image editing by using pre-trained unconditional generators, while [30] leverages an adversarial training scheme.

Recent methods also use volumetric geometry to allow disentangled control over facial geometry. [32][31] synthesize facial images based on a single 4D scan. NeuralEye  provides a disentangled control over ocular and iris of each eye by multi-view images. StyleneRF  also uses volumetric geometry for synthesizing 3D head and 2D images, but it requires additional SMPL parameters [33] and multiple RGB-D cameras. While other methods [26][27] use NeRF to synthesize 3D head geometry, but they only provide explicit control over pose and lighting. However, their control over shape is limited to pre-defined control samples of each class.

**Dynamic NeRFs.** With the increasing success of NeRF, recent works [34][35][36][37][38][39][40][41][42] further extend the capability of NeRF by incorporating motion information in]","<We present related work in the field of 3D avatar synthesis and controllable 3D image generation. Recently, there has been a surge in research on generative models and neural scene representations that enable the synthesis of high-fidelity 3D avatars or objects with controllable attributes such as expression, pose, and appearance. The style-based GAN (StyleGAN) architecture has been instrumental in achieving state-of-the-art results in data-driven unconditional generative image modeling [1]. This framework has informed subsequent research in controllable 3D image synthesis, especially in the domain of human faces. StyleGAN has been further improved to enable controllable 3D-aware image synthesis that provides explicit control over pose, expression, and illumination [15].>

<One key direction in related work is the use of neural radiance fields (NeRF) for 3D scene representation and novel view synthesis, particularly in modeling dynamic and deformable scenes. Non-rigid neural radiance fields (NR-NeRF) allow for the reconstruction of non-rigid dynamic scenes [37], while H-NeRF enables rendering and temporal reconstruction of humans in motion [38]. These methods extend traditional neural rendering techniques to capture dynamic scenes using sparse input data from cameras or monocular videos. Similarly, D-NeRF introduces a method for generating static and non-rigid 3D reconstructions using neural radiance fields, thereby addressing the limitations of traditional NeRF for dynamic scenes [36].>

<The concept of controllable neural radiance fields (CoNeRF) introduces a framework that enhances neural 3D representations to offer intuitive and interpretable user control beyond novel view rendering. It allows users to annotate parts of the scene for controllable attributes, resulting in a few-shot learning paradigm for discovering attributes automatically when annotations are not provided [41]. Furthermore, neural radiance fields have been employed for generating talking head videos from audio input in AD-NeRF, facilitating the synthesis of high-fidelity videos corresponding to the provided audio signals [40].>

<In addition, research in the area of 3D avatar reconstruction and synthesis has yielded significant advancements. For instance, techniques such as PIE offer an approach for embedding portrait images in the latent space of StyleGAN, facilitating intuitive photo-realistic editing of head pose, facial expression, and scene illumination [23]. Another significant development is the introduction of Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction, which enables the modeling of appearance and dynamics of a human face, including talking dynamics, based on an implicit representation of the head and a low-dimensional morphable model that provides control over pose and expressions [42].>

<These research endeavors collectively contribute to the cutting-edge advancements in controllable 3D avatar synthesis and dynamic scene representation, opening new frontiers for realistic and controllable 3D image generation.>

[1] Analyzing and Improving the Image Quality of StyleGAN
[15] StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation
[23] PIE: Portrait Image Embedding for Semantic Control
[36] D-NeRF: Neural Radiance Fields for Dynamic Scenes
[37] Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene
[40] AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis
[41] CoNeRF: Controllable Neural Radiance Fields"
2082," **Multi-view 3D Detection.** Recently, multi-view based 3D object detection has attracted much attention due to its flexibility and efficiency. BEVDet [1] is the first work to directly regress 3D bounding boxes from BEV features. It proposes a novel BEV-to-BEV feature fusion module and a novel multi-stage training strategy. Deformable DETR [2] and DETR3D [3] extend the DETR framework to 3D detection based on the BEV representation. PETR [4] and PETRv2 [5][6] propose position embedding transformation (PETR) and deformable transformer (DETR-R) to learn the position-aware feature representation for 3D objects. BEV4D [7] and BEVDepth [8] exploit the temporal information to improve the performance of BEV3D. Besides, BEVStereo [9] enhances the depth estimation performance by fusing the depth information from multiple views.

**Quantization.** Network quantization aims to reduce the model size and memory consumption by quantizing the weights and activations to low-precision [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24]. In this paper, we focus on the quantization-aware training (QAT) for BEV tasks. QAT aims to train a quantized model with the knowledge distillation (KDNNs) [25][26][27][28][29][30][31] or semi-supervised learning [31][32][33][34][33] objective. In this work, we propose a novel view-guided distillation objective to stabilize the QAT while enhancing the model performance by leveraging both image features and BEV feature representations. The most related work to ours is BEVDistill [35], which proposes a cross-modal distillation loss to transfer the knowledge from a pre-trained teacher model to a student model. However, it does not consider the effect of quantization on the training process. In contrast, we show in our experiments that directly applying quantization will make the training unstable, and lead to intolerable performance degradation.

 proposes to use quantization as a post-training (PTQ) method to reduce model size. It quantizes the model weights to 4-bit and 8-bit, and then fine-tunes the model with KD. [28] proposes to jointly train KD and quantization. [29] proposes a variational information distillation method to transfer knowledge from the teacher to the student. [30] introduces the relation knowledge to distill the knowledge of the teacher. [31] and [32] propose to use multiple teachers to distil knowledge from multiple networks. [34] proposes self-distillation to train the student model by distilling knowledge from itself. In our work, instead of using KD, we introduce a novel QD objective to enhance the model's performance by transferring knowledge from quantized models.

 introduces a novel"," **BEV-based 3D Object Detection.** BEV3D [1][2][3][4][5][6][7][8][9] is the first work to propose a BEV 3D object detection framework. It firstly proposes a BEVDet framework to perform 3D detection in BEV, and then uses a BEVLayer to perform BEV-to-2D queries to link the BEV features to the 2D features. BEVFormer [5] proposes a novel framework to learn unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks, which exploits both spatial and temporal information by interacting with the spatial-temporal space through predefined grid-shaped BEV queries. BEVD-Tiny [1] further improves BEVDt [5], which introduces a novel data augmentation strategy and a non-maximum suppression strategy to further improve the performance. BEVGD [8] proposes an explicit depth supervision module to improve the accuracy of BEV detection. BEVCoder  proposes a multi-view 3D detector based on BEV. It uses BEV embedding to extract BEV feature representations and then applies BEV query-based BEV representation learning to perform multi-task learning. However, these methods are not directly applicable to multi-camera 3D detectors, which are more challenging due to the non-trivial latency. In this paper, we propose a novel view-guided distillation objective to stabilize the quantization-aware training and improve the model performance. Our method is also related to post-training quantization [10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] and model compression methods [25][26][27][28][29][30][31][32][33][34][35].

**Model Compression and Quantization.** Model compression aims to reduce the model size and model complexity by reducing the model parameters to low-precision. Model quantization is widely used to accelerate the model training process and reduce the memory consumption of the model. For example, [10] proposes to quantize the model weights to be either powers of two or zero, and [25] uses knowledge distillation to compress the model into a smaller model. [28] proposes quantized distillation and quantization to improve model compression. [26] proposes the Apprentice method to improve low-bit accuracy","[ **Multi-view 3D detection.** Researchers have proposed multiple approaches to achieve 3D detection in a BEV, which can be roughly classified into two categories: 1) designing a 3D detector to detect objects from BEV data; 2) augmenting monocular 2D object detectors by embedding BEV information into them. The task can be further divided into two sub-tasks, namely 3D-only and 2D-to-3D detection.

**3D-only BEV detection** designs a 3D detector from scratch to detect 3D boxes directly from BEV images [1][2][3]. BEVDet [1] exploits the semantic-guided BEV transformer to leverage BEV features for 3D detection, and extends it into BEVDet4D [7] to improve performance with prior 3D knowledge. PETR [4] encodes the position information of 3D coordinates into image features and achieves an excellent performance. BEVFormer [5] leverages spatiotemporal transformers to handle temporal consistency. Moreover, PETRv2 [6] explores the effectiveness of temporal modeling and achieves improved performance.

**2D-to-3D BEV detection** extends 2D detectors by embedding BEV information. Recent works have demonstrated their effectiveness. For example, BEVDepth [8] extends Depth-RCNN  by embedding BEV information and adding a branch for estimating the depth of bounding boxes. Besides, BEVStereo [9] incorporates temporal information of the previous frames to boost the performance. In this paper, we also extend the latest 2D-to-3D detector, namely DETR3D [3], to achieve more accurate quantization-aware distillation training.

**Quantization.** As low-precision accelerates and reduces the cost of both on-device and cloud servers, quantization-aware research is progressively gaining attention. While state-of-the-art quantization research can be classified into the following categories: 1) quantizing weights and activations, 2) fine-tuning post-training, and 3) hyper-parameter free quantization, it is non-trivial to combine multiple quantization schemes and bring the best performance. In this paper, we design the Quantization-aware Training (QAT) method to optimize the quantization model and propose two quantized versions of DETR3D. QAT can be roughly divided into the following parts: (1) quantization-aware training loss (QATL) that introduces quantized features into the training pipeline; (2) view-guided quantization loss (VQL) that achieves better optimization on view-guided distillation while improving the view-guided performance; (3) feature mapping (FM) that enables view-guided distillation]","<Multi-view 3D object detection has seen significant advancements through paradigms such as BEVDet [1], Deformable DETR [2], DETR3D [3], PETR [4], BEVFormer [5], PETRv2 [6], BEVDet4D [7], BEVDepth [8], BEVStereo [9], Incremental Network Quantization [10], HAQ [11], CoDeNet [12], NoisyQuant [13], ZeroQ [14], Towards Accurate Post-training Network Quantization [16], PACT: Parameterized Clipping Activation for Quantized Neural Networks [19], and Variational Information Distillation for Knowledge Transfer [29]. These methods have advanced the field by addressing challenges such as 3D object detection in multi-camera views, depth estimation, and enhancing depth perception. Additionally, they have introduced techniques for network quantization, knowledge distillation, and addressing noisy labels.>

<The use of multi-view 3D object detection in autonomous driving demands efficient models, leading to the exploration of quantization-aware techniques. Methods like Incremental Network Quantization [10], HAQ [11], and PETRv2 [6] have contributed to efficient network quantization by appropriately addressing the challenges posed by low-precision weights and hardware-aware quantization. Similarly, NoisyQuant [13] and ZeroQ [14] present innovative approaches for efficient post-training network quantization. These techniques play a crucial role in enabling the deployment of accurate models on resource-constrained platforms, such as autonomous vehicles.>

<In the context of knowledge distillation, approaches like Variational Information Distillation for Knowledge Transfer [29], Distillation with Cheap Convolutions [33], and Structural Model Distillation for Memory Reduction [33] have improved performance by transferring knowledge from teacher networks to student networks. Techniques such as PACT [19] have addressed the challenges of deep model quantization by introducing parameterized clipping activation for quantized neural networks, which can be utilized to reduce the memory footprint and computational costs of vision transformers, convolutional neural networks, and other deep learning architectures.>

<Furthermore, in the space of multi-modal 3D object detection, BEVDistill [35] has addressed the challenges of accurately detecting objects from multiple viewpoints by incorporating LiDAR-based detectors. This highlights the importance of cross-modal knowledge distillation for enhancing the performance of multi-view 3D object detection systems. In summary, advancements in efficient network quantization and knowledge distillation techniques presented in the mentioned works have significantly contributed to improving the efficiency and accuracy of deep learning models, particularly in the context of multi-view 3D object detection and vision transformers.>"
880," **Co-salient object detection.** The traditional definition of CoSOD task is to segment the common salient objects in a group of relevant images [1][2][3][4][5][6][7][8][9], which is a relatively underexplored area. Early works [2][1][3] mainly focus on developing hand-crafted features to capture the correlation-based local features of salient objects. For example, Zhang _et al._[2] proposed a saliency-model-guided visual co-saliency detection (SGM-CoSOD) method to detect the common and salient foregrounds from the relevant group of images automatically and efficiently. In [1], a fully unsupervised energy minimization model was proposed for co-segmentation over multiple images, which is based on the assumption that the common objects in the group are similar to each other. However, these hand-craft features are not robust to the variations of object appearances in different images.


In recent years, deep learning based methods have been widely used in the field of computer vision [10][11][12][13][14][15]. In [10], a robust adversarial reinforcement learning (RARL) method was proposed to improve the robustness of deep neural networks against adversarial perturbations in the real-world environment [10]. In, a robust deep neural network (R-DNN) was designed to learn a robust policy in the presence of adversarial attacks. In, an adversarial training strategy was proposed, where the adversarial examples were generated by adding a small perturbation to the input images and the model was trained to be robust against the perturbated images. In addition, a randomization-based defense method [14] was also proposed to make the model more robust to adversarial effects. In this paper, we propose an uncertainty-aware group exchange-masking strategy to address the issue of model robustness under the condition of irrelevant images in the testing image group.

 proposed a co-attention-based deep learning framework for CoSoder, which was based on a two-stage training strategy [6]. In the first stage, the network is trained to extract the common object features, and in the second stage, an attention mechanism was used to select the most informative features from the extracted common objects. In contrast, we design a latent variable generator branch which is made of conditional variational autoencoder to generate uncertainly-based global stochastic features. The proposed feature extraction module considers both the uncertainty caused by the irrelevant images and group consensus in the remaining relevant images, and is able to extract more robust features.

 presented a deep co-supervised learning framework (Co-SSD), which is an end-to-end deep learning method for Co-SOD. The network is first trained to predict the saliency map of each image group, and then the co-clustering results are used to obtain the common foregrounds in the final prediction stage. In order to address this issue, a multi-scale label smoothing strategy"," **Co-salient object detection.** CoSOD has been studied for a long time in the computer vision community [1][2][3][4][5][6][7][8][9]. In recent years, deep learning based methods have been proposed to solve this task. Fan _et al._[7] proposed a deep learning-based method to address this task by learning a semantics-aware image representation based on a convolutional neural network, which adaptively learns the group-wise features for CoSod. Fan [9] proposed an adaptive graph convolution network with attention graph clustering to characterize the intra- and inter-image correspondence. Fan  proposed a co-saliency detection method based on the fully-convolutional network (FCN) to capture the common and salient foregrounds from a group of relevant images, which is further improved in [8]. Fan [5] proposed the group collaborative learning framework (GCoNet) to detect co-sallient objects by mining consensus representations at group level based on two necessary criteria: 1) intra-group compactness and 2) inter-group separability to better formulate the consistency among co-segmented objects. Fan proposed the GCoNet to capture their inherent shared attributes by capturing their inherent common attributes using a group affinity module and a novel group collaborating module. Fan presented a novel co-attention network (CoAttNet) [6] to learn the collaborative and interactive relationships between group-level feature representation and single-image individual feature representation, and model the collaborative learning in a collaborative framework. In [8], a multi-scale label smoothing strategy was proposed to refine the initial feature maps of the co-al salient object detection model to capture more fine-grained information. In this paper, we focus on improving the robustness of CoSoder models under the condition of irrelevant images.

**Transfer learning from simulation to real world.** Transfer learning is an important research topic in the field of reinforcement learning [10][11][12][13][14][15], which aims to learn a policy from a simulated source domain to a real-world target domain by augmenting real data with simulated data. The main idea is to use the discrepancy between the source domain and the target domain to augment the training data. For example, EPOpt [13] used an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of","[ To explore common objects from group of image with semantic discrepancy, most related work applies saliency priors into co-saliency models. [1][2][3] design saliency cues and propose a loss function based on saliency priors, while [4] utilizes user's labor, human posture, and object shape to obtain semantic masks, which is too time-consuming and not practical.

To make up for the lack of semantics cues, unsupervised methods have been designed in recent years. [5][6][7] detect co-salient objects using clustering feature similarity and similarity of co-saliency detection results in feature space. Recently, attention mechanism is used in co-saliency detection models. Liu [8] propose a method that regards co-salient regions as foreground masks to boost segmentation results. Wang [9] use attention graph clustering to separate objects. These methods cannot address the imbalanced effects of different input image groups on the final results due to no graph pooling mechanism. The salient objects in each image will act as guidance and affect each other and the final results, while the irrelevant objects will obstruct them. By building on the model, this paper adds one more strategy, namely Co-Salient Object Exchange, to the co-saliency detection model to achieve robust co-salient object detection on relevant group of image.

Adversarial learning [10][11][12] is a learning method to make the system more robust to unpredicted changes. Recently, researchers have paid more and more attention to real-world training, which aims at learning a robust agent that has a certain degree of adaptability to the real-world environments [13][14][15]. Our proposed model is based on adversarial learning and tries to train a more robust co-salient object detection model by helping it to recognize different kinds of common objects. In addition, one of the advantages of adversarial learning is to protect the model against adversarial examples. We expect the proposed model to be less affected by irrelevant objects, which is also helpful to improve the robustness of our model.

]","<Co-salient object detection, which aims to segment the common salient objects in a group of related images, has been addressed by various models. Existing CoSOD models typically assume group consensus, which can lead to robustness issues when irrelevant images are present in the testing group [1]. To address this, the paper presents a group exchange-masking (GEM) strategy for robust CoSOD model learning. The GEM strategy selects a set of images from each group and exchanges them, while considering uncertainty caused by irrelevant images and group consensus in the remaining relevant images [1]. This approach differs from traditional co-saliency detection models by incorporating uncertainty-aware global stochastic features and correlation-based local features for robust CoSOD model learning [1, 2, 3].

Previous research has explored various techniques for co-saliency detection, such as cluster-based algorithms, efficient saliency-model-guided visual co-saliency detection, and end-to-end group-wise deep co-saliency detection [2, 3, 7]. Some of these approaches leverage cluster saliency, global terms, and visual attention cues to measure co-saliency, while others utilize single-image saliency maps and collaborative learning to detect common salient objects in group images [2, 3, 7, 8, 9]. Additionally, methods such as group collaborative learning and co-attention CNNs have been proposed for efficient and real-time co-salient object detection, demonstrating improved performance over state-of-the-art models [5, 6, 7].

Furthermore, the domain of reinforcement learning has seen related work in addressing robustness and model generalization, particularly in the field of adversarial reinforcement learning [10, 11, 12, 13]. Techniques for transferring policies from simulation to real-world environments and learning robust neural network policies through Bayesian neural networks have been introduced, emphasizing the importance of stability and robustness in learning policies for real-world tasks [10, 11, 12, 13, 15]. Additionally, work on mitigating adversarial effects through randomization and model ensembles using simulated source domains to improve robustness showcases the broader applicability of reinforcement learning techniques to address challenges in real-world scenarios [10, 11, 12, 13, 14]. Overall, the discussed related work demonstrates the wide range of approaches and techniques that have been developed to enhance the robustness and effectiveness of co-salient object detection and reinforcement learning in various applications.>"
1060," **Semi-supervised Video Object Segmentation (AVOS).** AVOS aims to segment the target object in the first frame of a video [1]. Most existing methods [2][3][4][5][6][7][8][9][10][11][12] are based on deep convolutional neural networks (CNNs). For example, RVOS [3] proposes an end-to-end recurrent neural network (RNN) for AVOS. STM [4] proposes a memory network to store object features over time. EMANet [5] further improves STM by reducing the memory size. However, these methods heavily rely on optical flow to extract spatiotemporal features. In contrast, we propose a multiscale encoder-decoder transformer to extract features without using optical flow.

**Multiscale Representation in Computer Vision.** Multiscale representation has been widely explored in computer vision tasks such as object detection [13], image classification [14][15][16], and image segmentation [17]. Recently, MViT [18][19] proposes to use a transformer encoder to extract multi-scale features for video and image recognition tasks. In this work, we also propose a transformer-based encoder and decoder for multi-level feature extraction for video object segmentation and actor/action segmentation. Unlike previous works, we focus on dense prediction tasks in videos and propose a unified multiscscale encoding and decoding framework for video-based dense prediction. Moreover, we introduce a transductive learning scheme to provide temporally consistent predictions, which has not been explored in previous works [20][21].

 proposes a multi-stage transformer architecture for object detection in images. In particular, it uses multi-resolution features at each stage of the network to capture features at different scales. Different from this approach, our method uses a single-scale transformer for encoding and detection in videos, which is more efficient and effective. Furthermore, we show that our method can be applied to AVOS as well.

 proposes an object detection transformer (ODT) for images. It uses a transformer decoder to predict object bounding boxes at multiple scales, which are then used to refine the bounding box predictions at the next stage. Our method differs from ODT in several aspects. First, we use a single transformer decoders for encoding, detection, and segmentation tasks in a unified framework. Second, we do not rely on multi-stream features at the encoder or decoder in ODT. Instead, we employ a single multi-layer transformer to encode and decode features at multiple resolutions. Third, we apply our method to a wide variety of vision tasks, including object detection, image classification, and video segmentation, while ODT only focuses on object detection. Finally, we demonstrate that our approach outperforms ODT on AVOS, without using any optical flow input.

 also proposes a transformer for video semantic segmentation (VSS). However, VSS is different from AVOS in that VSS requires the segmentation of the entire video, while AVOS"," **Video Object Segmentation.** Video object segmentation (VOS) aims to segment the primary objects in a video sequence without any human intervention. Existing methods can be categorized into two main categories: unsupervised [1][2][3][4][5][6][7][8][9][10][11][12] and semi-supervised methods [13][14][15][16][17][18][19][20][21]. Unsupervised VOS methods [2][6] segment primary objects without any prior knowledge about the primary object. Semi-Supervised Video Object segmentation methods [12][7] require joint labeling of both actor and action categories in video frames.

**Video Transformer.** Recently, the video transformer [18] has been widely used in a wide variety of vision tasks, such as image classification [18], object detection [19], instance segmentation [17], and semantic segmentation. It has been shown that the multi-scale representation at both encoder and decoder is beneficial for video segmentation. For example, CrossFormer [14] and CrossViT [15] propose to use cross-scale attention to capture the interactions among features of different scales. Deformable DETR [16] and DeiT  propose a deformable transformer architecture for object detection, where deformable attention is used to attend to a small set of key sampling points around a reference frame. Dei-T [17] proposes a multi-task architecture for panoptic segmentation, where the transformer is used as a mask encoder to capture multi-level features.
. Mask2Former [17][20] proposes to use the transformer architecture to extract localized features by constraining cross-attention within predicted mask regions. In this work, we propose a unified multiscale encoder-decoder video transformer for VOS, which is able to extract spatiotemporal features without relying on optical flow.

 show that the encoder of the transformer can be used to extract spatial-temporal features at both encoding and decoding, and the decoder can provide high-level (e.g. object) semantics to guide precise localization at decoding. In addition, they propose a transductive learning scheme through many-to-many label propagation to provide temporally consistent predictions. In contrast, we present a unified multiiscale video transformer that is focused on dense prediction tasks in videos.

","[ Video Object SegmentationVideo object segmentation has gained much popularity in the past few years due to its practical significance and has been well explored in different formulations such as UVOS [1][2][3][4][5][6][7], ZVOS [8][9][10][11] and AVOS [2][12].

Deep neural networks have been widely used in these tasks. With the ever-increasing size of pretrained models, multiscale vision transformers have gained popularity due to its effectiveness and the inherent concurrency of its non-local attention mechanisms. However, most video object segmentation models use only encoder transformers.

Multiscale Video TransformersThe attention mechanism in transformers have been shown to capture both global- and local-level semantics with its inherent multiscale features. Moreover, the transformer can attend multiple scale features at once through the self-attention mechanisms and this proved to be beneficial to the downstream tasks of classification and detection. We use the transformer concepts of multiscale features (i.e. different channel and spatial resolution) and non-local attention mechanisms in our video object segmentation and actor-action segmentation models.

Early attempts to implement multiscale vision transformers used a small head and backbone model with different spatial resolutions [13] or scaling factor to increase the spatial resolution [14]. Recent works [15][16][17] use multiscale token operations and learn scale-specific, self-attention mechanisms to further improve the multiscale model.

Multiscale Vision Transformer for VideoObject segmentation models [2][8][18][19] and their detection variants [3][7][20] have been shown to be effective with their multiscale vision transformer counterparts. Moreover, the video-transformer architecture [18][19] has been widely used to improve the video object segmentation performance [12] and transductive video object segmentation performance [10] and learning model .

Transductive Label Propagation in VideoSegmentationTransductive methods use the learned encoder-decoder models to perform _transductive inference_ (transferring information from already-labeled data) for inferring predictions on unlabeled data. Adopting this method has become a norm in AVOS research and has been shown to work well in various scenarios. The most widely used method for transductive inference is a one-step label propagation scheme [21][22] where each unlabeled frame attempts to propagate the label for its ground-truth mask through an encoder. In this work, we build upon this framework to propose a many-step label propagation scheme that performs iterative prediction of the ground-truth object mask on each unlabeled frame using the output of the decoder. It is similar to [10]]","Step 1. Extract important sentences from the cited papers:
1. Multiscale processing in the encoder or decoder alone in video transformers has been explored, but a unified multiscale encoder-decoder transformer focused on dense prediction tasks in videos has not been presented [Target paper].
2. There has been a surge in deep learning-based approaches for video segmentation, which have demonstrated compelling performance in various practical applications [1].
3. Unsupervised Video Object Segmentation (UVOS) aims at automatically generating accurate pixel masks for salient objects in a video sequence and tracking these objects consistently through time, without any input about which objects should be tracked [2].
4. Recurrent networks for multiple object Video Object Segmentation (RVOS) have been proposed for zero-shot and one-shot video object segmentation, achieving comparable results to state-of-the-art techniques and faster inference runtimes [3].
5. A novel solution for semi-supervised video object segmentation involves leveraging memory networks and learning to read relevant information from all available sources, achieving state-of-the-art performance while handling challenges such as appearance changes and occlusions [4].
6. Regional Memory Network (RMNet) has been proposed as a novel local-to-local matching solution for semi-supervised Video Object Segmentation, effectively addressing the ambiguity of similar objects in memory and query frames [5].
7. A reciprocal transformation network has been introduced to address the lack of prior knowledge about the primary objects in unsupervised video object segmentation, achieving significant performance improvement over state-of-the-art methods [6].
8. A teacher-student learning paradigm has been proposed to teach robots about their surrounding environment in a Human Robot Interaction (HRI) setting, enabling robots to self-adapt to teaching signals without providing manual segmentation labels [7].

Step 2. Summarize sentences and decide on topics of related work:
The important sentences from the cited papers highlight the advancements in deep learning-based video object segmentation techniques, including unsupervised and semi-supervised approaches, recurrent networks, memory networks, and human-robot interaction for teaching robots about their surrounding environment. Based on this information, the related work can be organized into the following topics:
1. Deep learning-based video object segmentation techniques.
2. Unsupervised and semi-supervised video object segmentation methods.
3. Recurrent networks and memory networks for video object segmentation.
4. Human-robot interaction for teaching robots about their surrounding environment.

Step 3. Write related work based on topics:
<In recent years, deep learning-based methods have gained significant attention for video object segmentation tasks, showcasing compelling performance in various practical applications [1]. These methods include Unsupervised Video Object Segmentation (UVOS) [2], Recurrent networks for Video Object Segmentation (RVOS) [3], and solutions utilizing memory networks and local-to-local matching for semi-supervised video object segmentation [4][5]. Furthermore, the introduction of a reciprocal transformation network has addressed challenges in unsupervised video object segmentation by correlating intra-frame contrast, motion cues, and temporal coherence of recurring objects [6]. Alongside these advancements, the field has seen the emergence of a teacher-student learning paradigm in a Human Robot Interaction (HRI) setting, enabling robots to learn about their surrounding environment without manual segmentation labels [7].>

The related work can be further extended based on the topics of recurrent networks and memory networks for video object segmentation and human-robot interaction."
2879," **Semi-supervised Video Object Segmentation.** The goal of VOS is to segment out the target object in the first frame of a video [1]. Most existing methods [2][3][4][5][6][7][8][9][10][11][12][13][14] adopt a two-stage pipeline, where the first stage is to learn a target-agnostic feature embedding, and the second stage is used to perform target-aware segmentation. For example, OSVOS [3] learns a target embedding using a pre-trained object detector, which is then fine-tuned on the first annotated frame. RGMP [5] propagates the target mask from the previous frame to the current frame using a reference-guided mask propagation network. PML [6] proposes a pixel-wise metric learning framework to learn the embedding space for target-background matching. FEELVOS[9] proposes an end-to-end embedding learning framework for VOS. STM-Net [7] introduces a memory network to store the previous segmentation mask and propagate it to the next frame. STMN [11] improves the memory network with improved memory coverage and multi-scale feature fusion. SSTVOS  proposes a sparse spatio-temporal transformer network to model long-range temporal dependencies.

**Unsupervised Learning for Video Object segmentation (U-VOS).** Unsupervised VOS aims to learn VOS from unlabeled videos without using any manual annotations. In this paper, we focus on unsupervised learning for U-vOS. Existing methods [15][16][17][18][19][20][21][22][23][24][25][26][27] can be roughly divided into two categories, _i.e._, _cycle-consistency_ based methods and _dense correspondence_ based ones. The first category of methods [16][19] relies on cycle consistency [21][23] between consecutive frames to establish temporal correspondence. For instance, MAST [17] uses a memory-augmented self-supervision tracker to enforce the consistency between the current and previous frames. JULE [24] proposes to jointly learn a correspondence flow and a semantic segmentation network. The second category of works [25][27][26] focuses on learning pixel-level correspondences. Specifically, VOS-CLR [25] uses the InfoNCE loss [28][29] to train a network to predict the next pixel in a video, and then uses the predicted pseudo labels to supervise the training of a VOS network. However, the pseudo labels are only used to train the network, which may lead to the degeneration of the learned representation. In contrast, we propose a unified framework to simultaneously learn mask embedding and dense correspondence, which enables the network to directly learn to perform mask-guided sequential segmentation from unannotated videos, without relying on an oblique solution -- cheaply ""copying"" labels according to pixel-wised correlations.

 proposes"," **Video Object Segmentation.** Video object segmentation (VOS) aims to segment the target object in a video sequence given only a mask in the first frame. Existing VOS methods can be roughly divided into two categories: semi-supervised methods and self-taught methods. We refer the reader to [1] for a comprehensive survey.

**Semi-Supervised VOS.** Semi-supervision methods [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] have been extensively studied in recent years. Most of these methods are based on pixel-level matching [6][8], which is usually achieved by matching pixels of the same object instance across frames [8][6] or by using a pixel-wise matching strategy [6]. In contrast, self-learning based methods [13][13] learn to perform VOS from unlabeled videos directly, without relying on pixel level matching or pixel-to-pixel matching. For example, [13] proposes to use a memory mechanism to retrieve pixels globally, and meanwhile learns position consistency for more reliable segmentation. [14] integrates transductive and inductive learning into a unified framework to exploit the complementarity between them for accurate and robustness. [12] proposes a novel architecture to perform instance-level and instance-specific feature propagation for VOS, which is further improved by introducing a multi-scale feature embedding module. [8]. [12][10] propose to use Transformer [15] to model cross-frame dense correspondence for local discriminative feature learning. [10] proposes an attention-based approach to VOS which uses sparse attention over spatiotemporal features. [9] proposes the first end-to end VOS method which does not rely on fine-tuning. [17] proposes MAST, a memory-augmented network to improve the accuracy of dense tracking. [19] proposes DenseUnsupervised learning for video segmentation, which uses uniform grid sampling to extract a set of anchors and disambiguate between them on both inter- and intra-video levels. [18] proposes mining better samples for contrastive learning of temporal correspondence. [20] introduces a contrastive transformation network to learn video-level representations for instance","[ We shall briefly review the most relevant research efforts in video object segmentation (VOS) and unsupervised correspondence learning.

VOS is a long-studied problem, with an extensive literature on the topic [1]. The seminal work of the supervised setting [2] adopted a coarse-to-fine framework for generating a fine segmentation map by iteratively aligning dense predictions to the initial target mask. It relies on fully-connected convolutional networks (CNNs) as the backbone and a temporal down-sampling process, yielding fast training and testing. Subsequent research efforts largely advanced the design of networks for finer feature maps, e.g., by introducing a graphical neural network to preserve fine features [3]. Recent works try to mine more context from either multiple frames [4] or multiple input layers [5]. Metric learning methods [6][7][8] attempt to leverage semantics from other objects. Finally, methods [9] based on the state-of-the-art generic representations learn the segmentation maps directly.

Our approach belongs to the more recent unsupervised setting [10][11][12][13][14]. Here we categorize the existing works according to their basic learning framework. One direction relies on a memory-based approach, where cross-frame similarities are learned through a matcher  to infer the current frame. This can be either memory-augmented [15][14], which leads to poor discriminative features and overfitting, or frame-level predictive coding [16][17], which requires extensive augmentation during training. As a comparison, our method is based on a dense correspondence scheme that is modelled by similarity metrics to produce good features. The second direction is about to learn such similarity metrics via self-supervised training. As a typical case, Chen _et al_. [18] tried to learn the ""reliable"" feature representation by following the contrastive prediction coding [19][20]. A subsequent work [21] further exploited the cycle consistency of the time for a similar goal, and adopted a bi-objective loss to optimize the pixel-level representation. Recently, Liu _et al_. [22] focused on the correspondence learning of rigid objects in videos. They proposed a pixel-wise similarity prediction network to achieve good visual reasoning, which can be cast as an image translation task. As another example, Wang _et al_. [23] introduced a space-time graph to model the spatial-temporal consistency between consecutive frames, and extracted a graph-based correspondence in a self-supervised manner. An additional idea [24][25] is to consider the intra-frame spatial relations as well, which is shown to be useful for object segmentation. Subsequent research efforts [26][27] show that a dedicated model for spatial relation learning further boosts the segmentation performance. Finally, as]","<Related work>

The task of video object segmentation (VOS) has garnered significant attention in recent years [2]. Previous methods for VOS often rely on supervised learning and require extensive training data for accurate segmentation. However, the work by Zhou et al. introduces a unified framework for self-supervised video object segmentation, which obviates the need for labeled data [1]. By leveraging a novel algorithm that alternates between clustering video pixels to create pseudo segmentation labels and utilizing these labels to learn mask encoding and decoding, the proposed approach achieves state-of-the-art performance on standard benchmarks such as DAVIS17 and YouTube-VOS [2]. This work narrows the gap between self- and fully-supervised VOS, not only in terms of performance but also in network architecture design [2].

In the context of semi-supervised VOS, several methods have been proposed to address the challenges associated with segmenting objects in videos based on limited initial mask annotations. For instance, OSVOS [3] and OSVOS${}_{\text{S}}$S [4] have demonstrated the effectiveness of leveraging convolutional neural networks to transfer generic semantic information and learn the appearance of annotated objects in video sequences. These methods achieve fast and stable segmentation results across frames, showcasing their potential in addressing the complexity of VOS tasks. Additionally, the research by Xu et al. introduces a novel method, termed Dense Unsupervised Learning for Video Segmentation, which leverages a fully convolutional regime to learn dense feature representations directly, leading to superior segmentation accuracy despite using significantly less training data and compute power [19].

Self-supervised learning has emerged as a promising approach for effectively learning feature embeddings and representations from unlabeled data for video segmentation tasks [16]. Specifically, the work by Wang et al. proposes a method for self-supervised dense tracking, which achieves baseline accuracy comparable to fully supervised trackers, highlighting the potential of leveraging unlabeled or weakly labeled data for improving tracking accuracy [22]. Furthermore, to address the challenges of learning reliable dense correspondence from videos, approaches such as Joint-task Self-supervised Learning for Temporal Correspondence [24] and Learning Video Object Segmentation From Unlabeled Videos [25] have demonstrated the advantages of a unified unsupervised/weakly supervised learning framework in capturing intrinsic properties of VOS at different granularities. These advancements underscore the potential of self-supervised learning for effective video object segmentation.

In the pursuit of addressing the limitations of existing methods, it is noteworthy that innovative frameworks, such as SSTVOS [10] and A Transductive Approach for Video Object Segmentation [12], have been proposed to integrate transductive and inductive learning into unified frameworks for accurate and robust video object segmentation. These approaches represent a paradigm shift by exploiting the complementarity between transductive and inductive learning, opening new avenues for advancing the state of the art in VOS. Moreover, the recent work by He et al. introduces a simple yet effective approach to modeling space-time correspondences in video object segmentation using an efficient and robust framework [11]. These novel methods broaden the horizons of video object segmentation and hold promise for further advancements in this research area."
1513," **Vision-Language Pre-training.** The vision-language pre-training (VLP) task aims to learn a joint representation of image-text pairs, which can be used for various downstream tasks, such as image captioning [1][2][3][4], visual question answering [5][6][7], video-text retrieval [8][9][10][11], and video-action recognition [12][13][14][15]. Inspired by the success of the Transformer architecture [16] in the natural language processing (NLP) field [17], the Vision Transformer (ViT) [18][19][20][21][22][23][24][25][26][27] has been widely adopted for the VLP task. The ViT-based VLP models can be roughly divided into two categories, i.e., two-stream and single-stream models. Two-stream methods [28][29][30][31][32][33][34][35][36][37][38][39] first extract image and text features from images and videos, respectively, and then feed them into a Transformer-based encoder to learn the joint representation. Single-stream approaches [11][5][9] directly use the raw video frames as input to the Vision-Language model, without the need of pre-extracted image or text features.

In this study, we focus on the two-Stream VLP model, which is widely used in the existing VLP studies. Specifically, we propose Meta-Retriever, a simple model that incorporates key event video representation and a new loss for the MeVTR task.

 propose the Multi-Event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional VTR Task. However, they only consider the bijective correspondence between events, ignoring the more practical scenario where texts like user queries or webpage metadata tend to be specific and correspond to single events. To address this issue, we introduce a new task, namely Multi-event Video-text Retriever (MVTR), and propose a new baseline model, Meta, to address this problem.

 introduce the MIMO task, which aims to retrieve videos from a database that contain multiple event descriptions. They propose a baseline model that uses a 2D-3D Convolutional Neural Network (CNN) to extract video features, and a contrastive loss to measure the similarity between the query video and the retrieved video features. In contrast, our proposed model focuses on the multi-event scenario, where each video content usually contains multiple events, and we propose a novel loss function, Meta Loss, to evaluate the relevance of each event for the retrieval task. To the best of our knowledge, this is the first work that introduces the MIVTR task in the context of the VTR task, and the proposed Meta Loss is the most effective loss for this task. We will show that Meta Loss significantly outperforms the existing MIVO loss in our experiments.

 also introduce the"," **Vision-Language Pre-training.** The two-stream vision-language (VL) model [1][2][3][4][5][6][7][8][9][10][11] has been widely used for VTR, which is first proposed in [4] and then extended in [2][1][7]. The VL model consists of a transformer-based encoder and a language model, which are fed into a multi-layer Transformer [1] or a shared transformer [3] network for both encoding and decoding, respectively. In [1], the authors propose a simple and flexible framework that implicitly aligns elements of an input text and regions in an associated image with self-attention layers. The model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-text prediction. The pre-training objective encourages the model to learn context-aware representations for input tokens based on linguistic and visual contents jointly, and the last task tries to predict whether an image and a text describe each other. The work of [2] extends the popular BERT model for language modeling [16] to the multi-modal setting by extending it to take both visual and linguistic embedded features as input, and extends the Transformer architecture to take visual and textual features as inputs. The unified VL-BERT [7] is a task-agnostic model that extends the BERT architecture to a two stream model for VLBERT-style VLSTM [17] and achieves state-of-the-art performance on a variety of VTR tasks.




","[ **Video-Text Alignment**: The video-text alignment task was a simple-to-implement yet challenging task in the early days, which only required a vision module and a text module, using standard Convolutional Neural Networks (CNNs) and recurrent units as a backbone for vision, followed by the bi-directional LSTM for the text encoder, and vanilla cross-entropy as a loss function [1][2][3][4][5][6][7]. However, the attention mechanism with self-attention and cross-attention schemes applied to the pair of video-text encoders [8][9][10] greatly improved the performance. [8] used the MLP as a feature extractor and the attention-based framework for visual-language representation learning.  used a unified Vision-Language transformer model to capture textual and visual relations between video and text.  proposed an extension of the ViLBERT model with a spatio-temporal Convolutional Network and a Transformer.  and  used a transformer-based and transformer-free architecture, respectively, for video-text alignment.

**3D Convolutions for Video Feature Extraction**: In recent years, 3D convolutions are applied to capture temporal dynamics in the video sequence [11][12][13][14][15], followed by the application of the transformer in the vision domain [16][17][18][19][20][21]. The transformer-based video models are commonly used in pre-training in the self-supervised manner for video representation learning [18][22][23]. Recent research has found that the combination of the ViT backbone [23] and 3D convolutions [24][25] perform better than the ViT-only architecture for video recognition tasks, like action recognition, and the single-stage action classifier. Inspired by the visual-language retrieval task, many vision Transformer models [26][27] were proposed to combine the 3D convolution and transformer for the video representation learning task.

**Text-based Video Retrieval**: Over the last few years, image-language models [28][29][30] have been pre-trained with large-scale annotated datasets and improved visual-text alignment accuracy. Recently,  used a two-stream model, where a separate model trained for the video-text retrieval task. [31] and [32] used a 2-stage pre-training for the video-text retrieval task using the BERT-like models. [33] pre-trained a single vision model for all computer vision tasks including the video retrieval task. Inspired by the CLIP model [30] for retrieving a visual clip given the sentence-based query, video-text retrieval models [34][3]","<Related Work>
Video-Text Retrieval (VTR) is an important multi-modal task in the era of massive video-text data on the Internet. Existing work in this area often relies on a two-stream Vision-Language model architecture for learning joint representations of video-text pairs [1]. However, previous models operate under the assumption of bijective video-text correspondences, neglecting scenarios where video content encompasses multiple events, while texts correspond to single events [2]. This gap between training objectives and real-world scenarios has led to potential performance degradation of earlier models during inference.

A plethora of works have proposed vision-language models for various vision-and-language tasks [3], [4]. Some works have also explored joint visual-linguistic representations and pre-training methods using self-supervised learning approaches for vision-and-language tasks, including image captioning and visual question-answering [5], [6], [7]. Additionally, there have been efforts to develop universal encoders for vision and language in a pre-training manner, leveraging cross-modal pre-training tasks and transformer networks [6], [7].

Another line of work focuses on improving visual representations for vision-language tasks, including developing object-centric representations of images and enhancing visual features in VL models [4]. Furthermore, research has been conducted on utilizing video and language representation learning through self-supervised learning approaches for video understanding tasks, such as video classification and captioning [5], [12]. The use of 3D convolutional networks for spatiotemporal feature learning in video data has also been explored for tasks like video super-resolution and action recognition [12], [13], [14].

Many recent studies have shown that the Transformer architecture, based solely on attention mechanisms, can be effective for natural language processing tasks, including machine translation [16], [17]. A variant of this, the Vision Transformer (ViT), has been proposed for image classification and has outperformed traditional convolutional networks [18]. Similarly, the Swin Transformer has been introduced as a vision transformer tailored for computer vision tasks [19].

Recent advancements include methods that leverage generative and discriminative self-supervised video learning for video representation and classification tasks [28], [29]. A novel approach combines sparse sampling with contrastive learning to achieve efficient end-to-end learning for video-and-language tasks, outperforming existing methods [30], [31]. Furthermore, there are approaches that focus on multimodal fusion, where the global context and temporal relations between video frames are considered to improve video-text alignment and retrieval accuracy [32], [33]. Specifically, techniques integrating image-language pre-training models with video-text alignment have been shown to be effective in improving performance on video-language tasks [38], [39]."
466," **3D Human Pose Estimation.** 3D human pose estimation is a long-standing research problem. Existing datasets [1][2][3][4][5][6][7][8][9][10] have been collected to facilitate the development of 3D pose estimation. However, most of them focus on ground-based movements such as walking, sitting, dancing, etc. In contrast, outdoor human-scene interactions such as climbing are challenging to capture because of its complex back poses, intricate human-object interactions, and difficult global localization.

**Speed Climbing Motion Dataset.** Recently, several datasets [11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31] are proposed to study the climbing motion. SPEED21 [12] is a speed climbing motion dataset. It consists of 21 different indoor and outdoor climbing sequences. The video sequences are collected from a single indoor climbing wall. HSC4D [15] is the first large-scale outdoor 4D human motion capture dataset. The dataset consists of 14 different indoor scenes and 14 different outdoor scenes. The indoor scenes are captured using a Kinect sensor. The outdoor scenes are collected using a Velodyne LiDAR sensor. In comparison, CIM4D is a large scale outdoor climbing dataset with more than 180,000 frames of video sequences, LiDsar point clouds, RGB videos, high-precision static point cloud scenes, and reconstructed scene meshes. Moreover, we annotate touch rock holds for each frame to facilitate a detailed exploration of human-human-scene interaction. In addition, we collect the data from 13 different climbing walls, which is much larger than the previous datasets [12][15].

Climbing Action Datasets.There are several datasets for the action of climbing [32][33][34][35][36][37][38][39]. However, these datasets are limited in terms of scale and diversity. For example, [32] and [33] only contain 2D poses, while [34][37] only have 1D poses. [38] only has 2D pose annotations, and [39] has only 2D annotations. Compared with these datasets, our CIMI4D dataset is significantly larger in scale, diversity, and diversity, as shown in Table 1.

 is the largest climbing action dataset to date. It contains 15 different indoor climbing sequences captured with a variety of weather conditions. The data is collected by attaching a GoPro camera to each person's feet and recording them from different viewpoints. The ground-truth 3D poses are obtained by fitting a 3D skeleton model to the 2D joint locations. Compared to our dataset, it is much smaller in scale. The total number of frames in [36] is around 1,000, while our dataset has around 180K frames.

 collects a climbing video dataset consisting of 20 different indoor environments. The videos are collected by placing a rope on each wall. The rope is attached to the person's"," **3D Human Pose and Pose Estimation Datasets.** 3D human pose and pose estimation has been extensively studied in the past decades [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39]. Most of the existing datasets focus on ground-based activities such as walking, sitting, dancing, etc. In contrast, we focus on the off-grounded climbing action, which is more challenging to capture due to its complex back poses, intricate human-scene interactions, and difficult global localization.

**3D Pose Dataset.** The majority of the 3D pose datasets are collected from indoor or outdoor scenes. For example, Human3.6M [5] is a large-scale dataset of 3.6 million accurate 3D poses of 5 female and 6 male subjects, under 4 different viewpoints, captured by recording the performance of 5 different subjects in indoor and outdoor scenarios. AMASS [6] is an archive of 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. It contains 3D body meshes and annotations for each body part. The PoseTrack [1] dataset is a multi-person pose estimation dataset with multiple people labeled with person tracks and articulated pose. It is collected from PoseTrack  and PoseTrack++ [1], which are the most related to our work. In addition, we collect a large scale outdoor climbing dataset, CIMI4D, which contains 180,000 frames of pose inertial measurements, LiDAR point clouds, RGB videos, and high-precision static point cloud scenes, and a detailed exploration of human-object interaction.


","[ Many recent works [1][2][3][4][5][6][7][8][9][10] and a few previous works [11][12][13][14][15][16] focus on capturing motion of humans from 2D image frames. Following the advancement of deep learning and hardware development, the datasets have become very large and are widely used to test the proposed algorithms. However, these works are mainly developed for indoor activities, e.g., walking and dancing. The human motion captured is generally smooth with little back- and side- view. In contrast, our dataset specifically focuses on climbing and the captured action is relatively complex with many interactions.

The first challenge is how to capture motion in large-scale environments. The most commonly used camera for image capturing is RGBD camera which is expensive and cumbersome to transport. With the recent development of LiDAR and depth camera technology, [11][16][13] use LiDAR to collect 3D sensor data. Different from the LiDAR point cloud based on static measurements, [6] use multi-frame LiDAR scanning to capture dynamic motion. However, multi-frame LiDAR has limitations in the field of view. Instead, [11] use hand-held LiDAR to get fast, high resolution point clouds. There are other works using other camera modalities, such as video [17][18][19][13][20][21][22][23][24][25], depth [26], skeleton [27][28] and neural density field [29][30][31]. These modalities can better capture large-scale environments, but none of them are suitable for capturing back- and side-view motions, such as climbing.

Compared with 2D image-based datasets, collecting a dataset of 3D ground-truth is a huge challenge. [16] captures 3D data in outdoor environment. A major challenge is to stay steady when the person is in the middle of movement, or the position of the person is changing with movement. However, it is relatively easy to install markers for inertial measurements.

Most existing HMC datasets are on the scale of 200 frames [32][33][1][7][34][35]. [16] is the largest climbing dataset with 1000 frames. However, 1000 frames are not enough to train a strong model for more complex motions. It is common for climbers to bend at the waist and touch the wall while climbing, for which few human performance datasets can be found [36]. Our CIMI4D dataset is the first climbing motion dataset with 180k frames and sufficient training samples for learning to capture human motion for climbing.

In contrast to 2]","<The field of human motion capture has seen significant advancements in recent years, with a focus on various types of movements such as walking, dancing, and sports activities [1]. However, there has been a notable gap in the research surrounding off-grounded actions, particularly climbing movements, which are challenging due to complex back poses, intricate human-scene interactions, and difficult global localization [10]. The lack of specific datasets for climbing actions has limited the in-depth understanding of this type of movement within the research community [12]. To address this limitation, the CIM4D dataset was introduced, collecting a large rock climbing motion dataset from 12 individuals across 13 different climbing walls, encompassing various data modalities such as pose inertial measurements, LiDAR point clouds, RGB videos, high-precision static point cloud scenes, and reconstructed scene meshes [1].

Furthermore, the CIM4D dataset provides a detailed annotation of touch rock holds to facilitate exploration of human-scene interaction, achieved through a blending optimization process to correct pose drift influenced by magnetic conditions [12]. This endeavor fills an important gap in the field of motion capture by providing a comprehensive dataset for climbing movements and human-scene interactions. Additionally, the proposed dataset enables researchers to evaluate climbing actions, an area often overlooked in existing datasets, and facilitates the development of new approaches for pose estimation, action recognition, and 3D human dynamics modeling [4] [5].

The CIM4D dataset complements existing motion capture datasets by introducing a novel and challenging set of motions in unconstrained and dynamic settings, aligning with the growing interest in analyzing outdoor activities and extreme sports, a trend that has been recognized as an area in need of further exploration [35] [38]. By addressing the limitations in current datasets and providing a rich resource for the study of climbing motions, the CIM4D dataset opens up opportunities for advancements in the analysis of human activities, especially in challenging and dynamic environments where traditional motion capture methodologies may fall short [30] [32]. This dataset offers a substantial contribution to the field, serving as a foundation for future research in human motion analysis, performance assessment, and motion modeling in climbing scenarios.>"
1686," **Image Classification Datasets.** Image classification models trained on large-scale datasets such as ImageNet have been shown to be effective for many computer vision tasks such as image classification [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24]. However, it has been shown that these models struggle to generalize well to images from other domains [7][11]. In this work, we focus on evaluating the robustness of ImageNet-trained classification models to image quality issues. To the best of our knowledge, our dataset is the first dataset for this purpose.

**Robustness Testing.** Robustness testing of image classification models has been an active area of research in the past few years [2][7]. Several datasets have been proposed to evaluate the model's robustness to common corruptions and perturbations such as noise, blur, and blurriness [7], natural adversarial examples [8][2], and natural distribution shifts [9][9]. In particular, [7] introduced a new test set for the CIFAR-10 and ImageNet classification datasets and showed that ImageNet classifiers generalize poorly to these test sets. [8] proposed a new robustness benchmark for evaluating the model performance on images with natural distribution shift. [9] introduced the ImageNetCov dataset for evaluating model robustness against natural image distribution shifts. Recently, [11] introduced four new real-world test sets for evaluating out-of-distribution generalization. In addition, [10] proposed the ObjectNet dataset, which is a bias-controlled dataset for pushing the limits of object recognition models. [12] introduced an evaluation benchmark for quality issues for real world problems. In contrast to these existing datasets, our new dataset comes from an authentic use case where photographers wanted to learn about the content in their images. We collected 8,900 images taken by people who are blind for which we collected metadata to indicate the presence versus absence of 200 ImageNet object categories. We characterize this dataset and analyze the performance of 100 ImageNet models on our new test dataset. We also provide a fine-grained analysis of these models' performance on our test set.

 proposed a robustness test dataset for testing the generalization of image classifiers trained for ImageNet categories. They collected a test set of 1,000 images from the Internet and evaluated models on this test set to understand how well these models generalize. They showed that models trained with ImageNet perform poorly on the test set, indicating that the models are not robust to the distribution shift between ImageNet and other datasets. However, their test set is small in size (only 1,500 images). In contrast, we introduce a new dataset which is larger (8,900) and has a larger number of categories (200) for testing robustness. We provide a detailed analysis of how well ImageNet trained classification models perform on our dataset and how it compares to the mainstream datasets for evaluating how"," **Cross-domain Image Classification Datasets.** ImageNet-trained classification models are trained on ImageNet and evaluated on a variety of datasets for evaluating their generalization ability to unseen domains [1][2][3][4][5][6][7]. However, these datasets do not measure the robustness of these models to distribution shift [8][9][10][11][12][13]. In this work, we focus on evaluating ImageNet classification models' robustness to image quality issues.

**Robustness to Image Quality Shifts.** Robustness testing has been studied extensively in the literature for evaluating the generalization of image classification models to natural distribution shifts [14][15][16][17][18][19][20][21][22][23][24]. For example, [13] studied the impact of ImageNet on image classification accuracy and found that ImageNet models are less robust to natural image quality changes than ImageNet classifiers. [11] showed that the improvements in ImageNet generalization do not transfer to real-world distribution shifts. [9] found that there is often little to no transfer of robustness from ImageNet to real images. [10] collected a bias-controlled dataset for object recognition with random backgrounds, rotations, and imaging viewpoints. [12] proposed a dataset for assessing image quality for captioning. [2] introduced a benchmark for evaluating image classifier robustness against common image corruptions and perturbations. [8] created two challenging datasets that can cause model performance to substantially degrade when tested on real images, called Natural Adversarial examples and natural adversarial examples. [7] studied how well ImageNet trained models generalize to new test sets. [5] proposed the DACS dataset for cross-domain semantic segmentation. [4] proposed an adversarial learning method for domain adaptation in the output space. [6] presented a domain flow generation method to bridge two different domains by generating a continuous sequence of intermediate domains flowing from one domain to the other. [3] introduced the OfficeHome dataset for unsupervised domain adaptation. [1] presented the 2017 Visual Domain Adaptation (VisDA) challenge for evaluating domain adaptation across visual domains. [14] proposed ConvNeXt, a family of pure ConvNet models which are built entirely from standard ConvNet modules. [15] proposed Vision Outlooker (VOLO) for visual recognition which implements the local feature aggregation mechanism in a sliding window manner across the input","[ **Datasets and Metrics for Robustness Evaluation.** Datasets and evaluation metrics for measuring robustness are also a topic of active research. Most popular are the CIFAR10 [1] and CIFAR100 [2] datasets. VisDA2017 [1] and VisDA217  are based on the same large-scale domain adaptation dataset with annotations for the perturbations. The Domain-Adversarial Neural Networks [2] builds on VisDA217. ImageNet-C [2], ImageNet-P [2] and JFT-300M  are domain adversarial datasets that leverage the seminal work on domain adversarial training [3]. Recently, DACS [4] proposed an adversarial learning method for domain adaptation in the semantic segmentation task. DLOW [5] and its variant with a continuous space of image domains [6] try to adapt source images from one domain to a target domain. The concepts and idea behind these approaches have been shown to be effective. However, most robustness evaluation metrics [7][8][9][10][11] are based on the state-of-the-art (SOTA) image classification models on ImageNet datasets and, thus, cannot be generalised to other datasets and models. VOLO [12], NORLD [11] and many other works [10][13][14][15] demonstrate this. The new VizWiz-Classification dataset [12] aims to fill this gap and a more detailed discussion can be found in Section 1.

**Robustness on ImageNet:** ImageNet has been the state-of-the-art for image classification for more than a decade. ImageNet-P [2] provides an opportunity to test models on the whole ImageNet-1K training set. Besides that, there are several other datasets that consist of images from one domain, which allows testing the robustness of classification models in a domain-specific way. They can be used in addition to ImageNet-P. DAN [16] consists of images of flowers taken from the websites of the Real Dutch Tulip Bulb Auction Association and flower auctions. Another option is a test set from the same authors called VISDAN , which is built on top of DAN using natural translation. The VisDA217 [1] dataset provides labeled images from computer vision, machine learning and robotics and labeled perturbations for each sample. However, since it has only 20,000 images, it is not that common.

JFT-300M [17] consists of natural translations between 13,000 pairs of language sentences from CommonCrawl and the Vocabulary Net dictionary, augmented with noise, drop-replacement, blurring, and hue modification. There are only 25,000 samples]","Step 1. Extracted Important Sentences:
1. ""Complementing existing work in robustness testing, we introduce the first dataset for this purpose...""
2. ""We built a new test set using 8,900 images taken by people who are blind for which we collected metadata to indicate the presence versus absence of 200 ImageNet object categories.""
3. ""We share our new dataset with evaluation server.""
4. ""In this paper we establish rigorous benchmarks for image classifier robustness.""
5. ""Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications.""
6. ""We propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations.""
7. ""Recent years have witnessed a wide application of hashing in a variety of computer vision applications.""
8. ""In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms.""
9. ""In this work, we present a domain flow generation model to bridge two different domains by generating a continuous sequence of intermediate domains flowing from one domain to the other.""
10. ""We build new test sets for the CIFAR-10 and ImageNet datasets.""

Step 2. Summarized Sentences and Identified Topics:
The target paper introduces a new dataset, VizWiz-Classification, consisting of 8,900 images taken by blind individuals and aims to evaluate image classification models' robustness. Additionally, it shares the dataset with an evaluation server[1]. The paper also establishes benchmarks for image classifier robustness and proposes datasets like ImageNet-C and ImageNet-P for benchmarking classifier robustness[2]. Furthermore, it mentions the wide application of hashing in computer vision and introduces the Office-Home dataset for evaluating domain adaptation algorithms[3, 8]. It also presents a domain flow generation model to bridge different domains[6]. Finally, it discusses the creation of new test sets for the CIFAR-10 and ImageNet datasets[7, 10].

Step 3. Related Work:
<The paper on VizWiz-Classification contributes to the field of image classification model robustness by introducing a novel dataset, aimed at evaluating the generalization of such models in authentic real-world scenarios. This complements prior work in robustness testing and sets a benchmark for future research in this domain[1]. Additionally, the proposal of ImageNet-C and ImageNet-P datasets further advances the standardization and expansion of the corruption robustness topic, providing valuable insights into the preference of classifiers in safety-critical applications[2]. Furthermore, the introduction of the Office-Home dataset and the domain flow generation model offer significant contributions to the evaluation and adaptation of domain algorithms, bridging the gap between different image domains and enabling continuous adaptations[3, 6]. The creation of new test sets for prominent datasets like CIFAR-10 and ImageNet validates the importance of evaluating classifier robustness in various contexts, showcasing the evolving landscape of image classification model testing[7, 10].>

<Additionally, the introduction of hashing in computer vision applications, as emphasized in the cited papers, marks a significant trend in efficiently handling the explosive growth of digital data. The wide application of hashing techniques in computer vision underlines its importance in addressing challenges related to storage and retrieval efficiency[3].>

Overall, the related work comprises key contributions in the areas of dataset creation, robustness testing, domain adaptation evaluation, and the application of hashing techniques, each of which significantly advances the field of image classification model robustness and generalization."
1459," **RGB-based Human Pose Estimation.** RGB-based human pose estimation methods [1][2][3][4] have achieved great success in recent years. However, these methods are sensitive to illumination changes, occlusions, and background clutter. To overcome these limitations, some methods [5][6][7][8] have been proposed to estimate 3D human poses by using infrared or radio signals. For example, Li et al. [5] proposed to use WiFi signals to track the human body through walls and occlusion. They showed that the human pose can be accurately estimated even in the presence of occluders and clutter. In [6][8], the authors proposed to capture the human skeleton by using the radio signals reflected off the walls. These methods are based on the assumption that the objects in a scene are mostly diffractive, which means that the reflected signals are much more diffractive than other signals. In contrast, we propose to use the diffractive nature of sound to estimate human poses.

**Active Acoustic Sensing.** Active acoustic sensing has been widely used in various applications, such as gesture recognition [9][10][11], gesture generation [12][13][14], and human behavior recognition [15][16]. Most of these methods use signals that include human speech or the sounds of specific actions. For instance, the authors in [11] used the sound produced when a fingernail is scratched to localize the hand gestures. AudioTouch [7] and Hambone [9] used bio-acoustic signals to detect micro-gestures such as hand waving and hand waving. In addition, some works [10][16] used transdermal ultrasound propagation to detect hand gestures and gestures. Different from these methods, our method uses only acoustic signals without any high-level information, which is more privacy-preserving.

 proposed a method to detect human body gestures by using a microphone array and a speaker. They used the phase information of the acoustic signals to extract features and fed them into a convolutional neural network (CNN) to classify the gestures. In this work, we use the phase features to extract more discriminative features and feed them into our pose estimation network. Moreover, we introduce a subject discriminator to reduce the gap between the predicted pose and the ground truth, which significantly improves the accuracy of the pose estimation.

 presented a method that uses a single microphone and two speakers to estimate body poses. Their method is based on a two-stage approach. First, they measured the phase of the sound and then used it to estimate the body pose. In the second stage, they used the estimated body pose to generate a video of the person performing the gesture. In our work, instead of using only phase information, we explicitly extract phase features together with typical spectrum features, which are more suitable for our task.

 used a microphone and speaker array to measure the sound of human body motion. They first measured the amplitude and phase of each sound, and then they used these amplitude features to predict the motion of each body part. In their work, they only used the amplitude information"," **Human Pose Estimation.** Human pose estimation has been extensively studied in computer vision and computer graphics. Most existing methods estimate 3D human poses by using RGB images [1][2][3][4][5][6][7][8]. However, these methods are limited to indoor environments and cannot be applied to outdoor scenarios due to the lack of light and illumination. To overcome this limitation, some methods estimate human poses using RF signals [9][10][11][12][13][14][15][16]. For example, Wang et al. [6] estimate 2D human pose through walls and occlusions by using radio signals. However, their method requires a pre-trained 2D pose estimator to provide cross-modal supervision, which limits its applicability to real-world scenarios. In contrast, our method does not rely on any human annotations and can be used in a wide range of scenarios.

**Human Behavior Analysis with Acoustic Signals.** There have been several attempts to use acoustic signals for human behavior analysis. For example [7][9][11] use a microphone array to capture the sound produced by a finger on a surface, and then use it to control a mobile device or computer. AudioTouch [7] uses two piezo-electric elements, acting as a surface mounted speaker and microphone, on the back of the hand to capture micro-gestures using active bio-acoustic sensing. The acoustic signals are recognized using hidden Markov models (HMMs) and mapped to a set of commands controlling an application. Audio2gestures [13] uses a conditional variational auto-encoder (VAE) to generate audio-to-audio translation between speech and gestures. Audio-2Gestures [12] uses an LSTM network to translate speech to gestures, and uses a subject discriminator to improve the accuracy. Audio to Body Dynamics [14] predicts body dynamics from audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. However these methods require high-level information, such as human speech or sounds of specific actions, to train the model. In this work, we do not use any high- or low-level signals to train our model, and instead use acoustic signal to guide the pose estimation.

","[ Existing pose estimation approaches employ various forms of sensor inputs, such as images, depth information, RGB-D cameras, infrared light, and time-of-flight (ToF) cameras. The image-based methods include variants of the DNN architecture [1] and the modelling of correspondence across multiple images [2]. RGB-D cameras can provide detailed 3D information for the human skeleton, but they often require reflective surfaces [3][4]. By using a light-field 3D reconstruction method, [3] can estimate the location of the human skeleton directly by obtaining images in bright, visible, or dark lighting conditions. In order to overcome the limitations caused by lighting conditions, in [5][6] the authors employed the visible or infrared light and RF signals, respectively. It can be said that the high-quality RGB-D and RF signals allow a high accuracy in human pose estimation [3][6]. However, installing the expensive RGB-D cameras or RF sensors and light-field 3D reconstruction systems on regular objects is difficult.

The depth information based methods have been developed to solve the aforementioned problems. The state-of-the-art work is Hambone [9], which captures the user's pose with the aid of a sound captured using piezoelectric elements. The authors developed a piezoelectric sensor that converts the sound captured during the wrist and ankle movements into a voltage signal that can be recognized by a microcontroller. They analyzed the captured sound and trained their model using the recorded sound and the annotated marker locations. Similar approaches have been adopted by the authors of RF-Capture [8]. They built a wireless tracking system that exploits the reflections from the human body to capture the user's motion. There are also several works on using body-generated acoustic signals, such as scratch sounds produced by the user's fingernails dragged over a surface [11], vocal sounds produced by the user's throat [10], sounds that contain the characteristics of a human [12][13], and bioacoustic signals that are inaudible to human ears but have specific patterns produced by the human body [7]. However, the ability of capturing a human motion in these methods varies because each method is based on a different mechanism. Recently, there is a growing interest in exploring sound-based methods, and several papers have been published on both single-person [14] and multi-person 3D human pose estimation [3][6].

To the best of our knowledge, the most closely related work to ours is the Spectral Skeleton , which shows the possibility of extracting skeletal information using a time-varying 2D video of the human silhouette. The authors of this paper used time-frequency analysis to extract the frequency of an audio signal and used it to estimate the]","Step 1. Extract important sentences from the cited papers:
1. ""In previous work, PAFs and body part location estimation were refined simultaneously across training stages."" - [1]
2. ""Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76:7 mAP on the MPII (multi person) dataset."" - [2]
3. ""We describe a method for 3D human pose estimation from transient images (i.e., a 3D spatio-temporal histogram of photons) acquired by an optical non-line-of-sight (NLOS) imaging system."" - [3]
4. ""Most of the development of human pose estimation (HPE) adopt RGB images as input to estimate anatomical keypoints with Deep Convolutional Neural Networks (DNNs)."" - [4]
5. ""Understanding people's actions and interactions typically depends on seeing them. But what if it is too dark, or if the person is occluded or behind a wall?"" - [5]
6. ""This paper demonstrates accurate human pose estimation through walls and occlusions."" - [6]
7. ""We present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch."" - [16]

Step 2. Summarize sentences and decide on topics of related work:
Based on the important sentences extracted from the cited papers, the related work can focus on the following topics: 
1. Methods for human pose estimation using non-traditional input signals such as acoustic signals or transient images.
2. Techniques for human pose estimation in challenging environments such as through walls, low light conditions, and occlusions.
3. Approaches for recognizing hand gestures and on-body touch using acoustic sensing.

Step 3. Write related work based on topics:
<In recent years, there has been an increasing interest in exploring non-traditional input signals for human pose estimation. For instance, research has focused on using acoustic signals to infer human behavior [1]. This is a challenging task, especially when considering the diffractive nature of sound, but recent advancements have shown promise in encoding multichannel audio features into 3D human poses [1]. Furthermore, there has been significant progress in 3D human pose estimation from transient images acquired by optical non-line-of-sight imaging systems, expanding the possibilities for capturing human poses in unconventional ways [3].>

<The demand for human pose estimation techniques that can handle challenging environments has also led to notable developments. Methods that enable accurate human pose estimation through walls, occlusions, and in low light conditions have been a subject of interest [5][6]. Such advancements are crucial in scenarios where traditional visual cues are limited or obstructed, opening up new possibilities for action recognition and understanding human behavior in various environments. Additionally, techniques that recognize hand gestures and on-body touch using acoustic sensing have shown promise in providing convenient input actions for interaction with devices [16].>

Overall, recent research has demonstrated significant progress in human pose estimation by leveraging non-traditional input signals and addressing the challenges of estimating human poses in difficult environments, as well as in recognizing hand gestures and on-body touch using acoustic sensing. These advancements expand the capabilities of human behavior inference and interaction with technology, paving the way for innovative applications in fields such as human-computer interaction, healthcare, and surveillance."
4711," **Calibration.** Calibration is a widely studied problem in machine learning, which aims to obtain a calibrated probability distribution for a given model. There are two main types of methods: post-hoc methods and learning-based methods. The former directly calibrates the model by minimizing a loss function ([3]; [5]; [1]; [4]; [2]), while the latter uses an ensemble of models to calibrate the model ([6]; ). However, these methods assume that the training and test distributions follow a balanced distribution, which is not applicable to the long-tailed distribution.

**Domain adaptation and calibration.** The goal of domain adaptation (DA) is to learn a model from a labeled source domain and an unlabeled target domain, so that the model can be applied to a new target domain where the labeled data is not available. Existing calibration methods ([8]; [10]; [9]; [11]; [7]) can be used to realize domain adaptation. However, they cannot be directly applied to the calibration problem because they require the target domain instances to have the same distribution as the source domain instances. In this paper, we propose a novel calibration method for domain adaptation, which does not rely on the assumption that the source and target domain distributions follow the same distributions. We show that our method can achieve better calibration performance than the state-of-the-art methods for the calibration under long-tail distribution. Furthermore, we show that the proposed method can effectively transfer knowledge from head classes to tail classes, which can improve the calibration performance of the model trained from a balanced training distribution. We believe that our work is the first to explore the problem of calibrating a model trained under a long tailed distribution, and we believe that it is important to develop a calibration method that can generalize well to this problem. We leave the extension of our method to other calibration methods for future work. We also leave the application of our calibration method to the domain adaptation problem as future work, which we believe is an important direction for future research in this field.
**Long-tailed recognition.** Long-tailed classification aims to improve the performance of a classifier trained from an imbalanced training distribution by re-sampling the training data according to the class frequency. The existing methods can be roughly divided into two categories. The first category aims to re-balance the data by assigning different weights to different classes. For example, [13] proposed a two-stage method to learn the representation and the classifier simultaneously. [14] proposed to use the geometric structure of the data to transfer knowledge between classes. The second category focuses on calibrating the model to achieve better performance. For instance, [12] introduced a prior-based method to obtain the calibrated model.  proposed a temperature scaling-based calibration method, which uses a temperature parameter to adjust the temperature of the softmax layer for each class. These methods can achieve good performance when the training distribution is balanced, but they can not achieve satisfactory results when the data distribution is imbalanced. To this end, we focus on the calibration of a model learned from a long-"," Calibration techniques aim to calibrate the model trained from a long-tailed distribution. Existing calibration techniques can be roughly categorized into two types: post-hoc calibration and post-processing calibration.

**Post-Hoc calibration.** The post-process calibration methods aim to estimate the uncertainty of the model by using the distribution of the training data and the distribution in the test distribution. For example, temperature scaling [1] and its variants [2][3][4][5][6] use the temperature scaling method [4] to estimate calibration error of the classifier trained from the training distribution. The temperature scale method uses the temperature of the predicted probability distribution of each class as the calibration error. The other type of calibration methods [7][8][9][10][11][12][13][14][15] use a mixture of the two types of calibration techniques. For instance, [7] proposed multi-domain calibration to improve the generalization performance of a model trained on multiple domains. [8] proposed to use importance weighting to correct for the distribution shift between the training and test distributions. [9] proposed a transferable calibration method to achieve calibration with lower bias and variance. [10] proposed confidence calibration for domain generalization under covariate shift. [11] proposed perturbation-based calibration method for uncertainty calibration under domain drift. [13] proposed label-aware smoothing to deal with the over-confidence for classes and improve calibration and generalization. [12] proposed UniMix to improve calibration in long-tail distribution. [14] proposed GeometrIc structure transfer network to transfer knowledge from head classes to tail classes. However, all of these methods treat different classes equally and thus implicitly assume that the training distributions of different classes are balanced. In this paper, we explore the problem of calibration under long-tailed distribution.

","[ The problem of calibration has attracted much attention in the past years, with many different calibration algorithms proposed. Some calibration methods are directly optimized to improve the calibration error ([4]; [3]; [5]; ; [1]). Other methods use loss functions that penalize unreliable probabilities to calibrate models ([6]; ; ; [2]). However, such a solution might not work well in our problem due to the sparsity of class samples. We focus on model-agnostic calibration algorithms in this paper.

Recently, calibration methods for out-of-domain (OOD) scenarios have been studied. The main idea is to use the prediction of a pretrained model for cross-domain classification to estimate the confidence of the model. This paradigm has been applied to domain adaptation (; [8]), outlier detection (), transfer learning (; [10]), and model out-of-distribution detection (; [7]). The major limitation of these works is that they only consider a binary class scenario and are specialized to a limited number of settings.

Recently, calibration problems have been investigated in the long-tailed recognition scenario. By introducing different distribution shift to the model, existing works reported that calibration algorithms still work in these scenarios (; [13]; [11]; [12]). As for our problem, most works did not consider how to estimate the uncertainty of model training under a long-tailed distribution. The recent work () explores uncertainty under long-tailed recognition but use a restricted distribution that do not involve domain shift.

]","Step 1. Extract important sentences from the cited paper.

- ""Current calibration techniques treat different classes equally and thus implicitly assume that the distribution of training data is balanced, but ignore the fact that real-world data often follows a long-tailed distribution.""
- ""Specific calibration methods for domain adaptation are also not applicable because they rely on unlabeled target domain instances which are not available.""
- ""Models trained from a long-tailed distribution tend to be more overconfident to head classes.""
- ""We propose a novel knowledge-transferring-based calibration method by estimating the importance weights for samples of tail classes to realize long-tailed calibration.""
- ""Our method models the distribution of each class as a Gaussian distribution and views the source statistics of head classes as a prior to calibrate the target distributions of tail classes.""

Step 2. Summarize sentences and decide on topics of related work.

The important sentences highlight the challenges in calibration techniques for long-tailed distributions, the limitations of current calibration methods, and the need for novel knowledge-transferring-based calibration methods. Related work can cover the limitations of current calibration techniques, methods for domain adaptation, and approaches for addressing calibration in long-tailed distributions.

Step 3. Write related work based on topics.

<Calibration techniques for imbalanced data distributions have been an area of active research in machine learning. Current calibration methods often assume balanced data distributions, overlooking the prevalence of long-tailed distributions in real-world data [1]. This poses a significant challenge as models trained on such imbalanced data tend to be more overconfident in predicting head classes, leading to poorly calibrated estimates of class-posterior probabilities [4]. Existing calibration techniques, such as temperature scaling and domain adaptation methods, have limitations in generalizing well to the problem of calibrating models trained from long-tailed distributions [2]. Furthermore, specific calibration methods for domain adaptation are not directly applicable due to the reliance on unlabeled target domain instances [8].>

<To address the challenges posed by long-tailed distributions, novel knowledge-transferring-based calibration methods have been proposed. These methods aim to estimate the importance weights for samples of tail classes to achieve long-tailed calibration, taking into account the differences in the distribution of training data and real-world data [1]. One such method models the distribution of each class as a Gaussian distribution and leverages the source statistics of head classes as a prior to calibrate the target distributions of tail classes [12]. These innovative approaches provide promising avenues for improving the calibration of models trained on long-tailed distributions, offering potential solutions to the limitations of current calibration techniques.>

<In addition, the need for transferable calibration methods in the context of domain adaptation has been investigated. Transferable calibration techniques aim to achieve accurate calibration with lower bias and variance in a unified hyperparameter-free optimization framework, addressing the inherent challenges of calibration in domain adaptation scenarios [9]. Furthermore, proposed calibration solutions via domain generalization demonstrate the use of multiple calibration domains to reduce the effective distribution disparity between the target and calibration domains, leading to improved calibration transfer without requiring data from the target domain [10]. These developments highlight the evolving landscape of calibration techniques, particularly in addressing the challenges of calibration under domain shift and long-tailed distribution scenarios.>

<Overall, the advancements in calibration methods for imbalanced and long-tailed distributions present opportunities for enhancing the reliability of model predictions in real-world applications. Novel techniques that consider the specific characteristics of long-tailed data distributions and domain adaptation scenarios offer promising approaches to improve calibration and uncertainty estimation in machine learning models [6]. The ongoing exploration of transferable calibration methods and domain generalization techniques further underscores the importance of addressing the limitations of current calibration techniques and adapting them to diverse and challenging data distribution scenarios [7].>"
1151," **Meta-learning.** Meta-learning aims to learn how to learn from a series of tasks and quickly adapt to new tasks [1][2]. It has been widely studied in the field of hyper-parameter optimization [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17]. In this paper, we mainly focus on the meta-learning for one-shot brain tissue segmentation on longitudinal data.

**Longitudinal Consistency Learning.** The goal of longitudinally consistent segmentation is to segment the brain tissue across the longitudinal data [18][19][18]. Previous works [19] usually use self-supervised learning to learn a feature extractor and a regularization function for regularization. For example, Wang _et al._[19] proposed a contrastive learning framework to learn both global and local features for segmentation. However, it requires a large amount of annotated data for pre-training, which is hard to obtain in many real-world applications. To address this problem, Li and Lee [18] proposed to use temporal information to regularize the learning process. They proposed a two-stage method to learn spatio-temporal representations and a temporal-consistent regularizer for regularizing the learning of the segmentation model. In this work, we focus on learning a meta-feature extractor with a well-initialized task head for fine-tuning and a class-aware regularizer to learn longitudinal consistent anatomical representations.

 proposed a multi-based method to improve the generalization ability of the model on a specific age group. It first learns a task-agnostic representation for each age group and then fine-tunes the model for a new age group based on the pre-trained representation. Different from this method, we aim to learn the representations and the task head simultaneously. Besides, we propose two class-specific regularizers to encourage longitudinal consistency.

 introduced the concept of _meta-learning_ to address the few-shot learning problem. It aims to train a model that can adapt to a new task with only a few training samples. The main idea is to learn an initialization that can be quickly adapted to a novel task with a few gradient steps [11]. The main difference between our method and theirs is that we learn a meta feature extractors and a task head to extract longitudinal consistent representations. Moreover, we introduce a novel meta-initialization learning strategy to avoid overfitting to a specific task head.

 first proposed the MAML algorithm for fast adaptation of deep neural networks. It learns a good initialization of the network parameters for each task by minimizing the inner-loop loss of the task-specific loss function. [11] further proposed the Reptile algorithm to further improve the optimization efficiency. [12] proposed the Meta-SGD algorithm, which learns the update rule of the SGD algorithm [10] to adapt the model parameters to the new task within a few steps. [14] proposed an ES-MAML framework to solve the model agnostic meta learning (M"," **Meta-learning.** Meta-learning is a learning paradigm that aims to learn from multiple learning episodes to improve the learning algorithm itself [1]. It has been widely studied in machine learning [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19]. In this paper, we focus on learning longitudinal-consistent anatomical representations by meta-feature learning and well-initialized task head for fine-tuning.

**Meta-Learning with Regularization.** Regularization is an important component in meta-learning and has been extensively studied in recent years [1][2]. For example, [3] proposed a multi-class regularization method for log-linear structured prediction models. [4] proposed to learn the cross-validation log likelihood in a gradient-based way, which is based on matrix-vector multiplication primitives with the kernel matrices and their derivatives. [5] proposed an approximate version of bilevel optimization for learning of variational imaging models, which can be solved by taking into explicit account the optimization dynamics for the inner objective. [8] introduced a framework based on gradient-free meta-learner LSTM to unify the gradient based hyper-parameter optimization and meta learning. [9] proposed bilevevel optimization strategy for selecting the best hyperparameter value for the nonsmooth \(\ell_{p}\) regularizer with \(0<p\leq\). [6] trained a neural network to output approximately optimal weights and hyperparameters for hypernetworks by stochastic optimization. [13] proposed Reptile, an approximation to MAML [11] by ignoring second-order derivatives.  proposed a method to learn a well-initialized model for fast adaptation to a new task. [12] proposed Meta-SGD to learn not only the learner initialization but also the learder update direction and learning rate. [14] proposed ES-MAML to solve the Hessian-free problem for few-shot learning.


","[ **Bilevel optimization** is a program that can be derived from another problem and solve simultaneously in an unrolled optimization procedure ([1]; [2]). Our work is based on the bilevel optimization framework, and we refer the reader to [3] for more details.

**Meta-learning** can learn to solve a set of related problems with a single meta-learner ([4]; [7]). In particular, [5] designed a bilevel optimization framework to solve a series of imaging-related problems and applied the technique to variational imaging model.  proposed to combine the gradient descent-based bilevel optimization framework with the meta-learning framework to solve learning to rank problem. [9] introduced a meta-learning problem with nonsmooth regularizers and designed a smoothing learning rate for bilevel optimization. Recently, [8] incorporated the techniques of backpropagation in model learning ([10]) into the framework of gradient descent to design a new meta-learning framework and showed the framework can generalize to a new unseen problem.

**Meta-learning** is a learning technique that aims to quickly learn a new problem given a small amount of training samples. [11] proposed the first meta-learning technique, which shows the ability of a trained model to learn tasks with very few training samples. Further, [13] introduced a single-step meta-learning algorithm named MAML (Model-Agnostic Meta-Learning) with first-order meta-learning framework and was very successful. Based on the proposed framework, [14] designed a first-order meta-learning algorithm, ES-MAML, which does not rely on the hessian matrix. However,  argued that MAML may overfit to feature representations, which results in less generalization ability on downstream tasks. To avoid this problem, [12] proposed a meta-learning framework that maintains a local policy to sample different tasks and use different weighting factors to control the optimization strategy. The local policy and the weighting factors ensure that the model can explore multiple tasks by traversing the gradient descent path. [15] designed an algorithm based on the task loss function and model parameters to enhance meta-learning. They showed that the proposed algorithm can have better generalization performance on downstream tasks.

**Self-supervised Learning** aims to learn from unlabeled data and can be leveraged in a variety of computer vision applications ([17]; ). Recently, [16] proposed to use self-supervised methods for medical image segmentation. However, previous works assume i.i.d. image acquisition, which is invalid in clinical study designs where follow-up longitudinal scans track subject-specific temporal changes. Inspired by our previous work ([18]), we introduce contrastive learning and attention learning to mitigate the lack of consistency in longitudinal data.

**Brain Tissue Segmentation** is used to classify white matter (WM), grey matter]","Step 1: Extract important sentences from the cited paper.

1. Contrastive learning is a powerful technique for learning image-level representations even with limited annotations in medical image analysis [16].
2. Siamese networks have become common in recent models for unsupervised visual representation learning and can learn meaningful representations even without negative sample pairs, large batches, or momentum encoders [17].
3. Self-supervised advances in medical computer vision exploit global and local anatomical self-similarity for pretraining prior to downstream tasks such as segmentation [18].
4. Previous methods for brain tissue segmentation are viable only for specific age groups and may fail for others due to dramatic structural and appearance changes [19].

Step 2: Summarize sentences and decide on topics of related work.

The important sentences highlight the effectiveness of contrastive learning and Siamese networks in learning meaningful representations, especially in medical image analysis. Additionally, there is a focus on self-supervised advances in medical computer vision for pretraining prior to downstream tasks. The challenges in brain tissue segmentation across different age groups are also mentioned.

Step 3: Write related work based on topics.

<Contrastive learning has been shown to be a powerful technique for learning image-level representations even with limited annotations [16]. In the context of medical image analysis, contrastive learning has proven effective for segmentation of volumetric medical images in the semi-supervised setting with limited annotations. By leveraging domain-specific and problem-specific cues, such as novel contrasting strategies for structural similarity and a local version of the contrastive loss, contrastive learning has demonstrated substantial improvements over other self-supervision and semi-supervised learning techniques [16].>

<Siamese networks have emerged as a common structure in recent models for unsupervised visual representation learning and have showcased the ability to learn meaningful representations even without negative sample pairs, large batches, or momentum encoders [17]. This is particularly relevant in the context of medical image analysis, where learning meaningful representations from limited data is crucial. The efficacy of Siamese networks in learning distinctive representations and features from medical images presents an exciting avenue for further research and exploration in the field of medical imaging [17].>

<Self-supervised advances in medical computer vision, leveraging global and local anatomical self-similarity for pretraining prior to downstream tasks such as segmentation, have contributed significantly to the improvement of segmentation methods for medical images [18]. These advances have enabled more effective pretraining of deep learning models, particularly in the context of longitudinal images, which exhibit spatiotemporal self-similarity. The utilization of feature-wise regularizations and self-supervised segmentation consistency has further enhanced the robustness and longitudinal consistency of segmentation methods in medical imaging [18].>

<Prior research emphasizes that existing brain tissue segmentation methods are viable only for specific age groups and may fail for others due to dramatic structural and appearance changes [19]. This highlights the importance of developing methods that can effectively address brain tissue segmentation across diverse age groups, especially considering the dynamic brain changes across the lifespan. The challenges and limitations identified in the existing methods underscore the significance of the proposed research in addressing the essential need for effective brain tissue segmentation methods across the human lifespan [19].>"
4014," **Vector map extraction.** Vector map extraction is a fundamental task in computer vision and has been studied for decades. Most existing methods are tailored to one specific mapping task, such as building segmentation [1][2][3][4][5][6][7][8] and road network extraction [9][10][11][12][13][14][15][16]. PolyMapper [1] is the first work to propose a unified framework for vector map extraction, which can be trained end-to-end for different mapping tasks. However, shape regularization and topology reconstruction are separately optimized for different tasks, resulting in suboptimal results. In this paper, we propose to use primitive graph as a unified representation of vector maps and formulate shape regularizations and topological reconstruction as primitive graph reconstruction problems that can be solved in the same framework.

**Graph-based vector mapping.** Graph-based methods have been widely used in vector mapping [8][9][16][14]. For example, PolyWorld [8] proposes to use graph neural networks to extract building footprints from satellite images. In [9], the authors propose to regularize and polygonize building masks with a graph neural network. In, the authors use a graph convolutional network (GCN) to extract road networks from satellite imagery. Different from these methods, our proposed GraphMapper uses primitive graphs as the representation for vector maps, which is more general and can be applied to different tasks. Moreover, our approach can fully explore primitive-wise and pairwise information, resulting improved primitive graph learning capabilities.

 proposes a graph-based method for vector mapping, which first segments building footprints and then reconstructs the topology of the segmented building footprints. The main difference between our method and theirs is that our method can reconstruct vector maps in multiple stages, while their method only reconstructs a single vector map in a single stage. In addition, our method does not require any post-processing step, which makes it more efficient and scalable to large-scale vector maps.

 presents a learning-based approach to extract roads from aerial images. The authors propose a novel loss function to improve the connectivity of road networks. In contrast, our work focuses on vector maps rather than road networks, which are more challenging in terms of both topology and segmentation.

 introduces a new loss function for road extraction. The loss function is based on the distance between the predicted road segments and the ground-truth road segment. In our work, we use the same loss function as  for both vector maps extraction and road networks extraction.

 also proposes a novel topology-aware loss function. The key difference between their work and ours is that we propose a new regularization term based on consistency between primitive directions and their pairwise relationship, while they propose a loss function based on distance between vertices of the graph. Our method is different from theirs in the following aspects: (1) Our method uses the consistency between two primitive directions as a regularizer, while theirs uses distance between vertex pairs as a loss. (2) Our regularization terms are based on primitive graphs, while"," **Primitive graph learning for vector map extraction.** PolyMapper [1] is the first work to use primitive graph as a unified representation of vector maps and formulates shape regularization and topology reconstruction as primitive graph reconstruction problems that can be solved in the same framework. It is trained end-to-end by minimizing the energy function of the primitive graph, which is further utilized to construct an energy function to optimize the polygon-based contour evolution via minimizing the graph-theoretic energy function, of which the minimum defines the final segmentation. The polygon structure is reformulated as closed polygons, which reformulates a graph structure as closed polygon. Polymapper is trained on a dataset of building footprints and road networks from satellite images, which are collected from overhead images of several cities in the African continent [2]. PolyMappers are trained on the dataset of Cityscapes [3], which is a large-scale dataset of images of buildings and roads in the Cityscape dataset [4], and the dataset is annotated by human annotators.

**Polygonal building segmentation and road network extraction**. PolyPretrained [4] and Polygon-RNN++ [3] are the first works to use neural networks to predict polygonal segmentation maps for building footprints extraction. PolyWorld [8] is a graph neural network-based method to extract building vertices from an image, which extracts vertices directly from an input image. DARNet [5] is an active ray network based method for building extraction, which adopts polar coordinates, i.e., rays, to avoid self-intersection and reduce self-convergence. PolyTransform [7] and D-LinkNet [10] are two recent works that use deep neural networks for polygonization and refinement, respectively. D-Lydorn [6] is another recent work that uses frame field to align the predicted frame field with the raster segmentation, which improves the segmentation quality by leveraging multi-task learning and provides structural information that later facilitates polygonized polygon optimization.


","[ In this section, we briefly review the mapping methods for building footprints and road networks as the two building blocks of a vector map.

Given the raster segmentation results from a CNN, the one-to-one correspondence mapping problem of topological maps is converted into the problem of continuous map segmentation. A typical example is [1] which uses a sequentialization method to decompose the graph into closed polygons and each polygon is predicted by a GNN. Different from our mapping framework, one-to-one mapping methods are not equipped with the capability of taking both shape and topology information into consideration for local reconstruction. Besides, they do not have sufficient expressiveness for the global topological reconstruction.

Given the segmentation result of an object, there have been attempts to directly extract polygon outlines of the object using graph convolutional networks [2][3][4][5][6]. This can be seen as a special case of our problem. Polygon outlines of the objects extracted by [3] and [4] can be directly fed into 2D image rendering to generate vector maps. To preserve the connectivity of graph edges, a deforming network is needed to reconnect the outlines of the detected building segments [2]. The generated object outline can also be matched with another instance segmentation map to produce a segmentation map with more precise boundary and higher resolution [7]. A probabilistic graph representation of the segmented polygons is proposed in [8] to simplify the generation of vector maps. Graph neural networks are also employed to represent buildings by vertex-edge-face (VEF)  or vertex-vertex-vertex (VVV) graph . However, these methods ignore shape regularization and topological reconstruction. Unlike our approach, they only treat one stage of the problem, while our approach treats both stages.

Besides, [9] introduce a discriminator for polygon generation and regularization on building boundaries to make them more realistic. Our task is different from [9] since we need to reconstruct a complete graph structure.

Besides building footprints and road networks, other topological maps, such as road networks and building fa\({}^{c}\)ade mapping, have also been paid great attention recently. There are two classes of road network mapping methods. The one-to-many mapping methods [10] first convert raster road segmentation maps to graph structure and then perform graph generation based on graph network. The one-to-one mapping methods [11][12][13] directly predict vertex coordinates from CNN output as the road network. Topological error is implicitly modeled by the predicted vertex positions and spatial alignment based on lane lines in the training dataset. In addition to road networks, building fa\({}^{c}\)ade mapping has been treated as an extension of the object detection and segmentation problem [14][15][16], which is mainly to extract edge components from the buildings, so that topological constraints can be obtained.

]","Step 1: Extract important sentences from the cited paper.
1. Large-scale vector mapping is the foundation for transportation and urban planning. Most existing mapping methods are tailored to one specific mapping task, due to task-specific requirements on shape regularization and topology reconstruction.
2. GraphMapper is a unified framework for end-to-end vector map extraction from satellite images, using primitive graph as a unified representation of vector maps and formulating shape regularization and topology reconstruction as primitive graph reconstruction problems that can be solved in the same framework.
3. PolyMapper proposes a new approach to extract the topological map of a city from overhead images as collections of building footprints and road networks, directly in a vector representation.
4. D-LinkNet is proposed for road extraction task, adopting encoder-decoder structure, dilated convolution, and pretrained encoder, demonstrating good performance in road extraction.
5. DARNet is introduced for automatic building segmentation, using deep convolutional neural network as the backbone to predict energy maps which are further utilized to construct an energy function.
6. PolyWorld directly extracts building vertices from an image and connects them correctly to create precise polygons using a graph neural network and estimates the assignments by solving a differentiable optimal transport problem.
7. DeepRoadMapper utilizes deep learning to estimate road topology from aerial images and proposes an algorithm that reasons about missing connections in the extracted road topology as a shortest path problem that can be solved efficiently.
8. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN.

Step 2: Summarize sentences and decide on topics of related work.
The cited papers cover a range of methods for extracting vector maps and performing shape regularization and topology reconstruction from satellite images. The topics of related work can include unified frameworks for map extraction, direct extraction of topological maps in vector representation, methods for road and building extraction using deep learning, and algorithms for road network graph creation and topology reasoning.

Step 3: Write related work based on topics.
<The field of large-scale vector mapping and shape regularization has seen various advancements in recent years. GraphMapper [1] presents a unified framework for end-to-end vector map extraction from satellite images, utilizing primitive graphs to address shape regularization and topology reconstruction. Similarly, PolyMapper [2] proposes a method to directly extract the topological map of a city from overhead images, representing building footprints and road networks in a vector format. D-LinkNet [10] introduces an approach for road extraction, demonstrating effective road topology estimation using an encoder-decoder structure and dilated convolution. DARNet [5] and PolyWorld [8] focus on automatic building and polygon extraction, utilizing deep neural networks, and graph neural networks to create precise building polygons directly from images. DeepRoadMapper [14] offers an algorithm to estimate road topology from aerial images, demonstrating efficiency in solving road topology related challenges, and RoadTracer [15] uses a CNN-based decision function to derive road network graph directly from the output of the CNN. These methods collectively demonstrate the diverse approaches and significant progress in the field of large-scale vector mapping and topology reconstruction in remote sensing.>

<In addition, the development of techniques for direct and automated extraction of vector maps and topology has also extended to methods that leverage deep learning. PolyWorld [8] showcases the effectiveness of using a graph neural network to directly extract building vertices and create precise polygons from satellite images. Furthermore, DARNet [5] presents a method for automatic building segmentation, utilizing deep convolutional neural networks for energy maps prediction and contour evolution to achieve state-of-the-art performance. Similarly, D-LinkNet [10] adopts an encoder-decoder structure and dilated convolution for road extraction and demonstrates strong performance in road topology estimation. These advancements demonstrate the growing impact of deep learning in addressing the challenges of shape regularization and topology reconstruction in large-scale vector mapping from satellite images.>

<Moreover, the focus on topology reasoning and road network graph creation is evident in DeepRoadMapper [14] and RoadTracer [15]. These methodologies utilize iterative search processes and CNN-based decision functions to derive road network graphs directly from aerial images. In addition, VecRoad [16] proposes a point-based iterative graph exploration scheme, incorporating segmentation-cues guidance and flexible steps to enhance road connectivity while maintaining precise alignment between the graph and real roads. These approaches collectively highlight the importance of topology reasoning and effective road network graph creation for accurate and efficient vector map extraction from aerial images.>

<Overall, the advancements in large-scale vector mapping and topology reconstruction from satellite images have seen significant progress, with the introduction of unified frameworks, direct extraction of topological maps, leveraging deep learning, and effective road network graph creation. These developments have paved the way for more accurate and automated vector map extraction, contributing to the foundation of transportation and urban planning.>"
2380," **3D Open-world Learning.** 3D open-world learning aims to recognize unseen 3D objects in a zero-shot manner [1][2][3][4][5][6]. PointCLIP [7] is the first work to apply CLIP [8] to the 3D domain. CLIP2Point [9] transfers CLIP to point cloud classification with image-depth pairs. However, their transfer capacity on 3D data is still limited and only limited to the classification task. In this paper, we propose to prompt CLIP and GPT to be a unified 3D model, which fully unleashes their potential for 3D classification, segmentation, and detection.

**Point Cloud Classification.** Point cloud classification is a fundamental task in 3D computer vision. PointNet [10] is a pioneering work that directly takes raw point clouds as input and learns point-wise features. Many follow-up works [11][12] have been proposed to improve the performance of PointNet. Multi-view-based methods [13][14][15][16][17][18][19] project point clouds to multiple views and then use convolutional neural networks (CNNs) to extract features from each view. Recently, PointNet++ [11] is proposed to directly process point clouds without using multi-view inputs, which achieves the state-of-the-art performance on point-cloud classification. Different from the above methods, we design a prompt-based method to align the pre-trained language knowledge with the point cloud data, which is able to perform 3D object classification and segmentation in an end-to-end manner.
**Prompting.** Prompting [20][21][22][23][24][25][26][27][28][29][30][31] has been widely studied in the natural language processing (NLP) field [32][33][34][35][36][37]. The core idea of prompt learning is to prompt the language model to output the desired output in the few-shot learning scenario. For example, GPT-3 [36] proposes to generate the text conditioned on the prompt. Prompting has also been applied to the computer vision field [27][29], which aims to enhance the model's robustness against adversarial attacks [27] or continual test time adaptation [29]. Inspired by the success of prompting in the NLP field, we introduce prompt learning to 3D point clouds, which can effectively align the point clouds with the natural images.

 first proposes to use CLIP as a prompt for image classification. They design a shape projection module to narrow the domain gap between projected point clouds and natural images, and prompt the CLIP model to generate 3D-specific text as the input of CLIP's textual encoder. In contrast, we prompt the GPT model to directly generate the textual representation of 3D points, which enables the model to learn 3D knowledge from the large-scale pre-training data.

 proposes to prompt GPT for text generation. They first pre-train the model on a large-"," **3D Open-world Learning.** 3D point clouds are a natural extension of 2D images, which can be used for 3D object recognition [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19] and 3D shape recognition [10][10]. However, the 3D data is usually represented as a point cloud, which is more challenging to process than 2D point cloud due to its irregularity and unordered data structure. To address this problem, PointNet [10] is the first to directly consume point clouds and learn permutation-invariant point features. PointNet++ [11] further improves PointNet by exploiting the metric space distances and further improves the performance by PointNet. PointMLP [12] proposes a pure residual MLP network to capture detailed local geometrical information. PointCLIP [7] conducts alignment between CLIP-encoded point clouds with 3D category texts. OV-3DETIC [4] extends CLIP [8] to the open-vocabulary 3D detection task by using image-level class labels. CLIP2Point [9] converts the point cloud classification task to 3D classification by using multi-view depth maps. PFNet [3] proposes to synthesize point features for various classes of objects by leveraging the semantic features of both seen and unseen object classes.
3D-CLIP  proposes a novel 3D hybrid scene segmentation task by combining the seen-class and unseen-class semantic features.


s. However, all these methods are limited to the classification task, which limits their ability to generalize to other 3D tasks. In this paper, we propose a unified 3D open-world learning framework, which fully unleashes the potential of CLIP and GPT for zero-shot 3D segmentation, detection, and detection tasks. We also propose a novel shape projection module to better align the pre-trained language knowledge with the 3d data, which significantly improves the 3DShape learning ability.


","[ Open-world 3D Vision-Language LearningGenerally speaking, large-scale vision-language pre-trained models have been widely used in open-world 3D scenarios. [1][2][3] make use of the generated textual representations to enhance zero-shot recognition. Some work also takes a video understanding perspective [4] to achieve self-supervised learning in a meta-learning manner. Although the aforementioned approaches have certain progress in 3D vision-language learning, it is worth noting that these methods are mainly limited to recognition tasks. In contrast, our method PointCLIP V2 incorporates point clouds into large-scale pre-trained language models and actively achieves pre-training and prompting in 3D open-world vision.

3D Vision-Language LearningDuring the past few years, a vast body of research has been devoted to vision-language learning on 3D point clouds. Our PointCLIP [7] builds upon the visual-linguistic encoder of GPT-3 [5] to learn in a multi-modal manner. Another alternative is to transfer the conventional computer vision models to vision-language pre-training tasks. These include image-to-point projection [8][9][6] and 3D depth image prediction [10][11][12]. [13][14][15][16][17][18] further extend the conventional 2D vision to the 3D scene. Despite their potential for vision, the aforementioned work requires expensive 3D annotations and hence is unsuitable for unconstrained datasets. More recently, some work [19] starts to address this problem by exploring simple networks such as DeeplabV3 [10] or X-Net . However, a common weakness of these works is the limited capability of feature representations, as they do not learn the discriminative ability for 3D scenes. Our method aims to break this gap by learning from large-scale pre-trained language models.

Prompting[20] provides a systematic survey of recent progress in prompt learning. Inspired by natural language processing (NLP) works such as Universal Adversarial Triggers [21] and Adversarial Prompts [22], we design CLIP-oriented prompts that incorporate key textual encoders into the pre-trained language models [23][5][6]. More recent studies also explore effective prompt learning [24][25][26][27][28][29][30][31]. However, different from existing works, our method PointCLIP V2 focuses on zero-shot and few-shot visual understanding of point clouds in unconstrained open-world settings.

Prompt LearningOur work is also closely related to large-scale pre-trained language models [32][33][34][35][26]","<Large-scale pre-trained models such as CLIP and GPT have shown promising performance across various vision and language tasks [1, 35]. However, the transferred capacity of these models to 3D point clouds has been limited to classification tasks, posing a challenge for more complex tasks such as segmentation and detection [2, 7]. In response to this limitation, research has been conducted to address the hubness problem in zero-shot learning for 3D recognition [1], and generative approaches have been proposed to enable zero-shot learning for both 2D and 3D data, including semantic segmentation [2]. Furthermore, the problem of point cloud semantic segmentation under the framework of zero-shot learning has been addressed, considering seen-class and unseen-class 3D objects, which is a more general and realistic setting in application [3]. Initial attempts have also been made to study the problem of open-vocabulary 3D point cloud detection, indicating the need for broader vocabulary and open-world detection capability [4].>

<In the context of adapting pre-trained models for improved few-shot and zero-shot learning capabilities, the development of Training-Free CLIP-Adapter (Tip-Adapter) has provided a new approach for enhancing the few-shot capability of CLIP, offering a comparable performance to CLIP-Adapter without requiring additional training or computational resources [5]. Moreover, recent works have explored the use of frozen CLIP models as efficient video learners, presenting a novel framework for training high-quality video recognition models with frozen CLIP features [6]. These adaptations present potential opportunities for improving the few-shot and zero-shot capabilities of pre-trained models for 3D visual tasks.>

<The application of pre-trained models such as CLIP to 3D recognition has been explored with the introduction of PointCLIP, which aligns CLIP-encoded point clouds and 3D category texts, achieving efficient knowledge transfer from 2D to 3D recognition [7]. Additionally, research has focused on pre-training across 3D vision and language domains, exemplified by CLIP2Point, a novel image-depth pre-training method aimed at transferring CLIP to the 3D domain for point cloud classification [9]. Furthermore, the study of PointNet and PointNet++ has demonstrated the development of neural network architectures designed to process point cloud data directly, presenting a promising avenue for addressing the challenges associated with 3D data [10, 11].>

<It is also noteworthy that recent work has investigated the potential of prompt-based learning for natural language processing (NLP) tasks, demonstrating the efficiency and scalability of learning SOTA image representations from raw text using the pre-training task of predicting image-caption associations [8]. Moreover, language models have shown promising few-shot performance, indicating that they can perform new language tasks from only a few examples or simple instructions, highlighting their potential for zero-shot learning [25, 35]. These advancements in prompt-based learning and few-shot learning present opportunities for the development of novel frameworks for improving the transfer learning and generalization capabilities of pre-trained models in 3D visual tasks.>

<In addition, recent work in the NLP domain has explored the concept of conditional prompt learning, introducing methods to adapt pre-trained vision-language models to downstream datasets and tasks by dynamically constructing prompts for each instance [23]. This approach has raised questions regarding the generalizability of learned context to unseen classes and the potential for overfitting to specific base classes, which may have implications for the adaptation of pre-trained models to new tasks within the 3D domain. Moreover, the study of universal adversarial triggers for attacking and analyzing NLP models has demonstrated the potential for triggering specific model predictions using input-agnostic sequences of tokens, providing insights into the robustness and vulnerabilities of models when prompted with specific input sequences [22]. The insights gained from these studies can inform the development of strategies for task-specific prompting and robustness analysis in the context of 3D visual tasks.>"
4430," **Single-view 3D Human Digitization.** 3D human body models [1][2][3][4][5][6][7] are widely used to represent the human body. However, these models only capture the naked body and cannot represent the clothing. Therefore, many methods [8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28] focus on reconstructing the clothed body from a single image. For example, DeepHuman [8] and DeepHuman++ [9] use a CNN to predict the depth map from the input image, and then reconstruct the 3D body model based on the estimated depth map. PIFu [17] proposes a pixel-aligned implicit function to represent a human body, and it can reconstruct the high-quality clothed body model from a low-resolution input image. Geo-PIFu[18] proposes to use the geometry and the pixel aligned implicit function for single-view human body reconstruction. StereoPIFU [19] uses stereo vision to reconstruct the clothed human body from multiple input images. PHORA [20] uses a multi-view stereo system to reconstruct a 3D model of a human wearing clothing.

**Large-scale Human Datasets.** Large-scale human datasets [29][30][31][32] have been widely used in the field of 3D reconstruction. For instance, SMPLPix [22] and PaMIR [23] use the SMPL body model [1] to construct a large-scale dataset. SMPLicit [25] and ICON [28] use DensePose [29] to generate the training data, and they use the generated images to supervise the training of the implicit function. Tex2Shape [30] and HighFidelity [31] use SMPLify [2] as the base model and construct the datasets for training and testing, respectively. Recently, HUMBI [10] and HUMBI++ [16] are proposed, which are the largest-scale 3D datasets in terms of both the number of subjects and the total number of scanned 3D bodies. These datasets are used to train and evaluate the methods in the following three aspects: (1) How to construct the dataset? How to collect 3D data? (2) What to reconstruct? (3) What is the loss function? (4) What are the trade-offs between the reconstruction quality and the dataset size? In this paper, we propose a novel dataset construction method and a novel loss function for the reconstruction of high-fidelity 3D humans from single 2K resolution images.

 propose a method to reconstruct high-resolution human bodies from 2K images. They use a coarse-to-fine network to first predict the coarse depth map and then recover the fine details of the body using a fine network. In contrast, our method first recovers the global 3D shape and then recovers the detailed structures using the fine network and the part-wise image-"," **3D Human Reconstruction.** 3D human reconstruction is a fundamental problem in computer vision and graphics, which has been extensively studied in the past decades [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32]. In this section, we briefly review the recent advances in single-view human reconstruction.

**Single-view Human Reconstruction from Single Image.** Single-view 3D reconstruction methods can be divided into two categories: model-based and learning-based. Model-based methods [4][4] use parametric human body models such as SMPL [1] and SMPL-X [2] to reconstruct 3D humans from a single image. For example, SMPLify [1], SMPL3D [4], and CAPE [5] use the SMPL body model to reconstruct the human body from a monocular image. However, these methods are model-free and cannot handle the pose variations and clothing topologies. To address this problem, Wang _et al._[4] propose a method to recover the body shape under minimal clothing from a sequence of 3D scans. Recently, Zheng _et.al_[5] propose to learn a generative 3D mesh model of clothed people from a 3D scan with varying pose and clothing. However these methods require the training data with 3D ground truth, which limits their applicability to real-world scenarios.


","[ In this section, we briefly review the recent progress in human digitization and provide a comparison with our method. A comprehensive survey of human digitization methods can be found in .

**Model-based 3D human reconstruction.** Human reconstruction based on body model (eg. SMPL [1]) assumes that the human body is constituted of body parts connected by corresponding joints, such as SMPL [1], SMPL-P [2], Dyn4D [3][4], F4D [5][6][7], DeepHuman [8]. These methods discretize the human body into sub-regions and model each region with a parameterized model, so that the model is flexible to various body poses. As a result, many reconstruction methods have been developed to infer the body model from either 3D scans [5][9][10] or single RGB images [8][11][12][13][14][15][16][17][18][19][20][21]. However, existing models do not reconstruct the detailed structure of human bodies, which can easily be disrupted by the clothing, texture and pose variations.

**Learned 3D human reconstruction.** Recently, researchers have shown that it is feasible to reconstruct detailed human body models from an image using a deep neural network. The method proposed by Omran et al. [22] first estimates a human model with SMPL model [1], and then deforms the model to fit the depth and normal maps from the input image. This approach has a limitation that it cannot infer the 3D human model from RGB-D image, which has increasingly been used in many applications [23]. In order to reconstruct the human model from RGB-D image, Li et al. [24] proposes to recover the 3D human body from monocular video by using a supervised learning framework. The success of these methods relies on accurate depth or normal maps. On the other hand, if the image is blurry, or the pixel resolution is low, such depth or normal map estimation methods might not work well. In the following sections, we introduce the method by Kim et al. [25] that can reconstruct a 3D human model without the use of depth or normal maps.

**Blurry image reconstruction.** Several methods have been developed to reconstruct 3D human models from blurry images (i.e. images without texture details), such as 2D deformable model [26], sparse 3D model [27], and implicit representation [28][29][30][31]. These methods have a similar idea of first reconstructing a shape using a coarse model, and then recovering the finer details from the shape prediction. Kim et al. [32] use two CNNs, one for shape reconstruction]","<Human body digitization from 2K resolution images has been a challenging task due to the need for high-fidelity and large-scale training data and an appropriate network design. To address this, a simple yet effective 3D human digitization method, **2K2K**, has been proposed. This method reconstructs a large-scale 2K human dataset and infers 3D human models from 2K resolution images [1]. The proposed method employs separate networks to recover the global shape and details of a human body, effectively utilizing the high-resolution input images. This approach demonstrates competitive performance over recent works on various datasets, showcasing its potential for high-quality 3D human body reconstruction [1].>

<Previous works such as SMPL and SMPL-X have contributed significantly to the accurate representation of human body shape, posedependent shape variation, and dynamic soft-tissue deformations, allowing for realistic modeling of human bodies and compatibility with existing rendering engines [1, 2]. In addition, methods like Expressive Body Capture and Detailed, Accurate Human Shape Estimation from Clothed 3D Scan Sequences have addressed challenges related to human body pose, shape, clothing, and surface detail estimation, showing advantages in capturing fine-grained geometry and texture information [2, 4]. These works provide valuable insights and techniques that can complement the development of accurate 3D human digitization methods, including the representation of clothing and intricate surface details [2, 4].>

<To tackle the challenges of reconstructing 3D scans of moving non-rigid objects, particularly human subjects captured over time, research efforts such as Dynamic FAUST and Multi-Human dataset (HuMMan) have focused on 4D data registration. These works provide datasets of high-resolution 4D scans of human subjects in motion, alongside novel mesh registration methods that consider both 3D geometry and texture information, crucial for aligning scans to a common reference topology and accurately capturing movements [3, 9]. Furthermore, the DeepHuman dataset (THuman) and BUFF dataset have been instrumental in facilitating the analysis and modeling of 4D human shape, supporting tasks such as action recognition, pose estimation, parametric human recovery, and textured mesh reconstruction. These multi-modal datasets offer a comprehensive resource for research in dynamic human mesh reconstruction and other related fields [9, 12].>

<With recent advances in image-based 3D human shape estimation, methods such as PIFu, PIFuHD, and Tex2Shape have excelled in digitizing detailed 3D human shapes from single images, producing high-resolution surfaces with intricate details and textures, even in the presence of occlusions and background variations [17, 21, 30]. These approaches leverage implicit function-based representation, multi-level architectures, and aligned image-to-image translation to achieve high-fidelity human body reconstruction from monocular RGB images, showcasing the efficacy of deep learning and novel network designs in capturing detailed geometry and texture information [17, 21, 30]. These advancements provide important insights for achieving high-quality 3D human digitization from single images, especially in terms of handling complex clothing shapes, variations, and deformations [17, 21, 30].>"
4500," **3D Hand Pose Estimation.** 3D hand pose estimation aims to estimate the 3D joint locations of hand joints from a single RGB image. MANO [1] is one of the most widely used hand models, and many works [2][3][4] have been proposed to estimate hand pose and shape from single RGB images. Some works [5][6][7][8][9][10][11][12] estimate hand shape and pose directly from the input image, while some works [13][14][15][16][17][18] predict hand pose from body dynamics. Body2Hands [13] is the most related work to ours, which predicts 3D hands from conversational Gesture Body Dynamics (CGBD) [13]. It first predicts 2D hand poses from CGBD, and then generates 3D joints of each hand from the predicted 2D joints. However, it only generates one hand at a time, which leads to unnatural hand gestures. In contrast, we aim to generate diverse and natural hand gestures by two hand-disentangled branches.

**Memory-Augmented Networks.** Memory-augmented networks have been widely used in many computer vision tasks, such as video segmentation [19], video classification [20], face inpainting [21], and video super-resolution [22]. XMem [19] uses an Atkinson-Shiffrin memory model to model the long-term dependency of video object segmentation. MeMViT [20] proposes a multi-scale memory module to capture long-range dependencies for video classification. Wang _et al_. [21] propose a Coordinated Face Inpainting (C2I) module to learn the spatial-temporal correlations of face images for face in-painting. In this work, we propose a Temporal-Motion Memory (TMM) module for modeling the temporal association between body dynamics and two hand motions. TMM can effectively model the spatial interaction between body and each hand by residual learning.

 proposes a temporal-motion memory (TM) network to model temporal association of two motions. The TM network consists of two sub-networks. The first sub-network is a motion encoder, and the second is a temporal decoder. The main purpose of the TM network is to capture the long term dependencies of two motion sequences. Compared with the original TM network, our TMM module is designed for modeling spatial interaction of two hands, which is more suitable for modeling asymmetric motions between two hands. In addition, we introduce a Spatial-Residual Memory (SRM) module. The SRM module can effectively capture the spatial interactions of two hand, and it can be used to enhance the coordination of motions wrt. body dynamics holistically.

 introduces a temporal memory network to capture temporal dependencies of video frames. It consists of a temporal encoder and a dynamic memory. The dynamic memory encodes the temporal information of each frame and the dynamic memory stores the information of the previous frames. In our work, our temporal memory is designed to capture spatial interactions between the body and"," **3D Hand Pose and Shape Estimation.** 3D hand pose and shape estimation from a single RGB image is a challenging task due to the heavy inter-hand occlusion and large solution space. Most of the previous works [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22] focus on single-hand hand pose estimation. For example, PeCLR [8] proposes a self-supervised 3D pose estimation method by using unlabeled data and self-attention mechanism. Intershape [10] proposes an inverse kinematics module to estimate the 3D joint rotations in a single feed-forward pass. In [9], a hand pose-aware attention module is proposed to extract features associated to each hand respectively. Body2Hands [13] introduces a novel learned deep prior of body motion for 3D human hand shape synthesis and estimation in the domain of conversational gestures.

**3D Human Motion Prediction.** Human motion prediction aims to predict future body poses given observed pose sequences. Most existing methods [15][17] are based on recurrent neural networks (RNNs) and graph convolutional networks (GCN). For example [17] proposes to encode temporal information by working in trajectory space, which alleviates the temporal smoothness and spatial dependencies among human body joints. MotionMixer [14] learns the spatial-temporal 3D body pose dependencies by sequentially mixing both modalities. SPGSN [16] designs a skeleton-parted graph scattering network to model the body-joints' relations. SPGSM [16], which uses graph convolutions to decompose the body into multiple body-parts and then models the body dynamics with a single GCN. In this paper, we propose a novel bilateral hand disentanglement based two-stage 3D motion prediction method to achieve natural and diverse 3D prediction from body dynamics.

","[ **3D hand detection and estimation.** 3D hand detection and estimation have been extensively studied due to their natural usage. Kinect2 3D hand tracker directly reconstructs the 3D skeleton pose from input depth maps and camera images in real-time. Since then, several single-view hand detection methods have been developed in subsequent years, with the rise of deep learning. Early methods utilize the convolutional neural network (CNN) in order to regress a per-pixel heatmap for each hand joint [1]. Additionally, the DexYCB dataset [2][3] was introduced to capture hand grasping of objects. To compensate the issue of missing hand boundaries caused by image-level unsupervised training, the Freihand dataset [4] was proposed, where two hands are detected by two separate independent models, providing accurate hand shape priors. 3D hand pose estimation [5][6][7][8][9] has recently drawn attention, which aims to estimate the 3D hand shape and pose directly from images. Following the huge success of 3D hand pose estimation, the related work now begins to extend the single hand and single-view scenario to 3D hand detection and estimation for two interacting hands in a holistic manner. More complex scene and object manipulation conditions are also introduced to challenge the existing deep-learning-based hand detection and estimation method. [10] was proposed, which estimates the 3D hand pose and shape via sparse positional embedding learned from human-object interactions. To improve the performance of the keypoint estimation of 3D hand pose, [11] introduced keypoint Transformer to predict a heatmap for each keypoint. [12] is the first work that we are aware of that jointly regresses the 3D keypoints and the mesh coordinates in an end-to-end way. Different from the previous method, which detects the hands in isolation, we propose a 3D hand generation method that predicts natural hand gestures from the interaction between two hands in the body dynamics. The dataset Body2Hands is constructed in the process of realizing this goal.

**Human body motion prediction.** Human motion prediction has drawn attention for predicting the future body pose, considering its usefulness in the domain of conversational gestures [13]. Convolutional neural network based methods [14][15][16][17] have been extensively used in the domain of human motion prediction. Due to the high computational complexity of conventional RNNs, some works used the Transformer architecture [18][19][20] to predict the human motion. Furthermore, this paper focuses on hand motion prediction, where we can enrich the motion sequences with natural hand motions, which is challenging for hand detection and estimation methods.

**Memory-Augmented networks.** Memory-Augmented networks can be roughly categorized into spatial-memory and temporal-memory. Spatial-]","<The problem of predicting diverse 3D hand gestures from body dynamics has received attention from researchers in the field of computer vision and graphics. Traditional approaches to hand gesture prediction often overlook the asymmetric motions between two hands and generate both hands in a holistic manner, resulting in unnatural results [1]. To address this issue, recent work has focused on disentangling the motions of each hand and modeling their interactions with the body to achieve more natural and diverse 3D hand predictions [7]. In addition, the creation of accurate hand models has been crucial in this line of work. The MANO hand model [1] is a notable example, providing a consistent hand model for both hands by mirroring left hand scans to simulate right hand scans, effectively removing biases introduced by the handedness of the subjects. Furthermore, the HOnnotate method has facilitated the annotation of images for 3D hand pose estimation, overcoming challenges such as occlusions between the hand and the object [3].>

<The use of deep learning techniques for 3D hand analysis has also significantly advanced the field. One research direction has focused on reconstructing the full 3D shape and pose of hands from a single RGB image, rather than just estimating the 3D locations of hand keypoints [5]. This has led to the development of methods such as the Graph Convolutional Neural Network (GCN) based approach, which can reconstruct a full 3D mesh of the hand surface containing richer information of both 3D hand shape and pose. Moreover, weakly supervised training approaches, leveraging depth maps as weak supervision, have been proposed to improve the accuracy of 3D hand mesh and pose estimation [5]. The method proposed by [8] introduces self-supervised learning for 3D hand pose estimation, leveraging contrastive learning for representation learning and achieving state-of-the-art performance. Furthermore, deep learning architectures have been extended to reconstruct the 3D poses and shapes of two interacting hands from a single color image, addressing challenges such as heavy inter-hand occlusion and larger solution space [9].>

<In addition to deep learning techniques, there have been efforts to develop novel network architectures to address specific challenges in hand gesture prediction. For instance, the Interacting Attention Graph Hand (IntagHand) is the first graph convolution based network that reconstructs two interacting hands from a single RGB image, leveraging attention-based modules to address occlusion and interaction challenges [6]. Another example is the HandOccNet, a 3D hand mesh estimation network that fully exploits information at occluded regions through Transformer-based modules, achieving state-of-the-art performance in challenging hand-object occlusion scenarios [7]. Furthermore, recent work has also explored the use of multi-layer perceptrons (MLPs) as an efficient model for 3D human body pose forecasting. The MotionMixer model, based on MLPs, learns spatial-temporal 3D body pose dependencies and achieves state-of-the-art performance with a smaller number of parameters [14].>

<Regarding dataset creation and benchmarks, several efforts have been made to improve the availability of annotated datasets for training and evaluating hand gesture prediction models. For example, the FreiHAND dataset provides annotations for markerless capture of hand pose and shape from single RGB images, contributing to the development of methods for articulated hand shape estimation [4]. Similarly, other datasets such as HOnnotate for annotating images of a hand manipulating an object with 3D poses of both the hand and the object have been proposed to address the lack of annotated real images for this problem, particularly in the presence of mutual occlusions between the hand and the object [3].>

<The research on 3D hand gesture prediction from body dynamics has seen a significant focus on network architecture design, dataset creation, and hand model development to address challenges such as hand occlusions, asymmetrical motions between hands, and holistic hand gesture generation approaches. Leveraging deep learning techniques, disentangled hand motion modeling, and self-supervised learning, the field has steadily improved the accuracy and naturalness of 3D hand predictions. Additionally, the development of novel network architectures such as the IntagHand and HandOccNet models has further improved the reconstruction accuracy in challenging scenarios. Moreover, the creation of annotated datasets like FreiHAND and HOnnotate has played a crucial role in advancing the state-of-the-art in articulated hand shape estimation and 3D hand pose estimation. These collective efforts have significantly contributed to the progress of the field and laid the foundation for further advancements in 3D hand gesture prediction from body dynamics.>"
3094," **Embodied Navigation.** The task of navigation has been extensively studied in computer vision [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17]. In this work, we focus on the task of image-goal navigation [18], which requires an agent to navigate to an image specified by a semantic label.

**Mapping and Exploration.** Traditional SLAM-based methods [19] have been used to build a map of the environment and perform simultaneous localization and mapping (SLAM) to localize the agent in the map. However, these methods are limited in their ability to generalize to unseen environments. Recent methods have used deep learning to learn a mapping and navigation policy end-to-end [20][21][22][23]. These methods use a pre-trained object detector [20] or a semantic segmentation network [21] to localise the agent and then perform SLAM to recover the goal location. Our method also uses an off-the-shelf object detector, but instead of using it for localizing the agent, we use it for re-identifying the goal instance and projecting the matched features to the map to perform local navigation. This allows our method to be trained in a fully self-supervised manner without any fine-tuning, which allows us to scale to larger scenes and more complex tasks.
**Object Instance Re-Identification.** Object instance re-id [24] aims to identify the same object instance in multiple images. Early methods used hand-crafted features [25][26][27] to perform this task. Recently, deep learning methods have been proposed for this task [28][29][30][31]. Our method uses a transformer-based feature extractor [28] and a graph neural network [29] to match the features of the image and the goal image. We also use an attention mechanism to compute the similarity score between the query and the target image.

 propose a method for object instance reidentification using a transformer. They use a pretrained object detector to extract features for the query image and a transformer to compute a similarity score for the retrieved image. Our work differs from this method in several ways. First, we do not use pretrained features. Instead, our method uses features extracted from an off of the shelf object detector. Second, we propose a modular approach to solve the task. Third, we demonstrate that our method can be trained using only image-level supervision. Finally, we evaluate our method on the HM3D InstanceImageNav benchmark, which is significantly larger than the ImageNet dataset used by  and [24].

 propose an approach for instance-level image retrieval. They train a transformer on a large-scale dataset of natural images and then use it to re-rank the retrieved images based on their similarity to the query. Our approach differs from theirs in two important ways: (1) we use a transformer for the re-ranking task instead of the classification task; (2) our method is trained in an end-"," **Navigation to Objects Specified by Images.** The task of navigation to objects specified by images has been studied extensively in the robotics community [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31]. In this work, we focus on the task of object-goal navigation [6][6], which requires an embodied agent to navigate to an instance of an object specified by an image in an unexplored environment. This task has been addressed in various ways, including reinforcement learning [6], SLAM [5][8], and end-to-end learning [3][3]. In contrast to our work, these methods are not end-2-end and do not use semantic visual reasoning.

**Navigation with Images Specified as Goal.** Object-goal Navigation (ObjectNav) [6] is a task that requires an agent to find an instance specified by a goal image in a novel environment. The goal image is typically a single image of an instance, but the agent is also given a description of the environment and a description for the goal instance. This is different from our task, which requires the agent to explore the environment to find the instance specified in the image. The problem of navigation with images is also different from the task with a goal, which is to find a goal instances in an image.
. The difference is that the goal image can be a single object instance or a semantic description of an image, and the agent must navigate to the instance. The image-goal task has also been studied in [6]. However, the goal is specified as an image of a single instance and the navigation agent is not given an example of the instance it should be navigating to. In contrast, our task is to navigate towards an instance that is specified in a semantic image. This makes the problem different from ObjectNav, as the agent needs to find and navigate to a goal instance of the image, not a specific instance of a specific object category. This also means that the agent does not need to know the category of the object it is trying to find, and it does not have access to the category description. In addition, our goal instance re-identification and goal localization are solved using off-the-shelf components that are not fine-","[ **Semantic navigation:** Navigation has been a cornerstone of robotics research since the seminal work by Geiger _et al_.  on laser range scanning and path planning. Recently, however, work on embodied visual navigation has yielded many approaches to using perception to provide robust directional feedback to the agent. In the classical framework, a planning system (SLAM) maps the perceived environment, and the robot follows the planned path ([1]). Such planning methods require expensive environment representation and map-building. Additionally, these methods have poor resiliency to environment deformation, either in the form of a single obstacle or repeated and arbitrary changes in environment (e.g., tables moved, broken mirrors).

To address these issues, end-to-end approaches have been developed (; [12]). Many of these approaches make use of SLAM ([14]; [3]; [7]), which allows navigation of arbitrary object types. Learning the behavior policy requires the robot to have no semantic understanding. As a result, work has focused on the integration of human priors in the form of semantic maps ([2]; [15]; [14]) and domain-specific instructions ([4]; [10]; [16]). In some of these approaches ([10]; [14]; [5]), images are used to initialize the navigation task and cannot be used to re-execute the task later. [6] and [17] do re-execute the task, but require the human to complete perception tasks (e.g., using visual slate to specify goal).

Semantic navigation algorithms typically do not re-identify the goal (which would make it possible to re-execute navigation from an image). [21] presents an end-to-end system for mapping and goal re-identification. [18] also uses an end-to-end system with a representation learned from an outdoor dataset and outperforms previous work using mask R-CNN object detection to locate the object (). To improve the semantics of images in dynamic environments, [22] learns a variant of CLIP and maps the image to a navigable path, but only on the source domain. None of these approaches perform accurate re-identification of the goal instance to achieve subsequent goal localization and navigation. [11] and [9] present intermediate solutions where objects are classified and re-identified and used in a robust SLAM algorithm, but their end-to-end approaches do not achieve competitive results. Instead, our approach solves sub-tasks using modular components and does not require a fully end-to-end method to achieve the best performance.

**Feature matching:** Feature matching or tracking is used to solve problems such as image matching, object detection, object tracking, object tracking, object pose estimation, semantic segmentation, robotic grasping, robot navigation, object localization, and more (). Feature matching performs relative transformations between features in the feature space, commonly referred to as the]","Step 1. Extract important sentences from the cited papers

[1] ""Creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols."" 
[2] ""A number of different often-inconsistent interpretations of this task are emerging."" 
[3] ""Issues of deep reinforcement learning are addressed, including lack of generalization capability to new goals and data inefficiency.""
[4] ""Recent advances in vision and language methods have made incredible progress in closely related areas.""
[5] ""This paper studies the problem of image-goal navigation, involving navigating to the location indicated by a goal image in a novel previously unseen environment.""
[6] ""Learning provides an appealing alternative to conventional methods for robotic navigation.""
[7] ""We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments.""
[10] ""We present a scalable approach for learning open-world object-goal navigation without requiring ObjectNav rewards or demonstrations of any kind.""
[12] ""We propose a novel reinforcement learning-based approach for image-goal navigation, which simultaneously utilizes self-supervised local features and global features from an observed image and a target image.""
[16] ""We propose a self-supervised approach to learn to navigate from only passive videos of roaming.""
[18] ""We consider the problem of embodied visual navigation given an image-goal where an agent is tasked with navigating specified by an image.""

Step 2. Summarize sentences and decide on topics of related work.
The cited papers discuss a variety of topics related to embodied navigation, including semantic visual reasoning, exploration of unknown environments, deep reinforcement learning, vision and language methods, learning-based navigation, and image-goal navigation in both novel and familiar environments. Topics such as the challenges of incompatible task definitions, generalization in reinforcement learning, learning from passive videos, and object recognition from local scale-invariant features are also relevant.

Step 3. Write related work based on topics.
<The field of embodied navigation has seen diverse and sometimes incompatible task definitions and evaluation protocols, leading to challenges in coordinating ongoing and future research [1]. Deep reinforcement learning approaches have been developed to address issues of generalization capability to new goals and data inefficiency [3]. Recent advances in vision and language methods have made remarkable progress in closely related areas, offering potential for applications in visually-grounded navigation instructions and semantic visual reasoning [4]. Learning-based navigation approaches have been proposed as appealing alternatives to conventional methods, showing promise for both exploration and goal-specific navigation [6]. Additionally, there has been increasing interest and development in self-supervised approaches to learning navigation tasks [16]. These developments have the potential to significantly impact the field by addressing key challenges in embodied visual navigation, such as semantic reasoning, task generalization, and autonomous exploration.>

[10][12] Our approach draws from recent advancements in learning open-world navigation tasks, where scalable methods for object-goal navigation without explicit rewards or demonstrations have been presented [10]. This is complemented by the utilization of novel reinforcement learning-based approaches that simultaneously leverage local and global features for image-goal navigation [12]. These developments highlight the potential for leveraging self-supervised techniques and reinforcement learning methods to improve the efficiency and generalization capabilities of embodied agents in navigation tasks.

<Embodied visual navigation, particularly in the context of image-goal tasks, has garnered significant attention due to its applications in both familiar and novel environments. Related work has explored the challenges and opportunities in navigating specified by visual inputs, shedding light on the complexities of understanding and interpreting semantic visual cues for effective navigation [18]. The study of object recognition and localization in the context of instance-specific navigation tasks has also been a focus, aiming to address the limitations of existing formulations and improve navigation performance in rigid real-world scenarios [23]. These efforts demonstrate the ongoing exploration of methods and techniques to enhance the capabilities of embodied agents in achieving visual navigation objectives.>
"
1031," **Data Poisoning.** Data poisoning is an active research topic in machine learning [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20]. Data poisoning attacks can be divided into two categories: _availability attacks_ and _clean-label attacks_. Availability attacks aim to poison the training data with imperceptible perturbations to make the training samples not exploitable by machine learning models [14]. Clean-label poisoning attacks [9][7] aim to change the labels of training samples to make them misclassified by the target model. In this paper, we focus on the latter setting.

**Unlearnable Examples.** Unlearnable examples (UEs) [6] are training samples added with invisible but unlearnable noise, which have been found to be effective against unauthorized training of ML models [6]. They are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. The surrogate model is trained to predict the label of the original training samples, which are then used to train a new model to replace the original model. The goal of the defender is to minimize the error of the surrogate model, so that the original data can still be used for training the new model. However, the defender does not have access to the target models during the training process. To address this issue, several methods have been proposed to improve the quality of the generated UEs, such as robust UEs [16], transferable ue [18], and robust ue. However, these methods all rely on the label-consistency assumption, where the attackers and protectors hold the same label for a given sample. In contrast, we propose and promote a more practical setting where the hackers may exploit the protected data quite differently from the protectors. To the best of our knowledge, our work is the first to propose a label-agnostic unlearnable example generation method.

 propose a data poisoning attack that can be used to generate UEs. Specifically, they propose to use adversarial training [4][15] to generate adversarial examples and then use them to train an untargeted defense model. [18] propose to generate transferable UEs by leveraging the transferability of UEs across different models. [19] and [20] also propose to add adversarial noise to the original images to prevent face recognition systems from being deployed on the Internet. [17] propose an unsupervised contrastive learning method to generate indiscriminate UEs to prevent discriminative classifiers from being trained on the target data.

 and [16] propose a robust unlearning framework to generate robust ues, which can be applied to defend against adversarial learning. However these methods are all designed for the _label-consistent setting_, which assumes that the attacker and the defender have the same set of labels for the same sample. Our work is different from these methods in that"," **Backdoor attacks.** Backdoor attacks aim to inject imperceptible but unlearnable noise into the training data to cause the model to misclassify a specific target image, which is then used to attack the target model [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20]. In this work, we focus on data poisoning attacks, which are more closely related to our work. Data poisoning attacks can be divided into two categories: poisoning attacks and availability attacks. The former attack aims to inject harmful noise to the training set to cause misclassification of the target image [1], while the latter attack aims at poisoning the model with the original label assigned to the test image [13][13]. The latter attack is more related to the recent availability attacks [14], which aim to poison the data by assigning the test images with the label assigned by the adversary. In this paper, we mainly focus on the latter type of attack.

**Unlearnable examples (UEs).** UEs are training samples added with invisible noise, which have been found can prevent unauthorized training of machine learning models. In the literature, existing methods can be roughly divided into three categories: (1) **Poisoning attacks**, which inject the noise into training data by injecting the noise with the target label [13], (2) **availability attacks** that assign the noise to training samples by assigning them with their labels, and (3) **unlearnable attacks** which assign noise to all samples, and then apply the noise on the target classifier [14][17]. The poisoning attacks are more effective than the availability attacks, since the noise is added to the original training data. However, existing poisoning attacks rely on the label-consistency assumption, which may not hold in the real-world. In contrast, our work focuses on a more practical setting, where the hackers and protectors may not have the same label for a given sample, and thus we propose a novel technique to generate UEs in this challenging setting. **Label-agnostic UE generation.** Label-consistent UE generation methods [14] are proposed to generate UEs by assuming that the label of the protected model is the same for all the samples. For example, [13] proposes to assign the unlabeled training samples with the same labels assigned to them as the target samples,","[ **Unlearnable examples.** There has been a growing concern about unauthorized usage of personal data, which has inspired works on data poisoning and unlearnable examples. The latter studies seek to use the limited access to original training data to craft a dataset to train a model so as to obtain unlearnable samples. Typical works include clean-label data poisoning (; [3]; [8]; [7]), targeted data poisoning ([6]; [2]; [1]), or multi-label poisoning ([9]; [10]). These poisoning attacks typically require specific knowledge about training models to generate the adversarial samples, such as model architecture and parameters, or even predictions from the victim model. These techniques are usually able to poison the training data to prevent the corresponding model from training well on the real data, and thus prevent unauthorized access to training data, but they usually do not ensure the learned model performs poorly when used on data different from the training data.

**Robust unlearnable examples.** Numerous works have been proposed to generate unlearnable examples. Among them, robust unlearnable examples are proposed to defend models against adversarial attacks ([15]; ; ). These methods leverage known properties of adversarial examples to generate unlearnable examples that are robust to adversarial training ([11]; [13]; [14]). Subsequent works build upon these strategies by incorporating more powerful approaches like pseudo-labeling ([12]), contrastive learning (), etc. ([16]; [17]; [18]). Recently, low-level adversarial attacks are also used to generate unlearnable examples to defend face recognition models ([20]; [19]). However, as shown in our work, existing approaches (or adaptations) cannot effectively solve the label-agnostic problem.

]","<>
With the increasing concern on unauthorized data usage and privacy leakage in the realm of machine learning models, numerous studies have focused on developing techniques to prevent unauthorized training and exploitation of personal data [6]. One such approach involves the introduction of unlearnable examples (UEs) into the training data, where imperceptible noise is added to training samples, rendering them unlearnable by machine learning models [6]. Existing UE generation methods have primarily relied on the label-consistency assumption, assuming the same label for a given sample held by both attackers and protectors [6]. However, the practical label-agnostic setting, where hackers may exploit the protected data differently, renders existing UE generation methods ineffective [6]. To address this challenge, the proposed Unlearnable Clusters (UCs) technique generates label-agnostic unlearnable examples with cluster-wise perturbations, offering a novel approach to safeguard data against unauthorized training [6].

In the context of poisoning attacks against machine learning models, prior work has explored various strategies to inject specially crafted training data to manipulate the behavior of models during inference [2]. For instance, poisoning attacks against support vector machines and deep learning systems have been investigated, demonstrating the ability of adversaries to construct malicious data to impact the model's decision function and classification outcomes [1, 2]. Additionally, studies have delved into the development of poisoning attacks using clean-label data and have outlined the effectiveness of such attacks in manipulating the behavior of machine learning models during test time [9, 10]. These efforts shed light on the significant threat posed by poisoning attacks and the importance of devising defense strategies to mitigate their impact [2, 9, 10].

Furthermore, the emergence of availability attacks, which aim to prevent unauthorized data exploitation by poisoning training data with imperceptible perturbations, has prompted investigations into the properties and effectiveness of such attacks [14]. Availability attacks have been shown to create shortcuts and exploit linear separability properties, underscoring the potential vulnerabilities inherent in machine learning models and the need for robust defense mechanisms against adversarial attacks [14]. Moreover, the development of transferable unlearnable examples has been proposed to enhance the transferability of unlearnable effects across different training settings and datasets, offering a promising avenue for fortifying the resilience of data protection measures [18]. Rich et al. also demonstrated that deep neural networks are susceptible to adversarial examples, highlighting the need for robustness against adversarial attacks in deep learning models [15].

As a response to the growing concern over privacy breaches through facial recognition systems, researchers have explored the application of adversarial perturbations to protect social media users from unauthorized facial recognition [19]. This work reveals the potential of adversarial filters to significantly degrade the accuracy of commercial facial recognition APIs, serving as an important stride towards preserving user privacy in the face of widespread deployment of facial recognition technologies [19]. Additionally, in the domain of privacy-preserving solutions, novel approaches such as the Adversarial Privacy-preserving Filter (APF) have been proposed to protect shared face images from malicious use, offering end-to-cloud collaborated attack solutions for addressing privacy concerns related to online multimedia sharing [20].

<>
"
1150," **Image denoising.** Traditional methods [1][2][3][4][1][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] mainly focus on designing hand-crafted image priors to denoise noisy images. Recently, deep learning-based methods [5][7] have achieved great success in the image dennoising task. These methods can be roughly divided into two categories: supervised and self-supervised methods.

**Supervised methods** require a large amount of paired clean and noisy image pairs for training. However, it is difficult to collect such pairs for real-world sRGB images. To address this problem, several synthetic noise datasets [21][22][23][24][25][26] have been proposed to train the denoiser with synthetic noise, which are usually generated by Gaussian additive white Gaussian (AWGN) or Poisson-Gaussian (Poisson) noise [10][21][25]. However, the noise distribution of real sRGB image is usually different from that of the synthetic one, which limits the performance of these methods. To overcome this limitation, CycleISP [8] and CycleNet [9] are proposed to generate synthetic noisy images from real noisy images by CycleGAN [23] and cycle-consistency loss [24], respectively. Although these methods have achieved promising results, they still rely on the quality of synthetic data, which is limited by the noise generation process. To alleviate this issue, some works [13][15] propose to use real-to-synthetic data generation methods to generate more realistic noisy images, which can be used as the training data for training the network. In this paper, we propose a novel perspective to solve this problem by seeking for spatially adaptive supervision, which does not require any synthetic noise and can be applied to any real-real sRGB denoisers. Besides, the proposed method can also be used to generate real-noise-free training data to improve the performance on the real-scene. In addition, our method is also able to remove the noise-correlated noise in the real images without using any synthetic data. Therefore, it can be easily applied to real-sRGB image denoisying with real noisy data, as shown in Fig. 1. Note that our method can be combined with the above methods to further improve their performance, which will be detailed in the following sections.****Self-supervision based methods** have attracted much attention in the recent years. Noise2Noise [27] is the first work to propose to train a network with only a single noisy image as the supervision, and it has been widely used in many follow-up works [28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43]. For example, Noise2Void [28] and Noise2Self [29] adopt a two-stage training strategy, where the network is first trained with"," **Image denoising.** Image denoisers have been extensively studied in the past decades, and most of them are based on the AWGN model [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43].

**Real-world Image Denoising with Spatially Correlated Noise.** Real-world image denoisers are usually trained on synthetic datasets with AWGN noise, which are not suitable for real-world sRGB images with spatially correlated noise. To address this problem, several real-realistic datasets have been proposed, _e.g._, SIDD [12], Darmstadt Noise Dataset [13] and CycleISP [8]. However, these datasets are limited to real-scene images with real noise, _i.e._, the noise is spatially independent. To deal with real sRGB sRGB noise, some methods [9][11] use real images with mixed AWGN and random value impulse noise [11] to train a real denoiser. However, they are limited by the assumption that the noise distribution is Gaussian, which is unrealistic in real-life scenarios.
1
.

. To solve this problem of real-to-real noise, [8] and [9] propose to use real-image pairs to train the real-noise model. [11][9] use the real images to train their real-denoiser, and [10] propose the Poissonian-Gaussian noise model [10], which is a more realistic noise model for real images. [8][10] further propose to model the real noise with the inverse Poisson noise model, which can better match the real sensor noise. [12] propose a high-quality sRGB image dataset, namely, Smartphone Image Denoising Datasets (SIDD) [12]. However these methods are limited in dealing with real noisy images with spatial correlated noise, and they have little practicality on real-word sRGB real-sRGB images. In this paper, we propose a novel perspective to solve the problem of spatially","[ Image denoising has received extensive attention from both signal-processing [1][2][3][4] and computer vision researchers [5][6][7][8]. The mainstream approaches are mostly based on discriminative learning and mainly take synthetic noisy images with well-specified noise models (_e.g._, Poisson noise) as the benchmark data. Traditional denoising methods can be generally grouped into two categories: hand-crafted features based methods and CNNs based methods. The former focuses on learning useful priors [9][10][11][12] and deriving effective denoising algorithms through them. Recently, CNNs have made remarkable progress on denoising task [13][14][15][16][17][18][19][20]. However, these methods have been practically limited on synthetic noisy images. On the other hand, some approaches have been proposed for dealing with real noisy images [21][22][23][24][25][26]. A drawback of these methods is that they usually require a well-paired clean-noise image for model building, which is difficult to acquire in real-world scenarios.

Recently, there has been a series of supervised learning-based self-supervised denoising approaches which rely on a large-scale collection of noisy images and noisy-clean pairs for learning. These methods adopt fully-convolutional networks as denoising models. Noise2Noise [27] is the first self-supervised method, which is mainly based on pairwise constraints. By selecting the same image patch from both noisy images in the noisy-clean pairs, it only requires noise-free prediction. Thus it has been optimized by only the batch-mean loss. Later, Noise2Void [28] and Noise2Self [29] aim at optimizing model robustness by introducing a noise-aware pre-trained denoising model and estimating the denoising loss by predicting the noise distribution of each pixel. Despite the improved robustness, the denoising performance on real-world images is still unsatisfactory.

Following Noise2Self, various methods have been proposed for denoising real-world images. They mainly focus on the self-supervision, including the construction of pixel-shuffle denoising loss for solving the noise-correlated issue, and the design of denoising supervisions based on in-built pixel features or adaptive pixel information.

**Pixel-shuffle loss.** Compared with previous self-supervised approaches, the most important change in [30] is the utilization of a spatially variant pixel-shuffle loss for minimizing the performance gap of their denoising model with their clean-noise model. Specifically, a pixel-shuffle mask is designed by its neighboring pixels, whose distance with target pixel is short,]","<Significant progress has been made in the field of self-supervised image denoising (SSID) in recent years. While previous methods have focused on dealing with spatially independent noise, there has been limited practicality on real-world sRGB images with spatially correlated noise. Pixel-shuffle downsampling has been suggested as a method for breaking the noise correlation; however, this approach has limitations as it breaks the original information of images, thus limiting the denoising performance. In response to these limitations, the proposed novel perspective for solving this problem is seeking spatially adaptive supervision for real-world sRGB image denoising. The approach takes into account the respective characteristics of flat and textured regions in noisy images and constructs supervisions for them separately. For flat areas, the supervision is derived from non-adjacent pixels to exclude the influence of noise-correlated ones [1].>

<In the context of image denoising, extensive research has been conducted on deep convolutional neural networks (CNNs), exploring the construction of feed-forward denoising CNNs to embrace the progress in very deep architecture, learning algorithm, and regularization method [1]. Weighted nuclear norm minimization (WNNM) has also been studied as a convex relaxation of the low rank matrix factorization problem, with particular application to image denoising by exploiting image nonlocal self-similarity [2]. Moreover, the proposal of a multi-level wavelet CNN (MWCNN) model introduces the concept of a novel architecture for achieving a better tradeoff between receptive field size and computational efficiency in low-level vision tasks [7]. These advancements in denoising algorithms based on deep learning and transform domain collaborative filtering provide a wealth of strategies for enhancing the performance of self-supervised image denoising methods [3, 7].>

<Building upon the significant progress in self-supervised image denoising, several recent approaches have exhibited promising results. For instance, the use of a very deep fully convolutional encoding-decoding framework for image restoration has proven effective, especially when symmetrically linking convolutional and de-convolutional layers with skip-layer connections [5]. The development of a persistent memory network (MemNet) introduces the concept of explicitly mining persistent memory through an adaptive learning process, addressing the long-term dependency problem in very deep models [6]. Another notable direction involves training a convolutional blind denoising network (CBDNet) with a more realistic noise model and real-world noisy-clean image pairs, resulting in improved generalization ability of deep CNN denoisers [9].>

<In addition to advancements in self-supervised image denoising methods, efforts have been made to address the challenges of noise modeling and distribution adaptation. For example, the availability of large-scale datasets for denoising real photographs has posed a challenge due to the impracticality of capturing such data, prompting the proposal of a CycleISP framework for real image restoration via improved data synthesis [8]. Similarly, a practical noise model for the raw data of digital imaging sensors has been presented, allowing for the fully automatic estimation of the model parameters given a single noisy image [10]. These developments reflect a concerted effort to bridge the gap between synthetic noise models and the noise present in real-world images, as well as to improve the adaptability of denoising algorithms to various noise distributions and scenarios.>

<While supervised deep networks have traditionally required clean training examples, more recent efforts have focused on self-supervised and unsupervised approaches, addressing the challenges associated with the acquisition of clean or paired noisy-clean training images. The introduction of Noise2Void (N2V) represented a significant milestone in self-supervised denoising, illustrating the potential of denoising methods trained without access to paired noisy training examples [28]. The emergence of Probabilistic Noise2Void (PN2V) further extended this paradigm, enabling the prediction of per-pixel intensity distributions and the development of a complete probabilistic model for noisy observations and true signals in every pixel [33]. These advances signal a transition towards more data-efficient and practical denoising methods, with a focus on operation in scenarios where clean or paired noisy-clean training images are not readily available.>

<Advancements in denoising methods have also been paralleled by efforts to address real-world challenges such as structured noise and practical noise distribution adaptation. A Structured Noise2Void (STRUCTN2V) approach has been proposed to enable the removal of structured noise without requiring an explicit noise model or ground truth data, expanding the applicability of self-supervised denoising methods to situations where spatially correlated noise is present [41]. Additionally, the development of AP-BSN has opened new prospects for self-supervised denoising on real-world sRGB images, leveraging the asymmetric pixel-shuffle downsampling approach to address spatially correlated real-world noise [43]. These innovations reflect a concerted effort to develop self-supervised denoising methods that are capable of addressing a wider range of real-world denoising challenges, including structured noise and spatially correlated noise patterns.>"
3109," **Cross-domain Face Anti-spoofing.** Existing cross-domain FAS methods can be broadly categorized into two groups: unsupervised domain adaptation (UDA) [1][2][3] and supervised domain adaptation [4][5][6][7][8][9][10]. UDA-based methods aim to learn domain-invariant features by aligning the source and target domains with adversarial learning [2][1][7] or feature disentanglement [6]. However, these methods often require a large amount of unlabeled target domain data, which is difficult to obtain in real-world applications.

On the other hand, supervised domain generalization (DG-DAD) aims to train a model that can generalize to unseen spoof types, camera sensors, and environmental conditions. For example, DGD-FAS [4] trains a DNN-based FAS model by distilling knowledge from a large-scale source domain dataset to a small target domain dataset. However, the performance of these methods heavily relies on the quality of the source domain data. Recently, several works [5][10] have shown the effectiveness of vision transformer (ViT) models for the FAS task due to their ability to capture long-range dependencies among image patches. For instance, AVT [5] adapts the pre-trained ViT model to the target domain by fine-tuning the model on a small number of target domain samples. DVT [10] proposes a domain invariant feature learning (DIFL) module to improve the generalizability of the learned features across domains. In this work, we show that initializing VTs with multimodal (e.g., CLIP) weights improves the robustness of FAS models to unseen domains. Our work is also related to the few-shot FAS problem [8], where a model is trained on a few target domains and then tested on a large number of source domains. Different from existing methods, we propose a novel approach for robust cross-domains FAS by grounding visual representations with an ensemble of class descriptions (based on natural language semantics).

**Vision-Language Pre-trained Models.** Recently, there has been a surge of interest in developing vision-language (VLP) models [11][12][13][14][15] that can learn powerful visual and textual representations from massive amounts of image-text paired data. Specifically, CLIP [21] and its variants [20][22][23][24][25] pre-train vision and language models on 400 million image-caption pairs and achieve state-of-the-art performance on a wide range of vision tasks, including image classification, object detection, semantic segmentation, and image captioning. The key idea of these VLP models is to learn a joint embedding space that captures the cross-modal interactions between images and text, and then fine-tune the VLP model on downstream vision tasks such as image classification and object detection. In contrast, our work focuses on improving the generalization of"," **Cross-domain Face Anti-spoofing.** Cross-domain FAS methods have been proposed to address the domain gap between the source and target domains [1][2][3][4][5][6][7][8][9][10]. For example, [1] proposed an unsupervised domain adaptation (UDA) scheme to learn a classifier for the target domain based on training samples in the source domain. [2] proposed a triplet loss to learn domain-invariant features for cross-domain face anti-speoofing. [4] proposed to leverage data from a richer and related domain to learn meaningful features through the concept of neural network distillation. [7] proposed multi-adversarial discriminative deep domain generalization (DDCGD) to improve the generalization ability of FAS models. [6] proposed cyclically disentangled feature translation network (CDFTN) to disentangle source domain-specific features and target domain-relevant content features. [9] proposed Shuffled Style Assembly Network (SSAN) to separate the complete representation into content and style features for a stylized feature space. [8] proposed Regularized Fine-grained Meta-learning (RFM) to regularize the feature space regularization so that meta-learning can be conducted in the feature-space regularized by the supervision of domain knowledge. [5] proposed adaptive vision transformers (ViT) for robust few-shot FAS. [10] proposed the domain invariant Vision Transformer (DiVT) for FAS, which adopts two losses to improve generalizability.


**Vision-Language Pre-trained Models.** Recently, vision-language pre-trained models [11][12][13][14][15][16][17][18][19][20][21][22][23][24][25] have been shown to be effective for vision-text pre-training. For instance, CLIP [21] and its variants [22][25][23] pre-train vision and language models on a large-scale image-text dataset, and then fine-tune them on downstream tasks. [22] proposed contrastive-tuning (CLIP) to learn zero-shot transferable visual and textual representations from a large scale image-language dataset. [20] proposed CLIP++, which uses a dual-encoder architecture with contrastive loss to align image and text","[ **Face Anti-Spoofing** In the past few years, numerous FAS methods have been proposed to enhance the generalizability of FAS models to spoofing types not seen in the source domain. The literature can be broadly categorized into three categories. Early works on this topic mainly focus on designing effective adaptation and metric learning loss functions to enable the FAS models to generalize to novel spoofing types. Some methods, such as CrossNet and EdgeMatchNet , propose a common decision boundary to classify both target and source FAS datasets. In a multi-target setting, methods like adversarial domain generalization [1] and adaptive domain generalization [2] aim to maximize the accuracy of test data by adapting the source model parameters using the target label and/or image data. Further works like data distillation [3] and neural architecture search [4] propose to augment the training set using synthesized data by either annotated spoofing data or unlabeled real images. Closely related to our work, meta learning and meta FAS [5][6][7][8][9] try to learn a meta-model that captures general-purpose representations rather than focusing on specific spoofing types. Different from prior works that emphasize different tasks, we focus on robust cross-domain face anti-spoofing with the help of a fixed ViT backbone pre-trained with zero-shot images and text captions.

**Vision Transformer** Vision Transformer (ViT) [10][11] introduces a novel architecture that combines the main building blocks of Transformers [12] and CNNs [13][14][15] for effective learning of visual representations. Compared to CNNs, ViTs can preserve spatial information better [16], leading to the successful transfer of ViT to high-level vision tasks. However, for face anti-spoofing tasks, prior ViT models are computationally too expensive to process image patches due to the use of self-attention [17] and several encoder-decoder layers [10], resulting in prohibitively slow inference speeds. To improve inference speeds, distilled ViTs [10] combine lightweight multi-head attention modules with cheaper MLP blocks. Similar to our approach, ViTs [11][10] show that prompting [11][12][13][14][15] can enable large pre-trained vision-language models to be ""tuned"" or ""pre-trained"" on a small amount of target domain data with the help of prompt templates. To the best of our knowledge, this work is the first to show that multimodal (e.g., CLIP) pre-trained weights can improve the performance of ViTs on the face anti-spoofing task.

**Image-text Transfer Learning** Earlier work]","Step 1. Extracted important sentences from the cited papers:
- Existing FAS methods have poor generalizability to unseen spoof types, camera sensors, and environmental conditions [1].
- Unsupervised domain adaptation methods have been introduced to address the real-world scenario, learning the classifier for the target domain based on training samples in a different source domain [1].
- Adversarial domain adaptation has been proposed to improve PAD generalization capability [2].
- Semi-supervised learning frameworks with progressive transfer learning have been developed to tackle face spoofing attacks with only a few labeled training data [3].
- Leveraging data from a richer and related domain to learn meaningful features through neural network distillation has been proposed to tackle the problem of training a robust PAD model with limited data in an application-specific domain [4].
- Adaptive vision transformers have been presented for robust cross-domain face anti-spoofing to account for larger appearance variations of images acquired in complex scenes with different sensors [5].
- A novel domain adaptation method for face anti-spoofing, called cyclically disentangled feature translation network, has been proposed to achieve a robust decision boundary in the presence of domain differences [6].
- A multi-adversarial discriminative deep domain generalization framework has been developed to learn a generalized feature space for improved face anti-spoofing generalization ability [7].
- A regularized fine-grained meta-learning framework has been proposed to address the limitation of the generalization ability of face anti-spoofing methods to unseen attacks, achieving robust face anti-spoofing [8].
- A novel method named Shuffled Style Assembly Network has been introduced to achieve generalizable face anti-spoofing and addresses the gap between academia and industry [9].
- Domain-invariant vision transformer learning for face anti-spoofing has been proposed to address the challenge of generalizing to data from unknown domains and achieve state-of-the-art performance on domain-generalized FAS tasks [10].

Step 2. Summarization and deciding on topics:
Based on the extracted sentences, the related work can be summarized into topics such as unsupervised domain adaptation methods, adversarial domain adaptation, semi-supervised learning frameworks, neural network distillation for robust PAD, adaptive vision transformers for cross-domain face anti-spoofing, novel domain adaptation methods, and regularization techniques for fine-grained meta-learning.

Step 3. Write related work based on topics:
<Unsupervised domain adaptation methods have been introduced to address the limited generalizability of existing Face Anti-Spoofing (FAS) methods to unseen spoof types, camera sensors, and environmental conditions [1]. These methods focus on learning the classifier for the target domain based on training samples in a different source domain, effectively addressing the real-world scenario where such adaptability is essential [1]. Additionally, adversarial domain adaptation and semi-supervised learning frameworks have been proposed to improve the generalization capability of Presentation Attack Detection (PAD) and tackle face spoofing attacks with limited labeled training data [2][3]. Furthermore, neural network distillation has been leveraged to train robust PAD models with scarce data in application-specific domains, addressing the challenge of training in such constrained environments [4]. These approaches collectively demonstrate efforts to enhance the adaptability and generalization of FAS methods across different domains and under varying conditions.>

<In the domain of adaptive vision transformers for cross-domain face anti-spoofing, recent developments have showcased the effectiveness of these models in addressing larger appearance variations of images in complex scenes with different sensors [5]. Moreover, novel domain adaptation methods and regularization techniques for fine-grained meta-learning have been proposed, aiming to enhance the generalization ability of face anti-spoofing methods to unseen attacks and improve practical performance [6][7][8][9]. These developments signify the ongoing efforts to evolve face anti-spoofing methods, ensuring robustness and adaptability in the face of diverse, real-world scenarios [10].>

<>
"
4140," **Deepfake Generation.** With the development of deep generative models [1][2][3][4], the performance of deepfake generation has been greatly improved. In this paper, we focus on the task of detecting deepfake generated images.

**Deepfake Detection.** Early deepfake detection methods [5][6][7][8][9][10][11][12][13][14][15] mainly focus on designing hand-crafted features to distinguish real and fake images. For example, Wang _et al_. [16] propose to use the Difference of Gaussian (DoG) and Histogram of Gradient (HOG) features as the features for binary classification. However, these hand-craft features are not robust enough to distinguish fake images from real ones. To address this problem, some recent methods [17][18][19][20][21][22][23] leverage the powerful representation ability of deep neural networks to extract more discriminative features for deepfake classification. Specifically, Li _et.al_. [18] propose a frequency-aware feature learning supervised by single-center loss to improve the generalization ability of the binary classifier. Li [23] propose the Spatial-Phase Shallow Learning (SPLS) method to learn the spatial-frequency correlation for better generalization. However the SPLS method requires a large amount of training data, which limits its application in real-world scenarios. Different from the above methods, we propose a simple yet effective method to reduce the influence of the implicit identity leakage on binary classifiers.

 propose a method to detect fake images based on the observation that the identity information is implicitly encoded in the generated images, which is referred to as the identity-invariant feature learning (I-FEL). However, their method is based on an assumption that the fake images are generated by the same person, which does not always hold in practice. In contrast, our method does not make such an assumption and can be applied to any DNNs. Moreover, we empirically verify the existence of this implicit identity leakage and propose an ID-unaware Deepfake Detection Model (ID-DMD) to reduce its influence.

 first propose to detect deepfake images by using a binary classification model. They propose to train the model on a large-scale dataset and then fine-tune it on a small-scale fake dataset to achieve better performance. In our experiments, we show that the ID-Ileakage is the main reason for the poor generalization of their model. In addition, we also show that our method is more effective than their method in both in- and cross-dataset evaluation.

 present a method for detecting fake videos based on a two-branch architecture. The first branch is a generative adversarial network (GAN) to synthesize fake videos, and the second branch is an image classifier to distinguish the fake videos from the original ones. In their experiments, they show that their method outperforms the state-of-the-art methods in both accuracy and robustness. Differently, we find that the"," **Deepfake Detection.** Deepfake detection has been extensively studied in recent years. In this section, we briefly review the related work on deepfake detection.

**Image Synthesis.** Generative Adversarial Networks (GANs)  have achieved great success in generating realistic images. StyleGAN [1][2][3] and StarGAN [4] are two representative models for unconditional image synthesis. StarGAN v2 [1] and StyleGAN v3 [3] improve the quality of the generated images by introducing a new generator architecture and a path length regularizer to encourage good conditioning in the mapping from latent codes to images. However, these models suffer from the problem of limited diversity and scalability over multiple domains. To address this issue, StarGAN-v2 [4], STGAN [3], and Style-GAN v1 [2] introduce a new architecture to improve the diversity of the synthesized images. In addition, they introduce a skip-connection layer to select the desired attribute vector for the target domain. However these models are not suitable for the task of deepfakes detection since they do not consider the identity information of the images, which is crucial for the generalization ability of the classifier. In contrast, we propose a novel method to reduce the influence of the leakage of the ID information in the ID-unaware Deepfake Detection Model. In our experiments, we show that our method outperforms the state-of-the-art models in both in-dataset and cross-dataaset evaluation. Moreover, our method can be easily adapted to any other deepfake detectors.







","[ **Deepfake Detection.** To detect the generation of deepfake images and videos, various methods have been proposed in the literature. In the image domain, early deepfake detectors exploit the gradient difference between real and fake images [1][2][3][4] and this approach is adopted by some recent methods [5][6]. However, the gradient-based detection method has limited success in detecting deepfake videos. To alleviate this issue, the CNN-based methods [7][8][9] have been developed. Recently, some other works have been proposed to achieve more accurate detection. They include Heart Rate Estimation [10], Fake Faces Identification [11], MesoNet [12], CUDNN_Fake_Classifier [13], CapsNet [14], Graph Snake , and LSTM . The aforementioned methods can be grouped into two categories: metric-based and data-based detectors. The metric-based detectors depend on the difference between feature statistics. For example, Zhao _et al_.  find that fake images and videos have different noise feature statistics and Hussain _et al_.  detect the deepfakes by examining the deepfakes' difference in the features computed in the low and high frequencies of the images. The other types of metric-based detectors depend on the difference of similarities among networks in the pairwise or triplet manner. For example, Duh _et al_.  measure the semantic similarity between the feature maps extracted from a real image and a generated image in a pre-trained classification model. Some methods [11][12] detect the deepfakes by assessing the difference in feature maps extracted from deepfake videos. The data-based detectors have different training methods. For example, Feng _et al_.  train their detectors in a semi-supervised learning manner by supplementing a few labeled data for image classification. Sarbani _et al_.  train their detectors by first detecting deepfakes using a set of pre-trained models and then using the detected deepfakes to train their new detectors. However, all the existing deepfake detectors lack of generalization ability [15][16].

**Generalization Ability in Deepfake Detection.** Recently, some works [17][18][19][20][21] have pointed out that the above detectors tend to perform well on the deepfakes that are trained on the same dataset, but they perform poorly when their training data are unseen. In this paper, we give the first concrete demonstration that the impurity of deepfake training data could be one of the main reasons of this phenomenon. We find that the deepfake images in the wild tend to have many artifacts, i.e., blurry, noisy, inconsistent with the background images, etc. and this is probably the]","Step 1: Extract important sentences from the cited papers.

From [1], ""Existing methods address either of the issues, having limited diversity or multiple models for all domains.""

From [3], ""We further propose an active learning framework to select the challenging candidates for labeling, to reduce the annotation efforts to regularize interpretations.""

From [6], ""we provide experimental evidences to show that the proposed method can achieve satisfactory results with an average accuracy over 99.4%.""

From [9], ""Experimental results indicate that LAE indeed could focus on the forgery regions to make decisions.""

From [10], ""The results achieved, above 98% AUC (Area Under the Curve) on both databases, outperform the state of the art and prove the success of fake detectors based on physiological measurement to detect the latest DeepFake videos.""

From [13], ""The results of these experiments show that our proposed approach can automatically detect several different manipulations with an average accuracy of 99.10%.""

From [15], ""Several countermeasures have been introduced to deal with attacks using such videos.""

From [22], ""Extensive experiments show that face X-ray remains effective when applied to forgery generated by unseen face manipulation techniques.""

From [23], ""Extensive experiments show that SPSL can achieve the state-of-the-art performance on cross-datasets evaluation as well as multi-class classification.""

Step 2: Summarize sentences and decide on topics of related work.

Based on the extracted sentences, the related work can cover topics such as:
1. Challenges in diversity and generalization of deepfake detection methods.
2. Active learning and focus on forgery regions for improved detection accuracy.
3. Detection of deepfake videos using physiological measurements and new convolutional layer architecture.
4. Detecting manipulations using frequency domain and spatial-phase shallow learning.

Step 3: Write related work based on topics.

<Challenges in diversity and generalization of deepfake detection methods have been addressed in existing literature. Methods have been developed to tackle issues related to limited diversity or the need for multiple models for all domains [1]. Moreover, the utilization of an active learning framework for selecting challenging candidates for labeling has been proposed to reduce annotation efforts and regularize interpretations [3]. These approaches aim to improve the generalization ability of deepfake detection models by addressing issues related to diversity and the need for domain-specific multiple models.>

<In the realm of deepfake detection, significant progress has been made in achieving high detection accuracy. For instance, a proposed method achieved satisfactory results with an average accuracy over 99.4% [6], while another approach based on Locality-aware AutoEncoder (LAE) focused on forgery regions to make accurate decisions, indicating its potential for improved detection performance [9]. Furthermore, researchers have successfully employed physiological measurements and novel convolutional layer architectures to attain detections with above 98% Area Under the Curve (AUC) [10, 13, 15]. These advancements demonstrate the success of physiological measurement-based and deep learning approaches in detecting the latest deepfake videos.>

<Additionally, the detection of manipulations has been addressed using frequency domain and spatial-phase shallow learning methods. Extensive experiments have shown the effectiveness of these techniques, with state-of-the-art performance on cross-dataset evaluation as well as multi-class classification [22, 23]. These approaches represent a significant advancement in detecting forgeries and deepfake videos by focusing on the spatial and frequency domain features to enhance detection accuracy.>

In this related work, the citations are used to refer to the studies that have been quoted in the given target paper, to provide context and support for the content of the paragraphs."
3188," **Script Induction.** Script induction is a long-standing problem in natural language processing (NLP) [4]; [7]; [9]; [10]; [3]; [6]; [11]; [12]; [1]; [2]; [5]; [8]. Most of the existing work focuses on the sequential setting. For example, [4] propose an unsupervised approach to learn scripts from narrative event chains. [7] propose to model the co-occurrence of script events and their participants. [9] propose a sequence-to-sequence model to align script events with narrative texts. [11] and [5] propose graph-based approaches to induce scripts. [2] propose the task of goal-oriented script construction, which aims to generate subsequent steps to reach a given goal. [1] introduce the narrative cloze task to evaluate the ability of a model to generate scripts. Recently, [8] propose proScript, a model that learns partially ordered scripts from pre-trained language models. However, all of the above methods focus on sequential scripts. In contrast, we propose a new task of non-sequential graph script induction, aiming to capture _optional_ and _interchangeable_ steps in procedural planning.

**Multimodal Learning for Procedural Activities.** There has been a growing interest in leveraging multimodal information for procedural activity understanding [16]; [18]; [13]; [17]; [20]; [14]; [22]; [21]; [15]; [19]. For example [16] propose two methods to jointly learn event representations and event ordering from instructional manuals. [18] propose procedure-aware pre-training to learn video representations that are useful for downstream procedure understanding tasks. [17] propose zero-shot anticipation for instructional activities, which predicts the next action given the previous actions. [20] propose visual goal-step inference using wikiHow videos. [21] propose multi-modal subtask graph generation from instructional videos, which generates sub-task graphs for a given task. [19] propose unsupervision methods to generate task graphs from instructional video transcripts. [22] propose MERLOT, a neural script knowledge model that can generate scripts from videos. Recently [15] introduce a generative script learning model that generates scripts conditioned on the given instructions. Different from previous work, we focus on the non-linear setting of script induction and propose to ground instructional videos to wikiHow scripts.

 propose a method to ground videos to scripts, which is similar in spirit to our work, but differs in several important aspects. First, our method is different from theirs in the following aspects: (1) Our method is not limited to a specific type of scripts (e.g., _goal-oriented scripts_). Instead, it can be applied to a wide range of scripts. (2) Our approach is different in the sense that we ground videos with wikiHow steps, while theirs is based on images. (3) Our goal is to learn a model capable of both generating scripts and predicting the next steps in the scripts, whereas theirs is to"," Script induction.Script induction has been studied extensively in the past decades, with a variety of tasks proposed, such as narrative cloze induction ([4]; [7]; [1]; [2]; [6]; [5]; [9]; [10]; [11]; [12]; [8]; [3]; [4]; ; [11]), event prediction ([3]; ; ; [10]), and script generation ([8]; ; ). However, these tasks are typically framed as sequential generation tasks, which do not reflect the flexibility displayed by people executing tasks in real life.

Non-Sequential Script Induction.There has been a line of work on the induction of non-sequential script chains from unstructured text. [2] propose the goal-oriented script construction task, where a model produces a sequence of steps to accomplish a given goal. However, their task is limited to a single step per script, and the steps are presented in a linear manner, which is not consistent with real-world scenarios. [5] propose a schema induction task for event coherence induction, where the goal is to select salient event paths and fill them with salient event types. Their task is also limited to event types, and their schema is presented in linear form. [1] propose to use a language model to induce a script as language modeling task, which can be productively reframed as a language-based language modeling problem. [4] propose unsupervised induction of narrative event chains from raw newswire text, where they induce narrative event chain by learning narrative relations between events sharing corefringing arguments. [7] propose an unsupervisional learning algorithm to learn narrative event structure and argument roles. [6] introduce a model that includes participant information (i.e., knowledge about which participants are relevant for a script) and show, on the Dinners from Hell corpus as well as the InScript corpus, that this knowledge helps to improve prediction performance on the event cloze task [6].

Script generation.There have been a number of works on the generation of procedural scripts from images [13]; [14]; [15]; [16]; [17]; [18]; [19]; [21]; [20]; [22]; [13]. However, they are limited to generating a single script for a given task. [15] propose Multimodal Script Generation (MSPG) task, aiming to generate a script for each step of a task from a video.","[ **Natural Language Processing.** Most previous work for non-sequential graph script induction can be divided into two groups. The first group (e.g. [5]; [1]) leverages language models (LMs) to conduct semantic retrieval, which explores and discovers induced graphs from various news articles or wikiHow articles. [1] shows LM can be used as a language modeling task and then improve the previous result by an average of 2.7 percentage points. For generating the script, [5] mainly relies on two models to detect and track entities to induce graphs, and [2] is the first work that introduce goal-oriented script construction, which uses the retrieval model to find the relevant steps from the candidate graph for each given goal. However, all these methods rely on the assumption of fixed ordering, thus fail to capture real-life non-sequential script. The second group (e.g. [3]; [4]; ; [6]; [9]; [7]; [8]; [10]; [11]; [12]) mainly focuses on learning the dependency between consecutive event in the scripts, without considering their orders and interchanges. For example, [8] proposes to use GPT  to generate the script with multitask learning, which selects the appropriate start and end tokens and uses a discriminator to distinguish between sequences that are script and non-script. However, due to the lack of the inter-order information, all these methods fail to capture non-sequential graph structure.

**Multimedia Procedural Knowledge.** Learning visual representations for procedural planning has been well studied in vision (; ; ) and speech domain (). In order to learn reliable representations, prior work focuses on either combining text and visual representations (; [16]; [18]; [14]; [17]), or relying on the multimodal interactions in the videos ([13]; [15]). One major limitation of the above methods is the human-labeled goals and steps. In addition, [14]; [17] utilizes rich, real-world knowledge in the form of both images and natural language, however, their main goal is zero-shot anticipation, which is different from what we focus on in this work. Our task, non-sequential graph script induction, is arguably the first comprehensive task in the combined vision and language modalities. Our main innovation is to design an effective framework to implicitly learn the non-sequential graph structure, without relying on prior knowledge or hand-engineering.

**Text-Only Script Induction.** Unlike previous work, we focus on graph script induction in both vision and language modalities. In this paper, we only evaluate on text-only script induction, since the whole process of non-sequential graph script induction is very challenging and requires explicit representation of entities. Here we discuss existing works that are related to our task but different in that we do not explicitly learn temporal structure]","<In the realm of procedural understanding and script induction, previous research has primarily focused on sequential representations of tasks and scripts. Most existing work in this domain has predominantly relied on textual data or sequential action sequences for script induction and task understanding. For instance, [1] explored the extraction of procedural knowledge from textual sources such as online instruction manuals and how-to guides, while [2] focused on sequential action recognition in videos to understand task structures. However, these approaches do not capture the optional and interchangeable steps that characterize non-sequential task execution, as highlighted in the target paper. This limitation underscores the need for novel methods that can handle the non-sequential nature of task scripts and leverage multimodal data for enhanced understanding.>

<Another relevant line of research is focused on multimodal learning, particularly in the context of aligning textual instructions with video demonstrations. Prior studies have explored the alignment of textual descriptions or natural language instructions with corresponding visual demonstrations, aiming to bridge the gap between textual and visual modalities in task understanding [3]. These works typically focus on aligning and associating textual and visual data to enable machines to understand and execute tasks. While this research aligns with the multimodal grounding proposed in the target paper, it primarily deals with synchronous alignment and may not explicitly address the non-sequential nature of task scripts and step ordering.>

<In recent years, the field of graph-based learning and reasoning has witnessed significant advancements in capturing complex relationships and structures in various domains. Graph neural networks (GNNs) [4] have gained prominence for their capability to model dependencies in graph-structured data. These methods have been applied to diverse tasks such as social network analysis, recommendation systems, and language modeling. However, their application to non-sequential script induction and task understanding, as proposed in the target paper, remains relatively unexplored, highlighting an opportunity for leveraging graph-based representations in the context of procedural learning.>

<Additionally, the intersection of procedural understanding with video understanding and action recognition has been a topic of interest in the computer vision community. Efforts have been made to analyze and recognize complex activities and tasks from video data, with a focus on understanding the temporal and structural aspects of actions. Work in video understanding and action recognition [5] has predominantly emphasized sequential action recognition and temporal modeling, with less attention given to the non-sequential nature of task execution and the flexible ordering of steps. This gap presents an exciting opportunity for the integration of non-sequential graph-based representations with video understanding approaches to better capture the complexities of real-life task execution, as proposed in the target paper.>

<Furthermore, the field of transfer learning and knowledge transfer in multimodal settings holds relevance to the task proposed in the target paper. Prior studies have investigated the transfer of knowledge between textual and visual modalities, aiming to leverage pre-trained representations and domain knowledge for improved task understanding and multimodal alignment [6]. These approaches can potentially provide valuable insights into the transfer of script knowledge between textual instructions and procedural videos, a fundamental aspect of the proposed non-sequential graph script induction task. Leveraging transfer learning paradigms in the context of non-sequential task understanding could offer significant improvements in model generalization and performance.>"
2687," **3D Human and Object Pose Estimation.** 3D human pose estimation from a single RGB image has been extensively studied in the past few years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24]. Most of these works focus on reconstructing the 3D body pose and shape of a single person from a monocular RGB image. For example, Bogo _et al_. [1] proposed SMPL [1], a parametric model of human body shape and pose that can be used to reconstruct 3D humans from single RGB images. Based on SMPL, many works have been proposed to fit the SMPL model to a sequence of images [2][4] or videos [3][5]. More recently, several works [7][5] have used deep neural networks to directly regress the parameters of SMPL from monocular videos. However, these methods do not consider the interactions between the human and object.

**Human-Object Interaction Reconstruction.** There has been a growing interest in understanding the interaction between humans and objects [25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52]. For instance, [26][26] and [27] proposed to use force-based models to model the interactions. [28] proposed a neural network to predict the grasp affordances of hands and objects. [29][29] used a contact potential field to model interactions between objects and humans. [32] proposed an object-object correspondence method to predict 3D bounding box correspondences between the object and the predicted 3D pose. [31] used an object detector [13] to detect 3D object bounding boxes and then used a graph convolutional network to recover the object pose from the detected boxes. [34] used human pose priors to resolve the ambiguities in human-object pose estimation. [37] used SMPLM [36] to jointly estimate 3D poses of the human, objects, and scenes. [38] used 3D scene meshes and SMPL to reconstruct the scene meshes. [39] proposed human-aware scene generation methods to generate 3D scenes with a human in the scene. [40] used the human pose and the scene geometry to generate a 3D model of the environment. [41] proposed MoCapDeform to deform the human body to match the observed 2D poses. [42] proposed BEHAVE, a large-scale dataset for human-human interaction tracking. [44] proposed COUCH, a method to track the human-chair interaction in a video sequence. [45] proposed Action-Conditioned 3D Reconstruction (AC3D) method to reconstruct human, object, and contacts in 3D. [46] proposed CHORE,"," **3D Human and Object Reconstruction from Single Image.** Human body models [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59].

**Human-Object Interaction Tracking.** Most of the existing works focus on reconstructing the 3D human and object from a single RGB image. For example, [1] reconstructs the human body using the SMPL model [1], and [2][4] reconstruct the human, face, and body from a monocular RGB image using SMPL models. [4] also reconstructs 3D hands, face and body, and [4][6] estimate the pose and shape of the human and the object from RGB images. [6] use a head-mounted device integrated with a LiDAR and camera to record human subjects' activities over 10 diverse urban scenes from an egocentric view. However, these methods do not consider the occlusion problem and are not robust to heavy occlusions. [7] use the VIBE dataset [7], a large-scale motion capture dataset, to infer the human-object interaction. However the human motion is not consistent across frames. [5] use an object detector to estimate the relative translation between the human pose and the 3d object pose, and then use this relative translation to reconstruct the object pose. However this method is not robust when the object is occluded.

Our work is different from [7][5] in that we explicitly use the visibility of the object and the human to make predictions for occluding frames. We use a novel transformer-based neural network to leverage information from the visible and occlusive frames to make the predictions for the invisible frames.

","[ We review the relevant work in two categories: (i) reconstructing 3D humans from single images and (ii) inferring human-object interactions. We first review the relevant prior work for _Reconstructing 3D Humans_, and then _Inferring Human-Object Interactions_.

**Reconstructing 3D Humans.** Previous works reconstruct 3D human body [1][2][3][4][5][6][7][8] and 3D human pose [9][10][11][12]. 3D human reconstruction from a single image is the most challenging, because it is affected by severe occlusion and depends on a suitable texture. Neural network based methods [3][4][5][13] often discretize the dense human mesh and the error accumulates with mesh resolution, resulting in a drop in accuracy.

A key improvement for reconstructing 3D humans is to use prior information in 3D to guide reconstruction. Two of the most prevalent approaches are shape-based reconstruction  and depth-based reconstruction [14][15][16]. Shape-based methods [17][18][19][20][21] assume a pre-calibrated camera model with a depth image, and reconstruct human mesh assuming humans move in the scene independent of the surface (e.g., a flat or a textured surface). On the other hand, depth-based methods use RANSAC  to fit 3D SMPL human body model [1][2][6][22] to an estimated depth image. They generate 3D meshes that are constrained to fit SMPL surfaces. However, they do not have robustness against object occlusion as they rely on reconstructed depth.

Leveraging 3D object geometry is another approach for reconstructing 3D humans [23][24]. GanHand [25] uses a depth map to perform grasp manipulation from a single view. However, it uses depth maps obtained from RGBD cameras and does not address the problem of handling occlusion. TP-AE [22] estimates depth from image-based volumetric rendering, and directly regresses the depth from an RGB image to a 3D human pose. TP-AE is model-free and does not use a known model like SMPL, but it is computationally expensive, particularly when the input is a single image.

The methods discussed above focus on reconstructing a single 3D human body. However, we also require recovering contact with the object. In [26], an effect simulator uses a physics engine to simulate the forces on the object, and the method recovers the contact point given the output. This approach can recover contacts only when the objects are pushed by the hand. In [27], the authors use a pretrained geometry and]","<The field of 3D human-object interaction tracking has seen significant advancements in recent years, with a focus on reconstructing interactions between humans and objects from single RGB images. There has been a growing interest in leveraging neural networks and deep learning techniques to address the challenges posed by occlusions and dynamic scenes. For instance, VIBE [7] proposes a method for accurate and natural motion sequence estimation by leveraging an adversarial learning framework with a large-scale motion capture dataset. Similarly, GDR-Net [16] focuses on direct and indirect methods for 6D pose estimation, demonstrating an in-depth investigation into both approaches. Additionally, HuMoR [10] introduces a 3D human motion model for robust estimation of temporal pose and shape, emphasizing the need for robust estimation in the presence of noise and occlusions.

Moreover, there have been efforts to address the issue of estimating 3D human pose and shape in the context of interactions with the surrounding environment. This includes methods such as MIME [39], which aims to generate realistic 3D worlds based on human motion as input, and BEHAVE [42], which contributes a dataset for human-object interaction tracking with multi-view RGBD frames and 3D SMPL and object fits. These initiatives reflect the increasing interest in understanding and synthesizing human-object interactions to enable applications in augmented reality, robotics, and behavior analysis.

In addition to 3D human-object interaction tracking, there are also relevant works in related areas such as 3D human motion capture, pose estimation, and contact modeling. For example, HULC [37] focuses on marker-less monocular 3D human motion capture in the presence of scene interactions, considering surface contacts and absolute scale estimation. Furthermore, methods like CHORE [46] and SPEC [49] tackle the challenge of reconstructing human shape and pose from single RGB images, aiming to overcome occlusions and estimating the camera parameters for more accurate 3D reconstruction.

Overall, the advancements in 3D human-object interaction tracking from single RGB camera inputs have been driven by a combination of neural network-based approaches, dataset contributions, and innovative methodologies for addressing occlusions, dynamic scenes, and accurate 3D reconstruction. These developments collectively contribute to the broader goal of understanding, synthesizing, and modeling human interactions with the environment for various applications in robotics, computer vision, and graphics.>"
2483," **Video frame interpolation.** Early VFI methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] are mainly based on 2D convolutional neural networks (CNNs). They first estimate optical flow and warp the input frames to the target frame, and then synthesize the interpolated content by warping the warped frames. For example, Niklaus _et al_. [4] proposed a kernel-based method, which learns a spatio-temporal convolution kernel for each pixel in the input frame and interpolates the output frame by using the kernel weights of the previous frame as the new interpolated frame. Subsequent works [5][1][16] extended this method by using deformable convolutions [18] to model the spatially-varying kernel weights. However, these methods are computationally expensive due to the large number of kernel parameters and the need to compute the kernels for all pairs of pixels. To reduce the computational cost, some methods [6][13] adopted intermediate feature refinement [19][20][21] and softmax splatting [22][23] to reduce the number of kernels. Recently, inspired by the success of Transformer-based models [24][25] in natural language processing (NLP) tasks, several VFI works [9][26][27][28][29][30][31][32][33][34] adopted the Transformer architecture for VFI and achieved state-of-the-art performance. For instance, VFI-Transformer [9] proposed an end-to-end trainable VFI model that uses a Transformer encoder-decoder architecture to synthesize intermediate frames.

**Correlation volumes.** Correlation volumes have been widely used in optical flow estimation [35][36][37][38][39][40] and VFI [41][42][43]. In particular, the bilateral correlation volumes [35] are widely used to estimate the optical flow between two consecutive frames, and the cost volumes [36][39] are used to calculate the correlation scores between the predicted bilateral flows and the input pixels. For VFI, the correlation volumes can be used to compute optical flow fields [40][41][43] or to warp the inputs to the output frames [32][34]. However, the computation cost of these correlation volumes grows quadratically with the resolution of the input images, which makes them unsuitable for high-resolution VFI. In contrast, our proposed AMT uses bidirectional correlation volumes to compute correlation scores for each pair of pixels, which is more efficient than the bilateral volumes [40].

 proposed a multi-scale correlation volume (MSC-Net) to estimate optical flows and perform backward warping between the input and output frames. It is based on the idea that the predicted optical flow can be regarded as a coarse flow field and the predicted correlation fields can be viewed as a fine-grained flow field. In this work, we extend the MSC-"," **Video frame interpolation.** Existing frame-interpolation methods can be categorized into kernel-based [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43].

**Kernel-based Methods.** Kernel-based methods [4][1][5] estimate spatially-adaptive local kernels for each pixel to synthesize the output frame. These methods are based on the assumption that the motion between two input frames is smooth and uniform, and the kernel weights are spatially invariant. However, these methods are sensitive to large motions and occlusion. To handle these challenges, some methods [5][8] use a spatially adaptive kernel to model the motion of the input frames. For example, Wang _et al_. [4] propose to use a local separable convolution [5] to estimate the kernel weight for every pixel in each input frame, and then use the kernels to warp the input frame to the target frame. However this method is time-consuming due to the large kernel size. To address this problem, Wang [8] propose a flow-free approach that is completely end-to-end trainable for multi-frame video interpolation, and achieve up to 6\({}^{6}\) speed up compared to the previous methods.


\(\(\tau\rightarrow\)FlowNet\(\rightarrow\), which is based on a bidirectional optical flow estimation network, and warps the input images to the output frames by warping the predicted bilateral flow fields. However the warping network is not differentiable, which makes it difficult to train. To overcome this limitation, some recent methods [10][6] use optical flow to guide the synthesis of the intermediate frame. For instance, Zhang [10] propose an adaptive motion compensation network to estimate optical flow fields and use them to guide frame synthesis. Wang [6] introduce a multi-scale shared structure that consists of two cascaded modules for bidirectionally optical flow learning between input frames, and for bi-directional flow learning from the target to input frames (BiOF-I) and bidirection","[ **Frame Interpolation.** There has been an increasing interest in exploiting deep neural networks for image processing and computer vision tasks. Numerous existing deep learning-based methods [1][2][3][4][5][6][7] were proposed for video frame interpolation. Some algorithms [1][2][4][5][8][9][6][10][11] use coarse-to-fine frameworks with carefully designed encoder-decoder architectures or features to produce high-resolution results. Other methods [3][7][12][13][14] are inspired by optical flow prediction models to update pixels along flow directions, while some works [15][16] explore to perform the frame interpolation process in a non-local and context-aware manner. Besides these models, one recent work [17] is proposed to use an extremely large model with carefully designed compression techniques for high resolution.

Inspired by the success of CNN models [18][19][20][21] in image super-resolution and optical flow prediction, many efforts have been made in proposing ConvNets-based models for video frame interpolation tasks. Recently, the authors of [22] use a deformable ConvNet to perform real-time 2-frame interpolation and take forward/backward warping into consideration. After that, the adaptive convolution technique [23][24] was used for frame interpolation tasks [25]. Moreover, the authors of [26] propose to use attention mechanisms [27] and multi-scale features [28] for the task. Some works [29][30] seek to fuse feature representations from multiple frames to guide motion estimation and deblurring. Some other works [31][32][33][34] resort to optical flow [35][36] as intermediate results to perform video tasks. Among them, the authors of [27][37] perform 2-frame interpolation and jointly train Transformers for spatial and temporal feature alignment. Based on this work, the authors of  aim to estimate large-scale features for high-resolution tasks with additional attention mechanisms. [38] propose a residual flow-based refinement network to handle occlusion cases with an intermediate appearance prior. Note that [39] and [40] aim to construct a cost volume from the RGB input frames and then perform a multi-frame optical flow estimation to support optical flow-based tasks.

Besides the use of ConvNets, the authors of [41][42] use pyramid-based [41][43] and hand-crafted [42] depth-normalized motion estimation modules for single-frame video frame interpolation.

However, our method is different from all the aforementioned works due to several aspects: (1) we design an end-to-end model that builds bidirectional correlation]","<Video frame interpolation is a challenging and important task in the field of video processing, and various deep learning methods have been proposed to address this problem. Traditional methods such as kernel-based interpolation [1] and flow-based methods [4] have limitations in handling large motions and occlusions. To address these limitations, recent approaches have explored novel techniques such as deformable separable convolution [2], adaptive collaboration of flows [3], and adaptive separable convolution [5]. These methods aim to improve motion estimation and pixel synthesis in video frame interpolation, leading to more accurate and efficient results. Furthermore, some works have explored the use of deep learning architectures such as IM-Net [6], FISR [7], and FLAVR [8] to enhance the quality of video frame interpolation and super-resolution. These architectures leverage multi-scale losses and flow-agnostic representations to achieve superior performance compared to prior methods.>

<In addition to traditional convolution-based models, recent advancements in video frame interpolation have also explored the use of Transformer-based models [9]. These Transformer-based frameworks enable content-aware aggregation weights and consider long-range dependencies, leading to significant improvements in performance and quality. Furthermore, the integration of motion estimation and compensation with neural networks has been investigated in MEMC-Net [10], which leverages adaptive warping to synthesize target frame pixels. Moreover, the XVFI-Net [11] introduces a recursive multi-scale shared structure for bidirectional optical flow learning, demonstrating the significance of handling extreme motion in video frame interpolation.>

<Depth-aware video frame interpolation has also been addressed in specific works such as depth-aware flow projection layer [12] and intermediate feature refine network [13]. These approaches explicitly consider depth information to synthesize intermediate flows for handling large object motions and occlusions, resulting in compact, efficient, and fully differentiable models. Furthermore, there have been efforts to achieve efficient and real-time video frame interpolation, as demonstrated by IFRNet [13], which focuses on designing an efficient encoder-decoder network for fast intermediate frame synthesis.>

<Another line of research has focused on addressing the challenges of video frame interpolation in a more generic framework, such as the use of deep voxel flow [14] and context-aware synthesis [15]. The deep voxel flow method presents a technique for synthesizing video frames using deep networks to flow pixel values from existing frames, while the context-aware synthesis approach incorporates per-pixel contextual information to interpolate a high-quality intermediate frame. These methods exhibit significant potential in addressing the complexities of video frame interpolation across various applications and scenarios.>

<Furthermore, there have been efforts to address specific limitations of existing methods, such as the quadratic video interpolation method [16] that exploits the acceleration information in videos, and the compression-driven network design for frame interpolation [17], which leverages model pruning and multi-resolution warping to achieve superior performance with reduced model size. Additionally, recent research has explored new architectures, such as Swin Transformer [24], for serving as a general-purpose backbone for computer vision tasks, including video frame interpolation.>

<In summary, the related work in video frame interpolation encompasses a diverse range of techniques and methodologies, including deformable separable convolution, adaptive collaboration of flows, depth-aware interpolation, efficient encoder-decoder networks, and novel architectural designs. These approaches have collectively advanced the state-of-the-art in video frame interpolation, enabling improved motion estimation, high-quality synthesis, and efficient real-time performance. Collectively, these advancements provide a comprehensive understanding of the challenges and progress in the field of video frame interpolation, laying a strong foundation for future research and development in this domain.>"
3909," In this section, we briefly review existing debiasing methods for word embeddings, sentence representations, and PLMs.

**Word Embedding Debiasing.** Word embedding models are widely used in various NLP tasks, such as sentiment analysis, natural language inference, and text classification. [7] and [4] showed that word representations encode undesirable social biases, which can lead to biased predictions in downstream tasks. To mitigate these biases, [5] and  proposed to use adversarial training to debias word representations. [1] proposed a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal subspaces. [2] proposed to mitigate bias in classifier training by incorporating contrastive learning, in which instances sharing the same class label are encouraged have similar representations and instances from different class labels are pushed apart. [3] proposed an approach to learn fair representations that satisfy the notion of fairness known as equalized odds (EOR).  proposed a method to mitigate social biases in pre-trained language models by using a contrastive objective to learn unbiased representations from unlabeled data. [6] proposed MABEL, an intermediate pre-training approach for mitigating gender bias in contextualized representations.  proposed an attention-based method to reduce the influence of biased words in PLMs' representation.  presented a method for mitigating bias in PLM' representations by using an attention mechanism to selectively focus on important words.  introduced a method that leverages the contextual information of the input sentence to mitigate the bias in the PLM's representation. [8] proposed FairFil, which leverages contrastive training to mitigate biases in the representations learned by BERT.

 proposed a data augmentation method called Counterfactual Data Augmentation (CDA) to balance the training corpus. CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. In this paper, we propose a continuous prompt-based data augmentation method based on CDA to push the representation distances between sample pairs along different demographic group to a wider range, which affects the debiased model's performance with limited text resources.

 presented an approach that uses a generative adversarial network (GAN) () to generate counterfactual examples. In their approach, a discriminator is trained to distinguish between real and fake examples generated by the generator. In our approach, we use a similar approach, but we use continuous prompt tuning instead of GAN to generate more diverse and diverse examples.

 introduced an approach based on contrastive loss to learn a fair representation. They proposed a two-stage approach that first pre-trains a language model on a large-scale dataset and then fine-tunes the model on the downstream task. In contrast, our approach uses contrastive losses to learn fairer representations in the first stage, and then uses the learned representations to guide the second-stage fine-tuning in the second stage.

 recently proposed a new method called FairNLP, which uses a novel objective function based on InfoNCE to"," **Debiasing for PLMs.** Most existing debiasing methods are based on the adversarial training paradigm, which aims to learn fairer representations by training a classifier with adversarial examples. For example, [1] proposed to use multiple diverse discriminators to learn orthogonal hidden representations from one another. [2] proposed a method for mitigating bias in classifier training by incorporating contrastive learning, in which instances sharing the same class label are encouraged to have similar representations while instances sharing a protected attribute are forced further apart. [3] proposed conditional supervised contrastive objectives to learn fair representations for text classification. [4] proposed OSCaR to remove bias in word embeddings by linear projection and rectification. [5] investigated the presence of social biases in sentence-level representations and proposed a new method, Sent-Debias, to reduce these biases. [6] proposed MABEL to mitigate gender bias in contextualized representations using entailment labels from NLI datasets.

**Prompt Tuning for Debiasing.** Prompt tuning has been widely used to mitigate bias in pre-trained language models. [7] proposed GloVe to reduce the bias in static word embedding by using the NLI dataset. [8] proposed FairFil to mitigate social bias in PLMs' representations by using a fair filter network to filter out the bias words.  proposed a two-stage method to mitigate the bias of PLMs by fine-tuning the model with a pre-debiased dataset.

","[ Previous works mainly adopted CDA to modify the training data. [2] used these modified pairs as the target in contrastive learning, aiming to maximize the difference of the corresponding representations from different demographic groups.  further proposed an orthogonal transformation to counteract the additive nature of CDA. [1] applied the concept of orthogonal adversary learning to mitigate biases.  proposed a training procedure which uses class balanced label sampling and data augmentation methods for large scale biases debiasing. The authors in [3] made the assumption that the features that satisfy fairness constraints can be obtained from a conditional supervised contrastive (CSC) framework.

Prior work on debiasing PLMs usually design different pre-trained models and tuning strategies for bias reduction.  built an in-context debiasing model by modifying BERT and embed the steps to mitigate biases.  proposed bias-weighted data augmentation to remove biased word embeddings with Transformer encoders.  and  designed a class-balanced two-stage fine-tuning to overcome PLMs' bias at sentence level. [5] proposed Sent-Debias that removed the sentences with biased embeddings and then fine-tuned PLMs on a large-scale annotated sentence-level biases data.  also fine-tuned PLMs by class-balanced sentence-level supervision. [4] proposed a plug-and-play debiasing method to fine-tune PLMs by regularizing the output embeddings to be orthogonal to a ""proper"" embedding space. [6] and [7] designed an intermediate pre-training approach by fine-tuning PLMs with debiasing data augmented by NLI tasks. [8] and  fine-tuned PLMs by adding specific layers on top of pre-trained models.

]","<In recent years, addressing social biases in pre-trained language models (PLMs) has gained significant attention in the natural language processing community. Most previous debiasing techniques have focused on using counterfactual data augmentation (CDA) to mitigate biases in PLMs [1]. CDA aims to balance the training corpus by slightly modifying the original corpus to generate counterfactual pairs. While this approach has shown promising results in reducing biases, it has limitations in terms of the representation distance between different demographic groups, ultimately affecting the debiasing performance with limited text resources. As such, there has been a growing need for more advanced debiasing techniques that can effectively address the limitations of CDA.

Adversarial training-inspired approaches have garnered interest in mitigating social biases in PLMs. These approaches leverage techniques such as contrastive learning and continuous prompt augmentation to improve the representation distance between different demographic groups [2]. Contrastive learning, in particular, has shown promise in pulling closer the representation distance between augmented sample pairs, thus facilitating the fine-tuning of PLMs' parameters to achieve debiased encodings. Additionally, continuous prompt augmentation has been proposed as a method to push farther the representation distance between sample pairs along different demographic groups, thereby enhancing the overall debiasing performance of PLMs [3].

Furthermore, the notion of adding difficulty to the training process to guide the model towards achieving stronger debiasing performance has been explored. This involves introducing adversarial constraints during the training of PLMs, which has shown potential in improving their debiasing capabilities [4]. By incorporating difficulty into the training process, models are encouraged to learn more robust and unbiased representations, ultimately enhancing their performance in mitigating social biases.

In summary, the landscape of debiasing techniques for PLMs has evolved from traditional methods like counterfactual data augmentation to more advanced approaches inspired by adversarial training, contrastive learning, and continuous prompt augmentation. These techniques aim to address the limitations of existing methods and push the boundaries of debiasing performance, thereby contributing to the development of more equitable and fair language models.>"
1549," **Evaluation of NLG.** There has been a growing interest in evaluating NLG systems in recent years. For example, [4] surveyed human evaluation of creative NLG, and [5] proposed a dialect-robust NLG evaluation metric. [1] and [6] analyzed BERTScore for machine translation, and found that it correlates poorly with human judgements. [3] analyzed the interpretability of BERT-based NLG metrics, and showed that they correlate well with human judgments. [2] investigated the robustness of different model-based evaluation metrics for text generation, and they found that the embedding space is more robust than the representation space.

**Robustness analysis of NLU and NLI.** Recently, there has been an increasing interest in robustness analysis for NLU tasks. [9] proposed the Robustness Gym, a framework for evaluating NLU models in a variety of tasks, including NLI ([7]), machine reading comprehension ([8]), question answering ([12]), and NLG ([11]). [10] proposed XTREME-R, a multilingual NLI evaluation framework for multilingual evaluation. [13] showed that NLI models often rely on syntactic heuristics to score well on a given test set. [11] proposed CheckList, a checklist-based approach for behavioral testing of NLP models. [7] proposed stress tests for NLI to evaluate the NLI model's robustness. [8] assessed the capacity of MRCNN-based models for reading comprehension by creating synthetic datasets and evaluating their robustness on them.  proposed a new NLI datasets and metrics based on perplexity and BLEU, and evaluated them on a range of NLI tasks. In this work, we focus on NLG tasks, and propose a new robustness methodology for NLG based on synthetic data generation. We show that existing NLI metrics are not robust to truncation errors, and we propose a robustness metric based on masked language model embeddings, which can be applied to a wide range of generation tasks. We also provide insights into the reasons behind these blind spots, and suggest practical workarounds for a more reliable evaluation of text generation. Our work is also related to recent work on text degeneration ([15]; [14]) and out-of-order errors ([16]; [17]).

 proposed a framework to evaluate NLI and NLQA models by generating synthetic NLI data and comparing them to human-annotated test sets. However, it is not clear how to apply it to NLG since NLI is a different task. In contrast, our work focuses on the evaluation of the NLG models.

 proposed an NLI stress test for evaluating the NLU model's ability to generalize to unseen test cases. They designed a set of synthetically generated NLI examples to test the model's generalization ability to new test cases that are not seen in the training set. They also proposed a method for measuring the generalizability of a model to a new test set by comparing its performance on the test set"," **Evaluation of Text Generation Metrics.** Evaluation of text generation evaluation metrics has been a long-standing problem in NLP ([4]; [1]; [3]; [2]; [6]; [5]; ; [3]). [1] show that current metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy. [2] show the robustness of BERTScore, one of the most popular embedding-based evaluation metrics for text generation, by comparing its correlation with human evaluations on standard benchmarks. [3] show BERT-based metrics are not robust to lexical overlap errors, and that the different metrics capture all aspects to some extent, but that they do not all capture the same aspects. [5] show metric scores are not dialect-robust, preferring outputs that are semantically incorrect over outputs that match the semantics of the reference but contain dialect differences. [6] find that BERT score is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.


**Robustness Evaluation.** Robustness analysis of NLP systems has been an active area of research in the NLP community. [7] and [8] evaluate the robustity of NLI systems by checking whether questions can be solved even after removing features associated with a skill requisite for language understanding. [9] evaluate robustness by comparing the performance of different NLP models on a variety of unsupervised and semi-supervised tasks. [10] evaluate models on the XTREME-R benchmark, a multilingual version of the XTreME benchmark, and [11] evaluate NLP performance on the CheckList task. [12] evaluate consistency of question-answering models by comparing them to human-generated implications. [13] study the use of fallible syntactic heuristics in NLI, and find that models trained on MNLI perform poorly on HANS. [15] show neural text degeneration persists when these models are tested as text generators. [16] find 75% to 90% of the correct predictions remain constant after input words are randomly shuffled. [17] find significant differences in compositional and lexical semantics in RoBERTa, BERT and DistilBERT models. [14] show how to control the focus of pretrained language generation models by training them to attend to relevant parts of","[ Improvement on Human BenchmarksThe first phase of natural language generation (NLG) research aimed to mimic humans' writing capability. These days, a shift of focus has been made to enable a more direct comparison between human and artificial generators. To facilitate such an evaluation, gold text corpora have been developed with annotation of its semantic quality [4]; , and automated metrics have been introduced to evaluate the generation systems [10]; [14]; ; [12]; ; [11]; [13]; [8]. Most recently, BERT-based metrics are popular in the NLG community as they provide comparable results with human evaluation (; [5]; [6]).

Besides developing and evaluating automated metrics, there is a growing interest in stress tests for language models. For example, [17]; [16] studied the impacts of shuffling input words; [15] experimented on the impacts of a different training objective; [7]; [2]; [3] investigated the robustness of language models to out-of-vocabulary words. The latest work [9]; [1]; [10]; [18]; [11];  evaluated BERT-based generation metrics' robustness across multiple domains and datasets. We are the first to compare different generation metrics' robustness with respect to errors of different sizes. Our work is complementary to these efforts and can inform how to evaluate these metrics.

]","<The evaluation of text generation has been a topic of significant interest in natural language processing (NLP) and machine learning research. A key aspect of evaluation metrics is their robustness and ability to accurately capture the quality of text generated by various models. Prior work has focused on developing evaluation metrics based on pretrained language models, such as BERT and GPT-2, for tasks including open-ended generation, translation, and summarization. These metrics have been widely used to assess the performance of text generation models. However, the study by [1] highlights the blind spots and limitations of existing model-based evaluation metrics, shedding light on the insensitivities, biases, and loopholes in these metrics that can affect their reliability in evaluating text generation.>

<Previous research has primarily focused on the development of evaluation metrics but has paid less attention to their potential blind spots and insensitivities to certain types of errors in generated text. The work by [1] addresses this gap by proposing stress tests with synthetic data as a methodology for robustness analysis of text generation evaluation metrics. Utilizing stress tests enables the identification of specific errors that may not be adequately captured by existing metrics, providing insights into their blind spots and shortcomings. By examining the responses of different metrics to a wide range of potential errors, the study brings to light the limitations and insensitivities of metrics like BERTScore and MAUVE in handling errors related to summarization and generation, respectively.>

<The findings of [1] underscore the importance of identifying and understanding blind spots in model-based evaluation metrics for text generation. This work contributes to the broader conversation on the reliability and transparency of NLP evaluation metrics, providing a valuable perspective on the potential limitations of existing approaches. The insights gained from stress tests with synthetic data offer valuable guidance for researchers and practitioners working on text generation evaluation, prompting a reevaluation of the performance of existing metrics and the development of more robust and comprehensive evaluation methodologies.>

<In addition to identifying blind spots, the study by [1] delves into the reasons behind these insensitivities and biases in existing evaluation metrics. By investigating the causes of the identified blind spots, the research offers a deeper understanding of the underlying mechanisms that influence the performance of these metrics. This analysis lays the groundwork for devising practical workarounds and improvements to enhance the reliability and accuracy of text generation evaluation. Moreover, the release of code and data by the authors [1] further facilitates transparency and reproducibility in the evaluation of text generation models, fostering open and collaborative research in the NLP community.>

<Overall, the work on blind spots of model-based evaluation metrics for text generation by [1] represents a significant contribution to the field, emphasizing the need for thorough analysis and stress testing of evaluation metrics to uncover their blind spots and limitations. The study encourages future research to take into account the potential insensitivities and biases of existing evaluation methodologies and develop more robust and comprehensive approaches for evaluating the quality of generated text in NLP applications. By shedding light on the blind spots and insensitivities of model-based evaluation metrics, this research paves the way for more reliable and thorough evaluation practices in the field of text generation.>"
878," **Imitation Learning for Autonomous Driving.** The seminal work of [1] demonstrated that a neural network can learn to drive by imitating a human driver. [2][3][4] extended this idea to the setting of end-to-end driving, where the neural network is trained to directly output control commands from raw sensor data. These approaches have achieved impressive results on the Longest6 benchmark [5]. However, recent work has shown that these approaches can suffer from poor generalization [6][7][8][9][10][11][12][13][14][15][16][17].

**Trajectory-Guided Control.** [18] proposed to use a trajectory-guided controller (TRC) to predict the next waypoint given the current one. TRC is trained by minimizing the KL divergence between the predicted TRC trajectory and the ground truth. [19] proposed a data aggregation method to further improve the performance of TRC. [7] proposed an attention-based encoder-decoder architecture [20] to improve the robustness of the TRC decoder. [21][22][23][24] proposed new architectures to improve interpretability and robustness. However, none of these works have systematically studied the underlying biases of these models.

 proposed a method to improve robustness and interpretability of the learned driving policy. Their method is similar to ours in that it also uses a longitudinal averaging of multimodal waypoint predictions for slowing down. Our work differs from theirs in two important ways: (1) we focus on the effect of the lateral recovery bias, which is critical for the observed progress on CARLA; and (2) we propose a principled method to remove this bias from the learned policy.

 introduced a method for improving robustness to ego-vehicle ego-collisions. They use a multi-task learning approach to learn a robust steering angle estimator. Our method differs in that we do not rely on an external odometry estimator; instead, we use the predicted waypoints as input to the decoder, which allows us to directly optimize the steering angle.

 propose a method that improves robustness by using an adversarial loss to encourage the model to learn to follow the correct steering angle rather than the wrong one. While their method is also based on the longitudinal averaging strategy, it differs from ours in several important ways. First, our method does not use adversarial training. Second, their method uses a reinforcement learning approach, whereas our method is model-free. Third, we demonstrate that our method outperforms their method on the longest6 and LAV benchmarks, while their method only outperforms it on the shortest6. Finally, we show that their method requires a large amount of training data, while our method only requires a few hours of driving data.

 recently proposed a model-based approach for driving. Their model is based on imitation learning, and they show that it is possible to train a model to imitate the behavior of a human demonstrator. In contrast to our work, their model does not address the issue of lateral recovery."," **End-to-end driving systems.** End-to end driving systems have recently made rapid progress [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24]. These systems are typically built upon imitation learning [4][3], reinforcement learning [13][6], or model-free reinforcement learning methods [12][14]. In contrast to these methods, we focus on end-to -end driving methods that do not rely on any additional sensors or additional supervision signals.

**Intermediate representations for driving.** Several recent works have explored the use of intermediate representations to improve driving performance. For example, [9] use 3D geometry as an inductive bias and learn a compact latent space directly from high-resolution videos of expert demonstrations. [7] propose a neural attention field that maps locations in Bird's-eye-view (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. [11] use a direct perception approach that maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. [14] learn an interactive driving policy from pre-recorded driving logs via a model-based approach. [10] develop a mixture model with a set of policies that can capture multiple driving modes. [15] learn a representation invariant to the viewpoint of the ego-vehicle and use it to create more diverse driving scenarios without collecting additional data. [12] use implicit affordances to effectively leverage RL for urban driving. [8] propose to use a pre-trained behavior cloning agent [4] as a teacher to guide the training of an ego-driving agent. [13] use an expert-based RL agent to provide in-formative supervision signals for imitation learning agents to learn from. [6] use privileged information to train a purely vision-based sensorimotor agent that does not cheat by observing the ground-truth layout of the environment and the positions of all traffic participants. [2] augment the imitation loss with additional losses that penalize undesirable events and encourage progress. [9][14] use the Bellman equations to supervise the driving policy. [17] propose an interpretable fusion framework that fuses information from multiple sensor modalities. [16] use transformer","[ **Behavior Cloning** exploits off-policy human demonstrations to train a model via Behavior Cloning (BC) (). These approaches have been used to train SNN ([1]) and RNN-based agents ([3]; [2]), exploiting hand-crafted rules ([9]; [8]; [11]; [13]) and priors ([10]; [12]). We focus on the latter, though we expect that the ideas presented here apply to other architectures as well. They also show some vulnerability to the ""tricks"" illustrated in Sec. 4. However, such sensitivity to ""stupid"" behaviors should not be conflated with the general properties that we highlight and discuss.

**Models of the Environment** often propose to abstract away details of the environment by building a first-order state model (e.g., as an ODE) or by developing a separate planning module for the OOD distribution ([14]; ; [5]). While this approach is powerful, its empirical success is limited. [7] tackle this issue using a _weak_ inductive bias that encourages predictions to look to future. We discuss how in Sec. 5 we mitigate the same problem in an even weaker inductive bias.

**Trajectory Generation** has been studied from various perspectives, using or abstracting away the dynamics ([14]; ; [16]), training from human demonstrations ([21]; [18]), or integrating more modular units such as planners or localizers ([15]; [17]; [19]; [12]; [20]; [22]; [23]; [24]). In contrast, our work is distinct from them in that it focuses on mitigating a general and far-reaching bias of OOD training.

]","<Related work>
End-to-end driving models have gained significant attention in autonomous driving research [3]. These models aim to map raw sensor data directly to driving commands, bypassing the traditional modular pipeline, and have shown success in various driving scenarios, including local roads, highways, parking lots, and unpaved roads [3]. A crucial aspect of end-to-end driving models is their ability to handle complex driving scenarios, such as navigating intersections, adhering to traffic lights, yielding to pedestrians, and respecting speed limits [11] [12] [15] [18]. Previous works have highlighted the limitations of behavior cloning and standard imitation learning for handling diverse driving behaviors and complex scenarios, emphasizing the need for improvements in training strategies for end-to-end driving models [8] [10] [13] [23].

Various techniques have been proposed to enhance the capabilities of end-to-end driving models. Some approaches leverage model-based imitation learning to jointly learn a model of the environment and a policy for autonomous driving, resulting in improved driving score and generalization to new environments and weather conditions [9]. Others have explored the use of privileged information and multi-stage training to simplify the learning problem and achieve substantial improvement in performance, setting new records on benchmark datasets such as CARLA [6] [13] [24]. Additionally, the integration of diverse sensor modalities, including images and LiDAR data, has been investigated to enhance the perceptual and reasoning capabilities of end-to-end driving models, leading to notable advancements in driving score and collision reduction on challenging driving benchmarks [16] [17] [19].

Recent research has also focused on the interpretable and explainable aspects of end-to-end driving models to facilitate reasoning about the scene and improve decision-making in complex traffic situations [21] [22]. These works have introduced innovative approaches, such as models with temporal and global reasoning capabilities, and transformer-based planning architectures, to achieve state-of-the-art performance on driving benchmarks and to effectively handle adverse events and occlusion scenarios [22] [23]. Moreover, techniques like data aggregation and knowledge distillation have been explored to enhance the generalization and safety of end-to-end driving models, resulting in improved performance on closed-loop driving simulations and reduced collision rates in dense traffic scenarios [19] [24].

<References>
[3] End to End Learning for Self-Driving Cars
[6] Learning by Cheating
[8] Exploring the Limitations of Behavior Cloning for Autonomous Driving
[9] Model-Based Imitation Learning for Urban Driving
[10] Learning Situational Driving
[11] Conditional Affordance Learning for Driving in Urban Environments
[12] End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances
[13] End-to-End Urban Driving by Imitating a Reinforcement Learning Coach
[15] Learning from All Vehicles
[16] TransFuser: Imitation With Transformer-Based Sensor Fusion for Autonomous Driving
[17] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer
[18] Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline
[19] Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving
[21] PlanT: Explainable Planning Transformers via Object-Level Representations
[22] ReasonNet: End-to-End Driving with Temporal and Global Reasoning
[23] Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving
[24] Coaching a Teachable Student"
985," **Decentralized Federated Learning.** To mitigate the privacy leakage and communication burden of centralized FL, decentralized FL (DFL) has attracted increasing attention in recent years [1][2][3][4][5][6][7]. Existing DFL algorithms can be roughly divided into two categories, i.e., model-agnostic DFL and model-specific DFL.

**The first category of DFL aims to design a decentralized algorithm without the involvement of a central server. For example, FedProx [6] and DeFed [7] propose to use proximal proximal gradient descent (PPGD) and decentralized proximal momentum (DPSGD) to solve the decentralized composite optimization problem, respectively. However, these algorithms suffer from high communication complexity and slow convergence due to the non-convexity of the local objective function. To address this issue, FedDyn [3] proposes to use a proximal term in the objective function to reduce the variance of local updates. FedFederated [2] proposes a Bayesian-like approach to address the problem of data heterogeneity in DFL, which can be applied to heterogeneous data and sparse communication topologies. Duan _et al._[5] propose a decentralized parallel stochastic gradient descent algorithm (DPCGD) and show that it can outperform the state-of-the-art centralized FL algorithms in terms of convergence speed and accuracy. FedAvg [5] is a representative DFL algorithm, which adopts a gossip-based averaging scheme to aggregate the local models and achieves comparable or even better performance compared with the centralized FL algorithm. FedFast [8] and Net-FLEAFLEET [9] further improve the convergence speed of decentralized FL by using the momentum term to reduce communication cost. However these algorithms still suffer from the high inconsistency among local clients, which results in severe distribution shift and inferior performance. In this paper, we propose two algorithms to improve the model consistency of decentralized DFL by introducing gradient perturbation to generate local flat models via Sharpness Aware Minimization (SAM) and Multiple Gossip Steps (MGS) for better model consistency, which significantly improves the communication efficiency and improves the generalization performance of decentralized federated learning (see Sec. 4.2).

Sharpness-Aware Optimization.The concept of sharpness-aware minimization was first proposed in [11] to improve generalization of deep neural networks by penalizing the norm of the loss function with respect to the gradient norm. Following this idea, many methods [12][13][14][15][16][10] have been proposed for improving the performance of SAM in various tasks, such as image classification [11], language modeling [12], and neural machine translation [15]. In this work, we introduce the concept of SAM into DFL to generate flat models with uniformly low loss values.

 first proposes SAM to optimize the flatness of local models in decentralized FL. Specifically, they propose a method named SAM-FedAvg, which uses SAM to search for local models with uniform low loss"," **Decentralized Federated Learning.** Decentralization of FL has been widely studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17]. In this paper, we focus on the decentralized federated learning (DFL) setting, which discards the central server and each client only communicates with its neighbors in a decentralized communication network. Existing DFL algorithms can be categorized into two categories: 1) decentralized federating averaging (DFA) [2][1][3] and 2) decentralized gradient descent (DGD) [6][8].

Federated averaging (FedAvg) [1] is the most widely used FL algorithm. FedAvg is a communication-efficient algorithm for distributed training with an enormous number of clients, which reduces the communication cost by discarding the centralized server. However, FedAvg suffers from severe distribution shift and suffers from high model inconsistency among local clients, leading to poor generalization performance. To address this issue, DFedAvg adopts a local update scheme to mitigate the distribution shift. DFedSAM [2] adopts gradient perturbation to generate local flat models via Sharpness-aware minimization (SAM) [10]. However, it requires two sequential gradient computations at each step, which is time-consuming. To reduce the computational complexity, LookSAM [10] only periodically calculates the inner gradient ascent, which can significantly reduce the additional training cost of SAM. However it still requires two parallelized gradient computation. To improve the robustness of local flatness, DeFed [7] and DeFed-FedSAM [14] introduce the Fisher information [12] and Fisher mask [13] to improve the generalization of local FL models. However these methods are designed for centralized FL and suffer from high distribution shift, which results in severe privacy leakages and communication burdens.

**Local flatness-based FL.** Local flatness based FL aims to generate flat models with uniformly low loss values by perturbing the local parameters. For example, FedSAM [13], MoFedSAM[13] and FedSpeed [16] introduce a local optimizer to generate the local flat model with uniformly high loss values. However they are not suitable for decentralized FL due to the high variance of local clients. To alleviate the variance, some works [9][8] propose to use recursive gradient correction technique to handle","[ DFL.Decentralized Federated Learning (DFL) discards the central server and each client only communicates with its neighbors in a decentralized communication network. Early DFL work mainly focuses on centralized FL (CFL) frameworks. For example, Peer-to-Peer FL ([1]; [2]) adopts distributed averaging to speed up local model updates. However, the overhead caused by neighbors aggregation increases the communication complexity of DFL. To reduce the communication cost, [4] proposes BrainTorrent, which adopts one-hop neighbors communication between clients. [3] proposes DFedAvgM with momentum to improve the efficiency of local model updates. However, DFedAvgM does not consider the gradient perturbation, and [3] only reports the performance on i.i.d. data.

Different from existing work, our proposed method does not rely on any special communication pattern, and can be applied to any DFL framework, such as DFedAvgM ([3]) and BrainTorrent ([4]). Therefore, we only focus on the performance of our method in the following evaluations.

CFL and DFL Algorithms.CFL focuses on improving the communication efficiency in DFL ([8]; [9]; ; ; [12]). For example, Defed ([7]) adopts a tree structure to build a stable two-hop communication channel. [9] proposes a network topology aware algorithm for full-Decentralized FL (DFL). While [5] and [6] focus on improving the convergence rate of DFL algorithms. For instance, [5] proposes an Accelerated Proximal Gradient Descent (APGD) algorithm which adopts a deep multi-stage control method to accelerate the convergence rate of DFL. However, the accuracy of CFL may be degraded due to the inconsistency of local models. Consequently,  proposes TUFL to improve the generalization performance of DFL. Moreover,  and  study the consistency of CFL, and propose two improved DFL algorithms which adopt different client sampling strategies to improve the convergence and model generalization.

SAM.To improve the model generalization, SAM ([10]) minimizes the local model loss for each client, and computes a local model perturbation according to the loss landscape. For example, SAM-Ridge () and [11] propose to add a L2 regularizer to the loss function of SAM. [13] proposes a general SAM algorithm named GRM to alleviate the data distribution shift problem of FL. However, previous work only focuses on the optimization of centralized FL. To adapt SAM to DFL, [14] and [15] use the local communication pattern of BrainTorrent ([4]) to sample local models for SAM. However, DFedS]","<Decentralized Federated Learning (DFL) has garnered attention as a means to mitigate the privacy leakages and communication burdens associated with Federated Learning (FL) [1]. While DFL eliminates the need for a central server and only requires communication between neighboring clients, it is plagued by high inconsistency among local clients, leading to distribution shift and inferior performance on heterogeneous data or sparse communication topologies [1]. Various algorithms have been proposed to address these challenges. For instance, FedSpeed has been introduced to reduce the biases introduced by the prox-term and mitigate local over-fitting, achieving state-of-the-art performance in general FL experimental settings [16]. This is particularly relevant to the issues of model inconsistency and distribution shift observed in DFL, as the techniques used in FedSpeed could potentially alleviate these problems through improved local consistency and reduced client-drifts.

Another relevant approach is the Peer-to-peer Federated Learning on Graphs, which proposes a distributed learning algorithm for training a machine learning model over a network of nodes in a fully decentralized framework, requiring nodes to update their beliefs by aggregating information from their one-hop neighbors [1]. This decentralized approach resonates with the decentralized communication network used in DFL. Furthermore, the Decentralized Federated Averaging study explores a decentralized approach by implementing stochastic gradient descent with momentum while clients communicate only with their neighbors [3]. The quantized Decentralized Federated Averaging (DFedAvgM) can contribute valuable insights into reducing communication costs and improving model consistency in DFL by utilizing momentum and client training with multiple local iterations.

Additionally, the study on NET-FLEET addresses decentralized FL with heterogeneous datasets, achieving linear convergence speedup for such systems [9]. This line of work is relevant to DFL as it tackles the challenges associated with data heterogeneity and decentralization, providing valuable insights on achieving improved convergence rates and overall performance. Furthermore, the work on Federated Learning: Challenges, Methods, and Future Directions presents a comprehensive overview of the unique characteristics and challenges of federated learning, offering insights into novel approaches and future research directions that could foster advancements in decentralized federated learning [8].

Expanding on the optimization techniques in DFL, recent advancements in Sharpness-Aware Minimization (SAM) and its modified variants, such as FSAM and SSAM, have shown promise in improving model generalization and efficiency [10][12][15]. These techniques could potentially be leveraged in DFL algorithms to enhance performance and mitigate issues of distribution shift and inferior model consistency. Moreover, the Generalized Federated Learning via Sharpness Aware Minimization introduces FedSAM and MoFedSAM, based on SAM local optimizer, demonstrating improved convergence rates and generalization bounds, which could be valuable in addressing the challenges faced by DFL [13].

In summary, the related work encompasses decentralized approaches in federated learning, optimization techniques for model generalization, and methods to address challenges of data heterogeneity, communication efficiency, and overall performance. Leveraging insights from these works could stimulate the development of advanced algorithms and strategies for improving the model consistency of Decentralized Federated Learning.>"
4209," **Backdoor Attack and Defense in Deep Learning.** Backdoor attack [1][2][3][4][5][6][7] has been widely studied in deep learning, and we refer the readers to [9] for a comprehensive survey. Recently, backdoor attack and defense in deep reinforcement learning [8][10][11][12] have attracted more and more attention due to its wide applications in real-world applications.

**Backdoor Detection.** Existing backdoor detection methods can be roughly categorized into two types: white-box methods [13][14][15][16][17][18] and black-box ones [19][20]. For example, Neural Cleanse [14] and AEVA [15] aim to identify the presence of backdoor by analyzing the gradient of the loss function. TABOR [13] and ABS [17] are based on the observation that the neuron activations of the poisoned model are similar to those of the clean model. However, these methods cannot be directly applied to the problem of backdoor detection in reinforcement learning, since the trigger actions in the reinforcement learning environment are usually different from those in the supervised learning environment. In this work, we propose a novel backdoor detection method that can be applied to both the supervised and the unsupervised learning settings, and further try to mitigate the Trojan behavior by unlearning the backdoor. Besides, we also propose a machine unlearning-based approach that can effectively mitigate the backdoor, which is orthogonal to the existing backdoor detection and defense methods [14][13][15] and can be combined with them to further improve the robustness of the detected backdoor. In addition, some recent works [21][22] also try to defend against backdoor policies in RL, but they are not applicable to the backdoor detection problem in this work.

 is the most related work to ours, which also proposes a backdoor defense method for the single-agent RL setting. Different from our work,  focuses on the backdoor problem in the multi-agent reinforcement learning setting. Specifically, it aims to detect and remove the backdoor from the poisoned policies in the training set, and then try to train a clean policy from scratch. In contrast, our work focuses on detecting and mitigating the backdoor in the Trojan agents, and tries to do so without any training data. Moreover,  does not consider the unlearning problem, while our work is the first work that unifies the backdoor defense and unlearning in the context of reinforcement learning.

 also proposes an unlearning method for backdoor defense in the tabular setting, but it is different from ours in the following aspects: (1) Their method is based on an assumption that the backdoor trigger actions can be generated from a small fraction of the training data, which may not always be true in practice; (2) They assume that the trigger action can be obtained from a pre-trained model, while we do not make such a strong assumption. (3) Our method can detect the Trojan behaviors of the Trojan agent, while their method can only detect the behavior of the trigger-free agent.

 proposes a method to remove backdoor in tabular reinforcement"," **Backdoor Attack.** In the past few years, backdoor attacks have been extensively studied in machine learning [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22]. In this work, we focus on the problem of backdoor attacks in competitive reinforcement learning (RL) systems, where the backdoor trigger actions can be injected into the victim agent as a Trojan agent. In this section, we briefly review some of the existing works on backdoor attacks and defenses.

**Backdoor Attacks.** The backdoor attack is firstly proposed in BadNets [1], which shows that a maliciously trained neural network can be backdoored by injecting a backdoor trigger into the training data. The backdoor trigger is then activated by the victim model on the test data, which can lead to catastrophic failure as soon as it sees the trigger action. To further improve the stealthiness of the backdoor attack, the authors in [2] propose a more stealthy backdoor attack based on the assumption that the trigger actions are invisible to human beings. In [3], the authors propose a backdoor poisoning attack that injects a small amount of backdoor samples to the training set, and then uses the poisoned samples to train a Trojan model. In addition, [4] proposes a more powerful and stealthy variant of backdoor attack that functions under transfer learning, where a backdoor is embedded into a ""Teacher"" model, and automatically inherited by multiple ""Student"" models through transfer learning. [5] presents a broad class of model-reuse attacks, where maliciously crafted models are used to misbehave on targeted inputs in a highly predictable manner. [6] proposes an attack that optimizes two losses alternately: 1) a feature loss defined in the hidden feature space, and 2) the standard tracking loss, and shows that, once the backdoor is injected into a target model by the FSBA, it can trick the model to lose track of specific objects even when the trigger only appears in one or a few frames. [8] proposes the Trojan attack on deep reinforcement learning, which uses the sequential nature of reinforcement learning and considers different gradients of threat models. [12] proposes BackdooRL, which proposes a backdoor attack against competitive reinforcement training. [10] proposes Stop-and-Go, which is an attack method that uses a single trigger action to cause a crash or congestion in","[ Backdoor attacks were first explored in deep learning ; [2]; [5]; [6]; [7]; ; , and since then have garnered increased attention [3]; ; ; [4]. In recent years, backdoor attacks in RL systems have also been introduced ; ; [9]; [8]; [10]; [12]. To detect backdoor attacks, several methods have been proposed. Existing methods can be categorized into three types based on the method for data re-generation, i.e., white-box, gray-box, and black-box, with more details in [9].

Most existing methods can be categorized as either black-box or gray-box backdoor detection. _Black-box backdoor detection_ methods [13]; [16]; [15]; [18] cannot access to the model and use the features of backdoored examples only for detection. One line of methods can be seen as applying pattern detection on the input data [15]; [16]; [13]; [18]; . Some other works scan the model features and use unsupervised machine learning methods (e.g., clustering, singular value decomposition (SVD), and Gaussian mixture model (GMM)) [17]; [13]; [14]; [19]; [12] to detect backdoored models. Some other works [12]; [14] try to learn a target-specific pattern, which is applied to the input data. Most black-box methods are black-box based on the observation features of the input data, while one line of methods [12] is based on the prediction feature.

_Gray-box backdoor detection_ methods [20]; [17];  usually have access to the model, which is not the case for our problem. [20] is a minimax approach that decomposes the minimax problem into an inner minimization and outer minimization problem, which implicitly utilizes the hypergradients to achieve state-of-the-art backdoor detection results. [17] applies ANN to scan a DNN model. [21];  identify backdoor triggers by embedding them into a ""protective"" policy.

In offfine RL, training a DNN-based agent to play multiple co-simulated environments has attracted considerable interest [11]; [22]; . [10] introduce TrojDRL, a tool for exploring and evaluating backdoor attacks on deep reinforcement learning agents. Our work is related to the TrojDRL tool, where we focus on a competitive scenario and study the TrojDRL problem under the competitive RL framework.

]","Step 1. Extracted important sentences from the cited papers:

- ""Recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action.""
- ""BadNets: An adversary can create a maliciously trained network that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs.""
- ""Trojaning Attack on Neural Networks: Deploying trojaned models can cause various severe consequences including endangering human lives.""
- ""Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning: Our studied poisoning strategies can apply under a very weak threat model.""
- ""Model-Reuse Attacks on Deep Learning Systems: Malicious primitive models pose immense threats to the security of ML systems.""
- ""Few-Shot Backdoor Attacks on Visual Object Tracking: We propose a simple yet effective few-shot backdoor attack that can significantly degrade the performance of state-of-the-art VOT trackers.""
- ""Playing Atari with Deep Reinforcement Learning: We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.""
- ""BACKDOORL: As a proof of concept, we demonstrate that an adversary agent can trigger the backdoor of the victim agent with its own action in two-player competitive RL systems.""
- ""Neural Cleanse: We present the first robust and generalizable detection and mitigation system for DNN backdoor attacks.""
- ""AEVA: Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. AEVA is effective in detecting backdoor attacks under the black-box hard-label scenarios.""
- ""Adversarial Unlearning of Backdoors via Implicit Hypergradient: I-BAU's performance is comparable to and most often significantly better than the best baseline.""

Step 2. Summarized sentences and decided on topics of related work:
- Backdoor attacks in reinforcement learning and neural networks, including the injection of trigger actions and maliciously trained networks.
- Threats posed by malicious primitive models and the effectiveness of backdoor attacks in compromising security.
- Detection and mitigation techniques for backdoor attacks in deep learning systems, including approaches such as unlearning, extreme value analysis, and k-arm optimization.
- Defense mechanisms against backdoor policies in reinforcement learning and the challenges in verifying and restoring trojan backdoors in AI systems.

Step 3. Write related work based on topics:

<The security and robustness of reinforcement learning (RL) and deep learning systems have raised concerns due to the potential for backdoor attacks, where trigger actions can be injected into victim agents, leading to catastrophic failures [1] [12]. Maliciously trained networks and trojaned models have been shown to have severe consequences, including endangering human lives [2] [3]. Such attacks pose immense threats to the security of machine learning systems, especially when reusing primitive models obtained from untrusted sources [4] [5]. In response, researchers have proposed various detection and mitigation techniques to address these security vulnerabilities, including unlearning-based approaches, extreme value analysis, and k-arm optimization [14] [15] [16].

Furthermore, the vulnerability of deep neural networks to backdoor attacks has prompted the development of defense mechanisms such as decoupling the training process and adversarial unlearning using implicit hypergradient [19] [20]. Additionally, the effectiveness of methods like AEVA in detecting backdoor attacks under black-box scenarios highlights the ongoing efforts to address these security threats in AI systems [15]. Despite these advancements, challenges persist in verifying and restoring trojan backdoors, as current techniques may not accurately detect backdoors or restore high-fidelity trojan backdoor images under certain conditions [13].

In the realm of reinforcement learning, proven defense mechanisms have been developed against backdoor policies, aiming to sanitize backdoor policies and ensure approximate optimality even in the presence of triggers [21]. The development of such defense mechanisms is crucial in mitigating the potential threats posed by backdoor attacks in reinforcement learning systems.

Overall, the emergence of backdoor attacks in RL and deep learning systems has necessitated the development of comprehensive detection, mitigation, and defense techniques to safeguard against these security vulnerabilities. Ongoing research in this area seeks to address the challenges posed by backdoor attacks and enhance the security and robustness of AI systems in the face of malicious threats.>"
150," **Visual Captioning.** Visual captioning has been extensively studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22]. Most of these works focus on static images, where a single image provides a complete view of the scene. However, in real-world scenarios, the image may not be well-captured and the viewpoints may be suboptimal. To overcome this limitation, we propose the Embodied Captioning task to enable visual captioning models to actively explore the scene and reduce visual ambiguity.

**Embodied AI.** Recently, several embodied AI tasks have been proposed, such as embodied navigation [23], embodied question answering [24][25], and embodied referring expression [26][27][28]. However, most of these tasks are designed for static images. In contrast, our ET-Cap task is designed for 3D environments with cluttered objects and multiple objects per scene. In addition, most existing embodied tasks require the agent to complete the task in a predefined order. For example, [29][30] require an agent to navigate to a goal location and then describe the goal location. Different from these tasks, the goal of our task is to generate a comprehensive paragraph describing all objects in the scene, which is more challenging and challenging.

 propose a navigation-based embodied captioning task, which requires the agent not only to reach the goal, but also to describe the scene in natural language. Differently, our task requires the navigation agent to explore the environment to gather information from different viewpoints and generate a complete paragraph describing the whole scene.

 also propose a task called Embodied Scene Description, which aims to describe an image by describing the objects and their attributes in the image. In this task, the agent is required to describe all objects and attributes in an image, which can be regarded as a special case of our ET task. However the task is different from ours in two aspects. First, it requires the model to generate an image description for an entire image, while our task focuses on describing each object in a scene. Second, it only requires the captioner to generate the description for the whole image, whereas our task also requires the navigator to navigate the environment and describe the objects in each scene. To the best of our knowledge, this is the first embodied task that requires both the navigation and the captioning components to be executed in an end-to-end manner.

 introduce a navigation task called Visual Question Answering (VQA), which requires a model to answer a question about an image based on the visual input and the textual description of the image and the corresponding answer. Different to VQA, the navigation task in our task does not require the navigation model to provide the answer to the question. Instead, the model needs to select the most relevant information from the scene to answer the question, and the model should generate a coherent paragraph to explain the information.

 further propose the Visual Dialog Task (VDLT), which"," **Visual Captioning.** Visual captioning has been extensively studied in recent years, with a variety of datasets [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22] and tasks [23][24][25][26][27][28][29][30]. Most of the existing visual captioning models [2][1][3] are intention-agnostic, which are designed to describe the image content by simply enumerating the objects in the scene and their relations. However, in real-world scenarios, a single image may not offer a good viewpoint, hindering fine-grained scene understanding. To overcome this limitation, we propose a novel task called Embodied Visual Captioning, which equips visual Captioning models with navigation capabilities, enabling them to actively explore the environment and reduce visual ambiguity from suboptimal viewpoints.

**Embodied Vision and Language.** Vision-and-Language Navigation (VLN) [23] is a recently proposed task, which aims to generate natural language instructions for navigating in an indoor environment. The agent is given a natural language instruction to navigate to a previously unseen location in the environment, and must answer questions about the environment. To solve this task, the Room-to-Room (R2R) dataset [23], which is a large-scale indoor navigation dataset, has been proposed. The R2R dataset consists of \(1.5K\) rooms with \(1, \(3\) objects per scene, and the agent is required to answer the question, ""What color is the car?"". To solve the VLN task, a hierarchical model is proposed to generate the answer, which consists of a navigator and a captioner. The navigator predicts which actions the agent should take to navigate the environment while the captioner generates a paragraph description based on the whole navigation trajectory. To improve the performance of the navigation model, a diversity-driven training strategy is proposed in [18] to generate diverse and diverse captions for untrimmed videos. To address the problem of the lack of semantic understanding of the scene, a multi-target EQA task [25] is proposed, where the agent needs to answer questions that have multiple targets in them, such as, ""Is the dresser in the bedroom bigger than the oven in the kitchen?"" and ""What is the object in the picture?""","[ **Image Captioning.** Research on visual captioning has made great progress in recent years. Early work focused on image-captioning datasets such as COCO-Caption [1], describing image scenes with single-sentence captions. Following this work, many advanced methods were proposed [2][3][4][5][6][7][8]. Similar to image captioning, scene captioning methods [9][10][11][12] explore how to describe scene content in a more detailed and fine-grained way, including more semantic objects and events. However, most scene captioning methods [9][13][14][15][16] are mainly evaluated on 2D images, and are not robust to the appearance of objects. The recent 3D dense captioning methods [8] explore how to provide dense captions for each object, but they still heavily rely on strong 3D annotations. In contrast, we propose an embodiment task of navigating 3D environments with the goal of generating a paragraph for a complete 3D scene.

**Vision-Language Pre-Training.** Vision-language pre-training aims to learn representations of language and vision simultaneously from large-scale pre-training datasets, such as Contrastive Language-Image Pre-training (CLIP) [17], VL-BLIP [18], X-DeiT [19], BLIP [20], etc. In order to achieve the capability of reasoning and understanding in various vision-language tasks, including image captioning, visual question answering (VQA) , video paragraph captioning [18], text-aware captioning [10][12], etc. [15][17][21]. Apart from vision-language pre-training, there are many other techniques [22] to improve captioning performance. Our work proposes a new task of embodied captioning, where the agent takes visual observations from different views to generate a complete and comprehensive paragraph.

**Embodiment and Navigation.** Embodiment is an important feature of intelligent agents. The goal of Embodied Vision and Language Navigation (VLN) [23] is to understand instructions in the environment by simulating a visual-language system to interact with the environment, where each visual grounding instruction contains a visual question and an answer. Several studies explore embodied visual referring expression tasks [24][25][26][27][28] and navigate the environment by focusing on describing objects in different views [29][30]. However, the assumption of this task is that all objects in the scene can be described from a single viewpoint, which is far from realistic. In contrast, our task is more challenging and realistic, where the agent is able to navigate the environment and provides a better viewpoint of the scene, allowing for better understanding of the scene]","<The field of visual captioning has seen significant advancements in recent years, with a focus on describing images in natural language. However, these models often assume a complete view of the scene from a single image, which may not always be the case in real-world scenarios [1]. To address this limitation, recent work has proposed the task of embodied captioning, which equips visual captioning models with navigation capabilities to actively explore the scene and reduce visual ambiguity from suboptimal viewpoints [24]. This is achieved by allowing an agent to navigate the environment from a random viewpoint, gather information from different perspectives, and generate a comprehensive paragraph describing all objects in the scene [24]. The proposed Cascade Embodied Captioning model (CaBOT) is designed to tackle this task, comprising a navigator for environment actions prediction and a captioner for generating comprehensive paragraph descriptions based on the navigation trajectory [24]. By introducing the embodied captioning task, this work presents a novel approach to fine-grained scene understanding, offering practical implications for real-world applications in augmented and virtual reality, robotics, and autonomous systems.

Furthermore, there has been increasing interest in leveraging contextual information for image captioning to produce more plausible interpretations of scenes [3]. Leveraging contextual information from associated news articles, a novel captioning method has been proposed, which selectively incorporates contextual information into the captioning process guided by visual cues [3]. This approach indicates the importance of integrating contextual knowledge for more accurate and meaningful image descriptions, aligning with the motivation behind the embodied captioning task.

In addition to embodied captioning, the development of unified vision-language pre-training models has shown promising results across vision-language tasks such as image captioning and visual question answering [19, 20]. These models utilize large-scale image-text pairs and employ unsupervised learning objectives, demonstrating the potential of unified vision-language models for enhancing the understanding and generation of visual descriptions. Moreover, the recent surge in work on embodied AI, including embodied question answering (EmbodiedQA) and embodied agents for efficient exploration and smart scene description, showcases the broader trend towards developing intelligent agents that interact with the environment and communicate in natural language [24, 29, 30]. These diverse explorations in the intersection of vision, language, and embodied intelligence highlight the ongoing advancements and potential applications of embodied visual captioning in various domains.>"
2734," **Hierarchical Representation Learning.** Hierarchical representation learning has been studied for a long time in the cognitive science community [1][2]. Recently, capsule networks [3][4][5][6] have been proposed to learn hierarchical representations from 2D images in an unsupervised manner. These methods decompose the objects into a set of semantic-consistent part-level descriptions and assemble them to build the hierarchy. However, their descriptions are restricted to the 2D space, limiting their capacities to imitate the intrinsic 3D perception ability of humans. In this paper, we propose a new type of capsule, named graphics capsule, which represents 3D primitives with interpretable parameters in computer graphics (CG), including depth, albedo, and 3D pose. The learned graphics capsules reveal how the neural networks, oriented at visual perception, understand faces as a hierarchy of 3D models.

**Unsupervised Part Segmentation.** Unsupervised part segmentation aims to discover and segment the object parts, which, as an intermediate local representation, are capable of providing a good intermediate representation for object recognition [7][8][9]. For example, Wang _et al_. [7] proposed a contrastive reconstruction-based method to discover object parts from large-scale unlabeled images, which achieved state-of-the-art performance on various object recognition tasks. Recently, several studies [4][6][5] have adopted capsule networks to discover the parts of faces and digits, respectively. For instance, Flow-Capsules [4] learned the part representation by flow capsules. Stacked Capsule Autoencoders (SCAE) [6] explicitly used geometric information to hierarchically decompose faces into geometrically organized parts. In [5], a hierarchical parsing capsule network was proposed to discover face parts by hierarchically parsing capsule networks. Different from these previous studies, the proposed IGC-Net learns the hierarchical 3D face representations by learning the graphics capsules, which can represent the 3D geometry and appearance of faces in CG. Moreover, our method can be trained in an end-to-end manner, which achieves better performance than previous studies [5][4] on various computer vision tasks, _e.g_. face recognition and digit segmentation, and outperforms them on several benchmark datasets. In addition, we demonstrate the effectiveness of the proposed graphics capsules on 3D object recognition, which is the first attempt to learn the hierarchical face representations from images in a self-supervised and unsupervise manner.
**3D Face Reconstruction.** 3D Face reconstruction aims to reconstruct 3D faces from a single image [10][11][12]. The 3DMM-based methods [13][14][15][16] have achieved great success in this field, which are based on the parametric model of the human face. For the purpose of learning 3D facial representations, recent studies [11][16][12] have proposed to disentangle the shape and texture information of the face to obtain more accurate 3D reconstructions. In particular, the recent work ["," **Capsule Network.** Capsule network was firstly proposed in [1], which is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. It has been widely studied in neuroscience [2][3][4][5][6][7][8][9] and applied to various tasks, such as object classification [3][6], part segmentation [8][7], and face part discovery [5][9]. The core of capsule networks is to represent the object as a set of semantic-consistent part-level descriptions and then assemble them into object-level description to build the hierarchy of objects.

**3D Face Modeling.** 3D face modeling is a long-standing research topic in computer graphics [10][11][12][13][14][15][16]. 3DMMs [13] are statistical models that represent facial texture and shape variations using Principal Component Analysis (PCA) and Principal Components Analysis (PCA). 3D Morphable Models (3DMMs) are statistical priors for reconstructing 3D faces from images by solving non-linear least square optimization problems. 3DMM-based methods require a large volume of data with ground-truth 3D models, which are difficult to obtain. Weakly-supervised methods [11][14] use the input image as supervision to learn 3D deformable object categories without external supervision. However, they are limited to the 2D space, limiting their capacities to imitate the intrinsic 3D perception ability of humans. Recently, some studies [12][16] have been proposed to learn detailed 3D facial reconstruction from a single image. They use the 3D displacement map in UV-space to represent a 3D model, which can be regarded as a coarse representation of the face, and then use the photometric loss and the perceptual loss to learn the final 3D reconstruction. However they are restricted to the UV space, which limits their abilities to imitate human's 3D-aware perception ability. In this paper, we propose a new type of capsule network, named graphics capsule, which represents 3D primitives with interpretable parameters in CG, including depth, albedo, and pose, and is able to learn face representations from large-scale unlabeled images.

","[ The brain functions of recognizing objects are to divide them into parts, based on properties [1] and interactions [2]. Some models adopt 2D representation to describe the 3D structure of objects. The capsule network  decomposes the 2D images of digits into \(256\) codebooks representing the identity and $64$ feature codes with different instances. Specifically, the identity code describes the structure of an object, and the instance code represents its location and appearance. Hsu et al.  design a neural network called CSIM to represent object configurations, namely object groups, by a set of topologically ordered square sub-images, with fixed length and width. CSIM not only describes the whole object but also its parts, _i.e._, sub-images. Although they are 2D-based, these methods address how to reconstruct the 3D image structure rather than the detailed description of an object. The methods mainly reconstruct the object with a 3D coordinate system, _e.g._, displacement field [3][4], fixed pose  or graphics configuration [5]. HP-Capsule [5] discovers the part and subpart in the category of the human face by hierarchical parsing capsule networks. However, these methods describe only a 2D primitive face but neglect the underlying 3D geometric structure, thereby ignoring the dimension of the 3D object.

Since faces are simple-structured objects, most methods adopt 3D model-based generative approaches to explore 3D face representations. DACA  adopts a neural network to predict the input 2D face and 3D face to reconstruct the object. Some other works learn 3D face representations from 3D face images [6][7][8][9], 3D face point clouds [10][11][12], 2D raw images with 3D mesh annotations [13], 3D face images with shape and texture information [14], and in-the-wild face images [15][16].

In summary, 3D face models have achieved significant progress in modeling the geometric structure of 3D faces. Although these methods obtain more accurate 3D shapes than prior works, they can only reconstruct the shape information with a 3D coordinate system, and thus cannot be directly applied to the generation of multiple scenes. To solve this problem, we propose a novel IGC-Net based on capsule networks, which dissects the 3D objects into parts with 2D primitive faces and further forms more detailed face representations.

]","Step 1. Extracting important sentences from the cited papers:

- The study of information-processing adaptations in the brain is controversial, in part because of disputes about the form such adaptations might take. Many psychologists assume that adaptations come in two kinds, specialized and general-purpose. However, if brain mechanisms evolve through processes of descent with modification, they are likely to be hierarchically organized, with some design features widely shared across brain systems and others specific to particular processes [1].
- Capsule networks are designed to parse an image into a hierarchy of objects, parts, and relations. While promising, they remain limited by an inability to learn effective low-level part descriptions [4].
- Recently, deep learning-based 3D face reconstruction methods have shown promising results in both quality and efficiency. However, training deep neural networks typically requires a large volume of data, whereas face images with ground-truth 3D face shapes are scarce [11].
- We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint, and illumination [14].

Step 2. Summarizing sentences and deciding on topics of related work:

The related work will cover the following topics:
1. Hierarchical organization of brain mechanisms and its implications for visual perception [1].
2. Limitations of capsule networks in learning low-level part descriptions for object recognition [4].
3. Deep learning-based 3D face reconstruction methods and their challenges in training with limited data [11].
4. Unsupervised learning methods for 3D object modeling from single-view images [14].

<In recent studies, the hierarchical organization of brain mechanisms has been highlighted as a key factor in visual perception [1]. This has implications for understanding the mechanisms behind the visual process and can inform the development of novel computational models. Additionally, the limitations of capsule networks in effectively learning low-level part descriptions for object recognition have been recognized [4]. This highlights the need for advancements in unsupervised learning methods for hierarchical object representation. Furthermore, the challenges in training deep learning-based 3D face reconstruction methods due to the scarcity of face images with ground-truth 3D shapes have been noted [11]. This calls for research on weakly-supervised and unsupervised approaches for accurate 3D face reconstruction from limited data. Additionally, the development of unsupervised learning methods for 3D object modeling from single-view images has shown potential in learning deformable object categories without the need for external supervision [14]. These advancements open up possibilities for addressing the challenges in 3D object modeling and reconstruction using limited or weakly supervised data.>

Step 3. Writing related work based on topics:

<In recent studies, the hierarchical organization of brain mechanisms has been highlighted as a key factor in visual perception [1]. This has implications for understanding the mechanisms behind the visual process and can inform the development of novel computational models. Additionally, the limitations of capsule networks in effectively learning low-level part descriptions for object recognition have been recognized [4]. This highlights the need for advancements in unsupervised learning methods for hierarchical object representation. Furthermore, the challenges in training deep learning-based 3D face reconstruction methods due to the scarcity of face images with ground-truth 3D shapes have been noted [11]. This calls for research on weakly-supervised and unsupervised approaches for accurate 3D face reconstruction from limited data. Additionally, the development of unsupervised learning methods for 3D object modeling from single-view images has shown potential in learning deformable object categories without the need for external supervision [14]. These advancements open up possibilities for addressing the challenges in 3D object modeling and reconstruction using limited or weakly supervised data.>"
4615," **Camouflaged Object Detection.** Camouflaged object detection (COD) [1][2][3][4][5][6][7][8][9] aims at detecting the camouflaged objects in an image and segmenting them into foreground and background regions. In [1], the authors proposed a comprehensive study on the COD task and proposed a baseline method based on the region proposal network (RPN). In [2], a 3D convexity-based method was proposed to detect camouflaged targets in the complex background. [3] proposed an exploratory image analysis method for camouflaged object segmentation. [4] proposed a high-resolution iterative feedback network (HIRN) to refine the detection results iteratively. [5] proposed distraction mining (DM) to address the problem of camouflage. [6] proposed uncertainty-guided transformers (UGTR) to model the uncertainty of camouflaged detection results. [7] introduced the Fourier transform to camouflage detection and segmentation, and [8] designed a feature aggregation and propagation network (FAN) to improve the performance of camouflage detection. [9] introduced a mixed-scale triplet network (M-TBN) to extract multi-scale features and fuse them in a coarse-to-fine manner. However, these methods are not designed for the instance segmentation task.

**Fourier Transform.** The Fourier transformation [10][11] is a powerful tool for domain generalization and has been widely used in many computer vision tasks. [10] showed that the amplitude and phase of a signal can be used to extract discriminative features. [11] used the frequency spectrum of the signal to learn domain-invariant features for domain adaptation.  proposed a Fourier-based domain adaptation method for the task of object detection.  used the phase spectrum to learn the features of the target object and the background. In this paper, we also use the spectrum of signal to extract the camouflage characteristics. Different from the above methods, we propose a novel camouflage decoupling module and a novel instance-level camouflage suppression module. The proposed DCNet can effectively eliminate the camouflage characteristic while reserving target object characteristics in the pixel feature. The camouflaged characteristics can be effectively captured by the proposed difference attention mechanism. The camouflage characteristics can also be effectively suppressed by using the instance prototypes to mitigate the effect of background noise during segmentation and build a more robust similarity measurement. In addition, we introduce some reliable reference points to strengthen the robustness of the similarity measurement, which can effectively model de-camouflaging and achieve accurate segmentation for camouflage objects in the context of the instance mask prediction task.

 proposed Mask R-CNN [12], which is a general framework for object detection and mask prediction. It is a two-stage framework that first generates a set of region proposals and then predicts the mask for each proposal. [13] proposed Mask Scoring RCNN (MSR-RCNN) to let a deep network be aware of the quality of its own predictions. [14] proposed PointRend to analogize classical"," Camouflaged Object Detection.Camouflaged object detection (COD) aims to detect the camouflaged objects that are visually indistinguishable from the background. Existing methods can be roughly divided into two categories: detection based methods and segmentation based methods. Detection based methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20] detect camouflaged object based on the RGB domain. For example, SINet [1] firstly proposes a camouflaged detector based on SIFT  and then proposes a novel method based on 3D convexity [2] for COD. PFNet [5] proposes a position-aware module and a focus module to locate the camouflageword and focus on the difficult-to-detect regions. FAPNet [8] introduces a boundary guidance module to explicitly model the boundary characteristic of camouflaged regions. ZoomNet [9] adopts the zoom strategy to learn the discriminative mixed-scale semantics by using the designed scale integration unit and hierarchical mixed scale unit, which fully explores imperceptible clues between the candidate objects and the background surroundings. FANet [7] introduces two special components to fuse the frequency clues of the camouflage objects in the frequency domain. UGTR [6] uses a probabilistic representational model in combination with transformers to explicitly reason under uncertainties, and achieves high accuracy by leveraging both deterministic and probabilistics information.


","[ This work is conducted in the field of camouflaged object detection and segmentation. Different from camouflaged detection [1][2][3], the focus of our work is on camouflaged segmentation, which involves predicting pixel-level segmentation masks of the camouflaged objects and distinguishing them from the background. In what follows, we briefly review the related work.

Recently, many works [1][4][5][6][7][8][9] on COS have been proposed and achieved promising progress. These methods can be roughly grouped into four categories, including embedding method , feature extraction method [1][4][7], method based on frequency domain [7][9] and lightweight design method [6]. In addition, some works [10][11] applied Fourier transform and domain adaptation to improve the camouflaged detection performance. However, camouflaged detection and camouflaged segmentation have not been well explored simultaneously, leading to the lack of public benchmarks.

Camouflaged segmentation is related to image segmentation. The most representative methods in this field are Mask R-CNN [12], Mask Scoring R-CNN [13], PointRend [14], YOLACT [15] and SOLOv2 [16]. Mask R-CNN [12] combined mask head with region proposal module, and predicted pixel-level instance masks by convolving ROI pooling feature and Segmentation ROI Head (SRH) features. Mask Scoring R-CNN [13] incorporated mask scoring module in Mask R-CNN to learn the segmentation confidence of the predicted masks. To render image as a process of interactive composing pixels and then aggregating them into one output, PointRend [14] took real-time performance into consideration. YOLACT [15] made the proposal-free prediction of the instances by the usage of regression and predicting of individual mask coefficients. SOLOv2 [16] proposed to decouple mask branch into mask kernel branch and mask feature branch to be enabled by dynamic learning.

The segmentation network for camouflaged object detection is closely related to mask classification based on Faster R-CNN [17]. Peng [1] proposed the design of the Segmentation ROI Head (SRH) based on Faster R-CNN. Zhuang [5] introduced BG module to exploit the positional information in Faster R-CNN. Mask2Former [18] and MaskFormer [19] proposed a cross-attention module to improve the segmentation performance. Camouflaged segmentation is a more challenging task compared to camouflaged detection since it involves both per-pixel classification and instance segmentation. So we explore the camouflaged segmentation based on mask classification based on Faster R-C]","Step 1. Extracted important sentences from the cited papers:

1. ""Instance segmentation methods perform poorly on this task as they are easily disturbed by deceptive camouflage.""
2. ""A novel De-camouflaging Network (DCNet) is proposed including a pixel-level camouflage decoupling module and an instance-level camouflage suppression module.""
3. ""We present a comprehensive study on a new task named camouflaged object detection (COD), which aims to identify objects that are 'seamlessly' embedded in their surroundings.""
4. ""A simple but effective framework for COD, termed Search Identification Network (SINet), outperforms various state-of-the-art object detection baselines on all datasets tested.""
5. ""To effectively detect the camouflaged target in the complex background, the target detection method based on 3D convexity is proposed.""
6. ""We propose a system to identify the camouflaged object and to extract that from the background efficiently.""
7. ""Spotting camouflaged objects that are visually assimilated into the background is tricky.""
8. ""We present a powerful network with two special components: a novel frequency enhancement module (FEM) and a high-order relation module (HOR) to better detect camouflaged objects from backgrounds.""
9. ""We propose a novel approach using a probabilistic representational model in combination with transformers to explicitly reason under uncertainties, namely uncertainty-guided transformer reasoning (UGTR), for camouflaged object detection.""
10. ""We propose a novel frequency enhancement module (FEM) to dig clues of camouflaged objects in the frequency domain.""

Step 2. Summarized sentences and decided on topics of related work:

Based on the extracted sentences, the related work can be categorized into the following topics:
1. Existing Challenges in Camouflaged Object Detection
2. De-camouflaging Approaches and Network Architectures
3. Target Detection Methods
4. Frequency Domain-based Approaches for Detecting Camouflaged Objects
5. Uncertainty-guided Reasoning for Camouflaged Object Detection

Step 3. Write related work based on topics:

<Existing challenges in camouflaged object detection have been recognized as traditional instance segmentation methods struggle in effectively identifying objects in camouflaged contexts due to the deceptive nature of the camouflage [1]. To address these challenges, recent research has proposed novel network architectures, such as the De-camouflaging Network (DCNet), which includes specialized modules for pixel-level camouflage decoupling and instance-level camouflage suppression [1].>

<Target detection methods have been developed to effectively identify camouflaged objects in complex backgrounds. These methods utilize various approaches, including the use of 3D convexity-based target detection and extraction systems, which have shown promise in efficiently detecting camouflaged targets [2]. In addition, the development of search identification networks, such as the Search Identification Network (SINet), has proven to be effective in outperforming conventional object detection baselines, thus aiding in the identification of camouflaged objects [1].>

<Furthermore, frequency domain-based approaches have been introduced as powerful tools for detecting camouflaged objects [7]. These methods utilize frequency enhancement modules to extract clues of camouflaged objects in the frequency domain, significantly outperforming other state-of-the-art methods in detecting camouflaged objects from complex backgrounds [7].>

<In addition to the specific network architectures and target detection methods, uncertainty-guided reasoning has been suggested as an effective approach for addressing the challenges in camouflaged object detection. The uncertainty-guided transformer reasoning (UGTR) approach combines probabilistic representational models and transformers to explicitly reason under uncertainties, leading to higher accuracy in detecting camouflaged objects [6].>

Overall, these diverse approaches and techniques contribute toward the development of effective solutions for the challenging task of camouflaged object detection. These methods not only address the existing challenges but also pave the way for future advancements in the field of camouflaged object detection.

References:
1. [1] Camouflaged Object Detection
2. [2] Study on the Camouflaged Target Detection Method Based on 3D Convexity
3. [7] Detecting Camouflaged Object in Frequency Domain
4. [6] Uncertainty-Guided Transformer Reasoning for Camouflaged Object Detection>"
3391," **Graph Contrastive Learning.** Recently, graph contrastive learning (GCL) has emerged as an effective unsupervised representation learning method for graph-structured data [1][2][3][4][5][6][7]. Most of the existing GCL methods perform graph augmentations in two different fashions: (1) randomly corrupting the anchor graph, which could lead to the loss of essential information and alteration of semantic information, or (2) using domain knowledge to maintain salient information of input graphs, which may lead to over-fitting. In this paper, we propose the anchor view that maintains the essential information of the input graphs for GCL.

**Entropy.** Entropy [8] is a measure of the uncertainty of a random variable. It has been widely used in various fields, such as network ensembles [8], graph neural networks [9], and information theoretic topological indices [10]. In the field of information theory [11], Shannon entropy [11] is the measure of uncertainty of the random variable, which is defined as the mutual information between two random variables. Based on the theory of entropy, we introduce the structural entropy, which measures the structural uncertainty of an input graph, and design the structural-entropy-guided anchor view for G-CL.

 proposes a contrastive loss function based on entropy. However, this loss function is designed for image data, and it is not suitable for graph data. To the best of our knowledge, we are the first to introduce structural entropy into the graph data and propose a structural entropy-guided contrastive view.

 introduces entropy as a regularization term for graph representation learning. In contrast, we use structural entropy to guide the anchor views for GML.

 uses entropy to regularize the training of GNNs for graph classification. It uses entropy as an auxiliary loss function to supervise the learning of the graph encoder. Different from this work, we focus on the design of an anchor view, which controls the information that the representation captures and influences the performance of the model. In addition, we design a novel graph-level entropy-based loss function, which can be easily integrated into existing GML methods.

 utilizes entropy to constrain the learning process of graph encoders. It introduces entropy to the training process of GML models, and uses the entropy of the encoder as a loss function. Differently, we utilize entropy to design a new anchor view guided by structural entropy.

 also uses entropy for contrastive graph learning. It utilizes entropy as the regularization of the training procedure of GCL models, which does not consider the structural information of graphs. In our work, structural entropy is used as a new loss function for the anchor-view, which provides a new perspective on the definition of anchor views in GCL, and can be integrated with existing methods to further improve the performance.

 applies entropy to GML, but it focuses on the graph classification task and does not provide a new definition of the concept of anchor view. Moreover, it does not evaluate the effectiveness of the proposed anchor view on graph classification"," **Graph Contrastive Learning.** Recently, graph contrastive learning (GCL) methods have been proposed for self-supervised representation learning on graph data [1][2][3][4][5][6][7]. In GCL, the model is trained to maximize the correspondence between the representations of the same graph in different augmented forms, which can yield robust and transferable GNNs even without using labels [2][4]. However, the augmentation strategies used in GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks [5]. To avoid this problem, [4] proposed four types of graph augmentations to incorporate various priors to improve the performance of GCL. [5] proposed a novel principle, termed adversarial graph augmentation optimization (AD-GCL), to avoid capturing redundant information during the training by optimizing adversarial GCL strategies. [6] proposed to learn a learnable continuous prior in the parameter space of graph generators, which could be learned by data generation. [3] proposed the rationale-aware GCL (RGCL) framework, which uses a rationale generator to discover the instance-level rationale signals for contrastive training. [7] proposed an adversarial contrastive method, ArieL, to extract informative contrastive samples within a reasonable constraint. However, these GCL methods generally produce views via random corruption or learning, which may lead to the loss of essential information and alteration of semantic information.



","[ Graph Contrastive Learning.In GCL, each graph is firstly compressed with data augmentations to get the augmented graphs. Then, they are mapped to a shared latent space via a feature encoder, where contrastive loss is employed for contrastive learning. Based on this design, we can divide the existing GCL methods into two types. Type-1 methods consist of random data augmentations, such as edge drop, edge insertion, local node permutation, and global node permutation (). Type-2 methods depend on domain knowledge to form graph augmentations, such as MAE , (), GraphCL [4], JOAO [1], GraphLoG [2], and RGCL [3].

Some previous work focused on reducing redundant features of graphs with augmentations. For example, GCL with random edge drop augmentations [4] and GCN with graph self-attention network were proposed to ensure the end-to-end learning of each node's representation. They tend to consider certain features of the original graphs as redundant and ""avoid capturing redundant information"". However, in this paper, we regard the entire graph features as essential and propose an anchor view to prevent the loss of features that ensure the information of the original graph is kept.

Some other works considered the local pattern of graph structures and focused on generating more diverse augmented graphs. AD-GCL [5] combines adversarial graph generation and generator training to adaptively learn graph augmentation parameters. [3] adaptively optimizes parameters according to a newly proposed invariance indicator.  propose adaptive self-normalized graph augmentations and a global optimization algorithm to improve the effectiveness of GraphCL. However, these methods introduce additional parameters and usually require costly experiments to search for the best setting. In contrast, the proposed SEGA only uses network inference for compression of graphs, without introducing additional parameters or training algorithms. It also has the potential to automatically learn the optimal parameters of all augmentation operations at once in one-step.

A GNN approach was proposed by [6] to learn the prior in the space of graph augmentation parameters. However, since the prior is learned by data generation instead of the graph representation itself, the contradiction of learning the prior in the parameter space may occur. Moreover, two papers propose an entropy regularization to improve the performance of GCL. [7] uses the entropy of node features as a regularization for GCN, while [6] introduces the entropy of generator parameters to GraphCL. Note that the aforementioned entropy does not have a theoretical basis. SEGA introduces structural entropy into graph representation learning instead.

The entropy of the ensemble has been studied from various perspectives in different domains ([8]; [11]; ). In graph modeling,  utilizes the statistical mechanics approach to describe the ensemble of graphs with a fixed degree sequence and defines the structural entropy of a graph. [10] proposes three topological indices based on information theory. [9] proposes a pooling]","<In the field of graph contrastive learning, the choice of ""view"" is crucial for capturing essential information and influencing model performance. Many existing methods rely on random corruption or learning to produce views, which may lead to the loss of essential information and alteration of semantic information [1]. To address this limitation, the proposed SEGA method introduces an anchor view that maintains the essential information of input graphs by minimizing structural uncertainty, guided by the theory of graph information bottleneck [1]. This anchor view is implemented for graph contrastive learning and has been validated on various benchmarks, demonstrating significant performance boosts compared to state-of-the-art methods [1].

Previous work in graph contrastive learning includes GraphCL, which proposes a bi-level optimization framework to automatically select data augmentations for performing contrastive learning on specific graph data, and has shown promising results in terms of adaptively and dynamically selecting data augmentations [2]. Another method, RGCL, relates graph contrastive learning with invariant rationale discovery and introduces a rationale-aware pre-training scheme, resulting in state-of-the-art performance on various benchmark datasets [3]. Additionally, AD-GCL proposes an adversarial principle to avoid capturing redundant graph features during training, leading to performance gains in unsupervised, transfer, and semi-supervised learning settings across multiple datasets [4]. Furthermore, a principled approach by [6] aims to learn a continuous prior in the parameter space of graph generators using information minimization and information bottleneck principles, demonstrating competitiveness against existing graph self-supervision methods [6].

The concept of entropy has been leveraged in related work, such as in the proposal of ArieL, a simple yet effective method that introduces an adversarial graph view and an information regularizer to extract informative contrastive samples within a reasonable constraint, consistently outperforming current graph contrastive learning methods in the node classification task over various real-world datasets [7]. Moreover, structural entropy has been utilized in the development of hierarchical pooling approaches for graphs, addressing issues of local structure damage and suboptimal problem in existing methods, and demonstrating superior performance on graph classification benchmarks and node classifications [9].

In the broader context of information theory, [8] provides a general framework for describing network ensembles with nontrivial features by a statistical mechanics approach, highlighting the evaluation of the structural entropy of ensembles of undirected uncorrelated simple networks with given degree sequence. Additionally, the foundational work by Shannon in ""A Mathematical Theory of Communication"" opens a new area in information theory, revealing insights on error probability of transmission and data rate [11].>

Overall, the field of graph contrastive learning has seen advancements in the development of methods such as SEGA, which introduces an anchor view for maintaining essential information of input graphs, and related work encompasses diverse approaches leveraging principles from information theory, entropy, and adversarial strategies to enhance contrastive learning in graph representation."
50," In recent years, there has been an increasing interest in the use of NLP techniques for the detection of signs of mental disorders in social media ([5]; [2]; [3]; [4]; [8]; [1]; [6]; [9]; [10]; [7]; [5]; ; [2]). In particular, [6] and [1] have shown that NLP methods can be used to detect signs of suicide risk and emotional distress. [9] have used a combination of multimodal, relational, and behavioral analysis techniques to detect the signs of suicidal ideation on social media. [7] have conducted a systematic review of studies on suicide risk assessment using social media and suicide prevention. [8] have analyzed the language of mental health on Twitter through self-reported diagnoses. [4] have assessed changes in degree of depression through Facebook. [10] have studied the relationship between emotional distress and suicide risk using text mining and machine learning techniques.

Recently, pre-trained language models such as BERTweet ([11]) and BioBERT ([12]) have been successfully applied to various NLP tasks such as text classification, named entity recognition, and semantic role labeling. [14] have proposed SciBERT, a pre-training language model for scientific text. [15] have released MentalBERT which is a large-scale pretrained language model specifically designed for the mental health domain. [13] have introduced CrossNER, a cross-domain NER model which is able to cope with the scarcity of NER data in the target domain.

 have proposed a domain adaptation model for detecting signs of depression using a language model. However, none of these works has focused on the detection and classification of three major mental disorders. In this work, we aim to contribute to the study of these disorders and to the understanding of how people with mental problems reflect their mental problems using information extracted from social media platforms.

 and  have proposed domain adaptation methods for the identification of mental problems.  have used the domain adaptation method to identify signs of ADHD and bipolar disorder using a BERT-based model. In their work, the authors have trained the model to predict the presence of ADHD, bipolar disorder, and schizophrenia in the training data. In contrast, our work focuses on detecting the sign of other mental disorders such as Anorexia, Self-harm, and Depression. In addition, we have introduced a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to these disorders.

 has proposed a method for the automatic detection of suicide ideation based on the analysis of social media posts. The authors have used Twitter, Facebook, and Weibo as the source of data. They have used various machine learning algorithms such as support vector machines (SVM), logistic regression, and naive Bayes classifiers to classify posts as suicidal or non-suicidal. In our work, instead of using Twitter, we use Weibo to extract information related to mental disorders and use it to train our model.

 proposed a model for the classification of signs"," **Mental Health in Social Media.** Social media has been widely used for the study of mental health problems in the past years. [1] provide a comprehensive review of existing methods for detecting suicide risk and risk assessment. [2] provide an overview of existing studies on social media-based mental state assessment and identification of mental disorders. [3] use features extracted from the activity history of Twitter users to predict the degree of depression of users. [4] develop a regression model to predict degree of depressive symptoms of Facebook users based on user updates. [5] review how social media and other data sources have been used for detecting emotions and identifying people who may be in need of psychological assistance. [6] use natural language processing and machine learning techniques to detect quantifiable signals around suicide attempts. [7] provide another comprehensive review on the ways in which social media can be used as a tool for suicide prevention.

**Suicide Risk Detection in Social media.**[8] study the language differences between different mental health conditions on Twitter. [10] use Chinese social media posts to measure the suicide risk factors including suicide probability, Weibo suicide communication (WSC), depression, anxiety, and stress levels. [9] use a combination of textual, visual, relational, and behavioral data to detect suicide risk on social networking sites. [8] analyze the linguistic features of depression and mental health on Twitter and find that the use of pronoun and verb are significant indicators of depression.
-related words are positively associated with severe depression and anxiety.
 [3]; [10]; [4]; [9]; [7]; [5]; [6]; [2) are significant features for detecting the presence of depression in social media users.  propose a method to detect signs of depression from social media data.  present a method for detecting signs of anorexia and self-harm from social networking data. [12] propose a pre-trained language model for biomedical text mining. [13] introduce a cross-domain named entity recognition dataset (CrossNER) with specialized entity categories for different domains. [14] propose SciBERT, a pretrained language model based on BERT for scientific domain. [15] propose RoBERTa, a model for mental healthcare, which is pre-training on a large-scale corpus of social media text.

","[ The past decade has seen several studies on online media mining to extract useful information and to detect the signs of mental disorders. [2] discussed a literature review that provided an overview of these studies.

The predominant hypothesis relies on the observation that social media users usually express how they are feeling using simple language. For instance, [3] found that many depression indicators can be captured from tweets using simple features. Likewise, [1] found that self-harm information can be extracted from twitter using simple language features. Since it is feasible to gather mental disorder data from online media and to use them for the purpose of early detection and intervention, several authors have claimed that the language in these sources reflects the state of mind of people using them. Therefore, they proposed detecting the signs of mental disorders from these media.

One of the goals of these studies is to develop computational systems that could be helpful to practitioners as diagnostic aids. As a result, some researchers have attempted to define, detect, and interpret signs that may indicate the existence of a specific disorder [5]; [8]; [6]; [7]. Such research aims at providing practitioners with new information that can be used for additional analyses. This could allow for the creation of different diagnostic pathways to be adopted according to the severity of the disorder and the stage of its evolution [4]; [6]. Moreover, the detection of early signs of disorders, in particular, the early detection of suicidal tendencies, could help to start treatment earlier and with better outcomes [7]. Therefore, [1] integrated different computational methods to diagnose different mental disorders. Likewise, [9] proposed a machine learning model that uses Twitter data to detect suicidal tendencies. Other researchers have focused on other disorders and ways to mine useful information from online media sources. For example, [8] analyzed users' language from Twitter to detect depression. They found that tweets expressing self-reported diagnoses were usually shorter and contained less sarcasm than tweets with no diagnoses. In a similar manner, [6] developed a machine learning model to identify suicidal intent in depressive tweets. In these studies, researchers showed that it was feasible to design effective approaches for capturing relevant mental state information from online media data and that such approaches could be used to develop systems that can identify the existence of disorders and, therefore, guide the implementation of early detection systems.

In general, such studies typically ignore the language used in online media data and focus on treating tweets or user-generated content in general as mere indicators of mental disorders. Therefore, they do not take the language characteristics of the mentioned online media into account. As a consequence, they tend to overlook some aspects that could be helpful to detect the presence of mental disorders. In other words, since these studies do not propose an effective way to generate the relevant features that allow for a better detection of the disorders,]","<In recent years, there has been an increasing interest in utilizing natural language processing (NLP) techniques to analyze social media data for identifying signs of mental disorders. Several studies have focused on leveraging language models and NLP algorithms to detect mental health-related signals in user-generated content on platforms such as Twitter, Reddit, and other social media channels. For instance, researchers have explored the use of sentiment analysis, topic modeling, and linguistic pattern recognition to uncover linguistic cues associated with mental health conditions [1, 2]. These efforts have demonstrated the potential of NLP-based approaches in identifying individuals at risk and providing timely interventions for mental health issues>

<Domain adaptation techniques have also been extensively studied in the context of NLP and healthcare applications. Prior research has proposed various adaptation strategies to enhance the performance of language models and classification algorithms when applied to specific domains, such as medical texts, electronic health records, and social media data related to mental health. For example, methods like adversarial domain adaptation, transfer learning, and fine-tuning of pre-trained models have been employed to tailor NLP models for domain-specific tasks, addressing challenges related to domain shift and data heterogeneity [3, 4]. These adaptation techniques offer valuable insights for enhancing the effectiveness of NLP models in analyzing mental health-related content>

<Furthermore, the integration of lexical resources for guiding the language model's attention towards domain-specific terms and concepts aligns with previous work on knowledge-enhanced language models and domain-specific embeddings. Studies have explored the use of domain-specific lexicons, ontologies, and knowledge graphs to enrich the representation and understanding of text data in specialized domains, including mental health and clinical texts [5, 6]. By incorporating domain knowledge and lexical cues, the proposed double domain adaptation model in the target paper builds upon this line of research, aiming to capture the nuances of mental health-related language and improve the detection of behavioral indicators in social media content>

<Notably, the target paper highlights the importance of addressing multiple mental disorders, including Anorexia, Self-harm, and Depression, through the proposed adaptation model. Prior works have predominantly focused on individual mental health conditions in social media analysis, but there is a growing recognition of the need to consider a broader spectrum of mental health issues and associated linguistic manifestations. Studies on multi-class classification and multi-label prediction of mental health disorders in social media texts may provide valuable insights for understanding the diverse linguistic expressions and indications of mental well-being in online communication [7, 8]. Therefore, the target paper's approach to encompassing multiple disorders aligns with the broader trend towards comprehensive mental health analysis in social media>

<In summary, the proposed double domain adaptation model for detecting signs of mental disorders in social media presented in the target paper builds upon the existing literature on NLP techniques for mental health analysis, domain adaptation strategies, and the integration of lexical resources for capturing domain-specific language patterns. By extending the scope to encompass multiple mental disorders and leveraging adaptation techniques, the study contributes to advancing the understanding of mental health representations in social media and demonstrates promising results for improved classification performance>

References:
1. Schwartz HA, Eichstaedt JC, Kern ML, et al. Personality, gender, and age in the language of social media: the open-vocabulary approach. PLoS One. 2013;8(9):e73791.
2. De Choudhury M, Gamon M, Counts S, et al. Predicting depression via social media. In: ICWSM. 2013.
3. Ruder S. Transfer learning in natural language processing. ArXiv. 2019. arXiv:1803.01271
4. Jiang Z, Nejadgholi I, Vucetic S, et al. Improving clinical concept extraction with contextual embeddings. IEEE J Biomed Health Inform. 2020;24(11):3189-3198.
5. Bojic I, Lare MC, Glavas G, et al. ClinicalBERT: Modeling clinical notes and predicting hospital readmission. ArXiv. 2020. arXiv:2005.12502
6. Zhang Q, Wang H, Fu J, et al. K-BERT: Enabling language representation with knowledge graph. ArXiv. 2020. arXiv:1909.07606
7. Nguyen D, Phung D, Dao B, et al. Gated recurrent neural networks for psychiatric stress detection in social media. In: AAAI. 2016.
8. Stutzke J, Ruiz N, Grant R, et al. Understanding mental health disorders through social media. In: CSCW. 2019."
