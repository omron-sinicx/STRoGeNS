,Unnamed: 0,pred,gt,ref,LED,LLAMA2,BART_arxiv,GPT
3435,3435," In this section, we briefly review the most widely used color difference (CD) metrics and the recent advances in deep learning.

**Handcrafted Color Difference Metrics.** The widely used CIE-79 [1] and CIE94 [2] color difference metrics are handcrafted and subject-calibrated against uniformly colored image patches. They are based on the observation that the human visual system (HVS) tends to perceive images with similar colors as well as dissimilar ones. However, they do not generalize well to images captured by different cameras. To address this issue, the JPC79 color difference formula [3] and the BFD (l:c) color difference metric [4] have been proposed. Both of them are color-difference functions based on color histograms. The BFD color difference has been extended to the spatial domain [5][6][7] and to the frequency domain [8][9][10]. However, these handcrafted metrics do not scale well to high-resolution images. In contrast, the deep learning-based metrics [11][12][13][14][15][16][17][18][19][20][21] have shown superior performance to the handcrafted ones in terms of both perceptual similarity and computational efficiency. In this paper, we propose a deep learning framework to learn a deep CD metric for high-quality images. We show that the learned deep metric can be used to improve the performance of existing handcrafted CD metrics, which is a promising direction for future research.


**Normalizing Flow.** Normalizing flows [22][23][24][25][26][27][28] are a type of non-linear non-convex functions that have been widely used in the field of image processing. In particular, normalizing flows are used to model the data distribution in the Fourier domain and to estimate the Laplacian of the data. The Fourier transform is defined aswhere \(\mathbf{L}_{1}\) and \(\mathbb{R}_{2}\) are the Fouries of the image and the data distributions, respectively. Then, the normalizing flow transforms the data into a Fourier-domain domain and then the data is transformed back to the image domain. The normalizing function can be defined as

\[\mathrm{Invertible Function}(L_{1},L_{2},F_{2})=\sum_{i=1}^{n}\sum_{j=j}^{N}\frac{1}{n}L_{i}-L_{j}}{\frac{j}{n}}{\sum_{n}^{j}}{j^{n}}{n}+\frac{n}{n}\frac{\frac{\nabla_{j}{j}}{{i}}{{j}}^{n}^{\top}+ \frac{l_{j}}{j}{{i}^{nd}}{\top}\] (1)where \(n\) is the number of samples, \(n\in[0,1]^{n\times"," In this section, we first review CD formulae in a broader context, and then discuss normalizing flow-based models, which are core to the proposed CD metric.

**CD Formulae**. The CD assessment is necessary for day-to-day color control, and is indispensable for color matching in color industries. Admittedly, CD formulae have accelerated the instrumental pass/fail devices for color judgments, but much still needs to be done for complete satisfaction. The scientific investigation of perceptual CDs can be dated at least back to Young and Helmholtz, who proposed and developed the trichromatic theory of color, which is the foundation of the metameric color matching experiment. CIELAB [1] is one of the most successful CD metrics recommended by CIE in 1976, and has been widely adopted in industry for a long time. However, the CIELAB color space is not perceptually uniform [2], which motivates the development of CIE94 [3] and CIEDE2000  through the introduction of application-specific parameters. Other CIELAB-based CD metrics include JPC79 , BFD(\(l\):\(c\)) [4], and CMC(\(l\):\(c\)) [5]. The introduced parameters are primarily calibrated using uniformly colored patches, digital or printed, which are statistically and semantically different from photographic images. Thus, the generalization of these metrics to photographic images is somewhat limited, especially when misalignment due to geometric distortions is present.

To incorporate spatial context into CD assessment, Zhang and Wandell [6] presented S-CIELAB, which extends CIELAB by adding spatial low-pass filtering as preprocessing. Similarly, Ouni _et al_. [7] provided a spatial extension of CIEDE2000. Lee _et al_. [8] re-examined histogram intersection, which is widely used in color image index, for the purpose of color image similarity assessment. Hong and Luo [9] chose to give larger weights to areas with spatially homogeneous colors and pixels with larger CDs. This method was later augmented by spatial filtering . Lee and Plataniois [10] built upon the philosophy of color structural similarity, and gave the hue component careful treatment with circular statistics. Jaramillo _et al_.  grouped the same texture areas for human-like CDassessment, using local binary patterns as texture descriptors.

General-purpose image quality models, including full-reference ones - SSIM [11], VSI [12], LPIPS [13] and DISTS [14], reduced-reference ones - Wang05 [15] and Yu09 [16], and no-reference ones - BRISQUE [17], NIQE [18] and Gao13 [19] can be directly adopted for CD assessment, regarding CDs as a particular form of ""visual degradations."" Meanwhile, just-noticeable difference (JND) methods, _e.g._, Butteraugli [20] and 'HLIP , also attempt to characterize visually indistinguishable color changes between two images. In the era of deep learning, due to the lack of sufficient human-labeled training data, DNN-based CD formulae are rarely proposed. Wang _et al._[21] created the first largest image dataset, SPCL, for perceptual CD assessment, and made one of the first attempts to train a DNN-based CD measure for photographic images. However, the underlying feature transform is not mathematically bijective.

**Normalizing Flow-based Models**. Normalizing flow-based generative models are constructed by bijective functions \(f:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\), with typically easy-to-compute analytical inverse \(f^{-1}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\). The primary goal of \(f\) is to map raw data \(\mathbf{x}\) to samples \(\mathbf{z}=f(\mathbf{x})\) from a simple probability distribution \(p_{\mathcal{Z}}(\mathbf{z})\). Many classic machine learning algorithms can be cast in this framework, such as principal component analysis (PCA, where \(f\) is a linear transform and \(p_{\mathcal{Z}}(\mathbf{z})\) is standard Gaussian) and independent component analysis (ICA, where \(f\) is again linear and \(p_{\mathcal{Z}}\) is factorized and heavy-tailed).

In 2014, Dinh _et al._[22] proposed non-linear independent component estimation (NICE), as a generalization of ICA. NICE is considered the first normalizing flow with the introduction of the _additive coupling_ to ease the calculation of the Jacobian determinant. To make flow-based models more suitable for image-related tasks, Dinh _et al._[23] extended NICE to RealNVP, which admits a _multi-scale autoregressive_ architecture, implemented by _squeezing_ and _affine coupling_. Kingma and Dhariwal [24] introduced the _invertible_\(1\times 1\)_convolution_ (_i.e._, the linear transform in PCA) to replace the fixed random permutation for splitting the channel dimension during multi-scale processing. The batch normalization in RealNVP is also replaced with activation normalization (_i.e._, _actnorm_). To allow unconstrained architectural design, Grathwohl _et al._[25] leveraged the Hutchinson's trace estimator for scalable and unbiased estimation of the log-density. Similarly, Behrmann _et al._[26] proposed invertible residual networks (i-ResNet), introducing a tractable estimation to the Jacobian log-determinant of a residual block. Other representative normalizing flow work includes hierarchical recursive coupling [27] for increasing flow expressiveness, Wavelet Flow [28] for scaling flow to ultra-high dimensional data, and Discrete Flow [29] for discrete data modeling. In this paper, we do not use normalizing flow for generative modeling, but for invertible feature transform.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]"," **Color Difference Metrics.** The CIE94 [1] and CIELAB [2] color difference metrics are handcrafted and subject-calibrated against uniformly colored patches, which do not generalize well to photographic images. To address this issue, the CIE 94 [3] and BFD(l:c) [4] are proposed to measure color differences between two images. The JPC79 [5] and CIE-94 [6][7][8][9][10] measure the color difference between a gray image and its corresponding gray reference image. The CMC(l:,c) and CMC-CMC(c):c [3], which are the most widely used CD metrics for gray images, are also subject-categorized into three categories: spatial, color, and structural. The spatial CD metric [6] is based on the spatial extension of the CIELab color metric [2]. The color difference metric [7] is a full-reference based metric, which is a more accurate metric for gray image. It is also the most commonly used metric for color images.

**Perceptual Image Quality Metric.** Image quality assessment (IQA) aims to predict the visual quality of distorted images with only partial information about the reference images. Traditional IQA methods [11][12][13][14][15][16][17][18][19][20][21] use handcrafted features to measure the visual similarity between images. Recently, deep learning based methods [13][13] have been proposed to improve the performance of IQA. For example, the VSI [12] uses a deep neural network to learn a visual saliency-based CD metric. The VSI[12] is an IQA model that uses a neural network as a feature weighting function to represent the importance of a local region. The SSIM[13] model uses a convolutional neural network (CNN) to learn the similarity between two patches. The structure similarity [14] is proposed to learn an objective function to measure similarity between patches.


"," Layout generation.Previous layout generation works often make use of Transformer-based architectures [2]; [3]; ; . [1] propose to optimize the graphic elements with latent space optimization, but fail to control the elements explicitly. For conditional layout generation, [4] propose an attribute-constrained graphic layout generation approach, which decodes layouts according to a single attribute. Conditional layout generation requires different user-defined requirements. Although we leverage a Transformer-based architecture to predict layouts, our task is to generate conditional layouts, thus we need to find a way to flexibly accommodate diverse user requirements.

Constrained generation.Different requirements like spatial constraints or attribute constraints can be fulfilled by constrained generation, which is an important research direction in machine learning. As a subset of constrained generation, conditional generation allows model to learn the desired constraints and output. Existing works of conditional generation mainly focus on text generation [9]; [8]; [10]; [7] and image generation [6]; .

There are also works of constrained generation, which introduce different kinds of constraints to the generation process [11]; [14]; [15]; [13]; [12]. For example, [14] propose to generate a table of SQL queries that meet the type constraints. [11] introduce predicate logic constraints to generate fluent texts while satisfying lexical constraints. [15] propose PICARD, which decodes auto-regressively from language models with incremental constraints. [13] propose a generator which efficiently unifies different constrained generation settings via energy-based representation. [12] propose a decoder with dynamic beam allocation for lexically constrained decoding in translation.

The generator of LayoutFormer++ is mainly inspired by [12]; [11]. Unlike previous works, which focus on conditions like tokens, [12] focus on combinations of tokens and slots for constraints, e.g., spanning over several tokens. Previous works of constrained generation define constraints via template rules and/or manually crafted formula rules. While our work defines constraints via constrained sequence representation, thus bridging the gap between template-based constraints and formula-based constraints. In particular, we propose to utilize a constrained representation to replace the expensive formula-based constraints. Besides, most previous works of conditional generation can not handle diverse layout conditions flexibly, and constrained generation is applied for the pre-designed constraint models. In contrast, LayoutFormer++ is a fully-automated approach to fulfill diverse conditions, and can handle diverse layouts flexibly without prior design of constraint models.

",,"<In the field of color science and image quality assessment, there has been substantial research on developing color difference metrics and image quality evaluation methods. A common theme in the related work is the development of objective measures that correlate with human perception and can accurately assess color differences and image quality. This research has led to the creation of various color spaces, such as CIELAB [1], CIELUV, and their derivatives, which are widely used for color difference calculations. Additionally, several color difference formulas, including CIE94, BFD, JPC79, and CMC, have been introduced to address different industrial and perceptual needs [3], [4], [5]. These metrics are designed to capture the dynamics of color appearance and its relationship with human visual perception in various contexts, such as industrial pass/fail color-difference work and digital color-image reproduction [6]. A key consideration in the development of these metrics is their ability to handle different sources of color degradation and distortion in images, including geometric distortions, compression, and enhancement techniques [9]. The complexity of natural photographic images has also been a focus of recent research, where efforts have been made to develop perceptual color and image quality assessment models specifically for smartphone photography and complex modern imaging systems [21].>

<In recent years, deep learning has gained prominence in the field of image processing and color science. Deep learning methods have been utilized to develop new color difference metrics based on learned feature representations and hierarchical transformations, such as multi-scale autoregressive normalizing flow for feature transform [1]. These approaches aim to improve the alignment of color and form with the observations in vision science and to be robust to geometric distortions commonly found in photographic images captured by different digital cameras. Additionally, the potential of deep learning in perceptual color difference metrics is shown by the remarkable effectiveness of deep features as a perceptual metric, outperforming traditional metrics in terms of predictive accuracy on benchmark image quality assessment databases [13]. Furthermore, deep learning techniques have been applied to non-linear independent component estimation, density estimation, and wavelet flow architectures for fast training of high-resolution normalizing flows, demonstrating the versatility and scalability of deep learning-based methods in color science and image quality assessment [22], [23], [28].>

<The development of perceptual color difference metrics and image quality assessment methods has also benefited from the use of natural image statistics, scene statistics, and visual saliency models to quantify and evaluate the perceptual quality of images. These models, such as BRISQUE, NICE, and VSI, leverage statistical features of natural images and scene statistics to provide objective measures of image quality that align with human perception and can be used for applications such as image denoising, texture classification, and perceptual image quality assessment [17], [12], [14]. Additionally, the incorporation of visual saliency maps as a feature in image quality assessment models has shown promising results in predicting local and global image quality, reflecting the importance of human visual attention in assessing image quality [12]. The use of hierarchical architectures and continuous dynamics in invertible neural networks has contributed to the development of efficient and scalable models for density estimation, Bayesian inference, and generative modeling, showcasing the potential of advanced neural network structures in addressing complex problems in image quality assessment and color science [27], [25].>"
3779,3779," In-Context Learning.In-context learning (ICL) aims to infer the context of an input given a few examples. It has been widely studied in recent years ([2]; [3]; ; ; ; [1]; ; [4]; ; ). Most existing ICL methods can be categorized into two categories, i.e., prompt-based and example-based methods.

Prompt-based ICL.Prompt engineering aims to design a set of natural language instructions (or prompts) to guide the model to solve a given task. For example, [7] and [5] propose to automatically generate prompts based on pre-trained language models. [6] and  propose to reorder examples based on the given task to improve the few-shot performance. However, these methods require a lot of manual efforts to design the appropriate prompts. On the other hand, example selection methods ([10]; [11]; [9]; [12]) aim to automatically select the correct context from the given examples. In this paper, we propose a new principle for ICL, self-adaptive ICL (SICL), which can automatically find the optimal in-context example organization for each input.

 propose to learn the optimal order of the examples in a task-agnostic manner. They propose to use the knowledge distillation loss to train the model. In contrast, our SICL is a general method that can be applied to any task.

 proposed to learn a permutation-invariant embedding space for example selection. They use the learned embeddings to represent the order of examples in the task-specific embedding spaces. Our method differs from theirs in two aspects. First, our method learns the permutation of examples instead of embedding the examples. Second, their method only considers the input-level embedding, while our method considers the entire input and considers both the input and the embedding-level information.

 proposes to learn an embedding for each example in the context. Their embedding is learned by the model itself, whereas our method is learned from the input. In addition, they only consider the input embedding and do not consider the output embedding of the example.

 presents a general framework for instance-level example selection and ranking, which is similar to our method. They also propose a learn-then-rank algorithm to learn how to rank the examples according to their embedding similarity. Different from them, our algorithm learns how to adaptively select and rank the input examples.

 also proposes a method for example-level context selection. Their method is based on a heuristics-based method, while we propose an end-to-end trainable algorithm.

 is a heuristic-based example selection method. Our algorithm is a learnable algorithm, while their method is not.

 and  are the two most related work to ours. They both focus on example selection in a supervised learning setting. Our work differs from them in the following aspects: (1) We propose a general algorithm. (2) Our algorithm can be used in a zero-shot setting. (3)"," Despite the surprising zero-shot performance of PLMs, recent works show that ICL can bring the performance to the next level. Augmenting PLMs with ICL achieves SOTA results on a wide range of NLP tasks, ranging from question answering , information retrieval [1], math word problem , commonsense reasoning , and fact checking [2] etc. The instability of ICL, however, has encouraged researchers to explore methods that search for robust and high-performing prompts. These methods can be categorized as follows based on the target of searching/optimization:

Template searchfocuses on searching for the template that can guide PLM's behavior and steer its best performance. Great advances have been made in template searching using various methods: PLMs , heuristic rules ; [7]; [8]; [3], reinforcement learning , genetic algorithms [6], or by hands [4]; [5]. Nonetheless, all these methods require a high-quality validation set to do prompt selection or optimization. Unlike them, our framework does not require a validation set.

When the validation set is not available, researchers propose to search prompts using entropy [9] or mutual information [4]. It's worth mentioning that these two works and all aforementioned methods search at the _corpus-level_: they pick the best-performing template with or without a validation set and then equally apply this template to all test examples during inference. However, corpus-level methods might be sub-optimal. If we consider the _No Free Lunch Theorem_, finding one single template that works well for all testing examples is nearly impossible.

In-context example search,unlike template search, is rarely explored in the literature despite that they also have a huge impact on ICL performance ([5]; [9]). [9] first propose a learning-free corpus-level method for in-context example search. However, they only consider an impractical setting with only 4 examples and their 24 permutations (\({}^{4}P_{4}=4!=24\)). [10] find examples that are semantically similar to a test sample can serve as a good choice for its in-context examples. However, the reason why such a simple heuristic works is unclear. [12] extend this nearest neighbor search and further take the diversity of examples into consideration. Inspired by these methods, recent studies propose to learn to retrieve in-context examples ([11]).

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"," **Prompt engineering.** Prompt engineering aims to find the best prompt to serve as the context for a given input. Existing methods can be divided into two categories: example-based and prompt-based. Example-based methods [1]; [4]; [7]; [6]; [8]; [3]; [5]; [9]; [10]; [1] select examples based on the given context, which are manually designed. For example, [4] propose a template-based method to select the best prompts without ground-truth labels. [7] propose to use a masked language model (MLM) to generate prompts. [8] propose an automatic prompt search method based on a gradient-guided search. [6] propose PERO, a method to search over the set of permutations of the training examples in the right order. [9] propose Fantastically Ordered Prompt (FOP) to overcome the few-shot prompt order sensitivity problem. [5] propose Calibrate-GPT, which estimates the model's bias towards each answer by asking for its prediction when given the prompt and a content-free test input such as ""N/A"". [1], DSI, and ZeroPrompt [3] use a Transformer-based model to retrieve the context from the pre-trained knowledge base. [2] propose Gopher, a multi-task model that can be scaled to 1,000 tasks by scaling the model size [2].


1] and [10] propose prompt selection methods to improve the performance of GPT-3. [3], [6], and [1]) propose to search for the optimal prompt for each input.

**Selective annotation.** Selective annotation aims to select a subset of training examples to annotate for each new task. [12] propose Selective Annotation-based Learning (SAL) to select examples from a large pool of unlabeled examples. [11] propose the prompt selection method for in-context learning, which uses an LM to retrieve examples as prompts and label training examples as positive or negative based on this probability.  propose to select prompts based on an LSTM-based prompt selection model.

"," As sentence simplification is a long-standing problem in natural language processing, many approaches have been proposed to simplify sentences. Most current methods are designed in a sequential framework, where sequence-to-sequence (seq2seq) approaches are used to generate the simplified sentences ([6]; [3]; ; [4]; [8]). As a result, all methods in these approaches typically generate only a single sentence. In order to avoid repetition, most approaches employ RNNs for automatic data generation ([2]; ). Due to the success of GPT ([13]) and BERT ([12]), a great number of sequential simplification models are proposed ([5]; ; [9]). However, most of these models only produce a single sentence and thus fail to simplify documents consisting of more than one sentence.

In order to address the problem of document-level text simplification, ([10]) proposed a method to simplify Wikipedia articles, but the method only applies to one article at a time. ([9]) was the first to introduce the problem of document-level text simplification. In ([9]), they introduced the D-Wikipedia dataset and two main document-level simplification models. One model consists of a two-step model, in which an editing model is followed by a simplified generating model. The editing model selects the sentences to be kept, rewritten, or removed, while the generating model rewrites the selected sentences. However, the editing model is based on a memory and information retrieval. While the proposed editing model has performed well, it cannot be applied to a document that has many non-grammatical sentences because the two-step method might completely change the grammar structure of the source text.

SimSum can be seen as a simplified and parallel version of the original two-step model. Instead of using information retrieval and memory as we did in the original two-step method, we adopt a more efficient method to select the sentences to be kept, rewritten, or removed. Instead of generating a single sentence, SimSum has a more complex structure, generating multiple summary sentences. This allows us to take advantage of the large knowledge that Transformer pretrained models have learned from the large training set.

Summary datasets usually share similar characteristics. The majority of them involve summarization from a sentence or a paragraph into a single sentence. On the contrary, for document-level text simplification, the datasets generally involve the generation of several simplified sentences ([10]; [9]).

SimSum's generation of multiple simplified sentences makes it different from previous summarization datasets. The D-Wikipedia dataset ([9]) has multiple simplified sentences but is too small to be used for large-scale learning. Unlike SimSum, most abstractive summarization models such as BERT-SUMMA ([11]), BRIO ([14]), and BART ([17]) cannot be used for the generation of multiple simplified sentences. In this paper, we focus on the challenges of document-level text simplification by proposing SimSum. We also use a new dataset, D-DocSim",,
5565,5565," **Contrastive learning.** Recently, contrastive learning [1][2][3][4][5][6][7][8][9][10][11][12] has become a popular paradigm for self-supervised representation learning. The key idea is to learn representations by contrasting positive pairs against negative pairs in the feature space. The positive pairs are generated by matching the feature maps of the same image with those of different images, while the negative pairs are created by contrasting the features from different images. In contrast to the traditional instance-level contrastive loss [2][1][4], the pixel-wise contrastive losses [7][9] are more suitable for dense prediction tasks.

**Knowledge distillation.** Knowledge distillation [13][14][15][16][17] aims to transfer the knowledge from a large teacher network to a small student network by mimicking the distribution of the teacher's output features. The teacher network is usually a pre-trained deep network and the student network is trained by minimizing the KL-divergence between the student's and teacher's feature maps. Recent works [18][19][20][21][22][23][24][25][26][27][28] have shown that the teacher network can be used as a powerful teacher to guide the student to learn more effective representations. For example, SimCLR [18] uses a teacher-student distillation framework where the teacher distills the intermediate feature maps from the last fully connected layers of the student. SimCLS  uses the intermediate features from the teacher and student networks to distill the knowledge. However, these methods are designed for classification tasks and cannot be directly applied to dense prediction.



\[\begin{tabular}{c|c| \hline c| \multicolumn{2}{c}{c}| \multirow{c}{2}{3}{4}{5}{6}{7}{8}{9}{10}{11}{12}{13}{14}{15}{16}{17}{18}{19}{20}{21}{22} \cline{hline}{c,hline,c,c}{15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,47].\] (1)

In this section, we briefly review the most related works to our method.

 proposed a two-stage distillation method called InfoNCE [13], which first trains a teacher network and then trains a student network using the intermediate representations extracted from it. The intermediate representations are then transferred to the student by minimizing a KL divergence between the two networks' intermediate feature distributions. In this method, the teacher networks are trained using the output features of the intermediate layers while the student networks are pretrained using the features extracted from the final layers. In addition, the student is also pretrained with"," **Pixel/region-level self-supervised learning** aims to learn competitive representations specialized for dense prediction tasks. Following the philosophy of contrasting pixel/region-level features from different augmented views, these methods develop various rules to find the positive pairs.

Intuitive methods [1][2][3][4][5][6] record the offsets and the scaling factors induced by geometric transformations (_e.g._, cropping, resizing, and flipping) to locate the positive pairs of pixels/regions from different augmented views. In [7][8], all pixels or regions within the original image are classified into some appropriate categories by a heuristic way or some unsupervised semantic segmentation methods. Any two pixels or regions from the same category form a positive pair. SoCo [9] and ORL [10] utilize the selective search  to identify numerous regions containing a single object and perform region-level contrastive learning based on these regions. DenseCL [11] and Self-EMD [12] pair the pixels of feature maps from different views according to some certain rules, _e.g._, minimizing the cosine distances between pixels or finding the matching set with minimum earth mover's distance .

Our PCD does not rely on sophisticated rules or preparations to pair pixels or regions. Instead, we directly contrast the feature maps output by the student and the teacher from the _same_ view of an image, decoupling the

Feature-based knowledge distillation transfers knowledge by matching the intermediate features of students and teachers, which are often not comparable due to the difference in shapes, _i.e_., the number of channels and the spatial size. It is a common practice to reshape students' features to have the same shape as teachers' by a learnable module [13]. Some works [14][15][16][17] transform both students' and teachers' features into tensors of the same shape. In PCD, shape alignment (especially with regard to the number of channels) is achieved by a non-linear projection head, which is a widely recognized technique in SSL for enhancing the quality of learned representations [18][19][20]. Additionally, in cases where the student and the teacher have feature maps of different spatial sizes, we employ a simple interpolation to complete the necessary alignment.

Self-supervised distillation transfers knowledge in a self-supervised learning fashion. CompRess [21] and SEED [22] propose to minimize the feature similarity distributions between students and teachers. DoGo [23] and DisCo [24] add a distillation branch for easing the optimization problem of small models during self-supervised pre-training. Previous works train students to classify images [25][26] or minimize the intra-group distances [27] based on the clusterings generated by teachers. [28] simultaneously trains teachers and teacher from scratch. Students are guided by teachers' on-the-fly clustering results. With these methods, the notorious problem that small models pre-trained by SSL methods face performance degradation has been partially solved. Our PCD is proposed to address the unsolved part--to improve the transferring results on dense prediction tasks.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]"," **Self-supervised learning.** Self-supervision has been widely used to learn transferable representations from unlabeled images. The pretext tasks are usually designed to learn invariant representations that are invariant to different views of the same image. For example, [1][2][3][4][5][6][7][8][9][10][11][12] learn pixel-level representations by minimizing the distance between the feature maps of the teacher and student networks. [1] and [2][4] learn dense representations by forcing local features to remain constant over different viewing conditions. [3][10] learn object/group-level representation by aligning different augmented views of an image to the same object. [6][8] learn the pixel-wise representations by matching the corresponding pixels from different views. [7][9] learn to align the representations of different views by selecting the most similar ones. [10] use the object-centric priors of ImageNet as the prior to discover object-level semantic correspondence.

**Knowledge distillation.** Knowledge distillation [13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28] aims to transfer knowledge from a teacher network to a student network. The teacher network is trained to predict the softmax probabilities of similarity scores between the student network and the teacher network, which are then used to guide the student to learn from the teacher's knowledge. For instance, FitNet [13] and FitNet++ [14] use a softmax loss function to match the teacher feature maps with the student feature maps. [15][17] use convolutional operations to paraphrase the teacher features and translate them to the student features. [14][21] use attention maps of teacher network as the guidance for the student. BYOL [20] and SimCLR [18] use an online and a target network that interact and learn from each other. [21] uses a teacher-student architecture to distill the knowledge from the student model to the teacher model. SEED [22] and Distill-on-the-go [23] use online knowledge distillation to transfer the knowledge of a teacher model to a smaller student model. DisCo [24] uses contrastive learning to transfer a teacher's softmax probability distribution to guide a student model, which is then used as a teacher to distil the student's knowledge"," **Diffusion Models.** Diffusion models [1][2][3][4][5][6] have achieved state-of-the-art performance in the field of generative modeling in recent years. DDPM [6] formulates the image generation process as the reverse of a Markovian diffusion process. The flow of information goes from random noise to the latent space and then decoded to the real space. H3D [7] performs a mean-field version of DDPM. This makes the diffusion model capable of capturing semantic information and providing useful semantic information. In this work, we also use H3D for modeling the generation process of our diffusion model. ASyRP [8] proposes a modified H3D to eliminate the need for the generator and to encourage the generation of meaningful content in H3D. DDIBs [9] and EGSDE [10] formulate DDPM as a normalizing flow, to enable the generation of images in different domains.

**Few-Shot Generative Models.** Few-shot learning [11][12][13][14] is a hot topic in the research field. Few-shot learning in the generative modeling [15][16][17][18][19][20][21] has become a research direction in recent years. Some recent work [16][19] improves GANs by training them on high-dimensional data. FS-GAN  also introduces new architecture to learn better from low-dimensional data. Recent works [15][17] adapt the few-shot model with data augmentation to avoid model collapse. Instead of augmenting data, DAPG [17] adds self-attention to learn from a low-dimensional space. Joint training with the source domain and adaptation to the target domain are also explored [21][22]. Except the above works, the training strategy and the loss function are different from our work. Besides, we also improve the model by introducing a novel FSDL model.

",,"<The field of self-supervised learning for visual representation has seen significant developments in recent years, with a focus on training models without the need for labeled data. Several methods have been proposed to improve self-supervised distillation, including contrasting feature maps from teacher and student networks to provide more informative pixel-to-pixel distillation [1]. This approach is similar to Pixel-Wise Contrastive Distillation (PCD), where the method attracts corresponding pixels from the teacher's and student's output feature maps for self-supervised distillation [1]. Another related work, View-Agnostic Dense Representation (VADeR), also aims to learn dense representations and has been shown to outperform ImageNet supervised pretraining in multiple dense prediction tasks [1].>

<Similar to the proposed method, previous work has focused on capturing spatial consistency of local representations. For instance, Spatially Consistent Representation Learning (SCRL) is designed for multi-object and location-specific tasks, using geometric translations and zooming operations to produce coherent spatial representations of local regions [2]. This suggests a similarity with the proposed SpatialAdaptor in the Pixel-Wise Contrastive Distillation (PCD) framework, which reshapes a part of the teacher network to preserve the distribution of output features while performing pixel-to-pixel distillation [1]. Additionally, Region Similarity Representation Learning (ReSim) focuses on learning regional representations for localization and semantic image-level representations, aligning feature maps across views to improve spatial and semantic consistency [3]. These methods collectively emphasize the importance of spatial consistency in self-supervised learning, echoing the goals of the PCD framework.>

<In the context of contrastive learning, previous work has explored pixel-level pretext tasks for dense feature representations. Propagate Yourself introduces pixel-to-propagation consistency tasks, showing significant improvements in object detection, instance segmentation, and dense pose estimation [4]. Similarly, FitNets emphasizes the importance of utilizing intermediate representations learned by the teacher network to improve the training process and final performance of the student network through knowledge distillation [13]. These ideas align closely with the objectives of the Pixel-Wise Contrastive Distillation (PCD) framework, which aims to distill knowledge from the teacher network to the student network for dense prediction tasks in a self-supervised fashion.>

<Several methods also focus on enhancing the performance of self-supervised pretrained models specifically for object detection tasks. Instance Localization for Self-supervised Detection Pretraining proposes a new instance localization pretext task to improve the transfer learning results for object detection [5]. Similarly, Aligning Pretraining for Detection via Object-Level Contrastive Learning (SoCo) is designed specifically for object detection tasks, achieving state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework [9]. These methods echo the objectives of the PCD framework in aiming to pre-train a small model friendly to dense prediction tasks in a self-supervised fashion, further emphasizing the relevance of the proposed work.>

<Additionally, the use of attention mechanisms in self-supervised learning has been explored in previous work. Methods like Paying More Attention to Attention and Efficient Visual Pretraining with Contrastive Detection emphasize the importance of attention in improving the performance of self-supervised pretrained models and yielding powerful representations for transfer learning [7, 14]. This aligns with the emphasis in the PCD framework on utilizing multi-head self-attention modules to enhance the effective receptive field of the student network for improved performance in dense prediction tasks.>

<In summary, the proposed Pixel-Wise Contrastive Distillation (PCD) framework aligns closely with related work in the field of self-supervised learning for visual representations. It shares similarities with methods focusing on pixel-level pretext tasks, spatial consistency, attention mechanisms, and knowledge distillation to improve the performance of self-supervised pretrained models for dense prediction tasks. These related works collectively highlight the potential of self-supervised distillation methods, and the impact they have on improving transfer learning and performance in dense prediction tasks.>"
2924,2924," Event-based motion estimation methods can be roughly divided into two categories: global alignment methods and local alignment methods. Global alignment methods [1][2][3][4][5][6][7][8][9][10][11] directly estimate the motion parameters from the event data, which are more robust to noise and outliers. However, they are computationally expensive due to the large number of events to be estimated. To reduce the computational cost, some works [2][1][3] propose to align events by minimizing the motion-compensated event contrast. For example, Wang _et al._[2] minimize the contrast between the spatio-temporal events and the estimated motion, which is similar to the Euclidean distance between two point sets [4]. However, this method is sensitive to outliers and may fail to estimate accurate motion when the motion model is different from the estimated one. To address this issue, some methods [3][10] propose a local alignment method, which first aligns the events in a batch to the time-surface map, and then estimates the motion by minimizing a local loss function. For instance, the work in [10] minimizes the local contrast of the event-to-time map and the motion, and the method in [3] maximizes the difference between the aligned events and their estimated motion. Although these methods have achieved promising results on some motion models, they still suffer from the high computational cost.


Different from the above methods, we propose a progressive scheme to progressively align events. In this way, only a small portion of events are involved in the optimization process, which greatly reduces the total runtime. In addition, our batch size strategy can adaptively adjust the batch size to obtain a more accurate motion model.

 propose a global motion estimation method by minimizing an energy function. They first align the events to the point trajectories and then minimize the energy function to obtain the motion. The energy function is defined aswhere \(\mathcal{O}(n^{2})\) is the sum of the global energy function and the local energy function, \(\bm{\theta}_{i}\) is the error between the estimated and the ground-truth motion. In contrast, our method is based on the time surface map and directly estimates the angular velocity of the events, which does not require any energy function computation. Moreover, we do not need to calculate the energy functions for each pixel in the event map. Therefore, our framework is much more efficient than the global alignment method.

 and  propose an event-based local motion alignment method by optimizing the local and global energy functions, respectively. The local energy functions are defined as

"," **Entropy based method.** Nunes and Demiris [1] proposed the entropy minimization framework (EMin) to obviate the need for an explicit intermediate image-based representation. They estimate the motion transformations by minimizing the dispersion of events, measured via a family of entropy functions. Since the entropy function has quadratic complexity, they extended EMin with an approximated version (AEMin) that searches events within a certain distance and an online version (IncEMin [2]) that incrementally estimates the motion parameters, achieving real-time rotational motion estimation. In contrast, our TS loss obviates the pair-wise calculation of events, showing a much lower computational complexity than the entropy loss. In addition, our approach is capable of conducting real-time estimations with sampled events and exhibiting superior accuracy compared to IncEMin.

**3D Points based method.** Liu _et al._[3] considered events as 3D points in the spatio-temporal space. They proposed a spatio-temporal registration algorithm (STR) that estimates rotational motion transformations by splitting events into early and late parts and registering events one-by-one based on trimmed iterative closest points [4]. STR requires costly computation with a nearest-neighbor search strategy. In addition, it is prone to incorrect registrations with noise events. In our work, we handle the data association problem implicitly and we apply a denoising operation on the TS map to reduce the influence of the noise.

**Image based method.** To combine events with well-established frame-based vision tools, some works [5][6][7][8] chose to transform events into an image-based representation. Gallego _et al._[7][9] proposed the Contrast Maximization (CMax) framework, which accumulates events to produce an image of warped events and maximizes the image contrast with respect to the motion parameters. Cheng _et al._[8] developed a spatio-temporal Poisson point process (ST-PPP) that aligns events through a maximum likelihood approach. The hyper-parameter \(\lambda\) of the Poisson pro cess is environment-specific and requires re-measurement when switching scenes or event cameras. Mitrokhin [6] proposed a method to estimate similarity transformations with metrics in the time-image, where they minimize a time-image loss (, the sum of gradients of the time-image) with respect to the motion parameters. In [10][11], the CMax framework was evaluated with twenty more image-based loss metrics. The contrast loss [7] achieves the best performance in terms of accuracy and efficiency, while the time-image loss [6] reports the lowest accuracy. A distinct difference between our approach and [6] is that our approach constructs the TS map with the minimum timestamps of the events warped at the same pixel. Thus not all the event timestamps are involved in optimization.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"," **Event-based motion estimation.** Existing event-based methods can be categorized into two categories: local and global methods. Local methods [1][2][3][4][5][6][7][8][9] estimate the motion of each event in a batch by minimizing the motion-compensated event image of warped events, which is obtained by maximizing the sharpness of the warped event image. The motion compensation method [10][11] is a popular local motion estimation method, which aligns events to the motion model of the batch. The main idea is to maximize the contrast of an image of the event data with the warped events. For example, the work of [1] maximizes the contrast between warped events and warped events of the same motion model. The method of [2] minimizes the dispersion between warped and warped event images, which can be regarded as a local version of the contrast maximization method [12; 13; 14]. The work of  uses the spatio-temporal information of events to align events to a spatiotemporal Poisson point process [8]. The method in [3] aligns the events to an event-to-point process [4] by averaging the events of different motion models. The work in [5] uses branch-and-bound [9] to align the events in the batch to the time-surface map. The works in [6] and [7] use the motion compensation model [6][8] to find the point trajectories on the image plane that are aligned with the motion data. The methods in [10; 11; 12; 13] are all based on the local motion compensation methods.




"," Synthetic DataGenerating synthetic datasets has enabled deep learning to overcome the shortcomings of data scarcity and improve the performance on a wide range of computer vision tasks [1][2][3][4][5]. Most synthetic datasets rely on virtual worlds to generate artificial data, such as GTA [4], KITTI [2], or Next-Gen . Another category of synthetic datasets is generated by rendering computer graphics (CG) simulations from 3D object models. This kind of data is costly to produce but typically provides high fidelity. In contrast to generating artificial data, Generative Adversarial Networks (GAN)  can produce images in a completely unsupervised way, i.e., without human-annotated data. Some recent works [6][7][8][9][10] employ synthetic images generated by GAN as training data. The power of large-scale UAV datasets with human images can be further improved by augmenting them with synthetic virtual images. In this paper, we investigate the possibility of using PTL to enhance a large-scale UAV dataset.

Progressive TrainingProgressive training (PT) has received significant interest in the machine learning community for its enhanced training efficiency [11][12][13][14][15]. Instead of starting the training from scratch, PT takes the initial model with a small capacity and gradually adds one or more intermediate layers. Then the model capacity grows and gets optimized iteratively until the training converges. Several papers have investigated how to optimize the training of large networks [11][16][17][18][19][15]. In this paper, we focus on how to train a relatively small initial network with intermediate progressive training.

Our work is related to curriculum learning [20][21][22][23], which is a meta-learning method for optimizing neural networks. With curriculum learning, the training order is determined by the difficulty of the training data. One example is the Baby Steps algorithm [23], which incrementally increases the model capacity to learn increasingly difficult tasks. In contrast to curriculum learning, PTL progressively augments a model with transformed virtual images, with augmentation determined by our domain gap.

Self-paced learning has been studied in several recent works [24][25][26][27]. Self-paced learning incrementally increases the training samples for a network, which is used in multiple machine learning applications such as image classification. In this paper, we take a step further by integrating self-paced learning into virtual-to-real GANs.

",,"<>In recent years, there has been a growing interest in event-based vision sensor technology, particularly in the domain of motion estimation. The paper ""Progressive Spatio-temporal Alignment for Efficient Event-based Motion Estimation"" by [authors] introduces an innovative framework for efficient event-based motion estimation. This work builds upon the existing literature on event-based motion estimation, leveraging progressive event-to-map alignment and spatio-temporal correlations for accurate motion model refinement. Notably, the proposed framework demonstrates advantages such as iterative refinement of motion parameters, reduced runtime by involving only a small portion of events in optimization, and dynamic batch size strategy to ensure consistent motion model assumptions [1].

The framework proposed in the target paper is closely related to prior works in event-based motion estimation. For instance, [1] presents a framework for event-based vision model estimation, which shares similarities with the target paper in terms of avoiding explicit image-based representation and incremental estimation. Additionally, [3] proposes spatiotemporal registration for event-based visual odometry, highlighting the significance of spatiotemporal correlations in motion estimation. Furthermore, the target paper's focus on efficiency and state-of-the-art accuracy aligns with the work by [5], which introduces a globally optimal approach for event-based motion estimation, and the work by [6], which explores efficient moving object detection and tracking using event-based cameras [1, 3, 5, 6].

Moreover, the target paper's methodology bears resemblance to [7], which presents a unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation. These works provide valuable insights into the application of contrast maximization and motion-compensated edge-like images in the context of event-based vision sensors. Furthermore, [8] introduces a spatio-temporal Poisson point process model for event camera data, which is relevant to the target paper's focus on spatio-temporal alignment. Additionally, [10] presents a collection of focus loss functions for event-based vision, which can be beneficial in the context of the target paper's proposed framework [7, 8, 10].

In conclusion, the target paper makes significant contributions to the field of event-based motion estimation by introducing a progressive spatio-temporal alignment framework. It builds upon and aligns with prior works in the domain, leveraging concepts such as contrast maximization, spatiotemporal registration, and efficiency in motion estimation. The proposed framework demonstrates state-of-the-art accuracy and efficiency, paving the way for advancements in event-based motion estimation methods [1, 3, 5, 6, 7, 8, 10].<>"
1090,1090," **Pre-trained Vision-Language Models.** Pre-trained language models (PLMs) [1]; [2] have been widely used in various vision-language tasks, such as image captioning [3], visual question answering [4], visual grounding [5][6][7][8], video understanding [9][10][11][12][13][14], etc. Recently, several works [15][16][17][18] have proposed to use the PLMs to learn the ""soft prompts"" to condition the pre-trained models on the downstream tasks. For example, [15] proposed Visual Prompt Tuning (V2T), which learns the soft prompts conditioned on the image and text embeddings. [17] proposed Conditional Prompt Learning (CPL) that learns the prompt distribution to condition on the input image and textual embedding. [18] proposed Prompt Distribution Learning (PDL), which uses the learned prompt distribution as the initialization of the task-specific embedding space. However, these methods are not suitable for the few-shot scenario, since the learning of the prompt initialization is sensitive to the initialization and requires a time-consuming process to find a good initialization.


**Few-shot Learning.** Few-shot learning aims to learn a model that can generalize well to new tasks with only a few labeled examples [19][20][21][22][23][24]. Recently, meta-learning [25][26][27][28] has been proposed to solve this problem. In this work, we focus on the few shot learning problem, where we aim to improve the generalizability of PLMs.

 proposed to learn task-agnostic embedding spaces for image-text data.  proposed a simple yet effective meta-learner that learns a task-invariant embedding for image classification tasks.  designed a simple but effective model that learns to generate task-relevant embedding vectors for image caption generation tasks. In contrast, our GRAM is designed for generalizable PLMs, which can be applied to various downstream tasks with limited labeled data.



"," **Prompt Tuning.** Prompt tuning is first introduced in the NLP area [1] to close the gap between pre-training and downstream tasks. Petroni et al. [2] manually create cloze-style prompts to elicit knowledge from pre-trained language models in a ""fill-in-the-blank"" way. Further, prompt tuning is introduced in vision-language understanding , which can enhance the generalizability of large vision-language models [3][4][5] on a wide range of vision-language understanding tasks [6][7][8][9][10][11][12][13][14]. As manually designing suitable prompts for different tasks is time-consuming and usually sub-optimal, recent works [15][16] propose to optimize a set of continuous learnable prompt embeddings. Concretely, CoOp  optimizes continuous prompt embeddings to improve the few-shot generalizability of CLIP. CoCoOp [17] proposes to learn image-conditioned prompts to further improve the generalizability of CoOp. ProDA [18] learns a distribution of diverse prompts via Gaussian distribution to handle the varying visual representations. To further enhance CLIP's adaption capability, Tip-Adapter builds a key-value cache model from the few-shot training samples to perform feature retrieval. Instead of designing a specific prompt tuning method, we propose a model-agnostic meta-prompt learning framework to improve the adaptation ability and cross-domain generalizability of the prompt tuning methods, which can be incorporated into existing methods in a plug-and-play fashion.

**Meta-Learning.** Meta-learning aims to enable efficient adaptation ability of models by leveraging the experience from learning across a set of tasks. Meta-learning approaches can be categorized as: metric-based [19][20][21], memory-based [22][23][24], and optimization-based [25][26][27]. Our framework is based on the optimization-based method (_i.e._, MAML [26]). Rather than relying on human-annotated meta-training tasks, our method can automatically generate a diverse set of meta-training tasks by cross-modal hierarchical clustering. Li et al. [28] propose to synthesize domain shift during meta-training to learn a domain-generalizable initialization. Differently, we present a novel gradient regulating function that actively transforms the updated gradient into a domain-generalizable direction.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]"," **Prompt learning.** Prompt learning is a recently emerging paradigm in NLP [1][2], which aims to learn the ""soft prompts"" to condition the pre-trained models to the downstream tasks. It has been widely used in various vision-language tasks [3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28]. In this paper, we focus on the few-shot scenario, where prompt tuning is the most effective way to adapt pre-training models to new tasks.

**Prompt tuning.** The prompt tuning paradigm was first introduced in [15] as an efficient and effective alternative to full fine-tuning for large-scale vision-linguistic models. The authors proposed Visual Prompt Tuning (VPT) to learn soft prompt parameters that can be used to condition a frozen pre-train model on downstream tasks, which is similar to the prefix tuning [16] in the NLP domain. However, VPT requires a time-consuming process to find a good prompt initialization, which limits its scalability and effectiveness in the few shot scenario. To address this issue, we propose a novel Gradient-Regulated Meta-Prompt Learning (GRAM) framework that jointly meta-learns an efficient soft prompt initialization and a lightweight gradient regulating function for better adaptation.

"," **3D-Aware GANs.** The first few works on 3D-aware GANs [1][2][3] rely on 3D geometric models such as human models and scans to obtain 3D data. A more general setting is to learn an explicit generative model for 3D radiance fields [4][5][6][7][8][9][10][11][12][13][14][15][16][17][18]. GRAF [16] learns a novel latent variable in 3D space that is used for 3D generation. p-GAN [15] embeds an infinite grid onto a periodic box and approximates periodic sampling as an inverse discrete Fourier transform. CodeNeRF [13] is a recent work that allows GANs to represent a large category of objects. Instead of directly learning from unstructured 2D images, several works use deep generative models to generate multi-view slices as pre-images to be used for rendering 3D scenes. SliceGAN  and NeRF+ [12] render multi-view scenes from 2D slices. However, it is prohibitively expensive to generate huge amount of slices. Mip-NeRF [10] introduces the Mip-NeRF framework that first converts the original unstructured training set into 4M unstructured images by applying the indicated camera projection and then render the 4M images using Mip-NeRF. Then, these 4M images are converted back to 3D via the indicated camera projection and then processed by 3D methods such as NeRF. [11] uses a similar setup as [10] to enhance the performance of Mip-NeRF. Both [10] and [11] have to apply a 2D-to-3D and a 3D-to-3D projection which results in 2.5D-like representations. To overcome the problem of 2.5D-like representations, we learn 3D-aware GANs that generate 3D radiance fields.

**Multi-view Consistent GANs.** A key to enabling 3D-consistent generation is that a 3D-aware GAN generator must be able to retain the camera projection and the object shape in a radiance field at different viewpoints. This is different from earlier GAN-based 3D-aware image synthesis methods [1][2] which only consider only one camera viewpoint. MCVGAN [19] learns 2D networks for multiple views. AutoInt [20] uses a genetic algorithm to integrate multi-view radiance fields. GRAF [16] uses a novel 3D latent variable which can serve as the ground truth and enforce consistency during training. GIRAFFE [2",,"\<The proposed Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models addresses the limitations of prompt tuning, a paradigm for adapting vision-language pre-training models to downstream tasks efficiently. Prompt tuning demonstrates sensitivity to initialization in the few-shot scenario, leading to time-consuming searches for optimal initializations. Additionally, it can undermine generalizability due to overfitting of learnable prompt tokens to limited training samples [1]. To mitigate these issues, the authors introduce a novel Gradient-Regulated Meta-prompt learning (GRAM) framework that meta-learns an efficient soft prompt initialization and a lightweight gradient regulating function to ensure strong cross-domain generalizability. GRAM uses only unlabeled image-text pre-training data and can be incorporated into various prompt tuning methods in a model-agnostic manner. The comprehensive experiments demonstrate consistent improvement across several methods with the integration of GRAM [2].

The concept of meta-learning has gained prominence in the literature, particularly in the context of few-shot learning and domain generalization. Within the field of natural language processing, meta-learning has been explored in the context of unsupervised multitask learning [3]. Furthermore, recent progress in pretraining language models has led to the emergence of novel approaches such as Conditional Prompt Learning [4], which has demonstrated promising results, particularly in the adaptation of pre-trained models to downstream datasets. These advancements highlight the potential of meta-learning in addressing the challenges of rapid adaptation and generalization across various domains. An important direction for future research lies in investigating the application of meta-learning techniques to vision-language models to further enhance their adaptability and generalizability.

In the context of visual navigation, meta-learning has been utilized to learn transferable meta-skills for embodied navigation tasks [5]. This approach enables the agent to quickly adapt to new environments without requiring extensive training data. Additionally, recent work has explored the application of meta-learning in domain generalization, aiming to improve the generalizability of models to novel testing domains [6]. By simulating train/test domain shifts during training, meta-learning procedures enable models to exhibit robustness to domain shift, a crucial aspect in real-world applications. These developments underscore the potential of meta-learning paradigms in addressing challenges related to adaptation, generalization, and robustness across various domains and tasks.\<

References:
[1] Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models, <full reference>
[2] Language Models are Unsupervised Multitask Learners, <full reference>
[3] Learning Transferable Visual Models From Natural Language Supervision, <full reference>
[4] Conditional Prompt Learning for Vision-Language Models, <full reference>
[5] Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation, <full reference>
[6] Learning to Generalize: Meta-Learning for Domain Generalization, <full reference>"
5385,5385," **Domain generalization.** Domain generalization (DG) aims to generalize to unseen domains where labeled data is scarce [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31]. In this paper, we focus on the task of synthetic-to-real domain generalization [32].

**Fourier domain.** The Fourier domain [32][33][34][35][36][37][38][39][40][41][42] has been shown to be a powerful tool for domain adaptation. In particular, it has been used to improve the robustness of deep neural networks [43]. In [43], the authors use the Fourier transform to improve robustness to noise and outliers. In this work, we use the frequency domain to improve generalization in the domain of synthetic to real data.

 propose a domain-invariant feature alignment technique. They use the amplitude spectra of the input images to align the features of different domains. In contrast, we propose to use the high-frequency components of the images to learn domain invariant features. We show that our method outperforms their method on the tasks of semantic segmentation, object detection, and image classification.

 also propose to align features in the spatial domain. However, they do not consider the temporal domain.

In contrast, our method uses the amplitude spectrum of the synthetic images to improve domain invariance.

 proposes a domain adversarial training method. They apply adversarial perturbations to the feature maps of the synthesized images. They show that their method improves the generalization performance of the model. In comparison, we show that the proposed method is more effective than their method.

 introduces a domain adaptation method. Their method is based on the idea of adversarial dropout [27]. However, their method is not applicable to our task since we do not have access to the real-world data. In addition, their approach is not scalable to large-scale datasets.

 presents a domain adaptive model. Their model uses adversarial drops in the feature space. Their approach is applicable to the domain adaptation task, but not the DG task. They also propose a multi-task framework. Their framework consists of two components. The first component is a domain classifier. The second component is an ensemble of domain classifiers. Their work shows that their approach outperforms our approach.

 uses a domain discriminator to learn a domain agnostic feature extractor."," **Domain Generalization (DG).** DG typically involves training models on single or multiple labeled data sources to generalize well to novel test time data sources (unseen during training). Several approaches have been proposed to tackle domain generalization [1][2], such as decomposing a model into domain invariant and specific components and utilizing the former to make predictions [3], learning domain specific masks for generalization , using meta-learning to train a robust model [4][5][6][7], manipulating feature statistics to augment training data [8][9][10], and using models crafted based on risk minimization formalisms [11]. More recently, properly tuned ERMs (Empirical Risk Minimization) have proven to be a competitive DG approach [12], with follow-up work adopting various optimization and regularization techniques [13][14] on top.

**Single Domain Generalization (SDG).** Unlike DG which leverages diversity across multiple sources for better generalization, SDG considers generalizing from a single source. Notable approaches for SDG use meta-learning [15] by considering strongly augmented source images as meta-target data (by exposing the model to increasingly distinct augmented views of the source data [16][17]) and learning feature normalization schemes with auxiliary objectives [18].

**Synthetic-to-Real Generalization (Syn-to-Real).** Prior work on syn-to-real generalization has mostly focused on some specific methods, including learning feature normalization / whitening schemes [19], using external data for style injection [20][21], explicitly optimizing for robustness [22], leveraging strong augmentations / domain randomization [23][21], consistency objectives [24] and using contrastive techniques to aid generalization [25]. Some approaches have also considered adapting from synthetic to real images, using techniques such as adversarial training [26], adversarial alignment losses [27], balancing transferability and discriminability [28] and feature alignment [29]. Pasta is more similar to the kind of methods adopting augmentations for improving out-of-the-box generalization We consider \(3\) of the most commonly studied syn-to-real generalization settings - (1) Semantic Segmentation - GTAV \(\rightarrow\)Real, (2) Object Detection - Sim10K [30]\(\rightarrow\)Real and (3) Object Recognition - VisDA-C [31] Syn\(\rightarrow\)Real. [32] recently proposed tailoring synthetic data for better generalization.

**Fourier Generalization & Adaptation Methods.** Prior work that explored augmenting images in the Fourier domain (as opposed to the pixel space) rely on a key empirical observation [33][34][35][36] that the phase component of the Fourier spectrum tends to preserve high-level semantics, and therefore, they focused mostly on perturbing the amplitude. Pasta is in line with this style of approach. Amplitude Jitter (AJ) [37] and Amplitude Mixup (AM) [37] are methods similar to Pasta that augment images by perturbing their amplitude spectra. While AM mixes the amplitude spectra of different images, AJ applies uniform perturbation with a single jitter value \(\epsilon\). FSDR [38], on the other hand, isolates domain variant and invariant frequency components using extra data and sets up a learning paradigm. Building on top of [37][39] adds a significance mask when linearly in terpolating amplitudes. [40] only perturbs image-frequency components that capture little semantic information. [41] uses an encoder-decoder to obtain high/low frequency features and augments images by adding noise to high frequency phase and low frequency amplitude. [42][43] study how amplitude and phase perturbations impact robustness to natural corruptions [44]. In contrast to these works, Pasta's simple strategy of perturbing amplitude spectra in a structured way and leads to strong out-of-the-box generalization without the need for specialized components, extra data, task-specific design, or changes to learning rules.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]"," **Domain Generalization.** Domain generalization (DG) aims to generalize a model trained on multiple source domains to a novel unseen target domain [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44]. In this paper, we focus on DG for the tasks of semantic segmentation, object detection, and object recognition.

**Synthetic-to-Real Domain Generalization (S2R DG).** The goal of S2R is to learn a model that can generalize to unseen domains with only one source domain data for training. Existing methods for this task can be divided into two main categories: (1) _model-agnostic_ methods [4][7] that learn domain-invariant features by minimizing the discrepancy between source and target domains [2][7], and (2) _adversarial training-based methods_[4][6] that train a model to fool a domain discriminator [27][4] or a meta-learner [6] to fool the domain classifier [4].

1

] or meta-learners [4] to generate virtual training and test sets to simulate domain shift. (3) _domain augmentation-based approaches_[15][18] that synthesize new domains by adversarial domain augmentation [18][13] or synthesizing multiple domains [15][17]. (4) _image-level domain augmentations_[26][17] that generate multiple domains by changing the style of the source domain images [23][20], or by adding style information to the source images [24][19]. (5) _instance-level augmentation_[19] that whitens the images of the target domain to improve the generalization ability of the model. (6) _category-level adaptation_[28][25] that uses category information to improve generalization. (7) _feature-level alignment_[16][29] that enforces the model to learn domain invariant feature representations [28][17], or (8) _style-level transfer_[23][25"," **Fine-grained classification.** There is a substantial body of literature dedicated to the problems of fine-grained classification and fine-grained feature learning. We refer the reader to the recent survey [1] for an overview of the progress in both areas. In this work, we focus specifically on the problem of discovering object parts by using only image-level labels.

**Attribution methods.** One family of work aims at visualizing the model's attention, and proposing to explain decisions through human-understandable visual reasons. Among these works, TSN  proposes to compute gradients of feature maps, and interpret the image using the gradients in a region-wise manner. More advanced works also estimate confidence maps to visualize the importance of image regions. [4] uses the mean gradient over regions computed by PSF.  computes the partial derivatives to have spatially dense attention visualizations. Another class of work regards attention as a high-dimensional vector using linear projections, and visualizes the importance with cosine similarity. Typical choices for such projections include RISE [12], L2Loss , or B-cos [17], where each element of the projection is the similarity with each feature map. Except for [12], most of these works do not provide guarantees for the interpretability of their attribution. Recently, several papers [15]; [20]; [16]; [18]; [13]; [9]; [3]; [11] examine the limitations of these methods, and argue for more theoretical guarantees. Nevertheless, the differences between the gradients and the projections of attention are still not fully understood. The question remains unsolved if the gradients computed with respect to a particular part are specific to the part or can be interpreted to explain the rest of the model. [5] uses a linear projection of feature maps that only attends to part areas, but it does not consider the rest of the model. Our proposed losses are a part of the formulation of a Bayesian attribution method in [14].

The second line of work visualizes the attention on all inputs that contribute to the prediction, but the selection of inputs is not based on the input importance but by the manual attention visualization. For example, [6] uses a visual attention visualization method, and then, by hand picking the most important part regions that were predicted to be the class, it computes the contributions of the different parts to the classification. Also, [8] uses a reinforcement learning to discover significant parts and uses a similar localization to form a class activation map. [7] also uses a reinforcement learning, but instead of using a localization, it uses the entire parts as labels, and propagates labels to find out the most important parts for classification. [2] proposes a method that discovers important parts and uses them for a part-based fine-grained classification task. In contrast, our proposed method does not use an attention visual",,"\<In recent years, the problem of domain generalization has garnered significant attention in the field of machine learning. The goal of domain generalization is to enable models to generalize well to unseen domains, particularly when training data is available from multiple related domains [2]. One approach that has been explored is domain-invariant feature representation, which aims to minimize dissimilarity across domains while preserving the functional relationship between input and output variables [2].

Another prominent direction in domain generalization is the use of generative modeling and data augmentation strategies. For instance, the use of synthetic data and its augmentation has shown promise in improving out-of-the-box synthetic-to-real generalization performance. This is exemplified in the Proportional Amplitude Spectrum Training Augmentation (Pasta) strategy, which perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Pasta has demonstrated improved performance in syn-to-real shifts for tasks such as semantic segmentation, object detection, and object recognition [1].

Furthermore, the role of meta-learning and regularized empirical risk minimization in domain generalization has been extensively explored [4]. Meta-learning methods for domain generalization focus on training models with good generalization ability to novel domains by simulating virtual testing domains within each mini-batch, thus encouraging models to generalize well to unseen domains [4].

Moreover, recent advances in domain generalization have delved into multi-task learning frameworks. For instance, the Multi-Task Autoencoder (MTAE) algorithm has been proposed for feature learning that provides good generalization performance for cross-domain object recognition. MTAE extends the standard denoising autoencoder framework by learning to transform the original image into analogs in multiple related domains, thereby learning features that are robust to variations across domains [3]. These approaches demonstrate the diverse strategies and methodologies being investigated for domain generalization.

In addition to these strategies, the significance of frequency domain-based disentanglement and interaction in domain generalization has gained attention. The use of frequency-based disentanglement and amplitude-phase recombination has showcased potential in enhancing model robustness and generalization in computer vision tasks [42][43].

Overall, the domain generalization literature demonstrates a rich landscape of approaches encompassing generative modeling, meta-learning, multi-task learning, and frequency domain-based methods, aimed at tackling the challenge of generalizing to unseen domains. As the field continues to evolve, these diverse approaches open up new avenues for building robust and generalizable machine learning models across diverse domains.>

"
2654,2654," **Supervised Crowd Counting.** Supervised methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] rely on large-scale annotated datasets, which are expensive to collect. To reduce the cost of manual labeling, some methods [30][31][32][33] have been proposed to solve the crowd counting problem in an unsupervised manner. However, most of these methods are based on hand-crafted features. In this paper, we propose an end-to-end trainable deep neural network for crowd counting.

**Contrastive Learning.** The recent contrastive learning (CLIP) [34] has achieved impressive performance on various vision-language tasks. The key idea of CLIP is to learn a mapping between image patches and corresponding text embeddings. In contrast to previous CLIP-based methods [34][34], we propose a novel multi-modal ranking loss to learn the mapping from image patches to text embedding.

 propose a multi-task learning framework to improve the performance of object detection and semantic segmentation tasks.  propose a self-supervised learning framework for object detection, which learns a mapping from the input image to a set of object proposals. In our work, we focus on learning the mapping between crowd patches and the corresponding text.




"," The mainstream idea of supervised methods [1][2][3][4][5][6] is to regress a density map, which is generated from an elaborately labeled point map. In general, the labeled points are hard to reflect the size of the head, meaning the density-based paradigm easily meets the huge variation issue. To tackle the scale variations, various methods make many attempts. Specifically, some works [4][7] adopt multi-column networks to learn multi-scale feature information. Some methods propose to utilize the scaling mechanism [8][9][5] or scale selection [6] to relieve the scale variations. The attention mechanism is also a valuable tool to improve the feature representation, such as self-attention [10], spatial attention [11][12], and other customized attention blocks [13][14][15]. Different from regressing density maps, some methods [16][17][18] leverage supervised-classifier to classify the crowd into different intervals, achieving appealing performance.

Another trend is based on localization [19][20][21][22][23][24], which can be divided into three categories: predict pseudo-bounding boxes [25][22][23] or customize special localization-based maps [19][21], the other methods [26] directly regress the point coordinates, removing the need for pre-processing or post-processing.

The fully-supervised methods need expensive costs to label points for each head. To this end, weakly or semi-supervised methods are proposed to reduce the annotations burden. The weakly-supervised methods [27] suggest using count-level instead of point-level annotation as the supervision. The semi-supervised methods [28][29] leverage small-label data to train a model and further use massive unlabeled data to improve the performance. Method in [30] optimizes almost 99% of the model parameters with unlabeled data. However, all the above methods still require some annotated data. Once the labeled data is left, these models can not be trained.

So far, only one method, CSS-CCNN , focuses on pure unsupervised settings, _i.e_., training a model without any label. CSS-CCNN  argues that natural crowds follow a power law distribution, which could be leveraged to yield error signals for back-propagation. We empirically find that there is a significant performance gap between CSS-CCNN and some popular fully-supervised methods [31][7]. In this paper, we propose a novel method named CrowdCLIP, which transfers the counting into an image-text matching problem, to boost the performance for unsupervised crowd counting by a large margin.

Recently, vision-language pre-training (CLIP [32]) using massive image-text pairs from the Internet has attracted more and more attention. Several methods [33][34][35] transfer the vision-language correspondence to the downstream tasks, such as object detection [35], semantic segmentation , and generation . Benefiting from the strong zero-shot ability of CLIP, these unsupervised methods achieve promising performance. In this paper, we study how to transfer the vision-language knowledge to the unsupervised crowd counting task.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]"," **Supervised Crowd Counting.** Supervised crowd counting methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35] have been extensively studied in recent years. These methods can be roughly divided into two categories: regression-based methods and detection-based ones. Regression-based approaches [4][7] directly regress the density map from the image. However, they are prone to be inaccurate in dense crowd scenes due to the large scale variations. To address this issue, some regression-free methods [4] are proposed to directly regress density maps. However they are sensitive to the quality of the density maps, which may be inaccurate and error-prone. To alleviate this issue and improve the counting accuracy, some methods [5][16] propose to use the predicted density maps as the intermediate representation to guide the counting process. For example, [5] proposes to use a multi-level network to generate the patch-level density maps and then group them into several density levels. [6] proposes a scale-adaptive selection network to mitigate the scale variation. [10] introduces the multi-stage attention mechanism to improve the performance of the counting model. [8] proposes an attention-based network to address the scale variations in crowd counting. [9][14] introduce the attention mechanism into the counting task. [11] proposes the supervised focus from segmentation to improve counting performance. [12] proposes category-aware attention to suppress the negative interaction among different categories of objects.


**Unsupervised Crowd counting.** Unsupervised methods [28][30] have not been widely explored in recent trends of crowd counting due to its high cost and labor-intensive annotation process. [28] introduces a spatial uncertainty-aware semi-supervised approach via regularized surrogate task (binary segmentation) for crowd counting, which focuses on the noisy supervision from the unlabeled data in an end-to-end manner. [29] proposes three components to tackle those unannotated regions: i) in an Unannotated Regions Characterization (URC) module, a memory bank to only store the annotated features, which could help the visual features extracted from these annotated regions flow to these un"," **Adversarial attacks** aim to fool classifiers into misclassifying their outputs. In the context of deep learning, adversarial attacks focus on the natural input data samples. These are applied to samples from the data distribution ([3]) using either high-level perturbations (e.g., GLR methods ([2]) and deepfool ([1])) or lower-level perturbations ([6]). For example, in single-step adversarial attacks, the model outputs are estimated for different perturbed data points. Then the perturbed data points are determined by searching in the space of adversarial examples (; [4]; [5]).

In contrast, **backdoor attacks** focus on altering the training data to corrupt the model. Typical backdoor attacks are performed by adding a specific trigger pattern to an image. These data perturbations are typically very small, as they need to be undetectable by the model during the training process ([15]; [20]). Recent backdoor attacks include traffic light pattern attacks (), star shape attacks ([17]), jumping backdoor attacks (), dotted backdoor attacks ([16]), and outline attacks ([14]; [12]; [15]; [11]). Although backdoor attacks aim at the model training stage, there are also attacks aiming at the model prediction stage ([10]; [13]). While backdoor attacks succeed in crafting adversarial examples, they may not generalize well as the perturbations may be confined in a specific region of the image space. [18] highlight that even when the train and test data are sampled from the same distribution, backdoor attacks do not guarantee test-time performance.

**Poisoning attacks** focus on the training data to induce the model to misclassify the targeted test images ([10]; [19]; [24]; [18]; [16]). The data perturbations in poisoning attacks are typically small and crafted to target specific images. [24]; [18]; [16]; [10] implement these perturbations to identify targets. They present a pre-defined training data set of known targets that are intentionally poisoned to guarantee that models that use these attacks can discriminate the test samples into the poisoned subset and the non-poisoned subset. [24] and [10] conduct a local optimization to train the model, while [18]; [16]; [24] perform gradient ascent to achieve this goal. In addition, some recent works present ways to remove these poisoned samples to perform certified backdoor attacks (; [12]; [14]; ).

**Unlearnable data** aims to make test data unrecognizable to a specific model while retaining their label space. [25] show that images with regular-looking adversarial perturbations can be linearly separable when assigning the class label of the perturbation to the image, and thus they can work as shortcuts for the learning objective. [",,"<Related work>
Crowd counting has been a challenging task, especially in dense scenes, due to the costly manual labeling required for supervised methods [1]. Existing approaches, such as CSRNet [1], have proposed deep learning methods for accurate count estimation and high-quality density maps in congested scenes. However, unsupervised methods have also gained attention, as they alleviate the need for manual annotations. For instance, Exploiting sample correlation for crowd counting with multi-expert network [2] focuses on efficiently training more expert networks for crowd counting using similarity metrics. Meanwhile, Vision Meets Drone Crowd Counting Challenge Results [3] had a significant impact by providing a large drone-captured dataset for crowd counting research.

In the realm of crowd counting, numerous methods have sought to improve accuracy and address challenges such as extreme density variations. For instance, CNN-Based cascaded multi-task learning of high-level prior and density estimation for crowd counting [4] proposed a novel end-to-end cascaded network for jointly learning crowd count classification and density map estimation. Additionally, Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting [5] tackled extreme density variations by proposing a strategy to normalize dense crowd patches into several clusters for improved density map learning.

Furthermore, the challenge of large scale variation problem in crowd counting has been addressed by proposed methods such as To Choose or to Fuse? Scale Selection for Crowd Counting [6], which introduces a Scale-Adaptive Selection Network for automatically learning the internal correspondence between scales and feature levels. Additionally, Single-Image Crowd Counting via Multi-Column Convolutional Neural Network [7] developed a multi-column CNN architecture to accurately estimate crowd count from individual images with arbitrary crowd density and perspective. These methods demonstrate the ongoing advancements in addressing challenges related to crowd counting."
2338,2338," **Object Attributes.** Attributes have been widely studied in the computer vision community [1][2][3][4][5][6][7][8][9][10][11][12][13]. In this paper, we focus on the most related task, _i.e._, object affordance recognition [14][15][16][17][18][19][20][21][22][23][24]. Affordance recognition is a fundamental task in computer vision, which aims to predict the properties of an object (_e.g._, _position_, _action_, and _action type_) given an image. In this work, we study the problem of _what attributes an object has_ and _how we can use it_.

**Causal Reasoning.** Causal inference [25] is a long-standing research topic in machine learning [26][27][28][29][30][31][32][33][34]. Our work is closely related to the field of _causal reasoning_, where the goal is to infer the causal relationship between two or more variables. However, our work is different from previous works in two aspects: (1) we study a new concept (object concept), and (2) we propose a new task (object attribute reasoning).

 proposes a causal reasoning task for object recognition. It focuses on the relationship between objects and their attributes. In contrast, our OCR task aims to learn the relations between object categories and their affordances, which is a more challenging problem.

 propose a causal inference framework for object classification. Their model is based on a multi-layer perceptron (MLP) and a recurrent neural network (RNN). Our model differs from theirs in three aspects. First, our model is fully end-to-end trainable, while their model is trained in a fully-supervised manner. Second, their model only considers one level of object concept (category), while our model considers three levels (category, attribute, and affordance). Third, their causal reasoning model is only trained on synthetic data, while ours is trained on real-world data.

Our work is also different from other related works in the following aspects:

1. We propose a novel OCR benchmark for object concept learning. We also present a novel model (OCRN) for object attribute reasoning. Our model can be trained with a large amount of unlabeled data, and it is trained with an adversarial loss function.

 is a large-scale synthetic dataset for object compositional reasoning. It consists of synthetic scenes with diverse objects and attributes. Our proposed OCR dataset is much larger and more diverse.

 and  are two synthetic datasets for object composition reasoning. The former is a synthetic dataset with synthetic scenes and attributes, while the latter is a real-life dataset with realistic scenes and objects. Our work differs from them in several aspects: 1) our dataset is more diverse and more realistic; 2) our model has a more complex structure.

 provides a more detailed description of our dataset. We compare our model with theirs in Table 1"," **Object Attribute** depicts the physical properties like color, size, shape, _etc_. It usually plays the role of intermedia between pixels and higher-level concepts, _e.g_., prompting object recognition [1], affordance learning [2], zero-shot learning [3], and object detection . Recently, several large-scale datasets [1][4][5][6][7] are released. For attribute recognition, besides direct attribute classification [3][4] and leveraging the correlation between attribute-attribute and attribute-object [8][9][10], intrinsic properties (compositionality, contextuality [11], symmetry [12][13]) of attribute-object are also proven useful. [11] uses the model weight space to encode the attributes to model the compositionality and contextuality.  uses the attributes as linear operators to transform object embeddings. [12] leverages the symmetry property to model the attribute changes within attribute-object coupling and decoupling.

**Object Affordance.** is introduced by [14]. Affordance learning has two canonical paradigms: direct mapping [15] or indirect method [16][17] with intermediates like object category, attribute, and 3D contents. Some works learned affordance from human-object interactions (HOI) to encode the relation between object and action [18][19]. Visual Genome  provides relations between objects, including actions instead of affordances. However, these relations cover limited and sparse affordances. Differently, we use easily accessible object images as the knowledge source and densely annotate all attributes/affordances for all objects. Besides the vision community, the robot community pays much attention to affordance [20][21] for grasping and manipulation. For instance, [20] utilized the robot to discover the object affordance via self-supervised learning. Recently, several datasets [22][23][24] have been proposed. IIT-AFF [22] collected ten daily indoor objects and provided nine common affordances to construct a dataset for robot applications. Zhu _et al_.  built a dataset containing attribute, affordance, human pose, and HOI spatial configuration. But labeling pose and HOI are costly. Chao _et al_. [23] proposed a _semantic_ category-level affordance dataset including 91 objects  and 957 affordances.

**Causal Inference.** There is increasing literature on exploiting causal inference [25] in machine learning, especially with causal graphical models [26][25], including feature selection [27] and learning [28], video analysis [29][30], reinforcement learning [31][32], _etc_. Recently, Wang _et al_. [33] studied the causal relation between objects in images and used intervention [25] to alleviate the observation bias. Atzmon _et al_. [34] analyze the causal generative model of compositional zero-shot learning and disentangle the representations of attributes and objects. Here, we explore the causal relations between three object levels and apply backdoor adjustment [25] to alleviate the existing bias.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]"," **Object Concept Learning.** Object Concept Learning (OCL) is a challenging task that requires machines to infer the affordances of objects and reason why they possess them. It is closely related to object attribute recognition [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24], affordance detection [25][25][26][27][28][29][30][31][32][33][34], and visual causal reasoning [35][36][37][38].

**Object Concept Recognition.** OCRN is a new benchmark for object concept learning. It requires the model to predict the affordance of an object and reason the reason why it has this affordance. It differs from existing object recognition tasks in that OCL requires the models to learn higher-level object knowledge, _i.e.,_ what attributes an object has, and what we can do with it.


\[\begin{tabular}{l c l l l c l c c l r l r r l l r c c r l c r r r c r c l. \end{l l l } \] (1) **Object Recognition Benchmark.** The object recognition task in OCL is different from object recognition in that it requires the machine to predict affordances, _e.g.,_, _what attribute an object possesses, and _why_ it has it. It also differs from the object recognition problem in that object recognition is a single task, while OCL focuses on inferring the object knowledge at three levels: category, attribute, affordance, and the reason. **OCL Benchmark**: OCL Benchmark is a novel object concept reasoning task. It has three levels of object knowledge: category (category-attribute, attribute-affordance), affordance (attribute-attribute-affordable), and reason (reason-attribute). It is also different from the existing OCRB [1], which only focuses on the attribute level. OCL has three different levels of knowledge: _category, attribute and affordance_. The category-attribute level has been studied in [1] and [2]. The reason level has not been studied yet. The reason-attribute level has also not been explored in [2][1]. The main difference is that [2]"," **Zero-shot Object Segmentation.** The primary goal of zero-shot object segmentation is to apply the learned instance classifier to novel classes for segmentation with novel categories [1][2][3]. In a zero-shot setting, all classes in the training set are known to the model. However, in a more realistic zero-shot setting, classes in the training set are known to the model as well as some novel classes not in the training set [4][5][6][7]. This is much more difficult since unseen classes are learned using a set of weakly-supervised clustering methods [4][5][6][7]. The studies that utilized caption data are able to detect novel categories by matching captions to novel regions. However, such methods are not perfect as they rely on either lexical match to textual annotations [1], or textual comparison between categories [8].

**Open Vocabulary Object Detection.** Previous studies on open vocabulary detection [9][10][11] share similar objectives, with methods considering how to model the relationship between textual and image data. To account for the unseen categories, recent works apply prompting strategies to a pre-trained image and language model to generate a textual caption with the image [12][13][14][15]. When handling novel categories, the detection head of such methods can be trained for the categories with captions. However, for unseen categories without corresponding captions, one way is to leverage background knowledge in the unseen caption. [12][13] and [14] generate pseudo-captions by matching seen classes to unseen classes. This pseudo-labeling strategy is effective for learning boundary segmentation, where captions have high inter-correlation with boundary information. However, we find that these strategies are less effective when applying to the inside of novel classes, where our method can greatly boost detection accuracy.

**Caption Generation and Zero-shot Segmentation.** Caption generation models [16][17][18][19][20] generate natural language descriptions for images.  proposes a mixed-modality framework for caption generation and segmentation in the zero-shot settings. However, this approach only considers the caption data for seen classes, thus relying on lexical match to unseen classes [1] or adopting semantically related words to the classes [1][21]. To this end,  adopts multi-modal fusion by the mean of extracting global semantic features, while we propose an inter-modality fusion by matching image regions and words, which can handle unseen classes more efficiently.

",,"<Related work>

The development of advanced visual recognition models has significantly enhanced object recognition capabilities. However, learning higher-level knowledge about objects, including their attributes, functions, and affordances, remains a challenging task. One approach to address this challenge is the concept of attribute-based recognition [1]. This paradigm focuses on inferring the attributes of objects, enabling recognition beyond simple naming and facilitating the generalization of attributes across object categories. In addition, recent work has emphasized the importance of physical and visual attributes in predicting affordances for autonomous robots [2]. Leveraging attribute-based mid-level representations has shown superior performance in affordance prediction, indicating the potential of using attributes for understanding object functionalities.

Another relevant area of research is attribute-based classification for object detection, particularly when training and test classes are disjoint. This scenario is common in real-world applications due to the vast number of object classes without specific training examples. Attribute-based classification, using human-specified high-level descriptions of target objects, showcases potential in detecting new object classes based on their attributes [3]. Additionally, the concept of compositionality and contextuality in attribute and object recognition has been explored [11]. It highlights the importance of modeling context-driven attribute compositions, which is crucial for building comprehensive and context-aware recognition systems.

Within the field of computer vision, the development of extensive datasets for scene and object recognition has been pivotal. The SUN database, with its large-scale scene recognition incorporating 899 categories, has improved the performance bounds of state-of-the-art algorithms [4]. Furthermore, research has also focused on learning multifunctional binary codes for both category and attribute-oriented retrieval tasks, showcasing the capability of hashing methods in addressing multiple retrieval tasks simultaneously [5]. These advances demonstrate the potential for integrating category and attribute information for multifunctional visual retrieval tasks.

In the realm of embodied AI, understanding objects extends to grasping their functionalities and affordances. Research on object function learning has emphasized the significance of discovering all possible functionalities of objects, using weakly supervised methods and 3D image similarity evaluation for coherent clustering of human-object interactions [19]. Moreover, efforts to scale affordance learning from large-scale datasets extracted from sitcoms have emerged as a valuable approach, where a diverse set of scenes and human-object interactions were utilized to develop a substantial dataset for learning affordances [17].

<References>
[1] Describing objects by their attributes
[2] Affordance Prediction via Learned Object Attributes
[3] Learning to detect unseen object classes by between-class attribute transfer
[4] SUN database: Large-scale scene recognition from abbey to zoo
[5] Learning Multifunctional Binary Codes for Both Category and Attribute Oriented Retrieval Tasks
[11] From Red Wine to Red Tomato: Composition with Context
[19] Discovering Object Functionality"
3187,3187," **Human-Object Interaction.** Human-object interaction is a challenging task due to the large number of non-rigid and non-collinear deformations of the human body. Many methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] have been proposed to tackle this problem. For example, NeuralHumanFVV [13] and NeuralHumanRendering [1] use a multi-layer perceptron (MLP) to model the 3D human body as a voxel grid and a neural radiance field (NeRF) to represent the scene. However, these methods are not able to handle the complex human-object interactions. Human-Object-NeRF [15] is the closest work to ours in spirit. It also uses a neural network to model human and object interactions, but it requires a pre-extracted human skeleton as input. In contrast, HOSNeRF does not require any external human model.

**View Interpolation.** View interpolation methods [18][19][20][21][22][23][24][25][26][27][28][29] aim to generate novel views of a scene by interpolating between different views of the same scene at different time steps. These methods can be divided into two categories: temporal interpolation and view synthesis. Temporal interpolation based methods [19][21] interpolate between different frames of a video by rendering the scene from a single frame. For instance, DynamicFusion [23] and NeRF  reconstructs a scene from multiple frames of an RGB video. NeRF-based methods [24][26] reconstruct a dynamic scene from an RGB image and render it from different viewpoints. However these methods require a large amount of training data, which is difficult to obtain in real-world scenarios. In this work, we propose a novel 360\({}^{\circ}\) free-viewpoint rendering method that can be trained in an end-to-end manner.



Neural Radiance Fields.NeRF  is a neural scene representation that represents a scene as a continuous volumetric function. It has been widely used for novel view synthesis and rendering. Recently, NeRF has also been extended to dynamic scenes [29]. However, the performance of NeRF is limited by the limited expressive power of the static scene representation. To overcome this limitation, we introduce a novel dynamic scene representation, which can be used to synthesize novel views from a monocular video.

 is a recent work that learns a scene representation based on neural networks. It uses a recurrent neural network (RNN) to learn the appearance and motion of a static scene. Our method differs from it in two aspects. First, we use a differentiable renderer to generate the scene representation from a video. Second, we model the scene as an implicit function.

 uses an RNN to represent a scene. We use a single RGB image as input to our model. Our model is trained end-"," Dynamic human modeling has shown promising results in utilizing various representations such as point clouds [1], meshes , voxels [2], and neural implicit functions [3], with models like SMPL [4][5] being commonly used for parameterizing the human body. Since the introduction of NeRF , neural human representation [6][7][8][9] has achieved remarkable progress on representing dynamic human bodies from sparse-view videos. Among them, Neural Actor  and Neural Body [6] pioneer in combining NeRF  with SMPL deformable meshes to represent human bodies with complex motions. Subsequent works have further improved on the generalizability  and animatability [10] of human bodies. To support multi-person modeling, recent works [11][12] have proposed to segment each human into 3D bounding boxes and learn a separate layered dynamic NeRF for each person. Other works [13][14][15] are specifically designed to reconstruct the dynamic human and object with RGB-D or multi-view videos as inputs. They track the human and object pose, and separately reconstruct them with volumetric fusion [14], neural texture blending [15], or neural rendering [16].

Despite achieving promising results, these approaches require multi-view videos or RGB-D as input, limiting their real-world applications. To solve this problem, Human-NeRF [17] is proposed to represent moving humans from a monocular video by the human pose-driven deformation module and canonical space. NeuMan  is the first successful attempt at reconstructing both the dynamic human and static background from a single video. However, Neuman  does not support human-environment interactions

Most prior approaches on dynamic scene modeling require synchronized multi-view videos [18][19][20][11][21] or depth [22][23] as input. Recent studies have built upon NeRF  to reconstruct dynamic neural radiance fields from monocular videos by either learning a deformation field that maps dynamic observation to canonical field [24][25][26] or building 4D spatio-temporal radiance fields [27]. Among them, Nerfies [25] associates latent codes with the deformation field and HyperNeRF  represents motion in a high-dimensional space. D\({}^{2}\)NeRF [28] builds upon HyperNeRF  and further decouples the dynamic components from the static background, and represents them separately with a HyperNeRF  and NeRF . DynIBaR  proposes a motion-adjusted multi-view feature aggregation module to synthesize new viewpoints by aggregating features from nearby views. Other studies have introduced voxel grids [29] or planar representations  for fast dynamic radiance fields reconstruction. While these approaches have achieved high-fidelity dynamic view synthesis results, they are restricted to simple scene deformations. In contrast, our HOSNeRF is capable of representing significant human-object motions and interactions in complex environments.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]"," **Human-Object-Scene Rendering.** Human-object-scene rendering has been extensively studied in recent years. Most of the existing works [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] focus on static human-object interactions. For example, [1] proposes a multi-view neural human renderer (NHR) to synthesize novel views of dynamic humans from a single monocular video. However, NHR is limited to the human-only setting and cannot handle the complex human-human interactions. [3] proposes PIFuHD, a novel multi-level pixel-aligned implicit function for high-resolution 3D human digitization. [4] proposes SMPLpix, a model based on the SMPL body model [4], which is able to capture the pose and shape of the human body from a large number of aligned 3D meshes of different people in different poses. [6] proposes Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. [10] introduces the skeleton-driven deformation to animate the human model. [8] introduces a new implicit human body model, represented using signed distance functions, to learn human shape and appearance from monocular videos. [9] proposes H-NeRF, a neural radiance field for rendering and temporal reconstruction of humans in motion. [7] proposes DoubleField to combine the merits of both surface and radiance fields for human reconstruction and rendering. [5] proposes to learn the pose-dependent pose and appearance of the subject from a dataset of human models.


However, all these methods are designed for static human and object-scene, and cannot be directly applied to dynamic human and scene rendering. In this work, we propose a novel free-viewpoint rendering method that can render dynamic humans, objects, and backgrounds from arbitrary viewpoints.

 is a recent work that proposes a novel view synthesis method for dynamic human performance capture and rendering from a monocular RGB-D stream [14]. However, their method requires a pre-recorded human template to be used as input, which limits its application in real-world scenarios. ["," **Novel view synthesis for dynamic human performance.** Neural network has been successfully applied to human reconstruction and novel view synthesis task [1][2][3][4][5][6][7][8][9]. In particular, while there are multi-view video-based approaches [10][11][12][13][14][15] that enable the FVV of dynamic humans with rich geometry, rich detail and high fidelity, they have limitations in capturing dynamic complex interactions between humans and objects. Some of them are limited to a fixed point-cloud in their reconstruction pipeline.

Free-viewpoint video reconstruction for dynamic human-object interactions has been studied for many years. Early works focused on joint representations of humans and objects, using 3D optical motion capture systems [16], or RGB-D cameras [17], and prioritizing visible regions over occluded regions in order to reconstruct the entire performance in images [18][19][20]. However, these works require costly sensors and cannot capture dynamic free-viewpoint video scenes in real-world environments. More recently, research has shifted to modeling dynamic human-object interactions with neural networks [12][15][14]. However, as mentioned in Section 1, these works focus on deformable 3D geometry reconstruction and do not perform novel view synthesis. Furthermore, none of them can operate in a free-viewpoint style, i.e. they cannot capture and synthesize a full 360-degree view from arbitrary viewpoints. In contrast, HOSNeRF is able to reconstruct a dynamic human-object scene by free-viewpoint video in a fast real-time fashion, as shown in Figure 1.

**Neural radiance fields for dynamic scenes.** Recent work has explored the use of neural radiance fields for dynamic scenes [21][22][23]. Many works try to extend neural radiance fields [24][25][26][27][28][29] from monocular videos by modeling non-rigid dynamics with sparse deformation fields [25] or by combining static neural radiance fields with non-rigid deformation [26]. In this work, we aim to perform novel view synthesis for dynamic human-object scenes using a single monocular video. While FVV videos of human-object interactions are challenging to capture in a free-viewpoint style, the advantage of neural radiance fields is that they can be fast and compact to be streamed.

",,"<>
The task of reconstructing neural radiance fields for dynamic human-object-scene from a single video has garnered significant attention in recent research. Several approaches have been proposed to address the challenge of dynamic human captures and free-viewpoint rendering. For instance, Multi-View Neural Human Rendering (NHR) [1] adopts PointNet++ for feature extraction and anti-aliased CNN for handling holes and noises to render new views. DeepVoxels [2] addresses the lack of 3D understanding of generative neural networks using a learned representation that encodes view-dependent appearance without explicitly modeling geometry. Additionally, PIFuHD [3] formulates a multi-level architecture for high-resolution 3D human digitization by addressing the conflict between accurate and precise predictions. These approaches signify the efforts to improve human captures and scene rendering capabilities from multiple viewpoints.

Several models have focused on representing human body shape and pose, such as SMPL [4] and its successors like SMPLpix [5], which provide more control over pose and shape but rely on classic computer graphics pipelines for rendering. Furthermore, Neural Body [6] addresses the view synthesis quality challenge by integrating observations across video frames, assuming that learned neural representations share the same set of latent codes anchored to a deformable mesh. DoubleField [7] introduces a framework combining surface field and radiance field for high-fidelity human reconstruction and rendering, significantly improving reconstruction quality of geometry and appearance. These models augment the understanding of human representation and provide enhanced realism in rendering.

A number of approaches extend Neural Radiance Fields (NeRF) to learn generative neural body models from unlabelled monocular videos, such as A-NeRF [8] and H-NeRF [9], that aim to recover fine alignment via iterative refinement and support temporal reconstruction of humans in motion. Additionally, Animatable Neural Radiance Fields [10] introduce neural blend weight fields to produce deformation fields, enabling explicit control by input motions. Editable free-viewpoint video generation using a layered neural representation [11] also demonstrates the potential to manipulate visual perception in large dynamic scenes. 

Furthermore, the proposed system, HOSNeRF, addresses the challenges of dynamic human-object-scene rendering by introducing novel object bones and learnable object state embeddings to represent human, object, and scene interactions. It significantly outperforms state-of-the-art approaches on challenging datasets [target paper]. These related works collectively demonstrate the advancements in human-object-scene reconstruction and free-viewpoint rendering and provide valuable insights for the development of the HOSNeRF framework."
1086,1086," **Bayesian Deep Learning.** Bayesian neural networks (BNNs) [1][2][3][4][5][6][7][8][9] have been widely used to model uncertainty in deep neural networks and have achieved great success in various computer vision tasks, such as classification, detection, and segmentation. However, BNNs are known to be sensitive to over-fitting and over-smoothing. To address this issue, several methods have been proposed to approximate the posterior distribution with Gaussian mixture models (GMM) [2][1][3] or Gaussian Processes (GP) [4]. However, GMM-based methods are computationally expensive and difficult to be applied to large-scale datasets due to the non-convexity of the Gaussian process. In contrast, PGD-based models [1] are able to model the uncertainty in a probabilistic graphical model, which is computationally efficient and scalable. In this paper, we propose to model confusion and ignorance explicitly with the theory of Subjective Logic.

**Open-Set Recognition.** Open-set recognition [10][11][12][13][14][15][16][17][18][19][20][21][22][23] aims to identify out-of-distribution (OOD) samples when the input is entirely out of the training distribution. Existing methods can be roughly divided into two categories, _i.e_., detection-based and generation-based. Detection-based approaches [11][14] first detect OOD samples by matching the feature distributions of known and unknown samples. For example, [11] proposes to use the maximum mean discrepancy (MMD) between the softmax probability distribution of known samples and the probability of unknown samples to estimate the OOD probability. [14] further improves the detection performance by using the virtual logit matching [15] and Gram matrices [17] to match the logits of the known and OOD distributions. [12] introduces the generative adversarial network (GAN) to synthesize OOD examples. [20] proposes a reconstruction-reconstruction framework to generate OOD images. [21] proposes an open-set representation learning framework to learn the feature representations that are invariant to the distribution shift. [22] introduces a convolutional neural network (CNN) to predict multiple class labels for each OOD sample. [18] proposes the class conditioned auto-encoder (C2AE) to generate samples conditioned on the class label distribution. [16] proposes meta-learning to learn a meta-learner for OOD recognition. [23] uses a generative model to generate unknown-class samples by minimizing the KL divergence between the predicted distribution and the ground-truth distribution.

 proposes to model prediction uncertainty with a Gaussian distribution and a mixture of Gaussians.  proposes to learn an OOD classifier by maximizing the log-likelihood of the conditional distribution of the input samples.  models uncertainty with the mixture of conditional distributions of the features of known classes and unknown-"," **Uncertainty estimation.** Typical neural networks can not detect their own failure. However, this ability can be important in several real-world applications, like rejecting unseen samples, and providing prediction confidence, to name a few. Bayesian NN [1][2][3][4] predicts epistemic uncertainty as the mutual information between model parameters and samples. By assuming a probabilistic prior on the network, it approximates prediction variance by sampling weight during inference. Several works [5] choose to model epistemic and aleatoric uncertainties separately. The Subjective Logic on which our method is established falls within the realm of epistemology instead of a frequentist (aleatoric) view. In other words, we focus on further separating epistemic uncertainty into confusion and ignorance.

Evidential deep learning [6][7][8][9], in contrast, proposes to learn the prior of the predictions directly. The prior, known as evidential prior, is interpreted as beliefs in Dempster-Shafer Threory . In [9], they model the first- and second-order uncertainties by introducing an auxiliary uncertainty network to approximate the difference between Dirichlet distributions.

While uncertainty is provided to describe the variance of model prediction, it is not clear if uncertainty comes from different sources when dealing with in-distribution or out-of-distribution data. Orthogonal to previous approaches, we separate the uncertainty into confusion and ignorance in this work. Confusion depicts the uncertainty between different known classes, while ignorance decides whether the sample is unknown. With this separation, we can make dynamic predictions on known classes and reject unknown classes at the same time.

**Open Set Recognition.** Machine learning models are usually designed with the closed-set assumption, where testing data shares the same distribution as the training. Open-set recognition (OSR) [10] introduces semantic shifts to the problem. Samples are from the classes that are not in the training set. Out-of-distribution (OOD) [11] detection introduces domain shifts to the testing set. In both of the settings, models should have the ability to reject unknown samples.

In general, OSR and OOD methods reject unknown samples depending on reweighting outputs [10][12][13][11][14][15], getting better feature embedding metrics [16][17], and exploring reconstruction errors [18][19][20][21]. These metrics are all related to the quality of classifier prediction. However, the recognition can fail on closed-set samples because of the existence of confusion. In this work, we show that when the confusion between known classes is adequately modeled, unknown samples can be identified more accurately.

**Conformal Prediction.** Parallel to our task, conformal prediction is a paradigm that could provide single or multiple predictions by empirically constructing confidence regions [22][23]. However, conformal prediction is confined to closed scenarios without open samples, as the empirical quantile is established on a labeled validation set sampled from the same testing distribution.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"," **Bayesian Neural Networks.** Bayesian neural networks (BNNs) [1][2][3][4][5][6][7][8][9] have been widely used to improve the robustness and uncertainty quantification of modern neural networks. These methods approximate the posterior distribution of a neural network by placing a probability distribution over the network's kernels. For example, Bayesian CNN [1] approximates the posterior with Bernoulli variational distributions, and Bayesian Convolutional Neural Networks (BCNN) [4] approximate Gaussian distributions on the weights of a ReLU network. Bayesian Layers [2] extends BNNs with drop-in replacements for common layers and allows for scalability via a unified abstraction over deterministic and stochastic functions. BayesNet [3] proposes a rank-1 parameterization of BNN, where each weight matrix involves only a distribution on a subspace. Theoretically, the Bayesian Neural Network [4], which approximates a Gaussian distribution on weights, can be viewed as a bit-Bayesian approximation [4].

**Open Set Recognition.** Open set recognition [10][11][12][13][14][15][16][17][18][19][20][21][22][23] aims to detect unknown samples in the training set. OpenMax [10] is the first work to propose an open-set classifier, which estimates the probability of an input being from an unknown class. Open-Max is based on maximum softmax probability [11], which is trained on a closed-set of known classes and is then used as the classifier for open set recognition. OpenGAN [13] extends OpenMax to the open set scenario by using a GAN to generate synthetic open set samples. OpenReed [8] uses a contrastive learning method to mitigate evidential bias in the representation learning process. C2AE [18] uses class conditioned auto-encoders with novel training and testing methodologies. CGDL [19] proposes to detect known samples by forcing different latent features to approximate different Gaussian models.

Open-set recognition has been widely studied in recent years. ODIN [14] and ViM [15] use temperature scaling and adding small perturbations to the input to separate the softmax score distributions between in- and out-of-distribution images, respectively. OpenNet  proposes to use a meta-learning based approach to detect"," In recent years, there have been numerous efforts to make deep learning systems robust and confident. Bayesian deep learning [1][2][3] effectively addresses the overconfidence problem of deterministic deep networks by placing prior distribution over the network weights. Unfortunately, the Bayesian framework suffers from slow convergence and cumbersome inference for a large number of layers. By contrast, Bayes By Backprop [4] exploits the Bayesian approximation of maximum likelihood estimation to re-calibrate the prediction probabilities of a neural network. However, Bayes By Backprop ignores the out-of-distribution examples. To overcome this limitation, distinct works [5] attempt to distinguish the known and out-of-distribution classes with the ranked set classifier, where the model has to learn outlier score distributions for unknown classes. Yet, this approach is hard to deploy in real-world settings due to its black-box nature and restricted expressiveness. Instead, we use deep learning to attain explicit uncertainty estimates and rich model-dependent decision rules.

Evidential deep learning [6] directly leverages the Bayesian analysis on neural networks for robust recognition and accurate decision making. Motivated by this approach, we attempt to identify both unconfident and out-of-distribution samples to enable evidence-based decision-making. In this manner, our methodology can be viewed as a complement to Bayesian neural networks with unique interpretability for out-of-distribution detection [7][8][9][10][11][12][13][14][15]. In specific, we learn a set of subjective evidences for flexible out-of-distribution detection [16][17]. Our evaluation shows that our approach can distinguish unconfident and out-of-distribution samples on open-set benchmarks while performing comparably on in-distribution samples.

One closely-related research direction is using a generative model for out-of-distribution detection [12][13][18]. However, this method is solely focused on generating positive labels of out-of-distribution samples and is limited by the domain and diversity of real-world scenarios. Moreover, this method does not explicitly distinguish unconfident and out-of-distribution classes. A different approach to OOD detection is the conditional Gaussian distribution learning [19]. However, this method is unable to properly handle in-distribution samples. Besides, the conditional Gaussian distribution is incapable of representing other uncertain classes as OOD samples.

There are also a large number of traditional methods for OOD detection, including conformal prediction [20][21][22][23], variational inference , and latent feature generation  These methods are mostly limited to the setting of the explicit known classes and are not suitable for open-set and flexible recognition.

",,"<Related Work>
The problem of uncertain visual recognition and detection of out-of-distribution (OOD) samples has been a topic of interest in recent research [6]. Evidential deep learning methods strive to explicitly model prediction uncertainty, which is crucial for robust and flexible visual recognition [6][8]. These methods achieve this by considering the evidence underlying the predictions as subjective opinions, enabling the estimation of uncertainty with the theory of Subjective Logic [6][8]. In particular, these approaches aim to address the challenges of inter-class uncertainty (confusion) and out-of-distribution samples (ignorance) separately and represent them as conflicting evidence and absence of evidence, respectively [6].

Bayesian neural networks (BNNs) have also been a focus of related work in addressing uncertainty estimation in deep learning models [4][5]. BNNs have demonstrated success in improving the robustness and uncertainty quantification of deep learning models [4][5]. Additionally, there have been efforts to combine the strengths of BNNs with deep ensembles to address efficiency issues and underfitting at scale, thereby enhancing the ability to quantify uncertainty [3]. Moreover, the use of generative adversarial networks (GANs) has been proposed for multi-class open-set classification, demonstrating significant improvements over traditional methods [12].

Conformal prediction and distribution-free uncertainty quantification techniques have also emerged as important tools for creating statistically rigorous uncertainty sets for machine learning models [23]. These techniques provide finite-sample coverage guarantees without distributional or model assumptions, making them suitable for high-risk environments where uncertainty quantification is critical [23]. Furthermore, recent research has explored the use of conditional Gaussian distribution learning for open set recognition, offering the potential to both detect unknown samples and classify known samples by leveraging different Gaussian models for discriminative representations [19].

In summary, the related work has focused on addressing uncertain visual recognition and OOD sample detection through the explicit modeling of uncertainty, the use of Bayesian neural networks, and the development of conformal prediction and distribution-free uncertainty quantification techniques, among others. These approaches aim to improve the flexibility, robustness, and reliability of visual recognition systems in the face of uncertain and OOD data."
678,678," **Image Super-Resolution Networks.** SRCNN [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18] has achieved remarkable success in image super-resolution. However, the performance of these networks is limited due to the limited spatial range of input information. To address this problem, many works [19][20][21][22][23] have been proposed to improve the spatial resolution of the input images. For example, SRResNet [2] and EDSR [6] introduce residual connections to enlarge the receptive field of the convolutional layers, while RCAN [11] and RDN [12] introduce the recursive structure to reduce the computational cost. SRGAN [10] introduces a generative adversarial network (GAN) framework to generate high-resolution images. Recently, SRGAN  and SRGAN++ [14] propose to use the pre-trained image processing Transformer (IPT) [18] as the backbones to further improve the performance.

**Vision Transformer.** Transformer [24] was first proposed for natural language processing (NLP) tasks in [24]. Since then, it has been widely used in computer vision tasks [25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45]. Recently, ViT [27] shows promising results in image classification tasks. It is a pure Transformer architecture that uses a self-based self-attention mechanism to model the relationship between the input image and the output image. It has achieved state-of-the-art results on various vision tasks, such as image classification, object detection, semantic segmentation, and image segmentation. In this paper, we propose a hybrid attention-based architecture that combines the advantages of both channel-based and window-based attentions.

 first introduces the attention mechanism into image processing tasks. They propose a two-stream architecture, namely channel attention and window attention, to capture the long-range dependencies in an image. The channel attention module extracts the global information from the whole image, while the window attention module aggregates the local information from different windows of the image. In the training stage, they adopt a same-task pre-training strategy to exploit the potential of the model for further improvement. In addition, they propose a spatial attention module to aggregate the information from both the global and the local windows. In contrast, we introduce a novel Hybrid Attention Transformer that combines both channel and window attentions into a unified architecture. We show that these two attentions can be combined to better exploit the complementary advantages of being able to utilize global statistics and strong local fitting capability. Moreover, to better aggregate the cross-window information, we design an overlapping cross attention module, which can effectively capture the interaction between neighboring window features.

 proposes a multi-scale attention module for image super resolution. It first extracts global and local features from the"," Since SRCNN  first introduces deep convolution neural networks (CNNs) to the image SR task and obtains superior performance over conventional SR methods, numerous deep networks [1][2][3][4][5][6][7][8][9] have been proposed for SR to further improve the reconstruction quality. For instance, many methods apply more elaborate convolution module designs, such as residual block [6][10] and dense block [9], to enhance the model representation ability. Several works explore more different frameworks like recursive neural network [11][12] and graph neural network [13]. To improve perceptual quality, [10][14][15] introduce adversarial learning to generate more realistic results. By using attention mechanism, [1][16][7][17] achieve further improvement in terms of reconstruction fidelity. Recently, a series of Transformer-based networks [18][4][5] are proposed and constantly refresh the state-of-the-art of SR task, showing the powerful representation ability of Transformer.

To better understand the working mechanisms of SR networks, several works are proposed to analyze and interpret the SR networks. LAM [19] adopts the integral gradient method to explore which input pixels contribute most to the final performance. DDR [20] reveals the deep semantic representations in SR networks based on deep feature dimensionality reduction and visualization. FAIG [21] aims to find discriminative filters for specific degradations in blind SR. RDSR [22] introduces channel saliency map to demonstrate that Dropout can help prevent co-adapting for real-SR networks. SRGA [23] aims to evaluate the generalization ability of SR methods. In this work, we exploit LAM [19] to analyse and understand the behavior of SR networks.

Recently, Transformer [24] has attracted the attention of computer vision community due to its success in the field of natural language processing. A series of Transformer-based methods [25][26][27][28][29][30][31][32][33][34][35] have been developed for high-level vision tasks, including image classification [27][30][31][36], object detection [37][25], segmentation [38], _etc_. Although vision Transformer has shown its superiority on modeling long-range dependency [27][39], there are still many works demonstrating that the convolution can help Transformer achieve better visual representation [29][33][40][35]. Due to the impressive performance, Transformer has also been introduced for low-level vision tasks [41][18][4][42][5][43][44][45]. Specifically, IPT [18] develops a ViT-style network and introduces multi-task pre-training for image processing. SwinIR [5] proposes an image restoration Transformer based on [31]. VRT [42] introduces Transformer-based networks to video restoration. EDT [4] adopts self-attention mechanism and multi-related-task pre-training strategy to further refresh the state-of-the-art of SR. However, existing works still cannot fully exploit the potential of Transformer, while our method can activate more input pixels for better reconstruction.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]"," **Image Super-Resolution.** Image super-resolution (SISR) has been extensively studied in recent years, with the advent of deep learning methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49]. In this paper, we focus on the image super-resolution task, which is a classic low-level vision task with a long history in the field of computer vision.

**Vision Transformer.** Transformer-based methods have shown impressive performance in various low level vision tasks, such as image classification [24][27] and image restoration [18]. However, these networks are still limited in their ability to utilize the full spatial range of input information. To address this issue, several works have been proposed to improve the efficiency of self-attention in vision transformers. For example, Swin Transformer [31] and Swin-SVT [25] use shifted windows to reduce the computation cost of self attention, while CSwin [26] and CSwin  use cross-shaped windows to improve efficiency. Shuffle Transformer (Shuffle) [28] proposes to use spatial shuffle connections to reduce computation complexity. LocalViT [30] introduces locality mechanisms into vision transformer by introducing convolution into the feed-forward network. However, the above methods only utilize a limited range of spatial information, which limits the ability of Transformer to capture long-range dependencies. In this work, we propose a hybrid attention Transformer that combines the advantages of both channel-based and window-based attention schemes.

 is a recent work that proposes a hybrid Transformer architecture for image restoration. It proposes to combine channel attention and window attention in the Transformer block. It also proposes a novel overlapping cross-window attention module to enhance the interaction between neighboring window features. In contrast, our HAT combines both channel and window attentions in a unified framework.

"," **Self-Supervised Learning.** Unsupervised visual representation learning tasks with unpaired data include image colorization , domain adaptation , appearance-based datasets , instance discrimination [3], etc. By contrast, self-supervised learning (SSL) requires a large number of data pairs (_e.g._, images with and without jigsaw puzzles, same/different pairs, related-class pairs) and trains the network based on the data similarity or dissimilarity. The proposed pretraining paradigm with the selected samples can reduce the supervision cost of data collection for these two kinds of unsupervised learning tasks.

Recent SSL tasks for self-supervised learning can be broadly categorized as discriminative tasks and generative tasks, including, but not limited to, contrastive learning [4][5][1][2][6], gradient-based [7][8][9], masked prediction [10][11][12][13], and others [14][15]. For example, SimCLR [1] proposes a large-scale data collection method and the first self-supervised method for image classification and semantic segmentation. We will refer to it as the unlabeled pool in our work. In addition to the unlabeled pool, most of these approaches require additional labeled data (_e.g._, 200 for SimCLR and 1000 for BYOL [4]). We hope to make it possible to train the model solely on the unlabeled pool by selecting a smaller subset of data.

In this work, we focus on the most representative contrastive learning [1][2] as it becomes the most popular paradigm for SSL. They learn the representations in the data by computing the similarities between a specific pair of views of the same image (the same-view loss) and different views of two images (the cross-view loss). It has achieved promising results in both computer vision tasks, especially image classification [1], and semantic segmentation [9][10]. Based on the idea of the unsupervised fine-tuning, MoCo [2] proposes to reconstruct the masked input to reduce the difficulty of fine-tuning. And MAML  argues that all of these are effective to capture representation in the data since one can train a model with an unlabeled pool. This makes the finetuning process more data efficient. To confirm this effect, we compare the performances of the finetuning models on the unlabeled pool and the supervised finetuning.

**Active Learning.** The active learning paradigm aims to reduce the annotation cost by selecting representative samples [16][17][18][19][20]. Given the unlabeled pool and an oracle for labeling, an active learning method would choose the most informative samples to annotate. Some conventional methods directly define the informativeness of samples as entropy [21], entropy based on domain knowledge [22",,"<>
In recent years, image super-resolution has been revolutionized by the use of Transformer-based methods, which have demonstrated remarkable performance in low-level vision tasks. However, one of the limitations identified in these networks is their ability to utilize only a limited spatial range of input information. To address this issue, a novel Hybrid Attention Transformer (HAT) has been proposed [1]. The HAT combines channel attention and window-based self-attention schemes to exploit their complementary advantages, enabling the utilization of global statistics and strong local fitting capability. In addition, an overlapping cross-attention module has been introduced to enhance interaction between neighboring window features. Furthermore, the use of a same-task pre-training strategy has been incorporated to exploit the potential of the model for further improvement. The extensive experiments conducted have verified the effectiveness of the proposed modules, demonstrating that the performance of the task can be greatly improved.

In the realm of single image super-resolution (SISR), the use of deep convolutional neural networks (CNNs) has gained momentum, leading to remarkable performance in this domain [2]. Traditional sparse-coding-based SR methods have also been viewed as a deep convolutional network, and a deep learning method has been proposed for single image super-resolution using a deep CNN [2]. Additionally, an accurate single-image super-resolution method using a very deep convolutional network inspired by VGG-net has been presented [3]. These studies have paved the way for significant improvements in accuracy and the development of more efficient training procedures.

Moreover, in the context of efficient image pre-training for low-level vision, a study has undertaken a thorough investigation of image pre-training [4]. The authors have proposed a generic, cost-effective Transformer-based framework for image processing and highlighted its highly competitive performance across a range of low-level tasks. The study has provided a deeper understanding of the effects of pre-training in image processing systems, emphasizing its impact on internal network representations. Similarly, a strong baseline model, SwinIR, for image restoration has been proposed based on the Swin Transformer [5]. This model consists of shallow feature extraction, deep feature extraction, and high-quality image reconstruction. The effectiveness of the SwinIR model has been demonstrated through experiments on three representative tasks.

Furthermore, a novel enhanced deep super-resolution network (EDSR) has been developed, demonstrating performance exceeding that of current state-of-the-art SR methods [6]. The authors have proposed new multi-scale deep super-resolution systems (MDSR) and training methods, showcasing superior performance compared to existing methods. The study has also addressed the limitations of existing residual learning techniques, leading to substantial performance improvements. Overall, these studies have significantly contributed to the advancements in image super-resolution and laid the groundwork for further exploration in this field [7]."
975,975," **Multi-Task Learning.** Multi-task learning (MTL) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53].

**Mixture of Experts.** Mixture of experts (MoE) [29] is a popular framework for MTL. MoE separates the parameter space into task-specific subspaces, where each subspace is composed of a set of experts. Each expert is responsible for solving a specific task. The experts are trained jointly with a single task-agnostic network. The optimal number of experts for each task is then determined by the model capacity. In this way, the optimal model capacity is optimized jointly with the task capacity.

In this paper, we adopt the MoE framework to solve the MTL problem in the image classification task. We adopt the Mixture-of-Experts framework to tackle the problem of sparsely-activated MTL (SOTA) [54][55][56].

\(i\(ii\(iii\))\(iv\(v\(vi\)\))\) is a variant of MOTA, where \((i\) is the number of tasks and \((ii)\(v+1)\) is the model size. We use \(v\) to represent the task size and \(i\) to denote the model complexity.


\(\mathcalcal{O}(v)=\frac{1}{2}\sum_{i=1}^{n}\frac{i}{n\times n}^{2}\)where \(n\) is task size, \(i\in[0,1]\) and \(n\), respectively, are the task complexity and task size. \(N\) are the model parameters. \(i=\frac{\frac{2}{n}\) and \(\frac{n}{1}{n}\).

\(j=1}{j\times 1}^{N}\) is model complexity and \(j\) is network capacity. \(j=0\) is MOTA and \(k\) is training complexity. \(k\times k\) is inference complexity. \(\mathcal{A}(n)\) and \(t\times t\) are model size and network complexity, respectively. \(J=0\times 0\) are network capacity and MOTA respectively.

\({}^{1}\sim 0}^{3}^{4}\)[54][55][56][54][56]"," **Multi-Task Learning (MTL).** MTL resolves multiple objectives and produces corresponding predictions for input samples. It has been investigated for a long history, and numerous solutions are proposed ranged from classic learning algorithms [1][2][3][4][5][6][7][8] to modern deep neural networks. Deep learning methods generate shared feature representations to model the common information across tasks, which can be categorized into two groups, _i.e._, encoder- and decoder-focused pipelines. The former [9][10][11] allows the task interactions in the encoder and attaches task-specific heads on top of it as independent decoders. For example, [9] and [11] advocate the linear combination and attention mechanism to learn shared encoder representations among tasks, respectively. The latter [12][13] first creates initial task-dependent features from decoders and then aggregates them to form the final per-task prediction. Such pipelines consume heavy computations since they need to at least execute all tasks once for the initial decoder features, which limits their practical usage in resource-constrained scenarios. In this paper, we mainly study encoder-focused architectures.

A conventional encoder architecture is a convolutional neural network (CNN) [11][14][15][16]. As ViTs emerge, IPT [17] leveraged transformer-based models to solve multiple low-level vision tasks. [18] and [19] adopt similar architectures for the tasks of {object detection, semantic segmentation} and {scene and action understanding, score prediction} in the video, respectively. [20] further involves vision tasks from 3D domains. Our work considers jointly learning classification, object detection, and instance segmentation with ViT-based models. Note that it is highly non-trivial since classification and detection & segmentation emphasize location invariant [21] and sensitive features respectively, which potentially contradict each other. Besides, another theme in MTL investigates how to share and separate parameter spaces for learning task-agnostic and -specific knowledge respectively [22][23][24][25][26].

**Mixture-of-Experts (MoE).** MoE duplicates some network components into a series of copies (named experts)and embraces the conditional computation in an input-dependent way [27][28][29]. The earliest variant of MoEs densely activates all experts for each input, and therefore it is computation-intensive [30]. Later on, [31][32][33] advocate a sparsely activated style for utilizing experts, called sparse MoE (SMoE). It greatly reduces the cost at both the training and inference stages, which grants impressive scalability and even allows enormous language models with trillions of parameters [33]. The effectiveness of SMoEs has been widely proved in various NLP [31][32][34][35][36][37] and vision [38][30][39][40][41][42][43] tasks. Particularly, the pioneering work [38] offers the first vision transformer-based SMoE for the image recognition task.

With further investigations, several downsides of SMoE are revealed, including: \(i)\) Training instability. [44] conducts a trade-off study of SMoE between its training stability and quality, where they show many classic tricks like gradient clipping stabilize training but sacrifice performance and the router \(z\)-loss [44] seems to bring a win-win case. \(ii)\) Poor specialization. The ideal outcome of SMoE is to divide and conquer certain tasks by tackling each piece problem with selected experts [45][46][47][48][49]. Yet it is hard to reach unless explicitly enforcing specialization and trimming down the redundancy among experts [50] like pre-defining a diverse expert assignment [51] or involving multiple routing policies [46]. \(iii)\) Representation collapse. Naively trained SMoE is prone to load imbalance, _e.g._, only a few experts are frequently used while the others are scarcely activated. To alleviate this issue, [31] adds Gaussian noises to router networks; [32][33] propose an auxiliary loss as the regularization; [52] formulates and solves a balanced linear assignment problem; [34] distributes the top-k relevant input for each expert; [53][36] adopt deterministic hashing and stochastic routing; and [54] promotes diversity during training, respectively. In this paper, we not only examine the aforementioned bottlenecks but also investigate new properties of routers such as policy convergence.

Several recent studies also explore the possibility of SMoE in the MTL scenarios. To be specific, [47][45][46][55][56] use task-dependent router networks to select relevant parts of the model with a fixed size for each task. They show positive results in small-scale applications like classification for medical signals [45], digital number images (MNIST) [46], and recommendation systems [47]. [26] works on the efficient on-device MTL with a model-accelerator co-designed SMoE.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]"," **Multi-task learning (MTL).** MTL aims to learn multiple related tasks simultaneously by sharing parameters across them [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][55]. In this paper, we focus on Mixture-of-Experts (MoE) [27][27], which is a new paradigm for MTL. MoE is first proposed in [27] and has been widely used in natural language processing [28][30] and computer vision [31][34]. The key idea is to select a subset of experts for each input example, which is achieved by training a gating network that maps each input to a distribution over the experts. The gating mechanism can be trained via gating mechanisms such as random gating [27], gating-based routing [30][34], gated routing [36][36], or gating based routing [34][31]. The gated network is trained to determine the number of experts to be activated for each example.

 [31] proposes a sparsely gated MoE layer, which uses a trainable gating function to select the gating experts. [34] proposes an expert selection algorithm based on the importance of each expert. [36] proposes to use a sparse routing algorithm to select experts for input. [33] proposes the Switch Transformer [33], which adopts a switch-based gating algorithm to choose the experts to activate. [35] proposes MoEfication [35], which uses conditional computation to convert the MoE model into a Transformer model. [37] proposes Sparsely-Gated MoEs [37], which proposes a more efficient routing algorithm based upon the sparsely-activated model.  proposes a novel MoE architecture, which utilizes a fixed-size model for all tasks. [31], which introduces a new routing algorithm, called Expert Choice Routing [34], to improve the performance of MoE models. [38] proposes V-MoE [38], which"," **Agent Trajectory Prediction**: The trajectory prediction problem has been studied extensively in the literature [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][24][25]. Since accurate, real-world trajectory predictions are essential to improving the safety and efficiency of autonomous driving, it is necessary to learn a robust and efficient model that can make predictions for a large number of agents simultaneously. Although several methods have improved the model efficiency and robustness, there is still room for improvement in trajectory predictions with respect to the accuracy, complexity, and computational overhead.

In this work, we tackle the motion prediction problem for a large number of agents in the scene. Our approach allows accurate and efficient predictions for each agent and can be applied in the single-agent and multi-agent settings. Unlike previous methods, we use weight augmentation in conjunction with efficient dynamic head conditioning to scale our model. We show that this method can improve accuracy while reducing computation by one order of magnitude.

**Graph-based Methods for Trajectory Prediction**: Recently, graph-based methods for trajectory prediction have emerged with the growing popularity of graph neural networks (GNNs). GNNs [27] were proposed as an alternative to convolutions, and have proven to be effective for encoding the spatial dependencies of the scene. Some methods apply GNNs to the per-agent trajectory problem, such as RANK [4], PRANK [4], MAP [6], PRECOG [15], MultiPath [12], HOME [14], and Unicycle-Transformer . However, these methods work on the individual agent prediction problem and require an additional step to determine the trajectories of agents interacting with one another.

Graph-based methods that model interaction between multiple agents also exists. Beyond tackling the individual agent prediction problem, some methods apply GNNs to the joint prediction problem, such as LAP [7], MultiHead-DAP [7], and V-Train [8]. PGSA [9] and SGAN [10] also introduce graph structures in the problem definition and use them in a GAN framework to produce a more diverse set of predictions. However, as their training is based on samples of simulated scenarios, they require costly simulations to learn.

[23] introduced an end-to-end multiple agent trajectory prediction architecture using a stacked transformer to improve the accuracy of the prediction with a small number of parameters. SENSIT [21] leveraged RNN-GNNs with graph attention to improve accuracy. [22] proposed to optimize the parameters of a DAE using GNNs, leading to a promising model",,"<>
In recent years, the use of sparsely activated Mixture-of-Experts (MoE) models has gained traction in the field of multi-task learning (MTL) [31]. These models have shown promise in improving training efficiency and generalization, particularly in scenarios where tasks exhibit significant differences in complexity [2]. The concept of MoE separates the parameter space and only utilizes relevant model pieces given the task type and input, leading to stabilized MTL training and efficient inference [31]. Additionally, the adaptability of MoE models has been explored in the context of multi-task vision recognition [20].

The idea of adapting the number of activated experts for each task in a multi-task setting has been discussed in previous research [7] to avoid the over-fitting of simple tasks or the under-fitting of challenging scenarios. This adaptability is particularly important when dealing with tasks that exhibit significant discrepancies in their complexity [22]. Furthermore, the efficient utilization of network capacity for various tasks has been a focus of recent work, with the aim of preventing the over-fitting of certain tasks while improving the learning ability of others [51].

Research in the area of multi-task learning has also emphasized the importance of understanding the task relationships and inter-task dependencies [33]. This understanding proves critical in the context of diverse tasks, such as those encountered in image recognition and natural language processing [45]. Additionally, the incorporation of hierarchical structures and latent hierarchical relationships between tasks has been explored to improve generalization performance in multi-task learning [7][28].

The notion of adaptively sharing parameters with different domains and tasks has been a focus of recent work, particularly in the context of multi-domain learning [25]. This approach seeks to derive specialized deep models for each domain while ensuring an adjustable budget in terms of the number of network parameters and computational complexity. Moreover, advancements in multi-task learning have been made through the use of attention mechanisms that allow for learning task-specific features and sharing features across different tasks [11][28].

In summary, the research landscape surrounding adaptive multi-task learning and the utilization of sparsely activated MoE models showcases a promising direction for addressing the challenges in multi-task vision recognition and learning. Such approaches have proven beneficial in adapting model capacity and expert selection adaptively based on the training dynamics, task complexity, and inter-task relationships."
3037,3037," **Novel Category Discovery.** Novel category discovery (NCD) aims to discover novel categories without labeled data [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17]. It is a challenging task due to the lack of prior knowledge about the number and distribution of novel categories. Most of the existing methods focus on fine-grained categories. In this work, we focus on the more challenging problem of generalized categories.

**Deep Metric Learning.** Deep metric learning (DML) [18][19][20][21][22][23][24][25][26][27][28][29] aims to learn a discriminative embedding space where samples from the same class are closer and samples from different classes are farther apart. Recently, deep metric learning has achieved great success in many computer vision tasks, such as image classification, object detection, semantic segmentation, etc. However, the performance of DML is limited by the availability of labeled data. To address this problem, several methods have been proposed to leverage unlabeled data. For example, [30][31][32][33] propose to train a network with both labeled data and unlabelled data in an unsupervised manner. In particular, [31] proposes to train the network with a semi-supervised loss and a label smoothing loss. In contrast, [33] proposes a label-preserving loss to improve the generalization ability of the network. Different from these methods, in this paper, we propose a novel method for novel category discovery without any labels.




\[\begin{tabular}{l c c c l c l l l c c r c l \hline \l c l}{c l c r l \l l \r l c}{c}{c c c} \l r c c \l"," NCD techniques have been proposed to classify data with various constraints on unlabeled data. One category of the methods presented pre-training the model on the labeled set and fine-tuning it on the unlabeled set using unsupervised clustering losses [1][2][3][4][5]. Another category assumed the availability of both the labeled and unlabeled data, and trained networks jointly with a labeled novel class loss within the semi-supervised scheme [6][7][8][9][10][11]. Han _et al._[12] proposed transferring knowledge from labeled to unlabeled data using ranking statistics in the joint learning stage. Recently, GCD [13] and XCon [14] tackled the more realistic scenario of joint datasets and distinguished known and unknown classes using prior knowledge. However, these approaches did not consider the continual learning scheme. To address the limitation, FRoST [15] and NCDwF [16] froze feature extractors and added the second head for each novel class, as much as the given number of novel categories. However, the methods employed disjoint sets. GM [17] proposed to consider novel category discovery on the joint datasets, but still require prior knowledge, such as the proportion of novel samples.

Most of the image retrieval methods have utilized metric learning and can be categorized into two approaches. Pair-based methods exploited contrastive loss [18][19][20] and triplet loss [21][22], that pull together data pairs in the same class and push apart those in different classes. Multiple data-based [23][24] methods proposed considering the relations between multiple data. Entire data-based approaches [25][26] presented considering all data in a batch, leveraging fine-grained semantic relations between them while requiring high computation costs and slow convergence. In contrast, proxy-based methods [27][28][29] employed fewer proxies than the training set, reducing training complexity. While these methods improved training convergence, they did not consider data-to-data relations, as each data was associated with its proxy. PA [30] inherited the strength of pair- and proxy-based methods, achieving fast and reliable convergence, robustness opposing noisy data, and leveraging rich data-to-data relations.

Recently proposed methods for learning with noisy labels have highlighted the importance of discriminating between clean and noise-labeled data to improve performance. DivideMix [31] used GMM to distinguish between clean and noisy labeled data and treated the latter as unlabeled for semi-supervised learning. AugDesc [32] employed data augmentation to enhance the differentiation between clean and noisy labeled data, while INCV [33] introduced cross-validation to separate clean data from noisy training data. SplitNet [34] leveraged a compact network to perceive the difference between clean and noisy labels, improving model performance by more accurately differentiating noise.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]"," **Novel category discovery.** Novel category discovery (NCD) aims to discover novel categories in an unlabeled dataset without any prior knowledge about the number and nature of novel classes. Existing methods can be categorized into two types: semi-supervised and unsupervised methods. The semi-Supervised methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] assume that the labeled data is available for training the model, and the unlabelled data is labeled for clustering. For example, AutoNovel [12] and UNO [10] use self-supervision to train the model from scratch on the union of labeled and unlabeled data. UNO further introduces a unified objective function to optimize the joint objective function on both labeled and unlabeled data, and achieves state-of-the-art performance. However, these methods are limited by their reliance on labeled datasets and prior knowledge of the number of novel categories.


The Unsupervised Novel Category Discovery (UCD) [18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34] is a more challenging scenario, where no prior knowledge is available about the size and number of new categories. For instance, XCon [14] proposes to leverage the information from a set of seen classes to discover new categories on unlabstracted data. NCDwF [16] proposes a novel class incremental learning setting, where a model is trained to incrementally discover novel classes while maintaining its performance on the previously seen classes. In this setting, the model is required to generate pseudo-labels for the unlabelened data and train a joint classifier for both the base and novel classes, which is similar to our proposed method. However in this paper, we propose a novel method for novel category discovery without prior knowledge on the number or number of categories.

"," Prompting in VL.Prompt tuning () makes an efficient adaptation from a pre-trained VL to downstream tasks, so that VLs with even fewer parameters can achieve comparable performance as trained VLs. For example, [4] apply a joint-text-and-image encoder and pre-train on several VL tasks. There are also variations of the zero-shot learning like zero-shot few-shot learning () and zero-shot transfer learning ([6]). Some studies further improve the VL-to-downstream task learning through further fine-tuning or more parameter manipulation, such as few-shot fine-tuning (), finetuning based on prompt ([1]) and prompt distribution learning ([8]; [11]). Some works ([10]; [9]; [3]) investigate how to regularize the prompt in the test time or train the prompt while in continual learning setting, but they are not applicable to the fine-tuning process. While various prompt tuning methods ([2]; [7]; ; [5]; [13]) show the promising zero-shot prediction performance, they may easily suffer from the catastrophic forgetting problem of VLs. Therefore, prompt regularization ([12]) is introduced to prevent the VLs from forgetting their original knowledge to handle the different downstream tasks. However, the performance drops by a large margin, and is still unable to catch up to the prompting-only approaches.

**Continual Learning.** Continual learning aims to avoid forgetting previously learned tasks while sequentially adding new tasks. Different from incremental learning, where each task is sequentially learned in a ""online"" setting ([15]; ; ), continual learning usually operates in an ""offline"" way with the task-specific datasets stored offline. There are two kinds of continual learning methods: optimization-based ([19]; [14]; ; [18]) and regularization-based ([17]; ; [22]; [23]; ; [16]; [14]; ; [18]; [15]; [20]; [11]). For example, the causal distillation framework proposed by [18] reveals the mechanism of class-incremental learning (CIL) and emphasizes the ""momentum of forgetting"" as the forgetting mechanism of CIL. And the regularization-based method are often used to stabilize the learning process and ensure the accuracy in downstream tasks. However, none of the methods above are explicitly designed to handle the catastrophic forgetting of prompt tuning.

",,"<>
Recent advances in deep learning have been instrumental in addressing challenges in novel category discovery, particularly in scenarios with incremental learning and lack of prior knowledge about new categories [1]. The concept of learning to cluster has emerged as a solution for transfer learning across domains and tasks, by formulating it as a problem of learning to cluster, effectively transferring similarity information to learn a similarity function and clustering network [1]. Additionally, meta classification learning provides a new strategy for multi-class classification without class-specific labels, optimizing a binary classifier for pairwise similarity prediction and learning a multi-class classifier as a submodule [2]. There have also been significant developments in the transfer learning setting, where prior knowledge of related but different image classes is used to reduce ambiguity in clustering and improve the quality of newly discovered classes [3].

Methods such as residual tuning have been proposed to address the challenge of discovering novel visual categories from unlabeled images, enabling automatic learning of new concepts without human-annotated supervision by estimating a new residual feature and adding it with a previous basic feature to compute the clustering objective together [4]. The residual-tuning approach has demonstrated consistent and considerable gains over other state-of-the-art methods, highlighting its potential for novel category discovery [5]. Furthermore, self-supervised learning and rank statistics have been combined to develop a new framework for discovering novel classes in an image collection, resulting in improved supervised classification of labeled data and clustering of unlabeled data [6]. These approaches illustrate the potential of leveraging self-supervised learning and rank statistics for successful category discovery.

Neighborhood contrastive learning has provided a unique framework for learning discriminative representations important for clustering performance, leveraging a feature extractor to generate representations where a generic query sample and its neighbors are likely to share the same class, thereby significantly improving clustering performance [7]. OpenMix, on the other hand, introduced a methodology for dynamically mixing unlabeled examples from an open set and labeled examples from known classes to prevent overfitting on unlabeled samples while discovering new classes in unlabeled visual data [8]. These methods, along with joint representation learning and novel category discovery on single- and multi-modal data [9], have shown promising results in the context of discovering new visual categories.

<>
In the pursuit of novel category discovery, joint representation learning and novel category discovery on single and multi-modal data have been the focus of exploration. This framework proposes a reliable representation learning approach inspired by self-supervised representation learning with a novel cross-modal discrimination strategy, which has been shown to outperform state-of-the-art methods on large-scale multi-modal video benchmarks, including image benchmarks such as CIFAR10, CIFAR100, and ImageNet [9]. Additionally, the UNified Objective (UNO) has stood out in the domain of discovering novel classes, introducing a single classification objective operating on both known and unknown classes, outperforming the state-of-the-art by a significant margin on several benchmarks [10].

Furthermore, new strategies such as AutoNovel have been introduced to navigate the problem of discovering novel classes, achieving promising results and significantly outperforming current methods for novel category discovery, alongside demonstrating effectiveness for fully unsupervised image clustering [12]. Lastly, approaches such as Generalized Category Discovery (GCD) have redefined the task of image recognition, challenging traditional assumptions and proposing new methodologies for discovering and categorizing images in the unlabelled set, thus demonstrating the potential for substantial impact in the field of image recognition [13].

<>
The field of deep metric learning has also witnessed significant advancements in understanding and utilizing deep neural networks trained with noisy labels. Strategies like DivideMix have leveraged semi-supervised learning to model the per-sample loss distribution with a mixture model, dynamically dividing the training data into clean and noisy sets and training the model in a semi-supervised manner [31]. Additionally, augmentation strategies for learning with noisy labels have been thoroughly evaluated, with the proposed strategy demonstrating significant improvement across all evaluated noise levels [32]. Moreover, a framework like SplitNet has been introduced, offering a novel learnable clean-noisy label splitting strategy for learning with noisy labels to improve the generalization performance of deep neural networks under both synthetic and real-world training noise [34].

These advancements collectively indicate the progressive evolution of methodologies for addressing the challenges of novel category discovery and leveraging deep learning in the presence of noisy labels, signaling a promising trajectory for future developments in these domains. <References will be included in the final document.>"
2368,2368," In this section, we briefly review the related work on RL with linear mixture MDPs and regret minimization with linear function approximation.

**Linear MDP RL.** Recently, there has been a surge of interest in RL for MDP with linear transition kernel. [4] proposed a linear combined model ensemble (LCM) algorithm to learn a linear transition probability kernel of the Markov decision process (MDP). [3] proposed an algorithm with \(\widetilde{\mathcal{O}}(dB_{*}\sqrt{K})\) regret bound for the SSP problem with a linear mixture transition kernel, where \(d\) is the dimension of the MDP and \(K\) are the number of episodes. [2] extended this work to the goal-oriented RL setting and proposed a no-regret exploration algorithm based on extended value iteration with a variance-aware confidence set. [1] proposed to learn the optimal policy via posterior sampling. [5] used value-targeted regression to estimate the variance of the transition model and proposed the value-directed policy iteration (VETI) algorithm, which is a variant of the Viterbi algorithm. However, all these algorithms assume a strictly positive lower bound of the cost function or an upper bound on the expected length for the policy. In contrast, our algorithm does not make any such restrictive assumptions and achieves a nearly minimax optimal regret upper bound. Moreover, VETI and Viterative policy iteration are computationally efficient and can be used in a variety of RL algorithms, such as value-based reinforcement learning (; [5]; [6]; [1]; [7]; [8]; [2]; [4]; [9]; [10]; [11]; [12]; ; ; [1].


In this paper, we focus on the problem of learning linear mixture SSPP with linear MDP. [15] proposed the first algorithm for learning linear SSP with linear feature mapping. The algorithm is based on value iteration and the variance is estimated recursively from high-order moments. [14] extended the algorithm to the SDP setting, where the uncertainty of the reward function \(f_{\theta}\) is defined as the sum of the expected reward and the expected exploration costs. They proposed a regret upperbound of \(\mathbb{E}_{\gamma}(d_{*}^{2}/d_{k}\log d_{k}}\) where \(D_{k}=\frac{1}{K}\sum_{k=1}^{K}\) and \(\gamma=0}^{k}\). However, their algorithm is computationally expensive and requires a large number of steps to reach the goal. [10] proposed two new algorithms for learning SSP in the goal oriented RL setting, which are based on the extended Vectors-based algorithm ([13]) and the KL-divergence based algorithm ([6]). The algorithm in ([10]) achieves a \(\mathrm{O}(O\sqrt{\log K}\log K}\) regret bound,"," **Tabular SSP.** Stochastic Shortest Path (SSP) is a popular variant of Markov Decision Process, which can be dated back to ; ; . The regret minimization problem of SSP was first studied by [2], which proposed the first algorithm with a regret of \(\widetilde{\mathcal{O}}(D^{3/2}S\sqrt{AK/c_{\text{min}}})\), and a parameter-free algorithm with an \(\mathcal{O}(K^{3/2})\) regret bound. Here \(D\) is the smallest expected hitting time from any starting state to the goal state and \(c_{\text{min}}\) is the assumed positive lower bound of the cost function. It was improved by [3] to \(\mathcal{O}(B_{*}S\sqrt{AK})\) when \(B_{*}\) is known and \(\mathcal{O}(B_{*}^{3/2}S\sqrt{AK})\) in the parameter-free case. There is still a \(\sqrt{S}\) gap from the lower bound of \(\Omega(B_{*}\sqrt{SAK})\) proved in the same paper. Later,  proposed an algorithm using the technique of reducing SSP to a finite-horizon MDP with a large terminal cost. This algorithm achieves the lower bound, but it requires some prior knowledge of \(T_{*}\), which can be bypassed by using the trivial upper bound \(T_{*}\leq B_{*}/c_{\text{min}}\), and \(B_{*}\) to properly tune the horizon and terminal cost in the reduction. As mentioned in Remark 2 of , this large dependence on \(1/c_{\text{min}}\) will not work well without the assumption \(c_{\text{min}}>0\). Concurrently,  avoided this requirement. They first developed an algorithm that knows \(T_{\pm}\) without assuming \(c_{\text{min}}>0\). This algorithm achieves an \(\widetilde{\mathcal{O}}(B_{*}\sqrt{SAK})\) regret upper bound, matching the lower bound. They also introduced a parameter-free algorithm that does not require knowing \(T_{*}\) in advance. For the case where \(B_{*}\) is unknown,  proposed an algorithm with a 'doubling trick' to guess the unknown \(B_{*}\) from scratch. Using the analysis framework called implicit finite horizon approximation,  proposed the first model-free algorithm which is minimax optimal under strictly positive costs. They also introduced a model-based minimax optimal algorithm without this assumption that is computationally more efficient. In other aspects of the literature, [1] introduced the first posterior sampling algorithm for SSP.  studied the problem of SSP with access to a generative model. Moreover, ; ;  studied the problem with adversarial costs.

**RL with Linear Function Approximation.** There exists a large number of works studying RL with linear function approximation (; [7]; [8]; [13]; ; [11];; [12]; [6]). The counterpart of the SSP we study in episodic MDPs is called linear mixture MDPs, where the transition probability of the MDP is based on a linear mixture model ([4]; [5]; ; ; b). [9] proposed an algorithm to achieve a nearly minimax optimal regret bound in episodic MDP. Recently, a new work can achieve horizon-free regret bound for linear mixture MDPs ([6]). In the SSP setting,  is the first to study a linear SSP model, which assumes there exist some feature maps and that both the cost function and the transition probability are linear in the feature maps. They proposed a computationally inefficient algorithm with a regret of \(\widetilde{\mathcal{O}}(\sqrt{d^{3}B_{*}^{3}K/c_{\text{min}}})\). [10] improved this result by a computationally efficient algorithm with an \(\widetilde{\mathcal{O}}(\sqrt{d^{3}B_{*}^{2}T_{*}K})\) regret. To avoid the undesirable dependency on \(T_{*}\), [10] also proposed a computationally inefficient algorithm with a regret bound of \(\widetilde{\mathcal{O}}(d^{3.5}B_{*}\sqrt{K})\) by constructing some confidence sets.

**Linear Mixture SSP.** Linear Mixture SSP is a different type of linear function approximation from linear SSP, which was first studied by [15]. In their work, they proposed an algorithm (LEVIS) with a regret of \(\widetilde{\mathcal{O}}(dB_{*}^{1.5}\sqrt{K/c_{\text{min}}})\) and an improved version (LEVIS\({}^{+}\)) with a regret of \(\widetilde{\mathcal{O}}(dB_{*}\sqrt{K/c_{\text{min}}})\). [10] proposed another algorithm (UCRL-VTR-SSP) with an \(\widetilde{\mathcal{O}}(B_{*}\sqrt{dT_{*}K}+dB_{*}\sqrt{K})\) regret. When \(d\geq T_{*}\), the result is nearly optimal, but in other cases, the dependency on \(T_{*}\) is undesirable. Similar to the tabular setting, this dependency can be bypassed by replacing \(T_{*}\) with the upper bound \(T_{*}\leq B_{*}/c_{\text{min}}\).

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"," **Stochastic Shortest Path (SSP)**: The SSP problem has been studied in episodic reinforcement learning for a long time period (; [3]; [2]; [1]; [5]; [4]; [6]; [7]; [8]; [9]; [10]; [11]; [12]; [13]; [15]; [14]; [3]. The main difference between these works and ours is that they assume a strictly positive lower bound of the cost function or upper bound of expected length for the optimal policy, while our algorithm does not.

**Linear Mixture SSP**: [1] study the problem of SSP with an unknown MDP with an absorbing state. They assume that the transition probability kernel of the MDP is a linear mixture model, and the learning agent has access to either an integration or a sampling oracle of the individual basis kernels. Their algorithm achieves \(\widetilde{\mathcal{O}(d^{3}H^{2}K^{2/\sqrt{H})\) regret, which matches the \(\Omega(dB_{\star}^{2}\sqrt{\sqt{K})\)([3]) regret bound of [6]) for linear mixture MDPs. However, their algorithm is computationally expensive, and their regret upper bound is only for the linear mixture SSP setting. In contrast, our algorithm achieves an \(\wideta_{O}\wideta(d_{\sqtt{H^{3}}K^{1/\text{H}\sqt^{2}}\) regret, and our algorithm is nearly minimax optimal. [7] also study the linear MDPP problem, but their algorithm requires a positive sub-optimality gap. [9] study SDPs with a mixture of linear mixture models, and they also assume a positive Q-function and a Q-value function. Their algorithms achieve an \(\tilde{O}\mathbb{E}(dB^{K}^{1.5}/\log(H^{4}K)\)\) regret. [14] study RL with discounted MDP, where the optimal MDP admits a linear transition kernel and the transition kernel is represented as a mixture model. They propose an algorithm with Hoeffding-type confidence sets and achieve \(\widetta_{O}((d^{2.5/\frac{d}{3.5}}K/c_{\min}}"," Object NavigationAccording to the navigation mode, object navigation can be divided into task-specific navigation [1][2][3][4][5][6][7] and zero-shot navigation [8][9]. Task-specific navigation uses the navigation instructions given in the task description to navigate the object, and zero-shot navigation aims to handle unseen object navigation tasks. Our work also belongs to the latter, which will be used to further address the practical object navigation scenario.

In general, zero-shot navigation methods can be categorized into three groups: (i) underspecification (Fig. 1(b)) navigation methods which allow agents to navigate without complete descriptions [10]; (ii) fault-tolerance navigation methods which guide agents to learn implicit navigation skills by adapting to new scenarios [8]; and (iii) memory-based navigation methods which equip agents with external memory or scene graphs to learn object attributes and dependencies [9][4].

Recent studies adopt graph neural networks as scene graphs to extract scene-level features to enable agents to obtain more detailed 3D structural information. These methods learn more accurate semantic information through multi-modal input representations, e.g., point clouds, depth, and cameras, as well as external memory to predict the actions for different environments [11][8][9][3][2]. Also, the coarse-to-fine planning architecture makes scene navigation more general. A well-studied example is SAVN [4], which learns to navigate new environments without additional meta-learning. Specifically, SAVN contains three modules: the self-supervised module is used to adapt the agent's visual representation; the semantic prediction module, i.e., predicting the environment graph, is used to optimize the agent for the new environment; and the navigation module is used to generate actions by interacting with the environment.

We develop the MAD framework to solve the problems of mutual exclusivity and insufficient decoupling of different aspects in navigation agents. MAD defines three decoupled meta-abilities and integrates them into a multiple thinking paradigm to promote mutual cooperation. Then, we design the MTC module to intertwine the decoupled meta-abilities. Finally, we utilize MAD to decode the navigation skill from the flexible thinking ability that leverages various object navigation tasks. We aim to build an agent that can navigate to targets flexibly and successfully in various real-world environments.

",,"<Reinforcement learning (RL) in the context of stochastic shortest path (SSP) problems has been extensively studied. No-regret algorithms have been proposed for this problem under different settings. For instance, Jia et al. (2020) and Ayoub et al. (2020) introduced no-regret algorithms for RL with linear function approximation in SSP problems, while proposing lower bounds on the regret to demonstrate the minimax optimality of their algorithms [9] [12]. Vial et al. (2021) developed no-regret algorithms for SSP problems with linear MDPs, achieving significant improvements over existing results [10]. Furthermore, Risk-sensitive RL with function approximation has been investigated by several researchers, with algorithms proposed to achieve sublinear regret bounds and demonstrate a tradeoff between generality and efficiency [11]. Jin et al. (2019) and Cohen et al. (2021) have also contributed to this area, showing that logarithmic regret bounds for RL with linear function approximation are attainable under specific linear MDP assumptions [12].

In addition, the exploration-exploitation tradeoff in finite-horizon RL has led to the introduction of model-free algorithms that induce exploration through perturbing the action-value function's least-squares approximation. For instance, Frequentist Regret Bounds for Randomized Least-Squares Value Iteration (RLSVI) algorithm has been shown to achieve an \(\widetilde O(d^2 H^2 \sqrt{T})\) frequentist regret under low-rank transition dynamics assumption [13]. Furthermore, the sample complexity of RL with linear function approximation has been a topic of interest, with algorithms proposed that achieve polynomial regret bounds without accessing to a generative model or making strong assumptions such as ergodicity of the MDP [14]. The study of RL under different settings, such as reward-free RL with linear function approximation, has also drawn attention. Positive and negative results for reward-free RL in the linear Markov decision process setting were provided, along with an exponential lower bound for reward-free RL in the setting where only the optimal \(Q\)-function admits a linear representation [7].

Algorithms for RL in linear mixture SSPs have been developed to address the challenges posed by the linear mixture transition kernel. New algorithms have been proposed with improved regret guarantees, including sublinear regrets for learning linear mixture SSP and refined confidence sets for achieving near-optimal regrets with poly-logarithmic factors [15]. There is also a body of work on online learning for SSP problems, with algorithms proposed to eliminate restrictive assumptions and achieve nearly minimax optimal regrets, matching lower bounds of linear mixture SSPs [1]. Furthermore, the study of RL for linear mixture MDPs has seen the development of nearly minimax optimal algorithms that achieve optimal regret bounds with no polynomial dependency on certain parameters [9]. Additional work has focused on addressing the limitations of RL methods and providing sharp thresholds for efficient RL, highlighting the fundamental requirements for efficient reinforcement learning under various conditions [8].

Overall, the related work demonstrates a rich and diverse landscape of algorithms and theoretical results for reinforcement learning in the context of stochastic shortest path problems, including but not limited to model-based RL, risk-sensitive RL, no-regret algorithms, frequentist regret bounds, and sample complexity analysis.>

References:
[1] Online Learning for Stochastic Shortest Path Model via Posterior Sampling
[7] On Reward-Free Reinforcement Learning with Linear Function Approximation
[8] Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?
[9] Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes
[10] Improved No-Regret Algorithms for Stochastic Shortest Path with Linear MDP
[11] Risk-Sensitive Reinforcement Learning with Function Approximation: A Debiasing Approach
[12] Logarithmic Regret for Reinforcement Learning with Linear Function Approximation
[13] Frequentist Regret Bounds for Randomized Least-Squares Value Iteration
[14] Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping
[15] Learning Stochastic Shortest Path with Linear Function Approximation"
5384,5384," **Self-supervised pre-training.** Pre-training vision models is a long-standing research topic in computer vision. Recently, self-supervision has been successfully applied to image representation learning [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20]. In this work, we focus on the most relevant ones.

**Reinforcement learning.** Reinforcement learning (RL) is a popular paradigm for learning a policy that maximizes the expected return of a given agent. It has been widely used in a variety of tasks, such as imitation learning [21][22][23][24][25], planning [26][27], and policy gradient descent (PGD). In this paper, we mainly focus on policy learning with pre-trained vision models.

 and [20] show that the effectiveness of pre-train vision models for downstream policy learning is largely driven by the pre-processing phase. However, they do not conduct a thorough study on the downstream fine-tuning phase, which is critical to understand the impact of the learned representations on downstream policy performance. In contrast, we conduct a comprehensive study on 14 different downstream policies using 3 distinct classes of policy learning methods, including reinforcement learning, imitation learning, and PGD, and find that the performance of these methods is highly dependent on the choice of the downstream learning algorithm.

 propose to use a contrastive loss for pre-based imitation learning. They show that pre-pretraining improves the generalization ability of the model, while  show that it improves the robustness of the policy learning. In this study, they use the contrastive learning loss [12] to learn a discriminative embedding space for the downstream reward function, and show that this embedding is more effective than the standard cross-entropy loss. They also show that using contrastive losses leads to better generalization performance than using a single-layer perceptron (MLP) for downstream reward learning.  propose a multi-task learning framework, where the model is first trained on a large amount of image-text paired data, and then used to fine-tune the model on downstream tasks. They further show that their model is more generalizable than the MLP-based approach.  use a two-stage approach, in which the model first pre-trains on large-scale image datasets and then uses a policy-based reward function for downstream task-specific training. They demonstrate that the model can achieve better performance than the state-of-the-art model on both image-to-image translation and image retrieval tasks. In our study, we also conduct a similar study, but with a focus on evaluating different downstream reward functions and policy learning algorithms.

 use contrastive self-training to learn visual representations for policy learning and demonstrate that it is effective. They use a single image-level reward function to learn the visual representation for downstream tasks, while we show that different reward functions are effective for different policy learning tasks.

 also use contrast"," **Pre-training in computer vision.** Large-scale pre-training has become the new fuel empowering computer vision. Contrastive learning and related methods ([12]; [11]; [1]; [10]) learn visual representations by modeling image similarity ([4]) and dissimilarity ([5]) between two or more views. Masked Image Modeling (MIM) ([2]) pursues a different direction by learning to predict removed pixels ([6]), discrete visual tokens ([7]), or pre-computed features ([3]). Language-supervised pre-training, e.g., CLIP ([8]) and related works (; [9]), has been established as a powerful paradigm for learning visual representations. While pre-trained models attract increasing attention in the vision field, no large-scale evaluation has compared the various models available for motor control. This work aims to benchmark the plethora of pre-trained vision models to explore which ones are the most effective for visuomotor control.

**Pre-trained vision models for motor control.** The application of pre-trained vision models to problems in motor control is a rapidly growing field ([18]; [14]), with studies such as RRL (Shah & ), PIE-G ([13]), and MVP ([19]) demonstrating the effectiveness of supervised or self-supervised pre-trained vision models as visual representations for RL agents. PVR ([20]) and R3M ([15]) find that vision models pre-trained on real-world data enable data-efficient behavior cloning on diverse control tasks. VIP ([16]) proposes a self-supervised pre-trained vision model capable of producing dense reward signals. Concurrently, [17] show that a carefully designed Learning-from-Scratch (LfS) baseline is competitive with methods that leverage pre-trained vision models. However, most approaches train the agent with only BC or only RL, with limited or no discussion on how policy learning choices are made. Thus, it remains unclear whether the effectiveness of pre-trained vision models is consistent across different policy learning methods.

**Policy learning.** Reinforcement learning (RL) (Sutton & ) and imitation learning (IL) () are two mainstream approaches for policy learning. The gap between image-based RL and state-based RL has been significantly bridged, largely due to ideas like autoencoder-based architectures ([26]; ), self-supervised objectives (; [24]), and data augmentation ([25]; ). IL can be broadly categorized into Behavior Cloning (BC) ([27]) and Inverse Reinforcement Learning (IRL) ([23]). BC is extremely sample-efficient but may suffer on out-of-distributions samples () or copycat problems ([21]). IRL focuses on learning a robust reward function ([22]). In this work, we aim to contrast the merits of three different policy learning methods (i.e., RL, BC, IRL) and their properties with respect to re-appropriating pre-trained general vision models for downstream control-specific problems.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]"," **Pre-trained Vision Models.** Pre-trained vision models have been shown to be effective in learning visual representations for downstream tasks [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27]. In this work, we focus on pre-training vision models for motor control.

**Pre-training Vision Models for Visual Control.** In the past few years, a number of works have explored the use of pre-trained visual models for visual control tasks. These works can be broadly categorized into two groups: (1) methods that pre-train vision models on large-scale vision datasets and fine-tune them on downstream tasks, and (2) methods which pre- train vision models in an end-to-end fashion on a variety of downstream tasks. The former group of methods pre-trains a vision model on ImageNet and then finetunes it with downstream tasks such as reinforcement learning [14][13], imitation learning through behavior cloning (BC), and imitation learning with a visual reward function (VRF) [16]. The latter group of works [18][20] pre-treats visual control as a goal-conditioned reinforcement learning problem, and uses pre-learned vision models to fine-tuning downstream policies. In contrast to these works, we study the effect of pretraining on downstream policy learning, and show that the effectiveness of vision models is highly dependent on the choice of the downstream policy. We also propose a benchmark of 21 tasks for evaluating pre-pre-trained models, which we release in the supplementary material.

 is the most closely related work to ours. They pre-task a vision network with a pre-defined reward function, and then use it to finetune the model with BC. They also use a large number of control policies, and evaluate their model on a small number of tasks. However, they do not evaluate the model's performance under different downstream policies, which is the focus of our work. In addition, we also release a benchmark for evaluating their model performance on 21 tasks.

"," Many DP algorithms have been designed in the context of federated learning ([9]; [3]). For smooth loss functions, [10] proposed a _sparse local stochastic gradient descent (sSGLD)_ algorithm with \(\epsilon/2\)-DP. This approach was then extended to include loss regularization terms in [11], as well as heterogeneous data in [5]. DP SGD has also been shown to perform well for large-scale models, such as language models ([5]; [4]). However, these results depend on the sparsity structure of the model and the data distribution. These assumptions can be circumvented by sparse coordinate descent methods ([6]; [7]).

Unlike the prior work on differentially private federated learning, we focus on centralized and fully decentralized learning settings. In the federated setting, there is only a single central server that all of the clients communicate with, whereas in the centralized setting, the clients communicate with a single central server. Finally, in fully decentralized learning, the clients communicate with each other but have no global server (; [8]).

Recently, there has been a lot of work on designing private algorithms for ADMM. The primary approach has been to propose new privacy amplification techniques to bound the global Lipschitz constant of ADMM by differential privacy methods that protect local Lipschitz constants. These private algorithms are then analyzed by using the non-private convergence analysis for ADMM ([18]; [19]; [14]). We first consider the case where all agents are acting in an asynchronous gossip network (; ; [15]; [22]; [16]; [17]; [13]). For the synchronous ADMM (; [14]), the relevant noise parameters to protect the Lipschitz constant of ADMM are those on the multipliers. Recently, [20]; [12] proposed privacy amplification methods for ADMM under certain error bounds. To use these new methods, we need to leverage DP theory to analyze the differentially private algorithms. This is orthogonal to the work in the machine learning literature, and can be applied to several existing private algorithms including _DP-SGD_([22]; [11]) and _DP-ADMM_([21]; [18]).

In this work, we analyze a broad range of existing private ADMM algorithms, including non-local ADMM, SCAFFOLD, ADMM and ADMM with Lagrangian duality. All of these private ADMM algorithms have been analyzed in the literature in the non-private setting. For instance, the ADMM algorithm by [18] for the non-private case was shown to converge with \(\epsilon\)-DP. Similarly, _ADMM with Lagrangian duality_ has also been analyzed for the non-private setting (; [21]; [16]).

In our work, we used the analysis by [",,"<Related work>

Recent research has highlighted the critical role of pre-trained vision models for motor control tasks, emphasizing the importance of downstream policy learning during control-specific fine-tuning [20]. The effectiveness of pre-trained vision models under different control policies has been the subject of an extensive study that considered 14 pre-trained vision models and 3 distinct classes of policy learning methods, including reinforcement learning, imitation learning through behavior cloning, and imitation learning with a visual reward function [19]. The study found that the effectiveness of pre-training is highly dependent on the choice of the downstream policy learning algorithm, and conventional evaluations based on reinforcement learning methods are highly variable and therefore unreliable. Instead, the study advocated for using more robust methods like visual reward function and behavior cloning for evaluating pre-trained models and their policy learning methods [20].

In the domain of self-supervised learning for visual representations, several approaches have been proposed that have shown promising results for pre-trained vision models [8] [12]. For example, self-supervised methods such as Masked Image Modeling (MIM) and Masked Visual Pre-training have demonstrated effectiveness in learning visual representations for downstream tasks, including robot manipulation and motor control [7] [19]. Moreover, techniques like Non-parametric Instance Discrimination and Self-Predictive Representations (SPR) have shown to be effective in learning feature representations capturing apparent visual similarity among instances and training deep reinforcement learning algorithms directly from pixels, respectively [11] [24].

Furthermore, there has been growing interest in leveraging large-scale, in-the-wild human video datasets for pre-training visual representations, which could subsequently benefit motor control tasks. For instance, self-supervised visual representations pre-trained on diverse human video data have shown to be effective for enabling data-efficient learning of downstream robotic manipulation tasks [15]. Additionally, the potential of using visual representations pre-trained from diverse, in-the-wild videos for real-world robotic tasks has been explored, demonstrating the benefits of scaling visual pre-training for robot learning [18].

The study of pre-trained vision models and their applicability to motor control has also prompted investigations into approaches for using image augmentation and planning from pixels to further enhance the capabilities of pre-trained vision models in motor control [25] [26]. By leveraging data augmentation techniques and learning latent dynamics for planning from pixels, researchers have demonstrated improvements in the performance and sample efficiency of model-free reinforcement learning algorithms, surpassing the performance of model-based methods and recently proposed contrastive learning approaches [25] [26].

In summary, recent advancements in self-supervised visual pre-training, image augmentation, learning from pixels, and leveraging large-scale human video datasets have demonstrated the potential for enhancing the effectiveness and robustness of pre-trained vision models for motor control tasks. These approaches have shown to be effective in addressing the challenges of downstream policy learning during control-specific fine-tuning and in achieving significant improvements in sample efficiency and performance for motor control under different control policies.
"
1042,1042," **Transformers in Computer Vision.** Transformer [1][2] has achieved great success in natural language processing (NLP) and computer vision [3][4][5][6][7]. Inspired by the success of transformers in NLP, a number of works [8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] have applied transformers to point cloud registration. Deep Closest Point (DCP) [9] is the first transformer-based method to directly learn the correspondence between two point clouds. REGTR [8] further improves DCP by introducing a transformer encoder and a transformer decoder. Geometric Transformer (GT) [14] proposes a geometric transformer to learn the transformation between point clouds in an end-to-end manner. However, all these methods focus on designing specific encoding methods for transformer models, which sacrifice versatility and introduce significant computational complexity during the inference process. In contrast, our MRA can be easily inserted into other methods to further improve registration accuracy.

**Masked Autoencoders.** Masked autoencoder (MAE) [25] is a powerful self-supervised pre-training method that has been widely used in various computer vision tasks. The basic idea of MAE is to learn a masking function from the input data, which is then used to supervise the training of a pre-trained model. Recently, several works have applied MAE to various vision tasks, such as image classification [26; 27], object detection [15; 28], and semantic segmentation [29; 30; 31]. For example, Masked Image Recognition (MIR) [6] and Vision Transformers (ViTs) [4][7] pre-train a ViT model on ImageNet and then fine-tune it on downstream tasks. Masked Language Modeling (MLM) [24] and SpanBERT [19] propose to learn masking functions from large-scale unlabeled data. SimMMAE [21] and Mask2Vec [22] extend Mask2MMA [23] to the task of speech recognition. Mask3D  and SimMIME++ [20] further improve the performance of masking-based methods by introducing the spatial masking and the temporal masking, respectively. In this paper, we propose a novel masking reconstruction auxiliary network (MRA), which is a generic and concise auxiliary training network, and can be seamlessly integrated into existing methods to improve their registration performance.

 first proposes to use the attention mechanism [10] to capture long-range dependencies in the input sequence. Inspired by this work, we introduce the quad-tree attention mechanism into transformer models to capture the spatial dependencies between the input and the reconstructed sequence. In addition, we design the Masked Reconstruction Auxiliary Network (MRAN) to reconstruct the complete point cloud by separately using the encoded features of each point cloud obtained from the backbone, guiding the contextual features in the backbone to capture fine-grained"," Transformers have exhibited great success in NLP [1][2][3] and computer vision [4][5][6][7][8], which has motivated researchers to explore their application in point cloud registration. The deep closest point (DCP) [9] utilizes a dynamic graph CNN (DGCNN)  to separately extract features and introduces a transformer [10] to capture the correlations between point clouds. Predator [11] leverages attention mechanisms to conduct information aggregation across a pair of point clouds and predicts overlapping regions for feature sampling purposes, achieving significantly enhanced performance in low-overlap scenarios. CoFiNet [12] extracts features via attention mechanisms in a coarse-to-fine manner and achieves promising performance. The registration transformer (RegTR) [8] utilizes attention layers to directly generate correspondences.

In general, these methods introduce transformers to enable information exchange and encode contextual information, which facilitates the prediction of putative correspondences. However, the limited shared characteristics of the low-overlap point cloud pairs produce obstacles when identifying common structures. To address this issue, several methods have attempted to enhance the ability of networks to capture common structures with dedicated designed encoding modules. Lepard [13] disentangles point cloud representation and utilizes rotary positional encoding  to explicitly reveal 3D relative distance information. The geometric transformer [14] calculates pair-wise distances and triplet-wise angles and combines them with self-attention to capture geometric features. Nonetheless, these methods sacrifice versatility and introduce additional computational complexity during the inference process.

Auxiliary training has been extensively studied in the field of point cloud research, as it can provide a multifold regularization effect during the optimization process while remaining heterogeneous to the main task. SA-SSD [15] introduces an auxiliary network to convert the extracted features back to pointwise representations and then performs foreground segmentation and pointwise center estimation. LabelEnc  employs a novel label encoding function to learn latent embeddings from the given ground truth labels, thus providing auxiliary supervision for the training process. LG3D [16] serves as an auxiliary network to achieve enhanced feature learning by obtaining critical representations and fusing object point clouds into the original input point clouds. However, these works focus on single point cloud and are less suitable for registration tasks. In the domain of point cloud registration, DVD [17] additionally utilizes self-reconstruction and normal estimation tasks to consider the intrinsic structural consistency of point clouds. Although DVD also introduces reconstruction tasks, it reconstructs the input point cloud itself, ignoring the relations across point clouds. DeepMapping [18] utilizes deep neural networks (DNNs) as an auxiliary function to model the structure of a scene by estimating the occupancy status of the global coordinates. Different from these methods, we guide the backbone by reformulating the point cloud registration problem as a masking and reconstruction task.

As a promising self-supervised learning scheme, MDM has achieved promising performance in NLP [1][19] and computer vision [20][21]. Data2vec [22] extracts information based on a masked view of the input and predicts contextualized latent representations that contain information from the entire input. The masked autoencoder (MAE) [20] randomly masks patches of the input image. Then, the MAE learns the latent representations from the unmasked patches and reconstructs the missing pixels. Unlike the MAE, SimMIM [21] utilizes a linear layer as its decoder to directly generate the predicted pixels. Following the recent advances in MDM, adaptation has been investigated for use with point clouds. Point-MAE [23] utilizes an asymmetric transformer autoencoder with a shifting mask token operation and learns latent features from the unmasked points to reconstruct the masked points. Voxel-MAE [24] first voxelizes the input point cloud and then predicts the occupancy values of masked voxels instead of the coordinates of the points. However, fundamental differences between point cloud registration and other point cloud processing tasks pose barriers to applying MDM to point cloud registration.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]"," **Transformers in Computer Vision.** Recently, transformer-based methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] have achieved great success in natural language processing (NLP) and computer vision [6][20]. The Vision Transformer (ViT) [6] is the first attempt to apply the Transformer [10] to the computer vision domain. It firstly splits the input point cloud into a sequence of tokens and then applies multiple Transformer layers to model their global relation for classification. The T2T-ViT [5] and Volo [4] further propose a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring tokens into one Token (Tokens-toToken), which can more efficiently encode fine-grained features. The Volo model [4], which is based on the Vision Outlooker (VOLO) [7], proposes a novel outlook attention operation that dynamically conducts the local feature aggregation mechanism in a sliding window manner across the input image. The REGTR [8] proposes an end-to-(end) framework to predict the probability each point lies in the overlapping region and its corresponding position in the other point cloud, and the required rigid transformation can then be estimated directly from the predicted correspondences without further post-processing. The Deep Closest Point (DCPP [9] and Deep Versatile Descriptors (DVD) [17] are two representative methods that use the attention mechanism to learn the global and local geometric features of the point cloud.

**Point Cloud Registration.** Point cloud registration has been extensively studied in the past few years. The pioneering work, the Iterative Canny Point Cloud Registration (ICP) [9], firstly proposes to solve the registration problem by iteratively matching the points in the two point clouds. The key idea of ICP is to iteratively match the points of the two points clouds by the nearest neighbor matching. The ICP method is based upon the RANSAC algorithm [3] and the outlier filtering algorithm [13]. The Go-ICP [3], Go-LK [13], GoLK-LF [14] and GoLF-D [9][8] are the representative methods"," **Vision Transformer.** The seminal work by [1] proposed a general pre-training model based on the transformer architecture for language modeling. Subsequently, this architecture became popular and widely used in various computer vision tasks [2][3][4][5]. [6] extended this architecture to image classification tasks. However, transformer-based methods are computationally expensive for large-scale image classification tasks, mainly due to quadratic complexity. Therefore, multiple techniques are introduced to improve the efficiency, such as the pooling-based approach [7] and the shifted attention mechanism . These approaches, however, still have large computational requirements, and the methods can hardly apply to other registration tasks.

**Deep learning-based registration methods.** Various deep learning-based registration approaches have been proposed recently. For example, DCP  utilizes the point cloud correspondence prediction task as a supervised learning task, while DEEPCL [8] extends the Vision Transformer to perform this task. However, these methods still need to label dense point correspondences between point clouds. To this end, PCANet [9] proposes to directly learn a representation based on a nearest-neighbor search method, followed by iterative closest point (ICP). [10] introduces a pre-trained language model BERT to extract features for a matching cost function between point clouds. Some recent methods [11][12][13][14] even go beyond searching for dense matching points and seek to generate sparse correspondence sets directly from the point clouds. However, most of the methods mentioned above are computationally expensive and the methods introduce significant complexities. These methods are therefore not applicable to small datasets and embedded devices. The proposed MRA, on the other hand, is easy to apply since it contains only 2 fully connected layers, and is compatible with almost all existing registration approaches.

**Self-supervised methods.** Generally speaking, there are two types of self-supervised learning tasks: image-based and point cloud-based tasks. Recently, several point cloud-based self-supervised learning methods have been proposed [15][16][17]. [18] proposes to predict a sequence of landmarks for point clouds as supervision signals. However, the point cloud features are predicted independently, leading to poor global and local geometric reconstruction capabilities.  uses an end-to-end convolutional neural network to construct the position-aware guidance for transformer-based self-supervised learning. However, these two methods only use the high-level structure of the point clouds for self-supervision. [17] introduces a multi-branch network to perform both global and local reconstruction tasks. However, this method relies on a large amount of training data, which is not efficient for embedded devices.

**Masked autoencoders.** Masked autoencoders have been successfully applied to various learning tasks including images [19], point clouds [20][21][22],",,"<The problem of point cloud registration has been extensively studied in the fields of computer vision and robotics. Traditional approaches rely on iterative algorithms like the Iterative Closest Point (ICP) to find the rigid transformation between two point clouds. However, the recent trend has witnessed a shift towards leveraging deep learning methods for point cloud registration. One of the emerging paradigms in this area is the rethinking of point cloud registration as a masking and reconstruction task [1]. This approach takes advantage of inherent masks derived from the invisible parts of each point cloud, and reimagines the aligned point cloud pair as the reconstruction target. By proposing the Masked Reconstruction Auxiliary Network (MRA), the authors introduce a generic and concise auxiliary training network, which reconstructs complete point clouds by using encoded features to guide the contextual features to capture fine-grained geometric details and overall structures of point cloud pairs. 

Furthermore, various works in related areas have demonstrated the effectiveness of using deep learning methods for diverse tasks. Notably, BERT [1] introduces bidirectional encoders from transformers for language understanding, while recent models like GPT-3 [2] and ViTs [4, 5] have shown significant advancements in natural language processing and visual recognition. Additionally, attention mechanisms have been instrumental in revolutionizing sequential and spatial data processing, as seen in the success of transformer architectures like Vision Transformer [6] and QuadTree Attention [7]. These advancements not only provide inspiration for applying similar techniques to point cloud registration but also pave the way for leveraging attention mechanisms and deep learning for improved registration accuracy. Leveraging these existing developments in the related fields could shed light on innovative approaches that can be adapted to the point cloud registration problem.>


<In the context of point cloud processing and registration, several recent works have demonstrated the potential of leveraging deep learning methods to solve key challenges. For instance, recent work by RegTR [8] proposes an end-to-end framework that replaces explicit feature matching and RANSAC with attention mechanisms, achieving state-of-the-art performance on 3DMatch and ModelNet benchmarks. Similarly, Deep Closest Point (DCP) [9] presents a learning-based method that outperforms traditional iterative methods like ICP for point cloud registration, with applications for solving the local optima problem. Furthermore, recent methods have also explored the use of deep learning for learning representations and handling rigid and deformable scenes in point cloud registration. For example, Lepard [13] demonstrates improvements in state-of-the-art registration recall on both rigid and deformable cases, indicating the potential of deep learning-based methods in addressing complex point cloud registration scenarios.>

<In addition to addressing specific challenges in point cloud registration, recent works in related fields have also shown the potential of leveraging self-supervised learning and masked modeling techniques for improved feature learning. For instance, SpanBERT [19] and SimMIM [21] provide valuable insights into improving pre-training and representation learning in complex data domains, such as natural language and computer vision. These approaches, which focus on predicting and reconstructing masked spans in text and images, can provide inspiration for developing self-supervised learning frameworks for point cloud feature extraction and representation. Furthermore, the recent surge in self-supervised learning frameworks like data2vec [22] and Masked Autoencoders for Point Clouds [23] signal an emerging trend in leveraging masked modeling for learning representations in various domains, including point cloud data.>

<There is also a growing interest in exploring pre-training and self-supervised learning methods for point clouds to advance the robustness and generalization capabilities of models. For instance, DeepMapping [18] introduces a novel registration framework using deep learning methods to align multiple point clouds, which demonstrates improved global registration accuracy compared to traditional techniques. Moreover, recent studies have explored the effectiveness of masked autoencoders for pre-training large-scale point clouds, as shown in Voxel-MAE [24], where the authors leverage large-scale point clouds without manual annotations to enhance the perception ability of autonomous vehicles. These works underscore the potential of leveraging masked autoencoders and deep learning techniques for advancing the state-of-the-art in point cloud registration and related tasks.>

<In conclusion, the recent surge in deep learning-based approaches for point cloud registration, as well as the application of masked modeling and self-supervised learning techniques, presents exciting opportunities for reimagining the conventional paradigms in this domain. Leveraging insights from related work in natural language processing, computer vision, and representation learning could contribute to the development of innovative methods for addressing key challenges in point cloud registration and reconstruction tasks [23]. The potential for cross-pollination of ideas and methodologies across these domains opens up new avenues for advancing the state-of-the-art in point cloud processing and registration.>

References:
[1] Rethinking Point Cloud Registration as Masking and Reconstruction 
[2] Language Models are Few-Shot Learners 
[4] VOLO: Vision Outlooker for Visual Recognition
[5] Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
[6] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
[7] QuadTree Attention for Vision Transformers
[8] REGTR: End-to-end Point Cloud Correspondences with Transformers
[9] Deep Closest Point: Learning Representations for Point Cloud Registration
[13] Lepard: Learning partial point cloud matching in rigid and deformable scenes
[18] DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds
[19] SpanBERT: Improving Pre-training by Representing and Predicting Spans
[21] SimMIM: a Simple Framework for Masked Image Modeling
[22] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language
[23] Masked Autoencoders for Point Cloud Self-supervised Learning
[24] Voxel-MAE: Masked Autoencoders for Pre-training Large-scale Point Clouds"
1581,1581," **Scene Graph Generation.** Scene graph generation (SGG) aims to predict the relationships between objects in an image. Most SGG methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] are based on the two-stage pipeline. In the first stage, objects are first detected by object detectors and then classified into predicates. The second stage predicts the relationship between objects by aggregating information from the context. In this paper, we focus on the second stage.

**Unbiased SGG.** The long-tail bias problem in SGG has been widely recognized in the literature [18][19][20][21][22][23][24][25][26]. To alleviate the bias problem, many methods have been proposed. For example, Zhang _et al_. [18] proposed a bias correction method to generate unbiased training data from the biased training data. Zhang _al_. [19] introduced a balance adjustment method to improve the generalization ability of SGG models. Wang _et. al_.  proposed a graph-level contrastive loss to reduce the influence of object-predicate pairwise relations in the graph generation process. Wang and Zhang [24] proposed an adaptive label propagation method to alleviate the noise in the label propagation process. However, most of these methods focus on improving the visual quality of the generated scene graphs. In contrast, our work focuses on the contextual information in the scene graph.

 proposed a context-aware SGG method, which utilizes the context information to enhance the visual feature representation of objects. Different from them, we propose a context augmentation method for unbiased training of the SGG model.

 introduced a context guided visual scene graph generation model. They proposed to generate context information for each object by concatenating the object features extracted from a pre-trained object detector. In comparison, our method is different from them in two aspects. First, our model can generate diverse context descriptions by using the original dataset. Second, we introduce an unbiased training method for SGG, which can be applied to different datasets.

 is a concurrent work to this paper. They also proposed an unbiased SGG framework. They introduced a new dataset for training the model, which is based on a large-scale synthetic dataset. They used the synthetic dataset to train the model with the original SGG dataset. In addition, they proposed a new loss function for the training of their model. Our work differs from theirs in the following aspects: (1) We propose a new training method based on our dataset. (2) We introduce a novel training method. (3) Our model is more robust to the noise. (4) Different from theirs, we use the original data to train our model. (5) Our method is more efficient.

 and (6) Compared with them, our approach is more general.

 are the most related to our work.

 was the first work to introduce the context in the context graph generation task. We use the context to improve visual quality. We also use the"," Traditional research about SGG is also called visual relationship detection. VRD  first proposes the SGG task based on visual object proposals from RCNN [1][2]. Researchers have gradually realized the importance of SGG in image understanding, and many subsequent works including IMP [3], Motifs [4], VCTree [5] follow this task. These works respectively introduce message passing structures, such as IMP [3], MSDN [6], GPS-Net [7], GB-Net , CISC [8], tree structures including VCTree [5] and CogTree [9], graph structures including G-RCNN , KERN [10] and GCN-SGG [11]. Pixels2Graphs [12] and FCSGG [13] directly predict object pairs and relationships from images, without relying on RCNN results. Seq2Seq-RL [14] introduces using the global context and the seq2seq transformer to estimate the scene graph. SSC-RCNN [15] achieves one-stage SGG through triple query based on Sparse R-CNN. OpenPSG [16] combines the panoptic segmentation and the SGG, and uses the transformer structure to simultaneously predict panoptic masks and relationships. However, in these methods, visual features always play a dominant role in SGG and context features are often used as auxiliary information. For example, RelDN [17] predicts the predicate in the spatial, semantics and visual three channels respectively, and designs the contrastive losses. GPS-Net [7] concats visual features, class scores and spatial features as node features and predicts predicates between nodes based on node features. Motifs [4] has proposed to use the global bounding boxes and labels for edge prediction, but global context information cannot effectively deal with long-tail bias, and the inference speed of bidirectional LSTM is slow. In our methods, we only use local context and visual features are discarded. Our methods extract object pairs for contextual augmentation training, and uses the results of the contextual scene graph results to guide visual SGG.

In recent years, due to the extreme imbalance of predicate categories in the SGG dataset, some works have focused on the long-tail bias to improve the performance of tail predicate predictions. These works can be divided according to whether the training is biased or not. For biased SGG training, extra information is often learned to help remove bias during inference. TDE [18] proposes the causal graph and tries to make the model recognize the deep mean of object features. Cogtree [9] proposes a coarse-to-fine method and debris from biased predictions, while BPL-SA [19] introduces the confusion matrix. DLFE [20] proposes the label frequency estimation and learns the label frequencies in biased training to remove reporting bias.

For unbiased SGG training, additional data processing steps help the model to train unbiased. PCPL [21] proposes the predicate correlation and enables the model to distinguish similar predicates, such as '_on_' and '_parked on_'. GFAL [22] introduces the graph density-aware losses for unbiased training. DT2-ACBS [23] introduces rebalanced sampling strategy and discusses the impact of different sampling strategy on the SGG task. NICE [24] analyzes the samples in the dataset to optimize more accurate labels and generate pseudo-labels that are not labeled. IETrans [25] proposes internal transfer and external transfer to enhance SGG dataset. BGNN [26] introduces a bipartite graph network with bi-level data sampling that can account for the overall recall and the mean recall of predicates. We believe small changes in objects for producing fake images do not change predicates between objects, so we project fake images to the context level to increase the number of context samples. Then we can obtain diverse context samples for unbiased training of the contextual SGG.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]"," **Scene Graph Generation.** Scene graph generation (SGG) is a long-tail problem in computer vision, which aims to predict the relationship between objects in an image. Early SGG methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26] are mainly based on object detection [2][1] and predefined predicates [3][6]. Recently, SGG has been studied from a two-stage pipeline [6][13], where the first stage detects objects and the second stage predicts the predicate for each entity pair using a softmax distribution. However, these methods suffer from the long tail problem in the long-tailed distribution of predicates. To address this issue, many works [6] have been proposed to improve the performance of SGG by introducing additional information, such as context information [4][8], knowledge graph [10], and attention mechanism [14][11]. However, they still suffer from long tail bias and slow inference speed. In this paper, we propose a novel C-SGG method without using visual information and introduce a context augmentation method to alleviate long tail issue.




"," Instance segmentation has witnessed rapid development in recent years. Most current real-time instance segmentation models are built upon two-stage architectures, _e.g_., Faster RCNN [1] and Mask R-CNN [2]. These methods separate the detection process from segmentation by first locating bounding boxes with a region proposal network, and then refining the proposals to segmentation masks. Mask R-CNN introduces the segmentation head for refinement, and the two-stage architecture achieves higher accuracy. While real-time instance segmentation is not the focus of our work, we have evaluated our method with these methods.

Recently, there are some one-stage methods that are widely considered more efficient than two-stage methods. Cascade R-CNN [3] aggregates the class-agnostic information for accurate detection, but it is inaccurate for mask prediction. Mask-RCNN [2] and Mask-RCNN with segmentation head [2] are also based on classification, which is insufficient to do precise mask prediction.

Many instance segmentation methods consider it as a problem of instance refinement. Hybrid Task Cascade (HTC) [4] consists of a two-stage instance segmentation cascade module to improve the quality of boundaries. Boundary Patch Refinement [5] extends the two-stage Mask R-CNN [2] by refining the region with the smallest receptive field. We have evaluated our method with these methods.

As the seminal work of PointRender [6] reimagines segmentation as a point rendering problem, many instance segmentation methods adopt this idea, such as [7][8]. Some existing methods first detect regions by point render and then refine them, such as the first one [6] and [7]. Some methods predict masks without bounding box [9][10][11][12], while the methods above all need to provide the bounding box for the mask head to work.

Recently, transformer has been successfully applied to the field of computer vision. DeiT  directly leverages the Transformer architecture for image classification and achieves SOTA performance in many classification benchmarks. Mask Former [13] adapts the attention-based Transformer to mask prediction, which is the first step of instance segmentation. Many Mask Former-based methods [14][15][16][17][18][19] follow the architecture of Mask Former and extend the model with a new design. Mask Former+ [18] extends the mask-level classification to pixel-level and achieves a higher performance. Mask2Former [17] and Mask Former X [20] formulate the instance segmentation as a universal pixel-wise classification task and conduct image segmentation and instance segmentation on the same model. CMT-DeepLab [21",,"<Scene graph generation (SGG) has seen significant advancements in recent years, with a focus on addressing challenges such as long-tailed bias and slow inference speed [1]. Methods such as Faster R-CNN [2] and neural motifs [4] have contributed to improving the performance of object detection and semantic segmentation. Additionally, iterative message passing models [3] and visual context tree models [5] have been proposed to understand visual scenes beyond recognizing individual objects. These works have demonstrated the importance of contextual cues and hierarchical relationships in scene graph generation. GPS-Net [7] and CogTree [9] have introduced novel techniques to explore properties such as edge direction and node priority while mitigating the long-tailed distribution problem. They have shown significant improvements in SGG performance on popular datasets such as VG, OI, and VRD. Moreover, the application of transformers has been shown to enhance SGG task, as evident from the successes of models like R-CNN [2] and Seq2Seq Transformers [14]. Efforts in addressing long-tailed bias have resulted in methods such as Neural Motif [4], Unbiased SGG with Cognition Tree loss [9], and balance adjustment [19], all of which have sought to improve biases in SGG by utilizing cognitive structures, balance adjustment, and knowledge-embedded routing networks.>

<While various approaches have been explored to address challenges in scene graph generation (SGG), including long-tailed bias and data distribution problems, they have often been addressed from different perspectives. For example, the use of loss functions that target specific errors in scene graph parsing was demonstrated in the Graphical Contrastive Losses approach [17]. Meanwhile, the Devil is in the Tails hypothesis [23] has explored the idea of improving performance in visual relationship learning by simplifying the model and focusing on long-tailed distributions, showcasing the importance of tackling data imbalances. Significant progress has also been achieved in overcoming noisy annotations and imbalanced data distribution through innovative methods like NICE [24] and IETrans [25], which aim to correct noisy labels and transfer data for fine-grained scene graph generation. Furthermore, the introduction of bipartite graph neural networks with adaptive message propagation [26] has shown promise in addressing long-tailed class distribution and intra-class variation, underlining the importance of incorporating adaptive message passing mechanisms in SGG frameworks. Each of these methods contributes to the broader goal of improving performance and robustness in the field of scene graph generation.>

<In summary, the related work in scene graph generation demonstrates a diverse range of methodologies that target various challenges such as long-tailed bias, data distribution problems, and noisy annotations. Enhancements in SGG have been achieved through the use of contextual cues, hierarchical relationships, loss functions targeting specific errors, and adaptation mechanisms to address data imbalance. These approaches collectively contribute to advancing the field of scene graph generation and hold promise for further improvement in visual understanding and reasoning.>

[1] Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
[2] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
[3] Scene Graph Generation by Iterative Message Passing
[4] Neural Motifs: Scene Graph Parsing with Global Context
[5] Learning to Compose Dynamic Tree Structures for Visual Contexts
[7] GPS-Net: Graph Property Sensing Network for Scene Graph Generation
[9] CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation
[14] Context-aware Scene Graph Generation with Seq2Seq Transformers
[17] Graphical Contrastive Losses for Scene Graph Parsing
[19] From General to Specific: Informative Scene Graph Generation via Balance Adjustment
[23] Learning of Visual Relations: The Devil is in the Tails
[24] The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation
[25] Fine-Grained Scene Graph Generation with Data Transfer
[26] Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation"
2969,2969," **Uncalibrated Photometric Stereo.** The goal of photometric stereo is to recover the surface normals of a scene under unknown illumination and reflectance. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] estimate the photometric parameters of a surface by minimizing a photometric energy function. These methods assume that the surface reflectance is known and can be estimated from a single image. However, this assumption does not always hold in real-world scenarios. In this work, we propose to estimate the reflectance from a set of images captured under unknown lighting conditions.

"," In this section, we provide a succinct overview of photometric stereo literature focusing on the single orthographic camera assumption. Alternative setups (_e.g._, perspective, multi-view cameras) are beyond the scope of this work.

**Optimization-based Approach:** The majority of photometric stereo methods assume calibrated, directional lighting following Woodham [1] and optimize parameters by inversely solving a physics-based image formation model. This approach can be further categorized into robust methods, where non-Lambertian components are treated as outliers [2][3]; model-based methods, which explicitly account for non-Lambertian reflectance [4][5][6]; and example-based methods [7][8][9] that leverage the observations of known objects captured under identical conditions as the target scene. The uncalibrated task is akin to the calibrated one, but with unknown lighting parameters. Until recently, most uncalibrated photometric stereo algorithms assumed Lambertian integrable surfaces and aimed to resolve the General Bas-Relief ambiguity [10][11][12][13][14][15][16]. In contrast to these works, photometric stereo under natural lights has also been explored, wherein natural illumination is approximated using spherical harmonics [17], dominant sun lighting [18][19], or equivalent directional lighting [20][21]. Although most optimization-based methods do not require external training data, they are fundamentally limited in handling global illumination phenomena (_e.g._, inter-reflections) that cannot be described by the predefined point-wise image formation model.

**Learning-based Approach:** Learning-based methods are effective in addressing complex phenomena that are challenging to represent within simple image formation models. However, the first photometric stereo network [22] necessitated consistent lighting conditions during both training and testing. To address this limitation, various strategies have been investigated, such as observation maps [23], set-pooling [24], graph-convolution [25], and self-attention [26][27]. Furthermore, researchers have explored uncalibrated deep photometric stereo networks [28][29][30], where lighting parameters and surface normals are recovered sequentially. Self-supervised neural inverse rendering methods have been developed without the need for exter nal data supervision. Taniai and Maehara [31] used neural networks instead of parametric physical models, with images and lighting as input. This work was expanded by Li and Li [32][33], who incorporated recent neural coordinate-based representations . However, despite their tremendous efforts, these methods are designed to work with only single directional light source and have limited ability to generalize to more complex lighting environments.

**Universal Photometric Stereo Network:** The universal photometric stereo network (UniPS) [34] was the first to eliminate the prior lighting model assumption by leveraging a non-physical lighting representation called global lighting contexts. These global lighting contexts are recovered for each lighting condition through pixel-wise communication of hierarchical feature maps along the light-axis using Transformers [35]. During surface normal prediction, a single location is individually selected, and the network aggregates all the global lighting contexts (bilinearly interpolated from the canonical resolution) and raw observations at the location under different lighting conditions to pixel-wise predict the surface normal. This method introduced two strategies to handle high-resolution images: down-sampling images to the canonical resolution for recovering global lighting contexts, and employing pixel-wise surface normal prediction. Although these two concepts contributed to the scalability of image size, they resulted in performance degradation due to the loss of input information and the absence of a non-local perspective, as previously discussed.

Our work draws inspiration from [34] and shares some fundamental ideas, particularly the use of Transformers [35] for communicating and aggregating features along the light-axis. However, our method diverges from [34] by fully utilizing input information in a non-local manner, which leads to a significant enhancement in reconstruction quality.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]"," In this section, we review the most related work on uncalibrated photometric stereo and photometric networks.

**Uncalibrated Photometric Stereo.** The photometric stereoscopic stereo (PS) problem was first introduced in [1] and has been extensively studied in the literature since then [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35]. In this paper, we focus on the problem of uncalibration, i.e., recovering the surface normal of an object under unknown lighting conditions. This is an ill-posed problem due to the absence of any prior knowledge about the lighting condition or the object shape.


**Uncalibrating Photometric Networks.** Uncalibrating photometric (UPS) methods have been proposed to resolve the generalized bas relief (GBR) ambiguity [10][14], which refers to the case where the surface normals and albedo are unknown under known lighting conditions, but the light directions are known. The GBR ambiguity has been resolved using various prior knowledge, such as the Lambertian reflectance model [6][13], the color profile [9], the intensity profile [12], the virtual exemplars [8], and the color and intensity profiles [15][20]. However, these methods are limited to Lambertian surfaces with known reflectance and lighting. In contrast, our method is able to recover general, non-Lambertian surfaces under unknown light conditions, and without prior knowledge of the object's reflectance or lighting condition. Moreover, we are the first to propose a method that can recover detailed surface normal maps under unknown, spatially-varying lighting conditions in uncontrolled environments, without any prior information about the object or its reflectance condition. We show that our method outperforms all previous methods on public benchmarks, and that it can be trained with a significantly smaller number of input images.

 is the first work to address the problem under general, unknown lighting. However, their method is limited to a single-image setup and does not account for the non-local interactions among surface points. In this work, we propose a novel method that is scalable, detailed, and can be used in conjunction with any existing UPS"," **Knowledge Distillation** has been widely applied in various vision tasks, and SOD is not an exception. Existing work in SOD mostly uses KD to compress the large model to a lightweight model. Some knowledge distillation approaches[1][2] aim to distill the knowledge from the teachers to the student, while the others[3][4][5][6][7][8][9][10] distill the knowledge from the teacher to the student, which is more suitable for our task. In this paper, we use the latter.

In order to improve the performance of student model, existing methods propose different strategies for KD. One of the popular strategies is to align the features from teacher and student. E.g. [11][12] tried to find the information that is shared by the features from teacher and student and used it for distillation. Wang _et al._[13] found that some distilled features are inconsistent with each other. They proposed a regularization scheme to constrain the correlations between features. As for KD for object detection, Wang _et al._[14] tried to align features from multiple instances of the same object for distillation. Guo _et al._[15] proposed a general distillation framework which emphasizes the localization of the predicted instance in the spatial dimension and the confidence score in the frequency dimension. LGD[16] proposed a distillation scheme based on the label-guided self-distillation, which has similarities with our GCPC.

Another strategy to improve the performance of student model is to design a better feature extractor to extract the informative features for student. In SOD, Chen _et al._[17] use dilated convolution to enlarge receptive fields. [18] proposed to aggregate the features from all possible scales and used the fused features to improve the performance of the student model.

With the development of the NN, some existing work tried to reduce the size of the model while still being able to get a satisfactory performance. For example, Zhou _et al._[19] proposed a student model with a lightweight architecture which can be trained in real-time with high accuracy. Shen _et al._[20] designed a scalable feature pyramid network for SOD. Wang _et al._[21] proposed a depthwise separable convolution layer for feature pyramid network. The top-down connections in FPN[22][23] can also be viewed as an attempt to make FPN applicable for SOD.

Recently, with the development of NN, the performance of SOD can be improved significantly. Specifically, a lot of work tried to improve the performance of SOD by enhancing the robustness to scale variation. In fact, there are two different kinds of scales in object detection: object scales and feature scales. Existing methods tried to solve different kinds of",,"<Photometric stereo (PS) is a computer vision technique aimed at recovering detailed 3D surface normals from images captured under varying and unknown lighting conditions. Traditional methods of photometric stereo have mainly focused on using multiple images captured under different lighting conditions to estimate the surface normals of the objects [1]. These approaches typically require calibrated lighting settings or assume specific reflectance models, which inevitably limit their usability in uncontrolled environments. The task of universal photometric stereo aims to address these limitations by developing methods that can recover detailed surface normals under unknown, spatially-varying lighting conditions in uncontrolled environments [34]. The proposed approach in this paper introduces a groundbreaking universal photometric stereo network, termed SDM-UniPS, which is scalable, detailed, mask-free, and capable of recovering intricate surface normal maps rivaling the quality of 3D scanners [34].

Recent advancements in photometric stereo have also focused on incorporating deep learning techniques to improve the accuracy and robustness of surface normal estimation. For instance, there have been approaches leveraging deep learning models, such as deep photometric stereo network (DPSN) [22] and PS-Transformer [26], which utilize deep neural networks to learn the mapping between complex reflectance observations and surface normals per pixel, enabling accurate normal prediction for real-world objects under varying lighting conditions. Furthermore, self-calibrating deep photometric stereo networks [28] have been proposed to tackle uncalibrated photometric stereo and resolve the generalized bas-relief ambiguity using deep learning architectures, offering a promising solution for obtaining accurate surfaces of objects with unknown reflectances and lighting.

Additionally, attention-based mechanisms have recently gained traction in the field of computer vision. A prominent example is the Transformer architecture, which has demonstrated superior performance in sequence transduction tasks, such as machine translation, by relying solely on attention mechanisms, dispensing with recurrent or convolutional layers entirely [35]. This attention-based approach could potentially inspire the development of novel photometric stereo methods that leverage attention mechanisms to effectively capture complex inter-image interactions and exploit global lighting contexts for robust and accurate surface normal estimation under diverse and unknown lighting variations.>

Overall, the related work demonstrates the evolution of photometric stereo techniques from traditional methods relying on multiple images and specific reflectance models to modern approaches leveraging deep learning and attention mechanisms. The proposed universal photometric stereo network in this paper expands the scope of photometric stereo to encompass uncontrolled environments and unknown lighting conditions, aligning with the advances in attention-based models inspiring new directions for robust surface normal estimation."
4509,4509," In this section, we review related work on distortion models, camera calibration, and neural radiance fields.

**Distortion Models.** A wide variety of distortion models have been proposed in the literature to model the effects of lens distortion on the image formation process. These models can be classified into three main categories: parametric [1][2][3][4], non-parametric [5][6][7][8][9][10][11][12][13], and hybrid [14][15][16][17][18][19][20][21][22][23]. In this paper, we focus on the distortion model proposed in [13], which is the most similar to our approach. However, it differs from our approach in several important ways. First, [13] is a non-differentiable model, which means that it can be optimized in an end-to-end fashion. In contrast, our model is fully differentiable, which allows us to use it to optimize the camera parameters. Second, the lens distortion model is parameter-free, which makes it much easier to use. Finally, the camera calibration can be performed in a unified optimization framework, which is not the case for [13]. In our case, the calibration is performed in conjunction with the 3D reconstruction. In addition, our method can be used to optimize a radiance field, which can then be used for other applications, such as camera pose estimation and 3D point cloud reconstruction. Finally we show how our model can be combined with existing distortion models to improve the quality of the final 3D reconstructions.****Camera calibration.A common approach for calibrating cameras is to use a calibration target [24][25][26][27]. These calibration targets can be defined as a set of calibrated images and their corresponding calibration parameters. In this work, we use the calibration targets from [25] to calibrate our model. We show how to use these calibration targets to optimize our distortion model, and show that our model outperforms the calibration target in several different cases. We also show how the distortion can be integrated into our model to further improve the performance of our 3D model. **Neural Radiance Fields.** NeRF [28] uses a neural network to estimate the radiance of a spherical function. This function is then used to synthesize a 3D scene from a 2D image. NeRF can be viewed as a special case of [29][30][31][32][33][34][35][36] that uses a convolutional neural network (CNN) to learn the parameters of the function. In particular, NeRF uses a CNN to learn:

"," Existing camera calibration methods.Many 3D computer vision methods assume that lens distortion is radially symmetric around the center of the image. Various camera models such as the radial [1] (bicubic ), division [2], FOV models , and rational model [3] are used to simulate such radially symmetric distortion. Numerous calibration toolboxes and pipelines [4][5] have been developed and integrated to OpenCV . Recently, BabelCalib  proposed a robust optimization strategy for parametric models. However, parametric models are only approximate models of real lenses; in practice, the real distortion includes effects caused by complex lens systems (which lead to combinations of different types of distortions) determined by the camera geometry and by the (not perfectly planar) shape of the lens [6].

When calibrating a camera system with an unknown lens it is difficult to decide in advance which particular model fits the real type of camera projection best. To avoid having to choose, one can instead use a single generic model to approximate most common types of projection. A generic camera model [7][8][9][10][11] associates each pixel with a 3D ray. These methods are designed for generality and flexibility and introduce an extreme number of parameters. In practice, classical sparse calibration patterns do not provide enough measurements for such generic models. [12] uses these models to obtain dense matches using displays that can encode their pixel positions or interpolate between sparse features. However, interpolation leads to inaccurate and sub-optimal performance. Therefore, models with lower calibration data requirements have been proposed [13]. Recently, Schops _et al._[14] extends [13] with a new calibration patterns and detectors to improve the calibration accuracy for generic cameras. [15] replaces the explicit parametric model with a regularization term that forces the underlying distortion map to be smooth.

Neural network-based camera calibration.Several prior works treat the optical components of displays and cam eras as differentiable layers (neural network layers) that can be trained jointly with the computational blocks of an imaging/display system [16][17][18]. Other works estimate camera parameters from single image observations using CNNs [19][20]. For multi-view, joint optimization of camera parameters and neural scene representations, representative works include BARF [21], NeRF\(--\)[22], Self-Calibrating Neural Radiance Fields [23] and the point-based neural rendering pipeline of Ruckert et al. [24]

**Learned markers and keypoint detectors.** Lens models can either be optimized during 3D reconstruction or in a separate calibration stage that uses keypoint positions corresponding to a known 3D structure. Many calibration packages use a checkerboard pattern  due to its simplicity and to be able to utilize line fitting to increase corner detection accuracy. Schops et al. [14] propose a star-based pattern similar to Siemens stars  to increase the amount of gradient information available. They use AprilTags [25] to initialize their point search, while we use ArUco tags  in a similar way on our proposed marker board.

However, all these boards are manually designed. In contrast, [26] uses a random pattern optimized to produce strong feature responses for keypoint detectors. This leads to significantly more points (on the order of thousands), albeit with lower detection accuracy. Hu et al. [27] propose to use a deep-learning based detector. Grinchuk et al. [28] propose to use a learning-based approach for creating markers by generating binary codes and rendering them on distorted and transformed image patches. Peace et al. [29] use end-to-end trainable systems for marker detection, but focus on fiducial-like markers with a unique marker identification. These systems usually require larger markers with a unique identifier to enable direct estimation of camera pose relative to a single marker. In contrast, we base our board on a marker detector with very high accuracy keypoint detection, as we only care about point detection accuracy and identify points on the board using a few low-accuracy ArUco tags. This leads to a higher number of extracted keypoints and high center point extraction accuracy.

**Invertible Neural Networks.** Our paper models lens distortion using an invertible mapping enforced through the neural network architecture. Invertible neural networks have been studied extensively in the context of normalizing flows, where network inverses are required for computing log-likelihoods for generative models [30][31][32][33]. Since our application does not require the estimation of the Jacobian for generative tasks, we opt to use an invertible residual network due to its expressive power and convergence speed. Invertible residual networks have been applied to many tasks, such as shape deformation [34][35], image denoising [36], and tone mapping [37]. In this paper, we explore their applicability to the problem of lens distortion.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]"," **Camera calibration.** Camera calibration is a long-standing problem in computer vision and photogrammetry, and has a long history in both academia and industry [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49]. In this paper, we focus on the problem of camera calibration, which has been extensively studied in the literature.

**General Camera Calibration.** The most common approach to calibrating cameras is to use a parametric model, which is defined as the mapping from the 3D point cloud to the 2D image pixels. This approach has been widely used in the computer vision community, and is known as the pinhole model [8][1][4]. However, the model is limited to a few degrees of freedom, which limits its applicability to more general camera models. To address this limitation, several works have proposed to use the generic camera model [9][8], which is a non-parametric model that is defined by the projection of a ray in 3D space to every pixel in the image. The camera model can be viewed as a linear combination of the camera and ray, and the camera parameters can be estimated by minimizing the photometric error between the projected ray and the original image. In [8], the camera is represented as a set of virtual sensing elements called raxels. The raxel parameters are then used to calibrate the camera. The pinhole camera model has been extended to handle non-central cameras [10][12] and to handle radial distortion [3][12]. In [12], the authors propose a parameter-free method for calibrating the radial distortion of a pinhole lens. The center of distortion is estimated using a nonplanar calibration grid [7], which can be used to estimate the distortion center of a noncentral camera [7]. The authors of [12] propose a method for estimating the center of the distortion of non-planar cameras. The authors in [13] propose an approach for estimating a center-of-distortion map for non-axis cameras. In addition, the authors of"," Lens model.Classic camera calibration methods use a large set of calibration features in the form of checkerboards, stars, or coded apertures to estimate the intrinsic camera parameters. More recently, these classical methods were extended to calibrate non-pinhole-like cameras such as fisheye [1][2][3][4][5], radial [1][6], cata-dioptric [7], and hyperradial lenses [8][9][10]. The generic calibration procedure was recently extended to handle planar non-central cameras, requiring accurate estimation of image points on the focal plane [11][12]. While these methods have impressive performance, they typically assume a linear projection of the image plane onto the camera plane, which is a strong assumption in practice and may not be true for lenses with strong aberrations.

More recent approaches propose camera calibration using _implicit_ parametric models that are defined in 3D instead of in the image plane. It is possible to recover the calibration parameters from 3D measurements using an optimization problem that has several similarities with hypergraph matching. Such methods have been proposed in [13][14], and their quality was improved through the use of probabilistic models such as warped VAMPIRES [13] or alternating proxies [15]. Unfortunately, these methods are highly dependent on accurate parametric calibration as shown in .

Several studies have addressed camera calibration from a different perspective by proposing _black-box optimization_ (BBO) pipelines, which estimate the calibration parameters from unlabeled real-world images. BBO methods can achieve better calibration results in the presence of noisy and low-quality images . Because of the sophistication of current BBO methods, using a black-box optimization tool requires detailed knowledge about the camera and its hardware, which may not be the case when capturing real-world images with a consumer-grade camera. It is possible to circumvent this issue by performing calibration on realistic synthetic images [16], but this requires detailed knowledge about the camera hardware and the assumptions of the model used.

A recent trend in camera calibration aims to _end-to-end_ calibrate the camera without needing prior knowledge of the lens distortion or camera model. These methods perform camera calibration using a neural network. Leitzing  proposed to automatically solve the distortion problem using an iterative algorithm that optimizes the parameters of an unknown parametric camera model using a neural network. Other approaches proposed to optimize the camera model using a black-box optimization procedure [17][18], but the camera model remains unknown. Most recently, DeepCalib [19] presented a supervised approach that estimates a pinhole model with an RGB image as input and a 3D viewpoint as ground-truth. However, these methods estimate the camera parameters using hand-crafted 2",,"<>
In the realm of camera calibration and optical modeling for 3D reconstruction, various approaches have been proposed to address the challenges of modeling lenses, distortion, and calibration targets. Existing methods have made significant strides in addressing the limitations of traditional models and have paved the way for innovative techniques. Close-range camera calibration [1] tackles the variation of lens distortion within the photographic field and offers a practical method for calibrating radial and decentering distortion of close-range cameras. Simultaneous linear estimation of multiple view geometry and lens distortion [2] augments the fundamental matrix estimation to include radial lens distortion, providing a simpler form in homogeneous coordinates. A rational function lens distortion model for general cameras [3] introduces a new rational function model for radial lens distortion in wide-angle and catadioptric lenses, enabling linear estimation of motion and lens geometry from uncalibrated views. These contributions have significantly influenced the development of camera calibration methods and lens distortion modeling for various imaging systems.

In addition, the field has seen advancements in analyzing distortions in non-single viewpoint imaging systems, leading to the development of a taxonomy of distortions and a metric to quantify caustic distortions [4]. This has broadened the understanding of distortions in imaging systems and led to the development of algorithms for computing minimally distorted views using simple priors on scene structure. The proposal of a flexible new technique for camera calibration [5] has enabled easy and flexible calibration of a camera, advancing 3D computer vision from laboratory environments to real-world applications. Furthermore, a precision analysis of camera distortion models [6] has contributed to the identification of the right camera distortion model for real camera distortion and has compared the precision of classic camera distortion models for direct or inverse distortion. These contributions have laid the foundation for a deeper understanding of camera calibration and distortion modeling, paving the way for future research in this domain. The proposed model offers a promising avenue for improving the fidelity and accuracy of lens modeling and camera calibration for 3D reconstruction and rendering applications.

Moreover, recent developments in the domain have also seen a shift towards neural lens modeling and optimization, as exemplified by the proposed NeuroLens model in the target paper. The proposed model leverages neural networks to model lens distortion and vignetting, offering a novel approach to addressing the limitations of traditional lens models. This innovative neural lens modeling approach has the potential to significantly enhance the quality of camera calibration and the fidelity of 3D reconstruction results. Additionally, the proposed method presents a comprehensive evaluation of the performance of the NeuroLens model using real-world datasets, showcasing its superiority over standard packages and recent approaches while being much more user-friendly. This innovative modeling approach holds promise for advancing the field of lens modeling and camera calibration, opening new avenues for research and development in the domain. Overall, the proposed NeuroLens model represents a significant advancement in the field of optical hardware modeling and paves the way for future innovations in camera calibration and 3D reconstruction."
3420,3420," **Incomplete Multi-View Classification.** Incomplete multi-view clustering [1][2][3][4][5][6][7][8][9][10][11] aims at clustering multi-modal data with missing views. In this paper, we focus on the task of incomplete single-view classification, which is more challenging due to the high uncertainty nature of missing data.

**Uncertainty Estimation.** There are two types of uncertainty in machine learning, _i.e._ epistemic uncertainty and aleatoric uncertainty [12][13][14][15][16][17][18][19][20][21]. The former is the uncertainty of the model itself, while the latter is the distribution of the data. The uncertainty of missing views is usually modeled as a distribution over the missing views, and the distribution can be estimated by a single model. For example, in [14], a single deep deterministic neural network (DNN) is used to estimate the uncertainty. In [12], a deep ensemble of multiple DNNs is utilized to estimate uncertainty. However, it is difficult to obtain a single deterministic imputation for each missing view, since the missing data itself is of high uncertainty. To address this issue, [13] proposes a prior network to model the uncertainty in the data distribution, and [18] proposes an uncertainty-aware loss function for multi-task learning. In addition, [21] proposes to use a logistic regression model to model uncertainty in deep neural networks. In our work, we propose to model each missing data with a distribution conditioned on the available views.




\[\begin{tabular}{l c c c l l l c l c r l c c r c c \hline{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,57]. \[\langle\mathbb{E}_{p(\mathbf{x},\bf{y})\,\mathcal{P}_{q}(y|x,y|y|z)=\frac{1}{2}\sum_{y=1}^{n}x^{\prime}y^{n+1}q_{y}+\mathrm{p}(x,z)\geq 0.\] (1)where \(x\) is the missing view and \(y\) and \(z\) are the observed views. The goal of our work is to obtain an imputation model for each view, and \(p(\bm{x})\) is the imputation error of each view"," Incomplete multi-view learning can be generally divided into two main lines in terms of how to handle missing views. Specifically, existing works mainly focus on neglecting or complementing the missing views based on deep-learning methods. **Methods without imputation.** The methods only use present views and directly learn the common latent subspace or representation for all views in clustering [1][2] and classification [3][4]. **Generative Methods.** The methods impute missing views with the present views and then utilize the reconstructed complete data to conduct downstream tasks [5][6][7][8][9][10][11]. Specifically, one of the most popular ways is applying the structure of variational auto-encoder on partial multi-view data to reconstruct missing views [5][6][7]. Generative adversarial network is also used to generate missing views [10][11]. Besides, there are some methods to obtain imputations based on kernel CCA [8], spectral graph , and information theory [9]. Compared with the above algorithms, our method obtains multiple imputations instead of single imputation and then dynamically evaluates the imputation quality. Thus the more reliable downstream classification tasks can be performed.

One of the key points of our method is to explore and exploit the uncertainty in missing data. To achieve high-quality uncertainty estimation, many approaches have been proposed [12][13][14]. The uncertainty in deep learning can be generally divided into aleatoric uncertainty and epistemic uncertainty [15][16]. Aleatoric uncertainty refers to the uncertainty caused by data and it measures the inherent noise of data. Aleatoric uncertainty can be further divided into homoscedastic uncertainty and heteroscedastic uncertainty, while the first one which varies with different tasks is usually used to estimate the uncertainty in multi-task learning [17][18], and the latter one which varies with input is useful when the input space includes variable noise [19][20]. On the other hand, epistemic uncertainty refers to uncertainty caused by insufficient model training and can be eliminated in theory. It can be estimated by predicting an uncertain observation using models with different parameters, the instability of predicting results just reflects the epistemic uncertainty [15]. In this work, we estimate the aleatoric uncertainty of imputations by adopting subjective logic [21] and Dempster-Shafer theory  to construct a trustworthy and reliable multi-view classification network.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"," Incomplete multi-view data classification.Incomplete Multi-view clustering aims to leverage information from multiple views to improve clustering [1][2][3][4][5][6][7][8][9][10][11]. However, it is difficult to obtain a single deterministic imputation for each missing view due to the high uncertainty nature of missing views. To address this issue, some methods [2][1][4] project the original and incomplete data into a common latent space and then group the incomplete multi-modal data to form a complete representation. However, these methods cannot well handle the dependence and discrepancy among different views, and thus cannot guarantee the trustworthy integration of the imputed views. Some methods [8][10] attempt to recover the missing views by learning a joint latent representation for all views and then infer the missing data by clustering the latent representation. In [11], a generative adversarial network (GAN) is employed to infer the imputation of missing data and perform missing data inference simultaneously. Although these methods have achieved great progress, they cannot guarantee a trustworthy prediction due to their uncertainty nature. In this paper, we propose an uncertainty-induced model to explore and exploit the uncertainty of the missing view, and propose an evidence-based fusion strategy to achieve more perceivable imputation and controllable fusion, which is more trustworthy.


**Uncertainty Estimation.** Uncertainty in deep learning has been widely studied in machine learning [12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][28]. In [13][13], a prior distribution is used to model the distributional uncertainty of each data point. [15] propose a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. [16] represent the uncertainty as a combination of aleatory and epistemic uncertainties. [21] propose an opinion-based method to characterize the uncertainty. In our work, we introduce uncertainty to the missing-view classification problem by modeling the uncertainty with a distribution conditioning on the available views and thus introducing uncertainty.

"," **Transfer learning**. Transfer learning aims to learn a transferable representation and generalize to a novel task from the same domain [1]. To this end, representation learning with meta-learning is proposed to implicitly discover and encode transferable knowledge [2]. These representation-learning based methods improve the performance of new downstream tasks by transforming the structure of the data [3][4] and adopting meta-learning . Our work also belongs to the transfer learning domain, but with a unique motivation of incompatible knowledge transfer from the source generator. Besides, we demonstrate that incompatible knowledge transfer results in undesirable performance degeneration.

**Few-shot learning.** Few-shot learning is commonly studied in different sub-domains, including action recognition [5][6], image classification [7], visual question answering , _etc._ As few-shot tasks are computationally expensive, great efforts have been made to accelerate them, _e.g._, increasing model capacity [8], using few-shot learning techniques such as pretraining and fine-tuning [9][10][11], _etc._ Our work is also related to few-shot learning, where a fine-tuned source generator should produce high-fidelity samples on the target domain. The source generator contains a large amount of shared features, but each image class in the source and target domain may exhibit different variations. Different filters in the source generator could produce various semantic meanings in the target domain due to incompatible knowledge transfer.

**Open-world object detection.** Compared with the fully supervised object detection problem, open-world object detection is to simultaneously learn the base classes and novel classes in a semi-supervised setting. Previous works have made remarkable progress, including adapting a detector [12] trained on a pretrained source model for the target domain, _i.e._, OW-DETR [13], and adopting an open-world detection transformer [14]. All of these works address the few-shot detection problem with a detector, which is not suitable for image generation. Differently, our work considers few-shot image generation.

**Knowledge preservation.** As a transfer learning paradigm, knowledge preservation aims to preserve the prior knowledge from a large-scale pretrained model to a target model with limited training samples [15][16][17]. Theoretically, knowledge preservation is beneficial to the model performance because it is assumed that a pretrained model can generate high-quality outputs for unseen samples in a target domain [1]. Differently, our work aims to preserve knowledge and remove incompatible knowledge from a pretrained model to improve the target model.

**Adversarial training.** Adversarial training  is the foundation of GANs . Adversarial training is also widely used in model adaptation to enhance the performance of target models. For example, [18] proposed to freeze",,"<In the realm of incomplete multi-view data classification, there has been a surge in research to address the challenges of uncertainty due to missing views. Prior work, such as [1], introduces the concept of partial multi-view clustering, acknowledging the common scenario of partial examples with missing views and proposing a method, PVC, to establish a latent subspace and generate clusters from such data. Similarly, [2] addresses the problem of incomplete multimodal data by proposing an unsupervised method to handle missing modalities and transform incomplete data into a new and complete representation in a latent space. Additionally, [3] focuses on the integration of data from multiple omics techniques and provides a deep variational information bottleneck approach for incomplete multi-view observations, addressing challenges related to view-missing patterns. Another relevant work is the proposal of CPM-Nets in [4], which emphasizes the completeness and versatility of multi-view representation, aiming to produce structured representation for interpretability under the context of view missing.

In the domain of uncertainty quantification, [12] presents an alternative to Bayesian neural networks for predictive uncertainty estimation, offering a method that is simple to implement, scalable, and capable of high-quality predictive uncertainty estimates. Along similar lines, [13] introduces Prior Networks (PNs), explicitly modeling distributional uncertainty for improved uncertainty estimation, with performance evaluations on synthetic and real-world datasets demonstrating superior performance compared to previous methods. Furthermore, [14] proposes a method for training a deterministic deep model, DUQ, that can identify and reject out-of-distribution data points at test time with a single forward pass, demonstrating robustness and scalability in predicting uncertainty.

Moreover, addressing the need for uncertainty-aware multi-task learning and predictive uncertainty, [17] presents a method that uses aleatoric homoscedastic uncertainty to capture relative confidence between tasks and sets weights for task loss, leading to reduced negative transfer and enabling robust multi-task learning. In a similar vein, [18] puts forward a principled approach to multi-task deep learning by weighing multiple loss functions based on the homoscedastic uncertainty of each task, demonstrating its effectiveness in learning per-pixel depth regression, semantic, and instance segmentation from a monocular input image. These works collectively signify the increasing attention and advancements in addressing uncertainty and incomplete data across various domains, ranging from multi-view data classification to uncertainty quantification and multi-task learning. More research is needed to further explore and refine these techniques for real-world applications.>"
22,22," **Animating Sign Language Notation.** Sign language production has been a long-standing goal of the deaf and hearing-impaired communities [1][2][3]. However, most of the work on sign language production focuses on spoken languages. For example, the ViSiCAST project [1] aims at translating spoken languages into sign languages, and the Virtual Sign Language Translating Project (ViSiCAST) [3] focuses on translating sign language from spoken languages to sign languages. In this work, we focus on translating text written in HamNoSys, a lexical sign language notation, into signed pose sequences.

HamNoSys was first introduced in [4] as a way to annotate sign language videos. It consists of a sequence of keypoints and a set of motion primitives, which are used to represent each keypoint's motion. Each keypoint is associated with a corresponding sign. The keypoints are associated with the corresponding sign in the corresponding video, and their motion is represented by the set of corresponding keypoints in the sign sequence. The main challenge of this task is to find the correspondence between the text and the keypoints. To this end,  proposed a method for learning a correspondence between keypoints, and  proposed an algorithm for learning the correspondence. However, these methods are limited by the quality of the hand-crafted features they use. In contrast, our method is able to learn a universal representation of the sign language by using a transformer encoder, which is trained on a large-scale autoencoder-decoder dataset, and can be applied to any sign language. Furthermore, we use DTW-MJE to measure the distance between pose sequences, which has been shown to be more accurate than DTW in previous work [5][6][7].

**Text-to-Image Diffusion Models.** Text-conditional image generation has been studied extensively [8][9][10][11][12][13][14][15][16][17]. In particular, text-conditioned diffusion models have been applied to image generation [7][8][5][9] and motion generation [12][14]. In these models, text and image are treated as two separate latent variables, and a diffusion model is used to map them into a joint latent space. The latent space is then used to synthesize images from the joint distribution of the two latent variables. Diffusion models have also been used to generate pose sequences from text [10][13] and robot motion [11][13]. In contrast to these works, our work focuses on animating sign language notation into pose sequences using text, and we do not rely on a separate latent space for text and pose. Instead, we propose a transformer-based model that can be trained on autoencoders [16][15] to directly generate poses from text.

 proposed the first method for animating text into sign language, and they use a conditional VAE to model text and poses. They use a VAE-based encoder to encode text into a fixed-length vector, and then use a diffusion"," In this section, we review related work in the field of SLP. We cover avatar approaches using HamNoSys as input and gloss approaches. Moreover, we cover HamNoSys generation work, including translating spoken language text or videos into HamNoSys, as these tasks allow for a full translation pipeline together with our work. Furthermore, we mention diffusion models as a source of inspiration for our method, and text-to-motion works, explaining how our problem and data are different from them.

Since the early 2000s, there have been several research projects exploring avatars animated from HamNoSys, such as VisiCast [1], eSign , dicta-sign , and JASigning . While these avatars produce sign sequences, they are not popular among the deaf community due to under-articulated and unnatural movements, making the avatars difficult to understand . Furthermore, the robotic movements of these avatars can make viewers uncomfortable due to the uncanny valley phenomenon [2]. In addition, as illustrated in Fig. 2, these avatars do not perform all hand motions correctly. A later work , uses motion capture data to create more stable and realistic avatars. However, this method is limited to a small set of phrases due to the high data collection and annotation cost.

To cope with these challenges, a recent work  suggests combining generative models with a motion graph (MG)  and Neural Machine Translation. They translate spoken language sentences into gloss sequences that condition an MG to find a pose sequence representing the input from a dictionary of poses. The sequence is then converted to a video using a GAN. A similar work  suggests progressive transformers for generating signed pose sequences from spoken language through glosses. Like Stoll _et al._, they use a closed set of dictionary signs as the signs in their output sequence, which makes these solutions language and data-specific. A later work [3] uses learned ""cheremes1"" to generate signs. Similarly to glosses and phonemes, cheremes are language specific. In contrast, since our method uses a universal notation, it works for languages it was trained on and for unseen languages, as long as the individual glyphs exist in our dataset.

Recently, different HamNoSys generation tasks have been researched. For example, Skobov _et al._ suggested a method for automatically annotating videos into HamNoSys [4]. Further research on this task could enhance the capabilities of our model, by creating more labeled data. Translating spoken language text into HamNoSys has also been researched , and if improved, can allow a complete translation pipeline from spoken language text into Sign languages using our model.

Diffusion models [5][6] recently showed impressive results on image and video generation tasks [7][8][9]. Generation is done using a learned gradual process, with equal input and output sizes. The model gradually changes the input to get the desired output. In this work, we take inspiration from diffusion models in the sense that our model learns to gradually convert the input (a sequence of a duplicated reference frame) into the desired pose sequence.

In recent years, works on motion generation from English text [10][11][12][13][14] showed impressive results. While these works may seem related to our task, they use 3D motion capture data, which is not available for our task. As detailed in Sec. 4, our data is collected using a pose estimation model over sign videos; thus, it is both 2D and imperfect, with many missing and incorrect keypoints. Moreover, since the text in these works is written in English, recent works [11][14][12] take advantage of large pre-trained language models such as BERT [15], CLIP [16], etc. As HamNoSys is not a common language, with limited available resources, we cannot use pre-trained models as they do.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"," **Sign language animation.** The goal of animating Sign language text into signed pose sequences has been studied for a long time [1][2][3][4]. Early work on virtual human signing [1] has focused on creating virtual avatars that look and act like humans [2] and animating them in a virtual valley [2]. However, these methods are limited to a limited number of keypoints and do not consider missing keypoints. In contrast, our method animates a text written in HamNoSys, a lexical Sign language notation, which is universal by design.

**Diffusion models.** Diffusion models [5][6][7] are a class of generative models inspired by statistical physics and have been widely used in image generation [5]. These models generate pose sequences by iteratively reconstructing the original pose distribution from a latent representation of the text. Our method is inspired by the idea of using a diffusion model to reconstruct the original text from the latent representation. However, we use a different approach to reconstruct pose sequences, where we use the latent representations of text and pose sequences to generate the pose sequences. Our approach is also related to the recent work on text-to-image generation [8][9][10][11][12][13][14], where text-conditioned image generation models have been proposed [9][8][10]. Our work is most closely related to [13], where the authors propose a two-stage approach to generate diverse and naturalistic human motions from text descriptions. Our work differs from theirs in that our method generates pose sequences using a single latent representation, and that we use weak supervision to train our model. Our model also differs from [13] in that we do not use any supervision for the training process, and instead use a weakly-supervised approach to learn from partial and inaccurate data. Our proposed distance measurement is based on DTW-MJE [15], which is a distance measurement that considers missing keypoint distance.


"," Pose Sequence Prediction in Sign LanguageNotation systems have been used for the animation of text [1]. [4] employed a system based on weights and rules (WAR) and applied it to HamNoSys. Their system received support in written form from sign language experts, resulting in sentences annotated with poses. They adapted an off-the-shelf skeletal model to use its skeletons as nodes in the tree grammar. [3] and [2] used HamNoSys as input. However, their approach was based on an off-the-shelf model and they have not evaluated it on a large-scale dataset, such as AUTSL. [5] used a transformer to encode visual information and the words of the sentence. [6] re-implemented a traditional transformer to use the noise-guided method from the works of [7][8][9], resulting in a highly expressive synthesis system. Since the noise-guided method is slow, they modified the sampling part to generate a new noise and use the original score, which is significantly faster. A system similar to ours was proposed by  and [10], but they used a two-stream encoder that jointly encodes the pose and text, in contrast to our system that separately encodes each of them.  built on top of the skeleton model of [11], pre-trained an off-the-shelf transformer on video-augmented scenes and used it to predict poses from HamNoSys.

Text-to-Motion GenerationText-to-motion generation has been explored by recent works. [12] proposed a diffusion-based motion model, but they used images of actions, resulting in limited human motion poses. [13] sampled motion snippet lengths using VAE, conditioned the VAE on a text encoder that used an off-the-shelf transformer and used the sampled snippets to predict human poses. [14] used VAE to pre-train a decoder on human motion and used the obtained embeddings as a motion prior in a text encoder that used an off-the-shelf transformer. However, they used human poses as input and did not evaluate them on a large-scale Sign language dataset. Finally, the works of [8] and [9] used CLIP to encode image and text, while using a pre-trained transformer to predict the image. Although these works use a unified text encoder, they do not offer any distance measure that considers the pose sequences. We therefore compare our method with their method.

Learning with TransformersA technique similar to ours has been used by  in the form of language-to-language translation (using characters as input instead of words), and by [16] in the form of image captioning (using sentences instead of characters as input). The works of [15] and [16] use a pre",,"<>
In the realm of sign language translation, the proposed method for animating text written in HamNoSys into signed pose sequences offers a significant contribution to open communication between the hearing and hearing-impaired communities [1]. By leveraging transformer encoders, the method generates pose predictions that consider spatial and temporal information while ensuring generic universality across various Sign languages. The use of weak supervision for training and the introduction of a new distance measurement using DTW-MJE for assessing the quality of pose sequences further augment the significance of the proposed method [1, 4]. This complements the existing research focusing on notations and virtual human signing in supporting technology development for sign languages [1]. The application of the proposed method to AUTSL, a large-scale Sign language dataset, demonstrates its effectiveness in learning from partial and inaccurate data, thereby aligning with the broader context of automated sign language representation and annotation [4].

Given the increasing interest in diffusion models for image synthesis and motion generation [5, 6, 7, 8, 9], the proposed method aligns with the trends in leveraging diffusion models for high-quality image and pose sequence synthesis. Additionally, the work contributes to the development of versatile and efficient generative models that learn from raw text to allow zero-shot transfer of the model to downstream tasks [16]. The proposed method also resonates with the exploration of hierarchical and joint-level mapping between natural language sentences and 3D pose sequences for motion synthesis [11]. The emphasis on generating diverse and natural 3D human motions from textual descriptions [13, 14] also aligns with the essence of the proposed method in animating text written in HamNoSys into signed pose sequences. Moreover, the utilization of language models for image synthesis and the joint embedding space of CLIP for language-guided image manipulations [8, 9] resonates with the proposal of animating sign language notation into pose sequences using a joint language-to-pose neural architecture [10].

In summary, the proposed method for animating sign language notation into pose sequences forms an integral part of the growing body of research that focuses on leveraging deep learning models, language representations, and diffusion models for image and motion synthesis, while addressing the challenges of sign language translation and representation. It contributes to the broader landscape of automated generation of 3D human motions from textual descriptions, language-guided image manipulations, and the development of generative models for diverse and natural motion synthesis. The release of the code for data pre-processing, the model, and the distance measurement further promotes future research and development in this domain."
4852,4852," **Visual Prompting.** Visual prompting [1][2][3][4] has been widely used in various NLP tasks, including image forensics [5][6][7][8][9], image denoising [10][11][12][13], image manipulation detection [14][15][16][17][18], and defocus blur detection [19][20][21][22][23][24][25][26][27][28][29][30][31][32]. In this paper, we use the visual prompting for low-level structure segmentation, which includes segmenting the manipulated parts, separating shadow regions, and detecting concealed objects.

**Low-level Structure Segmentation.** Traditional methods [33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51] have been proposed to segment out-of-focus and concealed objects in images. Recently, deep learning-based methods [11][10][13][14][16] have shown promising results for segmentation tasks. However, these methods are typically task-specific and require domain-specific pre-training and fine-tuning. In contrast, we propose a unified framework that can be trained end-to-end with the same amount of parameters for each task. Moreover, our method does not require any pre-trained models.

 proposed a visual prompting method for semantic segmentation. It leverages the convolutional neural networks (CNNs) to embed the input image into a latent space, and then uses the CNNs to predict the segmentation mask. Our method differs from theirs in two aspects. First, we focus on the explicit visual content from each individual image,, the features from frozen patch embeddings and the input's high-frequency components. Second, instead of using a dataset-level implicit embedding, we enforce the parameter-efficient prompt tuning protocol using the learned features from the frozen embedding.

 first proposed the prompt tuning method for visual question answering (VQA) task. It uses a CNN-based model to embed an image into an embedding space, then it uses a Convolutional Neural Network (CNN) to predict whether the embedding is correct or incorrect. The VQA task is formulated as a multi-class classification problem, where each class consists of a question, a support vector machine (SVM), and a classifier. Our proposed method is different from the previous visual prompting methods in that we do not use a CNN as the feature extractor. Instead, we directly use the features extracted from the patch embedding to generate the mask for each input image.

 is the most related work to ours. It also proposes a prompt-based method for image degradations detection. Different from our method, they only use the image-level embedding as the prompt, while our method leverages both the patch-level and high-level features.

 also proposes the prompt-free method for object detection, which is a convolution-based network that uses the conv"," Visual Prompting Tuning.Prompting is initially proposed in NLP [1][2]. [1] demonstrates strong generalization to downstream transfer learning tasks even in the few-shot or zero-shot settings with manually chosen prompts in GPT-3. Recently, prompting [3][4] has been adapted to vision tasks. [3] proposes memory tokens which is a set of learnable embedding vectors for each transformer layer. VPT [4] proposes similar ideas and investigates the generality and feasibility of visual prompting via extensive experiments spanning multiple kinds of recognition tasks across multiple domains and backbone architectures. Unlike VPT, whose main focus is on recognition tasks, our work aims at exploring optimal visual content for low-level structure segmentation.

Forgery Detection.The goal of forgery detection is to detect pixels that are manually manipulated, such as pixels that are removed, replaced, or edited. Early approaches [5] detect region splicing through inconsistencies in local noise levels, based on the fact that images of different origins might contain different noise characteristics introduced by the sensors or post-processing steps. Other clues are found to be helpful, such as SIFT [6], JPEG compression artifacts [7] and re-sampling artifacts [8][9]. Recently, approaches have moved towards end-to-end deep learning methods for solving specific forensics tasks using labeled training data [10][11][12]. Salloun  learn to detect splicing by training a fully convolutional network on labeled training data. [13][11][14][12] propose improved architectures. Islam [10] incorporate Generative Adversarial Network (GAN) to detect copy-move forgeries. Huh  propose to take photographic metadata as a free and plentiful supervisory signal for learning self-consistency and apply the trained model to detect splices. Recently, TransForensic [15] leverages vision transformers [16] to tackle the problem. High-frequency components still served as useful prior in this field. RGB-N [17] designs an additional noise stream. ObjectFormer [18] extracts high-frequency features as complementary signals to visual content. But unlike ObjectFormer, our main focus is to leverage high-frequency components as a prompting design to efficiently and effectively adapt to different low-level segmentation tasks.

Defocus Blur Detection.Given an image, defocus blur detection aims at separating in-focus and out-of-focus regions, which could be potentially useful for auto-refocus [19], salient object detection [20] and image retargeting [21]. Traditional approaches mainly focus on designing hand-crafted features based on gradient [22][23][24] or edge [25][26]. In the deep era, most methods delve into CNN architectures [27][28][29][30]. [27] proposes the first CNN-based method using both hand-crafted and deep features. BTBNet [29] develops a fully convolutional network to integrate low-level clues and high-level semantic information. DeFusionNet [28] recurrently fuses and refines multi-scale deep features for defocus blur detection. CENet [30] learns multiple smaller defocus blur detectors and ensembles them to enhance diversity.  further employs the depth information as additional supervision and proposes a joint learning framework inspired by knowledge distillation. [31] explores deep ensemble networks for defocus blur detection. [32] proposes to learn generator to generate mask in an adversarial manner.

Shadow Detection.Shadows occur frequently in natural scenes, and have hints for scene geometry [33], light conditions [33] and camera location  and lead to challenging cases in many vision tasks including image segmentation [34] and object tracking [35][36]. Early attempts explore illumination [37] and hand-crafted features [38][39]. In the deep era, some methods mainly focus on the design of CNN architectures [40] or involving the attention modules (_e.g._, the direction-aware attention [41], distraction-aware module [42]). Recent works [43] utilize the lighting as additional prior, for example, ADNet  generates the adversarial training samples for better detection and FDRNet [43] arguments the training samples by additionally adjusted brightness. MTMT [44] leverages the mean teacher model to explore unlabeled data for semi-supervised shadow detection.

Camouflaged Object Detection.Detecting camouflaged objects is a challenging task as foreground objects are often with visual similar patterns to the background. Early works distinguish the foreground and background through low-level clues such as texture [45], brightness [46], and color . Recently, deep learning-based methods [47][48][49][50][51] show their strong ability in detecting complex camouflage objects. Le  propose the first end-to-end network for camouflaged object detection, which is composed of a classification branch and a segmentation branch. Fan [47] develops a search-identification network and the largest camouflaged object detection dataset. PFNet [48] is a bio-inspired framework that mimics the process of positioning and identification in predation. FBNet [51] suggests disentangling frequency modeling and enhancing the important frequency component.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]"," **Prompting.** Prompting has been widely used in natural language processing [1][2] and computer vision [3][4]. In NLP, prompt-based learning is a paradigm of learning a language model to generate a textual prompt, which is then used to perform a downstream prediction task. In computer vision, prompt learning has been used in image recognition [5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52].

In this paper, we focus on the task of low-level structure segmentation, which aims to segment out-of-focus pixels in an image. The task is closely related to the traditional semantic object detection (SOD) task, which has been studied for decades. The SOD task aims to detect the most salient objects in the image, while the structure segmentations task is more challenging due to the high intrinsic similarities between the target object and the background.

\begin{tabular}{l c l c l l l c c l r l r r r c r c c r r d r c d c d r d c r d d r r a c c c d d d c a d c c a e d r a d r e d c e d e g c d a d e e g e g a e f e f f e g f e c d e f g e f a f e r e f r e g g f f f g f g gf f f h g f h f g h f h h f i g f i f g i f h i f i h f f i l f g a g f r g f c h f r d g f  end c c g f gu f g  g f a d f g.


**Task-specific solutions.** In contrast to the task-specific solution, we propose a unified solution that can be applied to all the tasks discussed above.

"," **Incomplete Multi-view Classification.** Existing incomplete multi-view data classification methods mainly include multi-view clustering methods ([8]; [1]; [2]; [5]; [4]) and imputation methods (; [6]; [9]). In general, multi-view clustering methods treat missing views as unknown dimensions and use an embedding framework with latent variables to model the data. [1] and [2] recover partial views by projecting the missing views in the unobserved data space. [3] employ a variational information bottleneck (VIB) to establish a constrained maximum likelihood optimization problem for the underlying distribution of the data.  propose to learn the probability of a view being missing as an auxiliary part of the imputation model. The aforementioned methods cannot directly perform inference for the imputed views and suffer from low uncertainty. Unlike the previous works, our method uses a stable and reliable framework to perform uncertainty inference, thus yielding a reliable and deterministic imputation model.

**Imputation for Incomplete Multi-view Data.** To achieve a deterministic imputation model, [11] utilizes a GAN to learn the data distribution and introduce a marginal discriminator that deals with the minority class. [9] proposes a novel objective that incorporates representation learning and data recovery into a unified framework from the view of information theory. [10] propose a cross partial multi-view networks (CPM-Nets) framework that aims to fully and flexibly take advantage of multiple partial views. However, the models proposed by these methods are still restricted by the homogeneous feature space and cannot reasonably deal with the various missing information of multi-modal data. Different from these methods, our method is designed to explore and exploit the uncertainty of imputation, thus realizing a more reliable and deterministic imputation.

**Uncertainty Quantification in DNNs.** The uncertainty quantification in DNNs aims to evaluate the uncertainty in model predictions. To quantify the uncertainty in a deterministic manner, [17] propose a task-uncertainty loss to re-weight losses for individual tasks. [18] propose an uncertainty quantification method that uses uncertainty for multi-task learning. [16] introduce uncertainty into image analysis to facilitate more informative decisions and effective visualization of results. [20] propose a test-time data augmentation method for estimating heteroscedastic aleatoric uncertainty. Besides the work above, some methods estimate the uncertainty in a probabilistic manner. For example, [14] introduce the DetNet model that can find and reject out-of-distribution data points. [19] utilize the heteroscedastic regression as a surrogate for uncertainty estimates. [13] propose a probabilistic network that measures the uncertainty from two aspects: model-based uncertainty and data-based uncertainty. Although the above methods are valuable for exploring the uncertainty, they either need the model training or",,"\<The problem of detecting low-level structures in images, such as segmenting manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects, has been typically addressed with domain-specific solutions. However, a unified approach that performs well across all these tasks has been introduced [reference 1]. Inspired by pre-training and prompt tuning protocols in natural language processing, a new visual prompting model, named Explicit Visual Prompting (EVP), has been proposed to enforce the tunable parameters focusing on the explicit visual content from each individual image. This approach significantly outperforms other parameter-efficient tuning protocols and achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions [reference 1].

Previous work in the field of natural language processing has shown substantial gains in task-agnostic, few-shot performance through the scaling up of language models [reference 2]. This approach greatly improves task-agnostic, few-shot performance in NLP tasks and may also be applicable to the task of low-level structure segmentation in images.

Furthermore, recent research has proposed the augmentation of Vision Transformer models with learnable memory tokens to adapt to new tasks with few parameters, preserving capabilities on previously learned tasks [reference 3]. This approach significantly improves accuracy and might provide insights for enhancing low-level structure segmentations in images.

Efficient tuning protocols for large-scale Transformer models in vision, such as Visual Prompt Tuning (VPT), have been introduced as an efficient and effective alternative to full fine-tuning [reference 4]. The approach introduces a small amount of trainable parameters in the input space while keeping the model backbone frozen and achieves significant performance gains compared to other parameter-efficient tuning protocols.

In the field of digital image forensics, strategies for steganography detection have been proposed, including novel general strategies for building steganography detectors for digital images using diverse submodels formed by joint distributions of neighboring samples from quantized image noise residuals [reference 5]. Additionally, effective methods for detecting copy-move forgery, analyzing JPEG errors, and localizing image forgeries have been introduced [references 6-9].

Lastly, recent research has progressed in using transformers for image recognition at scale, showing that a pure transformer applied directly to sequences of image patches can perform well on image classification tasks [reference 16]. These advancements in transformer architectures for image recognition might provide valuable insights for the task of low-level structure segmentation in images.>"
3798,3798," **Pretrained Vision-Language Models (VLMs).** Recently, large-scale pretrained VLMs ([2]; [5]; [6]; [3]) have achieved remarkable performance on various vision-language tasks. These models are mainly based on Transformer ([1]) encoder-decoder architecture, which can be trained end-to-end. BERT ([4]) is a representative VLM model, which consists of a bi-directional Transformer encoder and a masked language modeling (MLM) decoder. ViLBERT ([3]) and ALIGN ([6]) further improve the performance by introducing a memory bank and a momentum distillation module, respectively. However, these models are trained on a fixed set of pre-defined object categories, which limits their generalization ability to unseen object categories.

**Complex Question Decomposition.** Complex question decomposition (CQD) has been widely used in various NLP tasks, such as semantic parsing ([9]; [7]), multi-hop reading comprehension ([8]), and natural language understanding ([11]). CQD aims to decompose complex questions into simpler questions, which are easier to be parsed by a language model. In contrast, our NDCR aims to solve the problem of image retrieval from Linguistically Complex Text, which is a more challenging task. Moreover, our proposed NDCR is a neural-symbolic reasoning approach, which has been shown to be effective in solving sequential decision making problems ([10]; [13]; [12]).

 propose the Divide-and-Conquer algorithm, which divides the compound proposition text into simple propositions and produces their corresponding representations. They also propose a neural logic reasoning approach to solve this problem. Different from this work, we propose a new Divide and Conquer framework, dubbed NDCR, based on the dual-process theory. Our NDCR framework can solve the image retrieval problem from linguistically complex texts.

 propose a multi-task learning framework for image classification and image retrieval. Their framework consists of three main components: 1) an image classification module, 2) a visual-linguistic interactor module, and 3) a logic reasoning module. Their model is trained on the ImageNet dataset, and their model is evaluated on the COCO dataset. Our work differs from theirs in three aspects. First, our model is pretrained on ImageNet, and our model can be applied to any image retrieval task. Second, our approach is an end to end neural-logical reasoning approach. Third, our method can handle more complex tasks than their model.

 first propose the Knowledge Graph Model (KGM) for knowledge graph reasoning. It is a graph neural network (GNN) based model that learns a graph embedding for each node in the knowledge graph, and then uses the embeddings of the nodes in the graph to perform knowledge graph inference. In this paper, we adopt the KGM model as the backbone of our model, and use it to perform the logical reasoning in our proposed Divide and Conquer framework.

 is a GNN-based model for image captioning. It"," **Pretrained Vision-Language Models for Cross Modal Matching.** Owing to the success of Transformer ([1]) architecture equipped with pretrain-finetuning () learning method, pretrained VLMs have made a remarkable performance in cross-modal matching or reasoning tasks (), especially image-text retrieval. Early pretrained VLMs utilize BERT ([4])-like single encoder architecture to encode and fuse the image-text information, then perform image-text reasoning such as ViLBERT ([3]), VisualBERT ([5]), and Oscar (). In addition, dual-encoder architecture such as CLIP ([2]), and ALBERT ([6]), performs better than single-encoder architecture on image-text matching tasks and is widely used in industry because of its efficiency.

**Divide-and-Conquer for Question Answering.** The divide-and-conquer algorithm ()aims to divide the complex problem into multiple simple problems and then combine the subproblem results to achieve the final solution. This idea has been used in complex question-answering tasks in the natural language processing area. [9] proposed to utilize the decomposition of complex questions for semantic parsing. [8] adopt the question decomposition and rescoring method to perform multi-hop reading comprehension, which makes the reasoning path interpretable and robust. [7] utilized the QDMR structures of complex questions to conduct the decompose-synthesize text-to-SQL transformation. Previous pipeline approaches may lead to error cascades in the upper inference process due to the incompleteness or error of decomposed text. The image-text retrieval task has strict requirements on the correctness of text semantic understanding, thus we propose an end-to-end divide-and-conquer method for alleviating the error cascade issue via the whole learning process.

**Dual-Process Theory.** The dual-process theory shows that human brains have two different thinking Systems. System 1 performs analogical reasoning, and System 2 performs conscious logical reasoning. Combining this theory with practical tasks, some researchers designed various approaches. [13] believed that combining vector space models with external knowledge graphs could be regarded as thinking 'fast' in vector space along with thinking'slow' and 'deeply' by reasoning over the knowledge graph. [10] also proposed to use a deep learning network with a tree search engine as System 1 and System 2, respectively, for sequential decision-making problems. [12];  advocated the design of a conscious network to achieve the leap from System 1 to System 2. [11] designed a neural-symbolic system for natural language understanding tasks, which combines the explicit symbolic calculation-based System 2 and fast deep learning network-based System 1. For complex multi-modal reasoning problem, e.g., image retrieval from linguistically complex text, humans usually combine System 1 and System 2 to obtain the final solution. However, current methods relying mainly on deep learning networks resemble System 1 and lack the logical reasoning capability, thus suffering from image-text reasoning with the complex description. In this light, we make the first attempt to combine System 1 and System 2 to tackle this issue by designing a neural divide-and-conquer reasoning framework. We introduce a neural-symbolic reasoner in System 2 to conduct the logical operation. The overall framework contains analogical and logical reasoning as humans think, making appreciable gains.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]"," Vision-Language Models (VLMs).VLMs have achieved remarkable success in learning visual-linguistic representations [1][2][3][4][5][6] by pre-training on large-scale image-text pairs. They are based on the Transformer [1] architecture, which is based on a self-attention mechanism. The Transformer-based VLMs have been widely used in various vision-language tasks, such as VQA [2], image captioning [3][5], image retrieval [6] and image caption generation [2]. They are trained on a dataset of 400 million (image, text) pairs collected from the internet, and achieve remarkable performance on a wide range of vision and language tasks. For example, BERT [4] is a pre-trained language model that is designed to learn bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.8 after training for 3.5 days on eight GPUs.


Recently, several works [2][6][3] have extended the popular BERT architecture to a multi-stream model, which can learn task-agnostic visual and textual joint representations. For instance, CLIP [2] and VisualBERT [5] are two popular VLM-based models that are trained with the caption prediction task. They first pre-train the model on a large scale image caption dataset, and then fine-tune it on a downstream vision-text task. In this paper, we propose a novel Divide-and-Conquer reasoning framework for image retrieval from linguistically complex texts.


Question Decomposition.Question decomposition has been widely studied in natural language understanding [7][8][9][10][11][12][13]. It has been shown that the decomposition of complex questions into simpler sub-questions can significantly improve the performance of QA models [8][7][9]. For example in multi-hop reading comprehension, [8] decomposes a compositional question into a span prediction problem and uses it to generate sub-question that can be answered by single-hop RC models. [7] propose a weakly supervised QD approach for text-to-SQL parsing, where they train a QD model to synthesize the meaning of natural language utterances with"," Although traditional unsupervised ASR (; [6]; ) focuses on building acoustic models by exploring audio features, recent developments in neural language models have led to the unsupervised ASR (US-ASR) (; [1]; [2]; [3]; [4]; [5]). Specifically, [1] propose to classify phonemes by the harmonized HMM mapping with adversarial training. More recently,  propose to classify phonemes by mapping to an unsupervised representation space using GANs. Furthermore, recent approaches ([3]; [5]; [4]) focus on language modeling based on attention to text features, resulting in considerable improvements in speech recognition accuracy.

Various studies evaluate the robustness of US-ASR methods ([7]; [8]). For instance, [7] propose to train a US-ASR system by fine-tuning on the matching word segmentation mask and phoneme boundaries in unsupervised ASR and semi-supervised ASR systems. While they also show that the algorithm has a good training stability, the existence of local minima is still not completely clear. Another study by [8] reveals that US-ASR models suffer from the problem of domain mismatch between speech and text domains, which may easily lead to errors in the ASR task. In order to achieve a stable and accurate system, they propose a multi-task learning framework for the two domains. However, it still remains a question whether the robustness can be improved with more ASR methods ().

Early theoretical studies on GANs, specifically on the existence of equilibria and stability ([13]; ; ; [11]; [16]; [14]; [12]; [15]; [10]), have provided strong theoretical foundations for GANs. They mainly focus on solving the implicitness issue of the loss and the selection of specifications. For example, MMD-GAN was designed to improve the convergence of the Wasserstein distance loss ([16]), and improved discriminator ensures mode stability in a GAN trained on CIFAR-10 ([11]; [15]). In contrast, we take another perspective to derive a condition for convergence by characterizing the mapping properties of the network.

Most of the existing US-ASR methods mainly focus on the training algorithm (such as multi-task learning and adversarial training), model design, and hardware optimization ([3]; [4]; [1]; ; [5]; [8]). There have been a few theoretical analyses (e.g., ([5])), but they mainly focus on addressing non-convergence issues. Contrary to this, we propose to theoretically study the sample complexity and training stability of US-ASR, which has not yet been studied in previous works.

Recently, theoretical studies of GANs have attracted much attention. [17] proposed a saddle-point criterion for non-concave problems with multiple",,
5504,5504," **Vision-Language Models.** VLMs [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16] have been widely used in natural language processing (NLP) and computer vision [17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34]. In this work, we focus on VLM pre-training and fine-tuning.

**Multi-Task Learning.** Multi-task learning (MTL) [35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51] aims to learn a model that can perform well on multiple tasks simultaneously. In this paper, we mainly focus on adapting a pre-trained VLM to multiple downstream tasks.

 proposed a multi-task framework for image captioning, which learns a shared embedding space for image and text embeddings. However, it requires a large amount of training data for each task, which is expensive and time-consuming to collect. [35] proposed a co-training framework for MTL, where a model is trained on a set of tasks and then adapted to a new task with a small number of gradient updates. [46] proposed to learn task-shared representations by minimizing task-specific loss functions. [49] introduced a gradient reversal loss to mitigate the gradient vanishing/exploding problem. [51] proposed task-agnostic loss functions to reduce the influence of gradient vanishing and exploding problems. [44] proposed an adaptive loss function for multi-label MTL. [47] presented a novel loss function to alleviate the problem of mode collapse.

 presented a method for learning task-invariant and task-discriminative representations for image-text matching.  proposed a framework for learning multi-modal representations for multi tasks.  presented a framework to jointly learn representations for vision and language tasks. In contrast to these methods, our method learns task-aware representations for different tasks and leverages the information from different tasks to improve the performance of each task.

 introduced a method to jointly train a language model and a VLM. Their method is based on a two-stage training procedure, where the language model is first pre-train on a large-scale dataset and then fine-tune the VLM on a smaller dataset. Our method differs from theirs in that we propose a hierarchical task tree structure for learning multiple tasks. Moreover, we propose to learn multiple tasks with a unified prompt learning framework, which can be applied to a wide range of vision-language tasks. To the best of our knowledge, we are the first to propose a method that jointly learns multiple tasks in a MTL framework.

 and  proposed an MTL method for object detection. They proposed to jointly optimize a model for object classification and object detection tasks. Our work differs from them in two aspects: (1) Our method learns a unified"," Vision-Language Models (VLMs).Foundation models (e.g., GPT-3 [1], PaLM [2], and Florence [3]) trained on massive data show a surprising ability on many applications. In computer vision, milestone works, i.e., CLIP [4] and ALIGN [5], which learn the aligned embedding space of text and images via contrastive learning, demonstrate surprising transferability on downstream tasks. They inspire many researchers to explore better vision-language pre-training [6][7][8][9][10][3]. To this day, CLIP, trained on 400 million image-text pairs, is still one of the best VLM released publicly. VLMs also show great potential to address various visual tasks with the language prior, including detection [11][12][13], segmentation [14][15][16], and recognition [17][18][19].

Prompt Learning.Prompt learning is initially proposed for adapting the large pre-trained language models in natural language processing (NLP) [1][20]. Since various NLP tasks can be unified as the ""_text-to-text_"" problem [21], the specialized prompt is applied to guide the language model to answer the corresponding question [1][22][23]. However, manual crafting of prompts is difficult and often sub-optimal. Recently, automatic prompt generation [24][25][26][27][28] has emerged as a promising way to adapt language models effectively.

In computer vision, the pioneering work, Context Optimization (CoOp) , employs prompt learning to generate an appropriate prompt closer to the task context for improving the recognition of VLMs. Due to its simplicity and effectiveness, many works extend CoOp and apply prompt learning to board vision tasks [29][30][17][31][32][33][34]. Despite various progressions of existing works, adapting VLMs to multi-task learning with prompting is still an under-explored problem. In addition, although Conditional CoOp [33] also discusses the poor generalization of the task-individual prompt on _unseen classes_, it does not obtain better _in-distribution_ generalization, even worse than CoOp. Our HiPro demonstrates that training prompts with data from multiple tasks can effectively improve the in-distribution generalization of prompt learning. The most related work [35] is leveraging prompt learning for multiple perception tasks in autonomous driving scenarios. However, it has many specialized designing for autonomous driving, which is difficult to extend to other multi-task learning settings.

Multi-Task Learning.Multi-task learning (MTL) aims to improve the average performance of multiple target tasks from training together. Common methods design strategies or structures to share information across tasks, including hard sharing [36], soft sharing [37][38][39], and learnable sharing [40][41][42][43]. However, training different tasks on a shared model raises the difficulty of optimization and could lead to a negative transfer. Several works attempt to identify the suitable combination of tasks that can benefit from training together, also referred to as _task grouping_[44][45][46]. Other popular methods [47][48][49][50][51] aim to improve the optimization dynamics of MTL, e.g., modifying the gradient direction for mitigating conflict [51]. Despite significant progress, the exploration of MTL based on the modern large-scale VLM is still limited, which is an important step for developing the in-the-wild vision system. In addition, our method, guiding the frozen VLM to address various tasks with the lightweight prompt, is an efficient multi-task learner. We also compare HiPro with advanced MTL methods based on their variants of prompt learning. HiPro demonstrates clear improvements compared with MTL baselines.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]"," **Vision-language models.** Pre-training language models on large-scale text corpora [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53].

**Prompt learning.** Prompt learning [20][24] has been widely studied in the NLP community [22][20], where the model is trained to fill in the blanks of prompts such as ""Obama is a __ by profession"" or ""Obama worked as a _doctor_ before becoming president"". The prompt learning paradigm has also been applied to the vision domain [13][13]. For example, CLIP [5] and its variants [12][8] are pre-trained on a large corpus of image-text pairs, and then fine-tuned on downstream tasks such as image classification and semantic segmentation. CLIP has been extended to various vision-language tasks [13], such as object detection [11][19], object segmentation [15], object detection with context-aware prompt learning [16] and semantic image segmentation with multi-task learning [14]. CoCaE [10] is a contrastive captioner for image captioning, which is trained with contrastive loss and captioning loss. CoCa [10], a minimalist design for image-language pre-training, is a minimalist model that combines contrastive and caption loss.

VPT [17] is an efficient and effective method for prompt learning for large-capacity VLMs in vision, which only updates a small portion of model parameters in the input space while keeping the model backbone frozen. VPT is inspired by prompt learning in NLP [23][20] and has been used for zero-shot transfer learning [18][20]. VPT only updates only a small amount of model parameter in the parameter space, which makes it more efficient and scalable. However, VPT requires a large amount of training data for each task, which limits its adoption in real-world scenarios. In contrast, HiPro learns a task-specific prompt for each downstream task, and"," Multi-view stereo (MVS) algorithms aim to estimate a dense depth map from multiple views of a scene, which is an essential prerequisite of neural implicit surface learning. Classical MVS methods resort to estimating the disparity map across the different views [1] to measure the depth consistency among them. With the advances of deep learning techniques, MVS algorithms become more and more deep-learning-based and tend to either utilize deep priors on the camera pose [2][3], or 3D point clouds [4][5][6] to learn a discriminative feature for the depth map across multiple views. Some methods aim to learn the depth estimation based on one pretrained neural network, without any previous training data. DSM [7] learns a ""cost aggregation module"" to estimate the depth map based on single view alone. POSM  proposes to learn the plane consistency across multiple views.

Another line of research focuses on surface reconstruction. An object-level volumetric surface reconstruction is firstly proposed by poisson surface reconstruction [8]. As a common procedure, DPSNet [9] uses oriented points [10] to propose the framework of Pixelwise ViewSelection. More recently, some research [11][12] learn the patch match cost volume for MVS in a supervised manner. However, the accuracy of these supervised methods is relatively limited, and some other research [13][14] performs the MVS learning in an unsupervised manner. In this paper, we aim to reconstruct indoor scenes with non-uniform textures and non-rigid structure. Therefore, deep priors on surface geometry and textures are important.

Neural implicit surface learning is a representative technique for object-level surface reconstruction with deep priors. The recent progress of neural implicit surface learning can be roughly categorized into two families. One family learns the depth maps, either directly [15][16][17] or by density estimation [18][19]. These methods are unsupervised and form the baseline for the modern methods. In addition, the other family of researches [20][21][22][23][24][25] learn neural networks that directly model the geometry of object-level surfaces. These methods can be considered as a neural radiance field. Compared with previous research in neural radiance fields [26][27][28][29][30], these methods leverage more geometric priors on 3D shapes such as normals and planes. Differently, we introduce the deep prior of structure regularization for improving the quality of surface reconstruction, by solving a co-adaptive optimization between neural implicit surface learning and MVS.

",,"<>
In recent years, multi-task learning (MTL) has gained significant attention as a powerful approach to training a single model to perform multiple related tasks simultaneously [37]. By leveraging shared representations and learning to perform multiple tasks in parallel, MTL offers advantages in terms of computational efficiency and generalization. However, one of the primary challenges in MTL is managing task-relatedness and avoiding negative transferring between tasks [44]. A key consideration in MTL is determining which tasks should be learned together and how to balance the learning process to ensure that each task receives sufficient attention [46] and [49]. Conflict-averse gradient descent, proposed by Chaudhry et al., addresses the issue of conflicting gradients in MTL by dynamically tuning gradient magnitudes [48].

In the context of vision-language models, hierarchical prompt learning has emerged as an effective method for jointly adapting a pre-trained vision-language model to multiple downstream tasks [30]. This approach leverages the quantification of inter-task affinity to construct a hierarchical task tree and subsequently learn task-shared prompts at internal nodes and task-individual prompts at leaf nodes [25]. The concept of prompt-aligned gradient for prompt tuning, as introduced in recent work, has focused on optimizing the fine-tuning of prompts for vision-language models to ensure effective knowledge transfer to downstream tasks [34].

Furthermore, the taskonomy approach, proposed by Zamir et al., aims to disentangle the structure of visual tasks and understand their inter-relationships, offering a principled approach to transfer learning and identifying redundancies across tasks [45]. Another interesting direction in MTL includes the exploration of controllable multi-task networks, as presented by Baldi et al., where the architecture and weights dynamically adjust to match desired task preferences and resource constraints, offering adaptable multi-task learning capabilities [42]. Finally, research on imparting impartiality to multi-task learning aims to address the partial training issue and ensure a fair distribution of training resources among multiple tasks [49].

Overall, the research in MTL for vision-language models has expanded to encompass a diverse range of techniques and methodologies, aiming to address the challenges of task-relatedness, gradient conflicts, and optimal task grouping, providing new insights and approaches in the field [37] and [45]. The continuous development of these methods and the exploration of dynamic and adaptable MTL architectures offer promising avenues for advancing multi-task learning in vision-language models."
3978,3978," MIL for WSI classification. Current MIL methods can be roughly divided into two categories: attention-based and contrastive-based methods.

Attention-based MIL methods [1][2][3][4][5][6][7][8][9][10][11] focus on identifying salient instances via attention mechanisms. For example, Zhang _et al._[1] proposed a dual-stream MIL framework with self-supervised contrastive learning to improve the discriminative power of the MIL model. In [2], a transformer based MIL model was proposed to learn the correlation between the positive and negative samples. In, a multi-task learning framework was proposed with a Siamese MIL model to learn both the instance embedding and the classifier simultaneously. Different from these methods, our proposed MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention- or contrastive MIL model, and thus enables the student to learn a better discriminating boundary more accurately. In addition, the student is updated with an exponential moving average (EMA) to update the teacher with a consistency constraint to explore the potential hard instances, which has not been explored in existing MIL methods. In contrast, the proposed method mines hard instances at the instance level, which is more efficient and effective than the existing attention methods. Besides, our method can also be applied to other MIL tasks, _e.g._, medical image segmentation and whole slide image classification, _etc_.

Hard example mining (HEM) has been widely studied in computer vision tasks. Hard example mining has been applied to various tasks, such as face recognition [12], object detection [13][14], person re-identification [15][16][17][18] and metric learning [19][20][21][22][23]. In [15], a triplet loss [15] was proposed for hard example mining in person ReID, which was later extended to a quadruplet loss [22] and a triple loss [23] to further improve the performance. In face recognition, a cross-batch HEM method was proposed in [18] to mine hard examples for ID vs spot face recognition. In object detection, an online HEM [13] method was introduced to mine the hard examples in the form of hard negative samples for training a region-based object detector. In the field of metric learning, a variety of loss functions have been proposed for mining hard examples [20][19][17]. In this paper, we propose a novel HEM framework with a teacher-student framework, which uses a momentum-based HEM strategy to mine and update the hard instances in the MIL framework.

 proposed a method for mining the hard samples in the context of image classification. However, this method is designed for image classification and does not consider the MIL problem. In this work, we apply HEM to the MIL task, and propose a new HEM-based training strategy for MIL.

 introduced a novel MIL framework for medical image analysis. This method uses a two-stage training strategy, _i."," Multiple Instance Learning (MIL)  has been widely used in WSI analysis with its unique learning paradigm in recent years [1][2][3]. MIL is a weakly supervised learning framework that utilizes coarse-grained bag labels for training instead of fine-grained instance annotations. Previous algorithms can be broadly categorized into two groups: instance-level [4][5][3] and embedding-level [6][7][8][9]. The former obtain instance labels and aggregate them to obtain the bag label, whereas the latter aggregate all instance features into a high-level bag embedding for bag prediction. Most embedding-level methods share the basic idea of AB-MIL [10], which employs learnable weights to aggregate salient instance features into bag embedding. Furthermore, some MIL frameworks [1][3][9] mine more salient instances making classification easier and facilitating classification. For example, Lu _et al_. selected the most salient instances based on their attention scores (e.g., maximum and minimum scores) to compute instance-level loss and improve performance . Zhang _et al_. proposed a class activation map (CAM) based on the AB-MIL paradigm to better mine salient instances and used AB-MIL to aggregate them into bag embedding [9]. In addition, feature clustering methods [7][11] computed cluster centroids of all feature embeddings and used representative embeddings for the final prediction. However, all these methods focused excessively on salient instances in training, which are easy instances with high confidence scores and can be easily classified. As a result, they overlook the importance of hard instances for training. In this paper, we intend to mine hard instances for improving WSI classification performance.

Hard sample mining is a popular technique to speed up convergence and enhance the discriminative power of the model in many deep learning areas, such as face recognition [12], object detection [13][14], person re-identification [15][16][17][18], and deep metric learning [19][20]. The main idea behind this technique is to select the samples which are hard to classify correctly (i.e., hard negatives and hard positives) for alleviating the imbalance between positive and negative samples and facilitating model training. There are generally three groups of approachesfor evaluating sample difficulty: loss-based [21], similarity-based [22], and learnable weight-based [23]. Typically, these strategies require complete sample supervision information. Drawing on the ideas of the above works, we propose a hard instance mining approach in MIL, mining hard examples at the instance level. In this, there are no complete instance labels, only the bag label is available. Similar to our approach, Li _et al_. utilized attention scores to identify salient instances from false negative bags to serve as hard negative instances and used them to compose the hard bags for improving classification performance . A key difference is that we indirectly mine hard instances by masking out the most salient instances rather than directly locating hard negative instances.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"," **MIL for WSI classification.** MIL has been widely used in WSI image classification [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23]. The MIL methods can be divided into two categories: single-label MIL methods and multi-label learning methods. Single-label based MIL methods [3][1][4] treat the WSI as a bag of instances, and assign each instance with a single class label. The MIL aggregator is usually based on the independent and identical distribution hypothesis, thus neglecting the correlation among different instances. To address this problem, some methods [2][5] propose to model the relations of the instances in a dual-stream architecture with trainable distance measurement. However, these methods are usually trained on the positive instances only, which leads to a bias towards easy-to-classify instances while neglecting hard instances.

**Multi-instance learning (MIL).** MIL methods for WSIs have been widely studied in recent years [4][8]. The attention-based MIL methods have been proposed to explore the instance-level information. The attention mechanism is firstly proposed in [10] to learn the Bernoulli distribution of the bag label, and then the instance probability distribution is parameterized by neural networks. [8] proposes to use the attention mechanism to learn instance-label relation. [4] proposes a deep MIL model with a sub-concept learning component. [7] proposes an adaptive attention mechanism for the whole slide image classification. [11] proposes the first MIL method for WSHI classification. The main difference between our method and the existing MIL methods is that our method uses a Siamese structure with a consistency constraint to explore hard instances for training the student model, which can be any MIL model. [6] proposes dual attention multiple instance learning (DA-CMIL) for the classification of COVID-19 and bacterial pneumonia based on MIL. [9] proposes DTFD-MIL to enlarge the number of bags by introducing the concept of pseudo-bags, and a double-tier MIL framework is built to effectively use the intrinsic features. [2] proposes TransMIL, which explores both morphological and spatial information for WPI classification. However these MIL methods focus on identifying salient instances via attention mechanisms, which is not suitable for the instance level."," The first line of research on WSI classification proposed traditional MIL frameworks. Xue _et al_.  introduced a Gaussian mixture class model (GMCM) for WSI classification, which was further improved in  by using neural networks. Eisemann _et al_. [1] designed a Maximum Mean Discrepancy (MMD) based framework for WSI classification. However, most WSI datasets are scarce and hard to annotate, which significantly limits the performance of traditional MIL methods.

With the emergence of CNN, more CNN based WSI classification methods were proposed. Xu _et al_.  introduced a CNN based MIL framework with a diffusion confusion operator and an aggregation module. Qin _et al_. [2] proposed a Correlated MIL framework for WSI classification, which showed promising results. However, these CNN based MIL methods do not sufficiently exploit the high-dimensional semantic information, resulting in poor performance on high resolution WSIs. Recently, some works proposed to extract semantic features from WSIs by attending to instance patches [3][4][5][6]. Nevertheless, they are limited in capacity to generate the attentive maps, which results in inconsistent attention maps. With the introduction of Siamese networks, Ettedgui _et al_. [7] proposed a state-of-the-art method with attention-based MIL for WSI classification. Fan _et al_. [8] used attention-based networks to propose a framework for CT hemorrhage detection. However, these attention-based MIL methods do not mine hard examples, which is crucial for model training and may significantly impact on performance.

In contrast to the CNN based MIL methods, some works tried to learn instance features via instance classification [9][10][11]. However, these works neglected the attention mechanism to capture instance-level semantics, leading to poor performance on high resolution WSIs. Some literature tried to improve the performance by using attention-based network. Martinez _et al_. [10] used attention-based networks to model instance label as a Bernoulli distribution. Ma _et al_.  trained the model using the sampling method, and proposed a temperature-scaled attention mechanism. Fan _et al_. [11] proposed to explicitly learn the attention weights by a score estimation module. In this paper, we introduced an attention-based MIL framework with hard example mining. By implicitly mining hard examples, our method has the potential to significantly improve model performance.

To effectively mine hard examples, various literature tried to use different strategies to identify hard examples. In the field of object detection [12][13], hard examples are usually mined in a mini-batch, and the selected samples are stored in a queue for subsequent iterations. However, the selected samples can be filtered out due to poor quality. In contrast, most literature [14][15][16][17][18] tried to mine hard examples glob",,"<>
In recent years, the application of multiple instance learning (MIL) techniques in the field of whole slide image (WSI) classification has gained significant attention [1]. MIL approaches have been proposed to address the challenges posed by the large-scale and unbalanced nature of WSI datasets, where only slide-level labels are available. One such approach, the TransMIL framework, incorporates a Transformer-based MIL model to leverage both morphological and spatial information for effective WSI classification [2]. Similarly, the proposed MHIM-MIL framework in this paper introduces a Siamese structure with masked hard instance mining to explore the potential hard instances for improved WSI classification [1].

Another related area of research in the context of histopathology image analysis is the weakly supervised learning framework for histopathology image segmentation proposed by CAMEL [3]. By using a multiple instance learning (MIL)-based label enrichment, CAMEL achieves pixel-level segmentation using only image-level labels, showcasing the potential of MIL in addressing challenges in histopathology image analysis. Additionally, the DeepMIML network proposed in [4] demonstrates the effectiveness of deep neural network formation in generating instance representations for multi-instance multi-label (MIML) learning. These examples highlight the relevance of MIL and related techniques in addressing challenges in histopathology image analysis and WSI classification.

Furthermore, the application of MIL techniques is also evident in other medical imaging domains, such as in the case of CT-based analysis for COVID-19 screening [6]. The proposed DA-CMIL framework leverages attention-based multiple instance learning for the rapid diagnosis of COVID-19 and bacterial pneumonia, showcasing the versatility of MIL in the context of medical image analysis. Additionally, the study on intracranial hemorrhage (ICH) detection by [8] utilizes an attention-based multiple instance learning approach to train the model using scan-level labels and effectively improve predictions at both slice and scan levels. These instances reflect the relevance of MIL in addressing various challenges in medical imaging and diagnosis.

In the field of object detection, the study on training region-based object detectors with online hard example mining [13] presents a method that effectively utilizes hard example mining to improve detection performance. The proposed online hard example mining (OHEM) algorithm eliminates several heuristics and hyperparameters, leading to consistent and significant boosts in detection performance. Additionally, the study by [14] introduces a principled Self-supervised Sample Mining (SSM) process for object detection, addressing challenges related to the effective use of available image-level labels in object detection tasks. These studies provide insights into the significance of hard example mining in improving object detection performance.

The development of deep metric learning methods has also been a topic of interest, as evident from research on improved deep metric learning with the multi-class N-pair loss objective [19]. The proposed multi-class N-pair loss objective generalizes the triplet loss and reduces computational burden, achieving superior performance across a variety of visual recognition tasks. Moreover, the study on stochastic class-based hard example mining for deep metric learning [20] addresses the challenge of identifying hard negative examples in large-scale datasets, showcasing the significance of hard example mining in improving the performance of deep metric learning methods.

In the field of person re-identification, research on the deep quadruplet network [22] introduces a quadruplet loss, surpassing the limitations of the triplet loss and achieving improved generalization ability and performance. Additionally, the study on learning with batch-wise optimal transport loss for 3D shape recognition [23] introduces a new batch-wise optimal transport loss, accelerating convergence rate and achieving state-of-the-art recognition performance across benchmark datasets. These examples demonstrate the relevance of hard example mining and metric learning techniques in addressing challenges in visual recognition and person re-identification."
3814,3814," Dynamic Contextualised Word Embeddings (DCWEs) have been widely used in many NLP tasks such as semantic change detection ([5]) and time-aware language modelling ([3]; [2]; [4]). DCWEs represent the temporal semantic variations of a word in the context of a given context. However, most existing DCWE models are static and cannot adapt to changes over time. To address this issue, [2] proposed a dynamic language model called TempoBERT, which uses time as an additional context of texts. It learns to predict the future timestamps of a sentence based on the current context. [3] proposed to use a temporal knowledge base (TKB) to learn the temporal knowledge of a corpus. They use the TKB as a knowledge base to perform semantic changes detection and sentence time prediction tasks.  proposed a method for adapting a pretrained language model to a new domain by using a set of time-sensitive templates. They first generate templates for the new domain and then fine-tune the model using the templates. [1] proposed an example-based autoregressive prompt learning algorithm for on-the-fly any-domain adaptation, which learns to generate the appropriate prompts for a given task.

 proposed a template-based learning method for domain adaptation. They generate the templates for a new task based on a pre-defined set of templates and then use the templates to adapt the model to the task of interest. They also proposed an unsupervised method for learning the templates and fine-tuning the model on the task at hand.



\begin{tabular}{l c l c c c l l l c l \hline \l c c \langle c c t c c hline \r l \lline \hine \ledge{c l l \rangle{c}{l l l r l \cline}{c c l r c c } \hge{c} \hedge{hline}{c l c}{l r l c \hinge{c c}{c}{cline{l r c}{hge}{c hline } \ \redge{l l c  \hangle{hinge {c l}{c } \lge{l c}{ c c}{r l c} \tilde{hine{hangle}{c} & c c_{l c}\langle {c}{hline{hange{l}{c}} & c_{r c} \\ \hle{hale{l} &c}{c_{lc} \\ hle{l}\rangle {l}{l}{hangle}  \tangle{l,l,c}{r c}{ l}{hle {l} } \hlangle{r}{c}\cline {l,r}{l} c_{c}\\ \halle{l }{c }{halle {l l}{l}\hline"," Methods that use part-of-speech , entropy , latent semantic analysis  and temporal semantic indexing  have been proposed for detecting changes in word meanings. In SemEval-2020 Task 1  two subtasks were proposed for detecting lexical semantic change: a binary classification task (for a given set of target words, decide which words had their meaning altered, and which ones not) and a ranking task (rank a set of target words according to their degree of lexical semantic change between the two corpora). Giulianelli et al. giulianelli2020supervised showed that contextualised embeddings obtained from an MLM can be used to measure the change of word meaning. Rosin and Radinsky rosin2022supervised proposed a temporal attention mechanism by extending the self-attention mechanism in transformers, where time stamps of the documents are considered when computing the attention scores. Aida and Bollegala aida2023temporal proposed a method to predict semantic change of words by comparing the distributions of contextualised embeddings of the word between two given corpora, sampled at different points in time. Our goal in this paper extends beyond the detection of a subset of words with a change in lexical semantics, and to adapt MLMs over time.

DWEs [15, 14, 16] incorporate extralinguistic information such as time, demographic or social aspects of words with linguistic information. Welch et al. welch2020context learnt demographic word embeddings, covering attributes such as age, gender, location and religion. Zeng et al. zeng2017socialized learnt _socialised_ word embeddings considering both a social media user's personal characteristics of language use and that user's social relationships. However, Hofmann et al. hofmann2021supervised showed that temporal factors have a stronger impact than socio-cultural factors when determining the semantic variations of words. Consequently, in this paper we focus on the temporal adaptation of DCWEs.

Diachronic Language Models that capture the meanings of words at a particular point in time have been trained using historical corpora [14, 15]. These prior work learn independent word embedding models from different corpora. This is problematic because information related to a word is not shared across different models resulting in inefficient learning, especially when word occurrences within a single snapshot of a corpus are too sparse to learn accurate embeddings.

Rudolph and Blei rudolph2018supervised proposed a dynamic Bernoulli embedding method based on exponential family embeddings, where each word is represented by a one-hot vector with dimensionality set to the vocabulary size. This model is extended to the temporal case by considering different time-slices where only the word embedding vector is time-specific and the context vectors are shared across the corpus and over time-slices. Because the joint distribution over time and context is intractable, they maximise the pseudo log-likelihoodof the conditional distribution for learning the parameters of their DWE model. [1] proposed a domain adaptation method based on automatically learnt prompts. Given a test example, they generate a unique prompt and conditioned on it, then predict labels for test examples. Although their method uses prompts to adapt a model, they do _not_ consider temporal adaptation of MLMs, which is our focus. Moreover, we do not require any labelled examples in our proposal.

Amba [2] proposed a model updating method using vocabulary composition and data sampling to adapt language models to continuously evolving web content. However, their work is specific to one dataset and two classification tasks, and focuses on incremental training.  introduced a benchmark for ever-evolving language models, utilising the difference between consecutive snapshots of datasets, to track language models' ability to retain existing knowledge while incorporating new knowledge at each time point.  studied the lifelong language model pretraining problem, where the goal is to continually update pretrained language models using emerging data. [3] introduced a diagnostic dataset to investigate language models for factual knowledge that changes over time and proposed an approach to jointly model texts with their timestamps. They also demonstrated that models trained with temporal context can be adapted to new data without retraining from scratch. [4] proposed TempoBERT, where they insert a special time-related token to each sentence and fine-tune BERT using a customised time masking. TempoBERT reports superior results in  Task 1 semantic variation detection benchmark. As shown later in SS4.1, our proposed method outperforms TempoBERT.

[5] proposed DCWEs, which are computed in two stages. First, words are mapped to dynamic type-level representations considering temporal and social information. The type-level representation of a word is formed by combining a non-dynamic embedding of a word and a dynamic offset that is specific to the social and temporal aspects of the word. Second, these dynamic embeddings are converted to context-dependent token-level representations. To the best of our knowledge, this is the only word embedding method that produces both dynamic as well as contextualised representations, thus mostly relates to us. As shown in SS4, our proposed method outperforms their DCWEs on four datasets.

","[1, 2, 3, 4, 5]"," **Temporal adaptation.** Temporal adaptation has been studied in the context of domain adaptation ([2]; [1]) and language model adaptation ([4]; [3]). [2] proposed a method for adapting a pretrained language model to a new domain by incrementally evolving the model from a base year. [3] proposed to use a pretrainable temporal language model as a knowledge base to mitigate the changes in factual facts from future time periods. [4] proposed an unsupervised method for using time-sensitive templates to adapt a pretraining language model from \(C_{1}\) to \(T_{2}\), without requiring any human supervision. [1] proposed PADA, an example-based autoregressive prompt learning algorithm for on-the-fly adaptation to unseen domains.

**Temporal contextualised word embeddings.** [5] proposed DCWEs, which represent the temporal semantic variations of words in different linguistic and extralinguistic contexts. However, they do not consider the case where the context is taken at two distinct timestamps \(T\) and \(T\), as we do in this work.

"," Few-shot NERThis work is closely related to few-shot NER, which aims to generalize model parameters from rich-resource source domains to unseen novel target domains. A typical few-shot NER system is usually consisted of three modules, i.e., a few-shot model trained on source domains, a feature extractor and a classifier. Most previous few-shot NER approaches [11]; [1]; [2]; ; [10]; [9]; [7]; ; [8] can be mainly classified into three categories: transfer learning, prototypical networks and memory-based networks.

Transfer learningTransfer learning is typically to learn model parameters from source domains, and then directly apply them to target domains. Representative approaches [1]; [2]; [9]; [6]; ;  optimize label dependencies in few-shot NER by designing a novel label space. [12]; [13] propose to optimize the contrastive distance of input text tokens in a few-shot scenario. However, transfer learning is restricted by the lack of high-quality annotations on target domains.

Prototypical networksA prototypical network usually extracts features from training samples, and classifies new samples by measuring the similarity of their features with the features of representative samples [10]. The classifier can be trained with a class-specific loss or a cross-entropy loss, while the feature extractor can be optimized through multi-task learning. The metapath is commonly used to alleviate the class imbalance and optimize entity types. Variants of prototypical networks have been developed for few-shot NER by adjusting the model architecture [7]; [1]; [10]; [11]; [12]; [9] and introducing other neural networks [2]; [4]; [3]. In addition, some works ([9]; [2]) propose a new formulation of prototypical networks for few-shot NER. However, the optimization objective of prototypical networks is different from that of transfer learning. It is challenging for these two approaches to be combined.

Memory-based networksMemory-based networks store rich-resource annotation information on target domains to train a classifier for few-shot NER. Memory-based networks usually store entity types and entity boundary annotations on target domains as demonstrations. [11]; [1] propose to store text spans and its entity type as demonstrations. [3] utilize multiple span-based demonstrations. Some works ([1]) aim to incorporate prior knowledge into memory-based networks.

Apart from the above-mentioned works, there are also some approaches designed specifically for a novel few-shot NER task, e.g., few-shot named entity disambiguation ([5]) and few-shot relation classification ([5]). They demonstrate that NLP systems can have an impressive in-context learning ability in a few-shot scenario. However, to the best of our knowledge",,
4268,4268," **Domain generalization.** Domain generalization (DG) aims to generalize well across different domains [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][62].

**Optimization for DG.** DG aims to learn a robust model that generalizes well across various domains. Recent DG methods can be divided into three categories. The first category is domain-invariant methods. These methods aim to learn an invariant model that can generalize across domains. The second category is feature-preserving methods. They aim to find a feature extractor that can extract features that are invariant across domains, such that the learned features are discriminative. The third category is ensemble-based methods."," Some works [1][2] have studied the connection between current optimization approaches, such as SGD , Adam [3], AdamW [4] and others [5][6] and in-distribution generalization. Some literature shows that Adam is more vulnerable to sharp minima than SGD [7], which may result in worse generalization [8][9][10]. Some follow-ups [11][12][1][13] propose generalizable optimizers to address this problem. However, the generalization ability and convergence speed is often a trade-off [14][1][6][13][5]. Different optimizers may favor different tasks and network architectures(e.g., SGD is often chosen for ResNet [15] while AdamW [4] for ViTs [16]). Selecting the right optimizer is critical for performance and convergence while the understanding of its relationship to model generalization remains nascent [2]. [17] finds that fine-tuned SGD outperforms Adam on OOD tasks. Yet all the previous works do not consider the effect of optimizers in current DG benchmarks and evaluation protocols. In this paper, we discuss the selection of the default optimizer in DG benchmarks.

Domain generalization (DG) aims to improve the generalization ability to novel domains [18][19]. A common approach is to learn domain-invariant or causal features over multiple source domains [20][21][22][23][24][25][26][27][28][29][30] or aggregating domain-specific modules [31][32]. Some works propose to enlarge the input space by augmentation of training data [33][34][35][36][37][38]. Exploit regularization with meta-learning [20][39] and Invariant Risk Minimization (IRM) framework [40] are also proposed for DG. Recently, several works propose to ensemble model weights for better generalization [41][42][43][44][45][46][47] and achieve outstanding performance.

Recent works study the relationship between flatter minima lead and better generalization on in-distribution data [14][48][49][50]. [51] reviews the literature related to generalization and the sharpness of minima. It highlights the role of maximum Hessian eigenvalue in deciding the sharpness of minima [14][52]. Some simple strategies are proposed to optimize maximum Hessian eigenvalue, such as choosing a large learning rate [53][54][55] and smaller batch size [53][56]. Sharpness-Aware Minimization (SAM) [2] and its variants [48][57][58][59][60][61][62][63], which are representative training algorithm to seek zeroth-order flatness, show outstanding performance on in-distribution generalization. Most recently, several works discuss the relationship between generalization and gradient norm [64][65]. GAM[66] proposes to optimize first-order flatness for better generalization. [41][67] show that flatness minima also lead to superior OOD generalization. However, none of the previous works consider the optimization in DG nor provide theoretical assurance of their approach.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]"," **Adaptive Optimization.** Adaptive optimization methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67].

**Generalization in DG.** Domain generalization (DG) aims to generalize a model trained on a source domain to a target domain with unknown distribution. Existing DG methods can be categorized into two main categories: (1) methods based on domain adaptation [18][17] and (2) methods focusing on domain generalization. Domain adaptation methods [17][19] aim to learn a model that can generalize well to unseen domains. For example, CrossGrad [36] and CrossGrad++ [20] learn a domain-invariant feature extractor by minimizing the cross-entropy loss between source and target domains. Domain adversarial training [17] trains a domain discriminator to distinguish the source domain from the target domain by maximizing the mutual information between the two domains. In contrastive loss [17], domain-adversarial training is used to train a domain classifier that can distinguish between the source and the target domains, and minimize the cross entropy loss between the domains.

Recently, meta-learning-based methods [22][39] have been proposed to improve the generalization performance of DG models. For instance, DAT [22] proposes a novel meta-training procedure that trains a single deep network in a way that exposes it to overconfident domain discriminators and environment label noise. DAT adopts gradient-based meta-train and meta-test procedures to expose the optimization procedure to domain shift. MDA [21] proposes to learn domain invariant feature transformation that aims to achieve appealing properties across different domains. SWAD [41] seeks a flat minima across domains by optimizing the maximum mean discrepancy (MMD) between the training and test distributions. DDAIG [37] proposes an image generation method to generate images"," **Vision Transformer.** The ViT [1] is a Transformer model designed for image recognition, in which images are tokenized into a sequence of patches and then processed by self-attention layers. Thanks to the excellent performance [2][3][4][5][6][7][8] and a more efficient patch-based feature encoding mechanism, Vision Transformers can achieve better results than CNNs on many classification benchmarks. However, the quadratic complexity of the softmax operation makes it difficult to apply vision Transformers to large images, such as high-resolution 1080P images. Some techniques are proposed to ease this computational burden by either eliminating the expensive self-attention [2][9] or designing more lightweight self-attention modules [10].

**Linear Attention.** Due to the lack of constraint on the global context, the most common approach to address the quadratic complexity issue of the softmax operation is to approximate the global matching with local matching. [11] show that the global matching can be replaced by a summation of local matchings. Similarly, [12] use the spatially weighted dot-product attention to approximate the global matching.  utilize the invariance of the global attention to replace the global matching with the pairwise matching. However, the hard constraint and the linear independence of the weights in the approximations of the global matching prohibit the design of complicated operations. More recently, several lightweight self-attention models [13][14][15][16][17][18][19][20][21][22][23][24][25][26] are proposed to address the quadratic complexity of the softmax operation. In this paper, we focus on linear attention that directly approximates the global matching.

Among them, [23][22][13][26] adopt a weight sharing mechanism for different layers. [25] propose to learn the shared weights on the embedding level to improve the performance. But these works are found to have little impact on the performance and are not discussed in this paper. [24] utilizes multiple heads to share the weights among them. However, it requires complex searching algorithms. [21] combines positive and negative sampling with orthogonal projection to approximate softmax function. [20] utilizes the associativity of matrix multiplication to reduce the complexity from \(\mathcal{O}(N^{2})\) to \(\mathcal{O}(N^{2}\log N)\). In this paper, we combine [20] with orthogonal projection to achieve fast linear attention.

",,"<Several recent studies have focused on improving the generalization of deep learning models by exploring the relationship between optimization techniques and loss landscape geometry. One notable line of work has examined the effects of gradient descent algorithms and their impact on achieving flat minima for improved generalization [1] [2] [3]. These studies have revealed that traditional optimization algorithms, such as Stochastic Gradient Descent (SGD) and its variants, can implicitly regularize models by penalizing trajectory paths with large loss gradients, which can bias the optimization towards finding flat minima and enhance generalization [64] [65] [66]. Moreover, the concept of adaptive sharpness-aware minimization has been proposed to address the sensitivity of sharpness measures to parameter re-scaling, aiming to enhance scale-invariant learning [57] [58]. Additionally, Fisher SAM has been introduced to improve efficiency and generalization by utilizing a Fisher mask during optimization [61].>

<Another significant area of research revolves around the connection between loss landscape flatness and generalization, particularly in the context of SAM and related methods [42] [50] [51]. Recent studies have demonstrated that flatness measures, such as the maximum eigenvalue of the Hessian of the loss, can influence the generalization of deep neural networks [51] [52]. However, concerns have been raised about the effectiveness of these measures and their direct impact on generalization [58] [64]. As a result, researchers have been exploring methods to strengthen the relationship between flatness measures and generalization, such as the Sparsified Perturbation Approach and the use of a Fisher mask [60] [61]. In addition, efforts have been made to improve computational efficiency and scalability in sharpness-aware minimization to enhance its applicability to large-scale models and optimize generalization [57] [58] [59].>

<Lastly, research has also explored the impact of smoothness in domain adversarial training on achieving invariant representations and complementary effects on generalization [67]. Understanding the influence of smoothness-enhancing formulations on domain adversarial training and its implications for generalization is an active area of investigation.>

Overall, the recent related work presents a comprehensive exploration of optimization techniques, loss landscape geometry, and their influence on generalization in deep learning models. Efforts have focused on understanding the connection between optimization algorithms, flatness measures, and generalization, while also striving to enhance the computational efficiency and scalability of methods. Additionally, there is ongoing research into the impact of smoothness on domain adversarial training and its implications for generalization."
1008,1008," **Super-Resolution Neural Network (SRN)**. SRCNN [1] and its variants [2][3][4] have achieved great success in the field of image SR. However, these methods are designed to learn the mapping between LR and HR images, and thus cannot be directly applied to the problem of SR with arbitrary scales. To address this issue, SRWarp [5] and SRSRNet [6] propose to learn a mapping between the LR image and the HR image with a multi-scale convolutional neural network. Meta-SR [7] further extends the SRN to handle the scale-arbitrary SR problem by introducing a meta-parameterization module and a multilayer perceptron (MLP). However, both of these methods require the LR-HR image pairs to be drawn from the same space, which limits their applicability to the SR problem.

**Continuous Neural Network**. Recently, implicit neural network (INN) [8][9][10][11][12][13][14] has been proposed to represent an image as a continuous function, which can be learned with a neural network without the need of any explicit mapping from the input to the output space. In particular, LIP [8] proposes a local implicit image function (LIIF) to represent the image in the spatial domain. LIIF learns the local image representation by minimizing the sum of the mean and variance of the feature maps of each layer in the network. LIEF [11] proposes the Fourier feature mapping (FPM) to learn high-frequency functions in the low-dimensional Fourier domain. The Fourier features are defined as a function approximator of the input feature maps, which is defined aswhere \(\mathbf{x}_{i}=\mathbb{R}^{n}(x,y)\) and \(\mathcal{F}_{x}\) is the spectral domain, and \(\bm{y}_{j}\) denotes the spatial dimension of the output feature map. In this paper, we focus on learning the mapping from LR to HR images with the help of the kernel integral mechanism.

 proposes to learn an inverse differential equation between the input and the output. The problem is formulated as:

\[\begin{split}\frac{1}{2}{3}\sum_{i=1}^{N}\frac{\partial_{i}{j}}{\partial\bm{f}_{n}+\frac{2}{n}^{2}^{3}-\frac{\frac{n}{n}{2}}{n}\frac{{1}{n-1}}}{n+1}}\end{split} \] (1)where \(n_{i}\in[0,1]\) and \(n\) are the number of channels in the input image. The \(n\)-th dimension is the input dimension, and the \(n-2\)th dimension denotes the output dimension. The objective function is to minimize the difference between the output and the input. The kernel function"," **Deep learning based SR methods**. [1][2][3][4] have achieved impressive performances, in multi-scale scenarios one has to train and store several models for each scale factor, which is unfeasible when considering time and memory budgets. In recent years, several methods [5][6][7] are proposed to achieve arbitrary-scale SR with a single model, but their performances are limited when dealing with out-of-distribution scaling factors. Inspired by INF, LIIF [8] takes continuous coordinates and latent variables as inputs, and employs an MLP to achieve outstanding performances for both in-distribution and out-of-distribution factors. In contrast, LTE [9] transforms input coordinates into the Fourier domain and uses the dominant frequencies extracted from latent variables to address the spectral bias problem [10][11]. In a nutshell, treating images as RGB-valued functions and sharing the implicit function space are the keys to the success of LIIF-like works [8][9]. Nevertheless, a purely local decoder, like MLP, is not able to accurately approximate arbitrary images, although it is rather sensitive to the input coordinates.

**Neural Operators**. Recently, a novel neural network architecture, Neural Operator (NO), was proposed for discretization invariant solutions of PDEs via infinite-dimensional operator learning [12][13][14]. Neural operators only need to be trained once and are capable of transferring solutions between differently discretized meshes while keeping a fixed approximation error. A valuable merit of NO is that it does not require knowledge of the underlying PDE, which allows us to introduce it by the following abstract form,where \(u:D\rightarrow\mathbb{R}^{d_{a}}\) is the solution function residing in the Banach space \(\mathcal{U}\), and \(\mathbf{L}:\mathcal{A}\rightarrow\mathbf{L}(\mathcal{U};\mathcal{U}^{*})\) is an operator-valued functional that maps the coefficient function \(a\in\mathcal{A}\) of the PDE to \(f\in\mathcal{U}^{*}\), the dual space of \(\mathcal{U}\). As in many cases the inverse operator of \(\mathbf{L}\) even does not exist, NO seeks a feasible operator \(\mathcal{G}:\mathcal{A}\rightarrow\mathcal{U},a\mapsto u\), directly mapping the coefficient to the solution within an acceptable tolerance.

The operator \(\mathcal{G}\) is numerically approximated by training a neural network \(\mathcal{G}_{\theta}:\mathcal{A}\rightarrow\mathcal{U}\), where \(\theta\) are the trainable parameters. Suppose we have \(N\) pairs of observations \(\{a_{j},u_{j}\}_{j=1}^{N}\) where the input functions \(a_{j}\) are sampled from probability measure \(\mu\) compactly supported on \(\mathcal{A}\), and \(u_{j}=\mathcal{G}(a_{j})\) are used as the supervisory output functions. The infinitely dimensional operator learning problem \(\mathcal{G}\leftarrow\mathcal{G}_{\theta}\) thus is associated with the empirical-risk minimization problem . In practice, we actually measure the approximation loss using the sampled observationsand \(a_{o}^{(j)}\), which are the direct results of discretization:

\[\min_{\theta}\mathbb{E}_{a\sim\mu}\|\mathcal{G}(a)-\mathcal{G}_{ \theta}(a)\|_{\mathcal{U}} \tag{2}\] \[\approx\min_{\theta}\frac{1}{N}\sum_{j=1}^{N}\|u_{o}^{(j)}- \mathcal{G}_{\theta}(a_{o}^{(j)}\|_{\mathcal{U}}.\]

Similar to classical feedforward neural networks (FFNs), the NO is of an iterative architecture. For ease of exposition, suppose \(\mathcal{A}\) is defined on the bounded domain \(D\subset\mathbb{R}^{d}\), and the inputs and outputs of the intermediate layers are all vector-valued functions, with dimension \(d_{z}\). Then \(\mathcal{G}_{\theta}:\mathcal{A}\rightarrow\mathcal{U}\) can be formulated as follows:

\[z_{0}(x) =\mathcal{L}(x,a(x)), \tag{3}\] \[z_{t+1}(x) =\sigma(W_{t}z_{t}(x)+(\mathcal{K}_{t}(z_{t};\Phi))(x)),\] (4) \[u(x) =\mathcal{P}(z_{T}(x)), \tag{5}\]where \(\mathcal{L}:\mathbb{R}^{d_{a}+d}\rightarrow\mathbb{R}^{d_{z}}\), and \(\mathcal{P}:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{u}}\) are the local lifting and projection functions respectively, mapping the input \(a\) to its first layer hidden representation \(z_{0}\) and the last layer hidden representation \(z_{T}\) back to the output function \(u\). \(W:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{z}}\) is a point-wise linear transformation, and \(\sigma:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d_{z}}\) is the nonlinear activation function.

Although the PDE in (1) point-wisely defines the behavior of the solution function \(u(x)\), the solution operator \(\mathcal{G}\) we are seeking should exhibit the non-local property such that \(\mathcal{G}(a)\) can approximate \(u\) everywhere rather than locally. For this purpose, NO may employ the kernel integral operators \(\mathcal{K}_{t}:\{z_{t}:D_{t}\rightarrow\mathbb{R}^{d_{z}}\}\mapsto\{z_{t+1}:D_ {t+1}\rightarrow\mathbb{R}^{d_{z}}\}\) to maintain the continuum in the spatial domain, and one of the most adopted forms [13] is defined as

\[(\mathcal{K}_{t}(z_{t};\Phi))(x)=\int_{D}K_{t}(x,y;\Phi)z_{t}(y)\mathrm{d}y, \quad\forall x\in D, \tag{6}\]where the kernel matrix \(K_{t}:\mathbb{R}^{d+d}\rightarrow\mathbb{R}^{d_{z}\times d_{z}}\) is parameterized by \(\Phi\).

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"," **Continuous SR.** Continuous SR methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] have been extensively studied in recent years. These methods can be categorized into two groups: interpolation-based methods and optimization-based ones. The interpolation based methods [2][4] interpolate the LR image into the HR space with a bicubic interpolation layer before the SR operation. For example, EDSR [1] and MDSR  interpolates the LR input into a higher-resolution (HR) space by applying a single filter, commonly bicube interpolation, before the upscaling operation. EDSSR [1], for example, upsamples the LR images by a residual block and uses the residual blocks to learn the mapping between the LR and HR image spaces. MDSSR  proposes a multi-scale residual block to reduce the computational complexity of the bicubi operation. RDN [4] introduces a residual dense block to extract hierarchical features from all the convolutional layers and a contiguous memory mechanism to fuse the hierarchical features. RDNet [4], for instance, introduces a dense dense block and a dense connection mechanism to exploit the hierarchical information from the LR-HR feature maps. SRWarp [5] generalizes the traditional image warping task, specifically when the input is enlarged, as a spatially-varying SR problem. It proposes several novel formulations, including the adaptive warping layer and multiscale blending, to reconstruct visually favorable results in the transformation process. SRUpscale [7] proposes a single-scale module to upscale the SR model by taking the input image scale factor into account. Meta-SR [6] proposes to learn a scale-aware feature adaptation module and a scale aware upsampling layer to adapt to the target scale factor. MetaSR [7], for the first time, predicts the scale-specific weights for the upsampled factor in a single model.

**Arbitrary SR methods.** These methods [6][5] aim to learn an arbitrary-scale SR model for the target factor. For instance, SRUPSR [6], Meta-PSR, and Meta-SSR  propose a single network to perform scale-arbitrary super-resolution. MetaPSR  proposes to generate the dynamic scale-adaptive weights for each factor and upsample the HR image with the"," **Optimization-based Methods:** Optimization-based methods search for a deblurring map based on a regularization term and are often trained using an iterative algorithm for an efficient solution. Many of the early optimization-based methods assume a specific blur kernel form [1][2][3][4][5][6], which is later relaxed to account for the fact that the blur kernels are, in general, unknown and are found by searching in a certain kernel space [7][8]. Iterative optimization methods, such as Richardson-Lucy [9] and Gauss-Newton , based on the prior assumption on the kernel and the image statistics have achieved impressive results in the non-photon-limited setting [10][11][12][13]. These methods have the main drawback of requiring the inversion of the blur operator. Conversely, variational methods [1][14][5], which perform a simple blur operator approximation and directly optimize the data, are widely used in modern deep deblurring systems. Recently, several neural network based approaches have been proposed. Learned deblurring networks use supervised loss functions to minimize photon-limited deblurring errors, i.e., they focus on a photon-limited setting. [15] is a pioneer work on a general architecture for deblurring. [16][17] have achieved state-of-the-art results on photon-limited scenes.

**Neural Network-based Methods:** Since neural network based deblurring methods do not have any prior assumptions on the kernel shape, they can be more generalized in nature. However, their training still requires large-scale annotated image datasets, which are typically difficult to obtain. Moreover, without clear ways to visualize the optimization progress, it is often difficult to make a proper choice of training parameters. For these reasons, end-to-end learning based methods focus on coarse-to-fine strategies, where they predict multiple sharp images [18][19][20][21], iteratively refine them and combine the outputs together. Recent researches on neural network based methods for blind deblurring include the use of data-driven generative models [22][23][24][25] and Transformers [26][27]. As alternative to the neural network-based methods, there is another category of methods that are based on a traditional algorithm, like ALR, and are trained using backpropagation techniques. [7][28][29][30][31].

",,"< Related work >

Several deep learning approaches have been proposed in the field of image super-resolution. The work by Tai et al. [1] introduces Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR) and a new multi-scale deep super-resolution system (MDSR), showcasing enhanced performance and stability by optimizing conventional residual networks. Similarly, Zhang et al. [3] present a real-time single image and video super-resolution CNN architecture capable of achieving super-resolution of 1080p videos on a single GPU, providing significant improvements over previous methods in terms of reconstruction accuracy and computational performance. The research on Residual Dense Network (RDN) by Zhang et al. [4] investigates the exploitation of hierarchical features from low-resolution images to address challenges in image super-resolution, revealing that the proposed RDN achieves notable performance gains against state-of-the-art methods. These methods demonstrate the advancements in image super-resolution achieved through improvements in network architectures.

Additionally, Yuan et al. [6] propose a learning framework for scale-arbitrary super-resolution, enabling existing single image super-resolution networks to handle non-integer and asymmetric scale factors, showcasing promising results while maintaining state-of-the-art performance for SR with integer scale factors. Furthermore, Li et al. [7] present Meta-SR, a novel method for super-resolution of arbitrary scale factor, offering a single model for super-resolution with different scale factors, addressing the inefficiency in computation associated with training specific models for each scale factor.


To improve the learning of high-frequency features in the context of image super-resolution, further advancements have been made. For instance, the work by Zhang et al. [2] introduces dynamic high-pass filtering and multi-spectral attention modules to address the bias towards learning low-frequency signals in current CNN models, achieving better accuracy and visual improvements against state-of-the-art methods. Similarly, the proposed Local Texture Estimator (LTE) by <9> characterizes image textures in 2D Fourier space, enhancing an implicit function's ability to capture fine details and achieve superior performance within an arbitrary-scale factor.

On the theoretical front, Li et al. [8] propose a Local Implicit Image Function (LIIF) to bridge the gap between discrete and continuous image representation and demonstrate the potential of continuous image representation. The work by Tancik et al. [10] introduces periodic activation functions for implicit neural representations, demonstrating the capability of representing complex natural signals and their derivatives, and their potential applications in solving challenging boundary value problems such as Poisson equation and wave equations. Moreover, Kottur et al. [11] propose Fourier features for enabling multilayer perceptrons to learn high-frequency functions in low-dimensional domains, addressing the spectral bias and enhancing the performance of MLPs for low-dimensional regression tasks, which are relevant to computer vision and graphics communities.

Lastly, Jiang et al. [12] generalize neural networks to learn operators, termed neural operators, for mapping between infinite-dimensional function spaces, demonstrating their universal approximation capability and superior performance in learning surrogate maps for the solution operators of partial differential equations, with significant speed improvements compared to conventional PDE solvers. Additionally, the work by Sirignano et al. [13] introduces a Fourier neural operator for parametric partial differential equations, offering an expressive and efficient architecture that outperforms existing neural network methodologies while significantly reducing computational time. Furthermore, Zhang et al.'s [14] multiwavelet-based neural operator learning scheme showcases the compression of the operator's kernel using fine-grained wavelets, resulting in resolution-independent learning and remarkable improvements in accuracy when applied to various equations such as KdV, Burgers', Darcy flow, and Navier-Stokes equation.

These works collectively highlight the diverse set of methods and theoretical advancements in the field of image super-resolution, demonstrating continued progress in improving performance, computational efficiency, and the ability to handle a wide range of scaling factors and operator learning."
2706,2706," **Image Deraining.** Existing image deraining methods can be roughly divided into two categories: model-based and deep learning-based. Model-based methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] usually formulate the deraining task as a multi-step optimization problem. For example, Zhang _et al._[1] proposed a rain streak removal method based on image decomposition. Li _et.al._[2] introduced a rain layer prior to guide the image formation process. Fu _et_. [3] proposed to use discriminative sparse coding (DSC) to remove rain from a single image. Recently, some deep learning based methods [6][10] have been proposed to solve the image de-raining task. For instance, Fu [6] proposed an unpaired learning based deraining method with rain direction regularizer. Li [10] developed a progressive deraining network (PDN) to progressively remove the rain streaks with a coarse-to-fine strategy. Fu [7] proposed the density-aware multi-stream densely connected network (MD-DenseNet) to estimate the rain density and then restore the clear image with the guidance of the density map. In addition, Li [8] proposed depth-attentional features (DFA) to capture the depth information for better image restoration.

However, the above methods mainly focus on modeling the low-level image statistics. However, they ignore the multi-scale information that is important for latent clear image restoration, which is vital for high-quality image reconstruction. In this paper, we propose an effective De-Raining Transformer that can adaptively keep the most useful attention scores for feature aggregation, which can effectively model the non-local information for effective image restoration and thus facilitate the clear images recovery. Moreover, we develop a learnable top-k selection operator to adaptively retain the most crucial attention scores from the keys for each query for better feature aggregation and thus improve the performance of the de-rained CNNs. Besides, we also develop a mixed-scale feed-forward network to model the latent clear images, which helps to generate more accurate and clear images with the help of the learned sparse attention weights for better deraining performance and robustness [15].

**Transformer-based Image Restoration.** Recently, Transformer [16] has achieved great success in various computer vision tasks [17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32]. Specifically, ViT [15] is the first work to apply the Transformer to image classification. It firstly proposed a pure Transformer architecture for image classification and achieved remarkable performance. Later, DETR  further improved DETR by introducing the residual blocks and a residual attention module. PIPT [18] further proposed a pre-trained image processing Transformer (PIPT) to improve the image restoration performance. In order to achieve high-resolution image reconstruction, Restormer"," **Single image deraining**. Since image deraining is an ill-posed problem, traditional methods [1][2][3][4][2] usually develop kinds of image priors to provide additional constraints. However, these handcrafted priors tend to rely on empirical observations and thus are not able to model the inherent properties of clear images. To overcome this problem, numerous CNN-based frameworks [5] have been developed to solve image deraining and achieved decent restoration performance. To better represent the rain distribution, several studies take rain characteristics such as rain direction [6], density [7], veiling effect [8] into account, and optimize the network structure via recursive computation [9][10] or transfer mechanism [11][12][13][14]. Although these methods achieve better performance than the hand-crafted prior-based ones, they have difficulty capturing the long-range dependencies due to the intrinsic limitations of convolution. Different CNN-based deraining approaches, we utilize the Transformer as the network backbone to model non-local information for image deraining.

**Vision Transformers**. Motivated by the great success of the Transformers [15] in natural language processing (NLP) [16] and high-level vision tasks [17], Transformers have been applied to image restoration [18][19][20][21][22] and perform better than the previous CNN-based baselines as they are able to model non-local information. For the field of image rain removal, Jiang _et al._[23] design a dynamic associated deraining network by incorporating self-attention in Transformer with a background recovery network. More recently, Xiao _et al._[24] elaborately develop image deraining Transformer (IDT) with window-based and spatial-based dual Transformer to achieve excellent results. Note that, most existing methods rely on the dense dot-product self-attention as the heart of Transformers. However, one shortcoming of this computation manner is that redundant or irrelevant features with smaller weights may interfere with the attention map, which makes the output features contain potential noises. In this work, we propose sparse attention in Transformer to relieve the negligence of the most relevant information faced by vanilla self-attention.

**Sparse representation**. With inspirations drawn from neural activity in biological brains, sparsity of hidden representation in deep neural networks as a tantalizing ""free lunch"" emerges for both vision and NLP tasks [25]. Indeed, it is widely proven that sparse representation also plays a critical role in handling low-level vision problems, such as image deraining [26] and super-resolution [27]. In principle, sparse attention can be categorized into data-based (fixed) sparse attention and content-based sparse attention [28][29]. For data-based sparse attention, several local attention operations are introduced into CNN backbone, which mainly considers attending only to local window size. Recent studies [30][31] have investigated enforcing sparsity to Transformer backbone. More recently, Zhang _et al._[32] design an attention retractable Transformer to allow tokens from sparse areas to interact features, which is data-based sparsity. Different from it, we implement a simple but effective approximation for self-attention based on top-\(k\) selection to achieve sparse attention, which is content-based sparsity.

**Top-k selection**. Zhao _et al._[25] first propose an explicit selection method based on top-\(k\) mechanism in NLP tasks. Driven by their success, \(k\)-NN attention [31] is further introduced for boosting vision Transformers. Unlike performing top-\(k\) selection in the spatial dimension , we design an efficient top-\(k\) useful channel selection operator.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]"," **Image Deraining.** Existing image deraining methods can be roughly divided into two categories: model-based methods and data-driven methods. Model-based approaches [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32] are more effective than data-based ones in terms of image restoration performance. For example, Wang _et al_. [1] proposed a rain removal method based on morphological component analysis based on the layer decomposition model and sparse coding. Li _et.al_[2] proposed to use Gaussian mixture models to model the background and rain layers separately. Li and Fattal [3] proposed the discriminative sparse coding (DSC) method to separate the rain layer and the de-rained image layer from an rainy image. Li [6] proposed an unpaired learning based method to solve the problem of rain streak removal from an unsupervised set of synthetic images and real rainy images by exploring the properties of rain maps. Recently, deep learning based methods have achieved significant performance improvement in image rain removal [5][4]. For example [4][9] used the convolutional neural networks (CNNs) to learn the low-rank representation of rain streaks from the rain images and then used the CNNs to remove the rain streaks in the rain image. Zhang [7] proposed DID-MDN to estimate the rain density information and then remove the corresponding rain-streaks guided by the estimated rain density label. Li  proposed a multi-scale progressive network (MSPN) to better characterize the rain streak with different scales and shapes in the input image. MSPFN [9] employed recurrent calculation to capture the global texture and hierarchical deep features in a unified framework, and further introduced the attention mechanism to guide the fine fusion of these correlated information from different scales.

Recently, deep-learning based methods [5] have dominated the deraining field. For instance, Li [5], Zhang [6], and He [13] proposed semi-supervised learning based deraining method [11][6] and disentangled image translation based method [13], respectively, to explore and exploit the properties from both synthetic and real rain images. To further improve the performance of deraining, Li"," **Image deraining** A few traditional methods are proposed in literature. Zhao _et al_. [1][2] proposed a framework that separates the rain streak into an additional layer and a foreground layer. This framework needs the rain information in the rain streak layer as prior, which limits the generalization to real rainy images. Discrepancies between synthetic data and real rainy data hinder the performance of rain streak separation. Shih _et al_. [3] proposed a sparse coding-based framework that reconstructs the foreground from the background using sparsity-based dictionary learning. They also employed non-local regularization to reduce the performance degradation when the number of basis functions is smaller. In [4], Chu _et al_. proposed a convolutional sparse and low-rank coding-based framework that jointly represents the rain and background image using the sparsity and low-rank constraints. This framework combines both sparse representation and low-rank constraint to obtain image deraining.

With the surge of the CNNs, a significant number of deraining methods [5][6][7][8] have been proposed. Hu _et al_. [5] proposed a Deep Image Deraining Network (DID-Net) that takes a pair of image as input and reconstructs the background from the rainy layer. Yang _et al_. [9] proposed a multi-scale feature fusion module that concatenates the rain streak layers of different sizes. Huang _et al_.  proposed a gradient-enhanced approach that aggregates gradient information from each rain streak pixel of the rain streak layer and the background layer. Li _et al_. [10] proposed a Progressive Image Deraining Network (PIDNet) that learns from the rainy image with small noise. Xue _et al_. [6][11][12][13][14] proposed transfer learning methods that transfer the prior knowledge from synthetic images to real images. However, these methods cannot be directly applied to the real deraining. Transformers also benefit the image deraining. Li _et al_. [15] proposed the Backbone-Transformer-Predictor (BTP) architecture, which learns the background from the rainy layer using a Transformer. Li _et al_. [16] proposed a common model for sequence-to-sequence tasks, which shows excellent performance in the neural machine translation. Inspired by the Transformer, other visual Transformer architectures have been proposed for other visual tasks, such as Swin Transformer [17] for computer vision, PITR [18] for image restoration, Restormer [19] for image restoration, Uformer [20] for image super-resolution, UFormer [21] for image deraining and image dehazing. Wu _et al_. [22] proposed a 3D transmission-aware UFormer to enhance the image dehazing",,"<The task of image deraining has garnered significant attention in the field of computer vision, with several pioneering methods being formulated. In the work of [1], the authors proposed a single-image-based rain removal framework using morphological component analysis, decomposing the image into low- and high-frequency parts to effectively remove the rain component. Similarly, [2] addressed rain streak removal by formulating it as a layer decomposition problem and proposed an effective method using patch-based priors for both the background and rain layers. Furthermore, [3] developed an algorithm for single-image de-raining based on discriminative sparse coding, utilizing discriminative sparse codes for accurate separation of rain and de-rained image layers. These approaches demonstrate the significance of decomposition-based methods for image deraining.

On the other hand, deep learning has also seen widespread application in image deraining. For instance, [5] provides a comprehensive survey of single-image deraining methods, discussing the development from model-based to data-driven approaches, including the use of convolutional neural networks, recurrent neural networks, and generative adversarial networks. Moreover, [6] explores unpaired learning-based image rain removal, where a semi-supervised learning part and knowledge distillation part are leveraged to effectively remove rain from images using synthetic and real rainy images. Similarly, [7] introduces a density-aware multi-stream densely connected convolutional neural network-based algorithm to jointly estimate rain density and remove corresponding rain streaks.

Furthermore, recent advancements in the Transformer architecture have paved the way for its application in image restoration tasks such as image deraining. The work of [15] demonstrates the effectiveness of directly applying a Transformer to sequences of image patches for image classification tasks. Moreover, [25] introduces the concept of adaptively sparse Transformers, effectively leveraging dynamic sparse attention patterns. These approaches showcase the potential of Transformers in image restoration tasks, indicating a promising direction for future research in image deraining.>"
2148,2148," **Equivariant CNNs.** Group equivariant convolutional networks (GE-CNNs) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] have been proposed to learn rotation-equivariant features from 3D point clouds. These methods can be roughly divided into two categories, i.e., \(SO(3)\)-CNNs and \(SE(2)\)-ConvNets. The first category aims to learn \(\mathcal{SO}(3,\mathbb{R}^{3})\) equivariance by discretizing the input space into SO(3) subspaces. For example, [4] proposed SE(3)-Convs to learn equivariants from point clouds, which can be viewed as a generalization of kernel point convolutions (KPConv) [29]. However, it is computationally expensive due to the high dimensionality of the input data. To address this issue, [10] proposed 3D Steerable CNNs (3D-S-CNN) to learn rotations and translations in a coarse-to-fine manner. However, this method requires a large number of memory and computation resources, which limits its applicability in real-world applications. In contrast, our method is lightweight and can be easily integrated into existing point cloud learning pipelines.


**SE(3D)-CNN.** The second category is to learn invariant features by decomposing a point cloud into a set of SO(2,3)\(3\) groups. For instance, [9] proposed Tensor Field Networks (TFN) to represent the input point cloud as a tensor field, which is a compact representation of the 3D space. Then, the convolution operation can be performed on the tensor fields. The main drawback of TSN is that it cannot handle the rotation-invariant nature of point clouds due to its cubic complexity. To solve this problem, [24] proposed to use a spherical harmonics-based representation to represent point clouds as a set, which has the advantage of being compact and efficient. The drawback of this representation is that the equivariances are not guaranteed to be invariant to rotations. In this paper, we propose to use the spherical harmonic basis as the basis of the group representation, which preserves the geometric properties of the original point cloud.

 proposed a convolution operator that can be used to learn SO(1) and SE(2) invariant representations. It can be regarded as a special case of our method, where \(SO(\mathbf{x}^{2}\) is the group of the rotation group. The difference between our method and theirs is that our method uses the spherical basis to encode the rotation invariance, while theirs uses the harmonic basis to represent rotation.

 is an extension of TensorField Networks. It is a"," **Group convolutions:** In 2016, Cohen and Welling proposed G-CNN [1], enabling equivariance beyond translations on 2D images with generalized G-convolutions (group convolutions) over the group of 90-degree rotations, which is one of the earliest efforts in equivariant deep learning. Group convolution is similar to conventional convolutions but has an extended domain for feature maps and kernels. The idea was then applied to different networks to enable equivariance for \(\mathrm{SE}(2)\)[1][2], \(\mathrm{SO}(3)\)[3], \(\mathrm{SE}(3)\)[4], and \(\mathrm{E}(3)\)[5] groups up to some discretization. The idea mainly works with finite (discretized) groups, as it is convenient to parameterize feature maps and kernels on the discretized group elements just as on pixel grids. Group convolutions have a relatively simple structure, making them straightforward to apply, but a major downside is that the lifted domain of features and kernels causes higher computational and memory costs, and the problem is more prominent when the group is large. Groups convs can also work with continuous groups, for example, with the help of Monte Carlo (MC) estimation as in [6], but they suffer from a large memory burden in MC sampling when the number of layers grows [7].

**Steerable CNNs:** Another line of work is steerable CNNs. Instead of augmenting the domain of feature maps, steerable CNNs generalize the space of feature values to be _steerable_, i.e., the feature values transform predictably as the input transforms. The way the feature transforms is called the group representation in the feature space, governed by the feature _type_. Features of _scalar_ type are kept unchanged under group actions, and we call the group representation trivial. For _vector_ or _tensor_ feature types, the representation is not trivial, and the features will change with group actions, for example, through rotation matrix multiplication. Steerable CNNs work for both discretized and continuous groups. One may freely design the feature types based on pre-determined basic types and corresponding representations (irreducible representations, i.e., _irreps_) as building blocks for a given group. Group convolutions can be viewed as a special case of steerable CNNs with _regular_ representations, i.e., channels of a feature vector undergo permutations when transformed. Examples of steerable CNNs include [8][9][10][11]. While the framework of steerable CNNs generalizes group convolutions with more flexibility, it requires a good understanding of representation theory and involves generalized Fourier analysis when working with a continuous group, which makes the structure complicated and challenging to apply in real-world applications broadly. The work of [12][13] also found empirically that steerable CNNs with irreps underperform regular representations in certain tasks.

**Theoretical progress:** There has been a lot of progress in the theoretical development of equivariant networks that are not group-specific. [14] developed a general formulation for steerable CNNs with scalar type features, and [15][16] generalized it to arbitrary feature types. It is proven that all equivariant linear maps can be written as convolutions. The formulation from the perspective of Fourier analysis was presented in [17]. [13] found the general form of solution for the equivariant kernels for \(E(2)\) group, later generalized to any compact group [18]. [19] proposed an algorithm to solve for the equivariance constraint for arbitrary matrix group. The formulation of group convolution based on MC estimation was proposed for any Lie group with [6] or without [7] surjective exponential maps. Equivariant non-linear layers like transformers [12][20] and equivariant set and graph networks [21][22][19] are also proposed, but they are not the focus of this paper.

**Applications of equivariant learning in perception tasks:** We want to highlight a few equivariant networks that gain attention in perception applications due to their simplicity and practicality. Vector Neurons [23] is a PointNet-like \(\mathrm{SO}(3)\)-equivariant network for 3D point cloud learning, later applied to point cloud registration [24] and manipulation [25]. It can be viewed as a special case of TFN [9] with type-1 features and self-interactions only. E2CNN [13] as an \(\mathrm{SE}(2)\)-equivariant framework was applied in several image processing tasks [26][27], given its generality and user-friendly library. DEVIANT [28] applied scale-equivariant convolutions in monocular 3D object detection. EPN [4] is a group convolution network with \(\mathrm{SE}(3)\)-equivariance for 3D point cloud learning based on KPConv [29] and was used in practice, for example in place recognition task . We also position this proposed work in this category, aiming to promote the application of equivariant learning with our efficient and easy-to-use design. Our work is developed based on EPN, which also serves as a major baseline in this paper.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]"," **Group equivariant networks.** Group convolutional neural networks (G-CNNs) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29] are the most widely used convolution networks for various tasks. They are based on the equivariance principle and can be viewed as a special case of group convolutions, which are equivariants to translation, rotation, and permutation transformations. Theoretical results [6][13] prove that group convolution kernels are necessarily convolutions for any finite-dimensional Lie group, which is a generalization of the group equivariancy theorem [6]. However, these methods are limited to the translation group and cannot be applied to the rotation group. To overcome this limitation, [7] proposes a general framework to generalize the group-equivariant convolution to arbitrary Lie groups. [6] proposes to use the surjective exponential map and logarithm map to construct a convolution layer that is invariant to transformations from any Lie group. [7][14] propose to use a general method to construct equivariantly convolution layers for any compact group. However, all these methods require the group of warps to be known in advance, which limits their applicability to general tasks.

\(\mathrm{SO(3)\(\mathbb{SO}(3}=\mathbb[SO(2}+\lambda\(\bm{SO},\bm{3})\(\cdot)\(\lambda\),\(\lambda=\lambda+\bm{\mathcal},\cdot}\),\(SO(1)\(\bm{\sqrt}\)(\lambda)=\lambda-\(\beta)\(\gamma)\(\beta\)(\beta)(\lambda)\(\alpha)\(\bta)\(\Delta)\(\delta)\(\tilde{\bm{\sigma}(\lambda,\lambda)\) (\(b\)-\lambda(\lambda \(\beta\))\(\Delta\(\gammas)\(\sigma)\(\eta)\(\theta)=\eta\(\alpha\), \(\beta\), \(b\), and \(\eta\), where \(\theta\) and \(\beta\) are the group coefficients and \(\the"," Since the introduction of the first convolutional neural network (CNN) for images [1], the idea of _equivariance_ has gradually attracted increasing attention. Early works focus on equivariant convolution [1][2][3][4][5][6][7][8], which can largely save computational resources by reducing the number of parameters. As equivariance is more generally applied to multi-dimensional data, various representations have been proposed to form equivariant features, such as steerable or spherical filters [9][10][11][12][13]. However, these equivariant representations are generally sensitive to small deformations in input data. The adaptation of intertwiners [14] has been applied to more realistic domains to reduce sensitivity to small deformations [15][16][17][18]. More recently, the design of equivariant layers has been developed to allow networks to be equivariant to Lie groups [19][15][14][16][17][18]. Some of the above works are orthogonal to ours.

The continuous action of the group on the data provides equivariance property for each layer of the model. There are different points in the parameter space of a group, and they can all be accessed in different manners. For example, the action of continuous rotations on the Euclidean plane is from the rotation group \(SO(2)\) represented by \(-\pi<\phi<\pi\), and the action of continuous \(3\times 3\) rotations on the Euclidean space is from the rotation group \(SO(3)\) represented by \(-\pi<\phi<2\pi\). \(SO(2)\) is usually more commonly used because it is compact and much easier to work with compared with \(SO(3)\) in the deep learning area. It is obvious that \(SO(3)\) has higher theoretical capacities and provides more robustness in the practical case. This is why our proposed E2P module is inspired by a more general form of group convolution that allows the group \(SO(3)\) to appear on any position on the parameter space in the design. However, one should be careful in treating this form of group convolution. The application of this idea to any equivariant layers will lead to an enormous growth of the number of parameters. Hence the proposed sphere computation as a generalization of KPConv [9] is a new approach, different from previous works [2][3][20][13][21][22][23][24].

This paper proposes a CNN that is equivariant to \(SO(3)\) and \(SE(3)\). SE(3)-equivariance has been used in several areas in deep learning such as shape analysis [25][26], detection and recognition [27][26], and 3D object detection [28]. However, there is no work that systematically demonstrates that these methods have the same theoretical capacity",,"<>
The field of deep learning for 3D point clouds has seen remarkable advancements in recent years. The paper ""E2PN: Efficient SE(3)-Equivariant Point Network"" proposes a novel convolution structure for learning SE(3)-equivariant features from 3D point clouds. This work represents an important contribution to the growing body of research on equivariant neural networks for point cloud data. Several prior studies have also focused on group equivariant neural networks, such as G-CNNs [1], HexaConv [2], and Gauge Equivariant Convolutional Networks [3], which have demonstrated improved performance and data efficiency on tasks with symmetries.

The concept of equivariance to symmetry transformations has driven the development of various approaches for different data types. For instance, Equivariant Point Network for 3D Point Cloud Analysis [4] offers a practical SE(3) equivariant network for point cloud analysis, showcasing the potential advantages of equivariant frameworks for addressing computational cost and leveraging rotation-equivariant features in 3D shape alignment tasks. Furthermore, 3D G-CNNs for Pulmonary Nodule Detection [5] highlight the effectiveness of 3D roto-translation group convolutions for improving network performance, sensitivity, and convergence speed in medical imaging tasks.

In the context of handling various symmetries, Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data [6] introduces a general method for constructing convolutional layers that are equivariant to transformations from any specified Lie group. This approach demonstrates the flexibility and applicability of equivariant convolutional layers across diverse data types and symmetry groups. Additionally, the work on LieTransformer: Equivariant self-attention for Lie Groups [12] extends the concept of group equivariant neural networks to self-attention modules, showcasing the versatility of equivariant architectures in a wide range of tasks.

Continuing on the theme of equivariant networks for different data types, Harmonic Networks: Deep Translation and Rotation Equivariance [11] presents CNNs exhibiting equivariance to patch-wise translation and 360-rotation, showcasing their ability to achieve state-of-the-art classification on rotated-MNIST. In the domain of spherical signals, Convolutional Networks for Spherical Signals [8] demonstrate the effectiveness of spherical convolutional networks for solving rotationally invariant classification problems, with applications in areas such as climate and weather science, and astrophysics.

Moreover, Tensor Field Networks [9] and 3D Steerable CNNs [10] bring attention to tensor field neural networks and 3D convolutional networks equivariant to rigid body motions, providing advancements relevant to geometry, physics, and chemistry tasks. Additionally, DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection [28] introduces depth-equivariant networks designed for monocular 3D detection, showcasing the practical implications of equivariance in specialized domains."
1723,1723," **Super-resolution (SISR).** SISR aims at recovering a high-resolution image from a low-resolution one. Most of the existing methods are based on deep neural networks [1][2][3][4][5][6][7][8][9]. In particular, SRGAN [3] and its variants [10][11][12][13][14] use a residual network to learn the mapping between LR and HR images. However, these methods cannot be directly applied to the problem of novel view synthesis.

**Novel view synthesis (NVS).** NVS aims at synthesizing an image from an arbitrary viewpoint using multi-view images and camera poses. Recently, several methods have been proposed to solve this problem [15][16][17][18][19][20][21][22][23][24][25]. Most of these methods use coordinate-based neural networks to represent the image. For example, SPIRAL [21] and Mip-NeRF [19] represent an image as a set of voxels and use a multi-layer perceptron (MLP) to model the scene. These methods can synthesize high-quality images at arbitrary resolutions, but they are not suitable for the task of NVS. In this paper, we propose a novel framework to jointly optimize the SR and NVS tasks.

 proposed a novel SR-NVS framework. They use a convolutional neural network (CNN) to extract features from the input image and synthesize the novel view image from the features extracted from the output image. Their method is based on the idea that the feature map of the original image can be used as the reference image for the subsequent optimization of the NVS task. In contrast, our method uses the feature maps from both the original and the synthesized image to guide the optimization process of the SR task. Our method is more general and can be applied to various NVS problems.

 also proposed a new framework to improve the performance of SR. Their framework uses a CNN to extract the features from an LR image and an HR image, and then uses the features of the two images to synthesize a novel view. Their work is based upon the idea of using a CNN as a feature extractor and a decoder as a synthesizer. Our work is different from theirs in two aspects. First, they use a CNN for feature extraction and an encoder-decoder network for the synthesis task. Second, their network is trained in a supervised manner, while our network is end-to-end trainable.

 first proposed a framework for image super-resolution. Their network consists of three sub-networks. The first sub-network extracts features from LR images and synthesizes the HR image. The other two sub-nets are used for the reconstruction task. Their model consists of a CNN and a deconvolutional network. Their results are shown in Fig. 1.

 introduced a new architecture for the SR problem. Their architecture is inspired by the seminal work of [3].

"," SISR aims to learn mapping functions between LR and HR image pairs. It has improved dramatically with the advent of learning-based methods using large-scale datasets. SRCNN  first proposes a learning-based SR framework using CNN, and after that, EDSR [1] and RCAN  suggested a deeper network structure using residual blocks and an attention mechanism respectively. Also, with the advent of transformer-based architecture [2] together, researches started to solve vision problems using the corresponding architecture, and SwinIR  improved the performance of SISR by using swin transformer [3]. However, SISR is an inherently ill-posed problem, and there is no unique solution, which causes the SR results to produce blurry images. To address this, some studies have improved the perceptual quality of SISR using discriminative networks [4][5] and adaptive targets [6]. Still, reconstruction accuracy and perceptual quality of SISR are a trade-off. To solve this problem, our method proposes an SR update module that receives guidance from radiance fields and refines the results from SISR features.

Unlike SISR, there are studies that perform SR from multiple images. Video super-resolution (VSR) has the additional problem of exploiting the information from multiple frames of video with deep correlation. Some studies propose a sliding window framework to predict the optical flow of LR frames or a framework using a recurrent model architecture [7][8][9][10]. Reference-based super-resolution (RSR) is an approach to improve the details of LR images through HR images given as reference images. Some studies propose a deformable convolution or the cosine similarity between the reference and LR images [11][12][13][14]. Recently, a study proposes a method to perform MVSR, which generates HR reference images using given LR inputs and paired depths [15]. However, it requires depth maps as inputs, and since each image is processed independently, it is difficult to maintain multi-view consistency. Our method improves the performance of MVSR by updating the SR outputs during the process of optimizing the radiance fields from the given LR inputs. In addition, we demonstrate that our method is superior through quantitative comparison with existing methods for performing VSR and MVSR.

Through the development of implicit neural representation [16] (INR), various studies have been actively conducted to represent 2D images and 3D spaces as multi-scale representations. LIIF [17] and SphereSR [18] propose continuous image representations that enable SISR with an arbitrary resolution on planar and spherical images. In radiance fields, mip-NeRF [19] proposes a scale-dependent positional encoding, allowing for multiple-scale supervision. BACON [20] enables multi-scale decomposition without multi-scale supervision through bandwidth constraints. Both studies [19][20] improve down-scaling performance in radiance fields. In contrast, NeRF-SR [21] proposes a super-sampling strategy that improves up-scaling performance by learning in an unsupervised manner. We present a new methodology that can improve HRNVS performance through the interaction of 2D SISR and 5D radiance fields, breaking away from the current methodology that depends on the characteristics of INR.

Some studies improve NVS performance through radiance fields using physics-based multi-view geometry techniques for train-view images requiring image enhancement. NeRF-W [22] solves the problem of inputs with variable illumination and transient occluders by relaxing strict consistency assumptions. Deblur-NeRF [23] solves the problem of blurry input by developing a module that models spatially-varying blur kernels. RawNeRF [24] enables high-dynamic range (HDR) novel view synthesis by learning NeRF from raw data inputs and synthesizing raw output images. HDR-NeRF [25] makes exposure control and HDR image rendering possible by learning two implicit functions, radiance field and tone mapper. We propose a new method that finally enables HRNVS by allowing SR input images to be appropriately super-resolved during radiance fields optimization simultaneously. Unlike the image enhancement method using the existing radiance fields, we solve the problem by repeatably updating the train-view images which is the source of the radiance fields optimization.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]"," **Single Image Super-Resolution (SISR).** SISR aims to recover a high-quality image from a low-resolution input image. It has been extensively studied in the past few years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25]. Most of the SISRs are based on the Transformer [2] architecture, which is based on self-attention mechanism. For example, SRCNN [3] and VRT [4] use the transformer architecture [2], which has been widely used in image restoration [1] and video restoration [4], to improve the performance of single image super-resolution. EnhanceNet [5] and DietNeRF [6] use a multi-view consistency loss to improve few-shot view synthesis.

**Novel View Synthesis (NVS).** Novel view synthesis aims to synthesize an image from an arbitrary viewpoint using a collection of images with different camera poses. Existing methods for NVS can be divided into two categories: single-view methods and multi-camera methods. Single-view NVS methods [25][25][24] use multi-image super-resolved images as the input. They use the multi-spectral information from multiple views to synthesise novel views. For instance, NeRF [21] uses a multi view image super resolution network to synthesized images from a single view image. However, these methods are limited to the resolution of the input images, which limits their performance. Multi-camera NVS [25] is a recent method for high-resolution NVS. It uses a single-camera system and a volume rendering technique to render novel views from a set of input LDR images. It also uses a volume-rendering technique to project the output radiance, colors and densities of the synthesized views into the LDR image. The volume rendering is a classic volume rendering method that has been used in many computer vision tasks, such as novel view synthesis [25]. However, it is not suitable for the NVS task due to the limitation of the volume rendering and the difficulty of the optimization process. In this paper, we propose a novel framework for cross-guided optimization of the single-image SR and the radiance fields, which can be used to improve HRNVS"," **Visual Transformer.** Transformer is a powerful module in various vision tasks [1][2][3][4]. To boost the feature representation ability,  and  use self-attention to encode a pair of input images to obtain global correspondences. In [2][3][4], a cross-context attention mechanism is used to generate spatial correspondences.

In addition to local-to-global matching, transformers can also be used to fuse multi-scale matching results [5][6]. In , a transformer is proposed to fuse the results of the multi-scale spatial pyramid matching network (SPMNet) , which achieves impressive performance on object detection and tracking. In [5], the feature representation power of the transformer is used to extract global matching results from a global cost volume, which has many similarities with the feature mapping module in our method.

**Stereo Matching.** Many convolutional neural networks are designed to generate high-quality stereo matching results [7]. In these methods, an encoder network is used to get effective feature representations. These features are then warped, propagated, refined, and fused by a decoder network to get accurate stereo matching results. Most of these methods adopt a cost volume formulation to optimize the performance. [8] directly considers stereo matching in the full 4D space of all possible correspondences. Different from these methods, our method does not use the cost volume formulation. Instead, it learns from sparse supervision and uses a transformer network to generate matching results.

**Sparse Correspondence Learning.** Because of the sparse supervision problem in semantic matching, existing works focus on improving the discriminative ability of the feature extraction network to make use of the limited annotated ground truths. In [9], a hierarchical matching enhancement network is designed, which fuses same-resolution features and cross-resolution features to generate accurate semantic matching results. In [8], an attention-based local feature matching module is proposed to mine relevant matching pairs from a cost volume. Our work mainly solves the problem of local feature map matching and the generated matching results are used as an additional supervision signal for an asymmetric feature extraction network.

**Coarse-to-fine Matching.** Coarse-to-fine matching is used in many works [9][6]. In these methods, a coarse feature map is produced from a high resolution to a low resolution. This coarse feature is then used to guide the feature representation learning in a low resolution.

",,"<Related work>

In recent years, there has been significant progress in novel view synthesis (NVS), particularly through the use of Neural Radiance Fields (NeRF). These methods aim to synthesize images from arbitrary viewpoints using multi-view images and camera poses. The effectiveness of NeRF lies in its continuous volumetric representation, allowing for NVS at arbitrary resolutions. However, limitations arise when it comes to enhancing the performance of high-resolution novel view synthesis (HRNVS), particularly due to the reliance on the spectral characteristics of coordinate-based networks [1].

To address this issue, researchers have proposed a novel framework utilizing cross-guided optimization of single-image super-resolution (SISR) and radiance fields. By performing multi-view image super-resolution (MVSR) on train-view images during the radiance fields optimization process, they derive the updated super-resolution result by fusing the feature map obtained from SISR and voxel-based uncertainty fields generated by integrated errors of train-view images. This iterative process ultimately leads to better performance of HRNVS. Experimental results demonstrated the significant advantages of this proposed method over existing techniques on various benchmark datasets [2].

The work on image restoration using Swin Transformer provides valuable insights into leveraging the capabilities of transformer-based methods for low-level vision problems. SwinIR, a strong baseline model for image restoration, demonstrates impressive performance and reduction in the number of parameters, particularly in the context of image super-resolution, denoising, and artifact reduction [3].

The development of advanced techniques for single image super-resolution has been prominent in the research literature. Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR) has demonstrated superior performance over existing state-of-the-art methods, along with the introduction of a new multi-scale deep super-resolution system and training method [4].

Furthermore, the study on Video Restoration Transformer (VRT) highlights the importance of handling video restoration tasks, including super-resolution, deblurring, and denoising. By proposing a Video Restoration Transformer with parallel frame prediction and long-range temporal dependency modeling abilities, researchers have shown significant advancements in video restoration tasks [5].

These studies collectively provide a comprehensive overview of the advancements in image and video restoration techniques, providing valuable insights into the collective efforts to enhance image quality and realism. These advancements demonstrate the potential for future integration of such enhancements into the domain of novel view synthesis for high-resolution image generation.

References:
[1] SwinIR: Image Restoration Using Swin Transformer
[2] Cross-Guided Optimization of Radiance Fields with Multi-View Image Super-Resolution
[3] Attention is All you Need
[4] Enhanced Deep Residual Networks for Single Image Super-Resolution
[5] VRT: A Video Restoration Transformer"
4880,4880," In this section, we briefly review the related work on adversarial attacks and defenses.

The adversarial example attack is a safety threat to deep learning, in which a small perturbation to the input is made such that the network gives wrong or desired labels of the adversary to specified inputs. The attack is known as the fast gradient sign method (FGSM) [1]. In this paper, we focus on the adversarial parameter attack (APA) [2], which is more stealthy than the FGSM attack. In APA, the adversary is allowed to modify the parameters of the network, and the network outputs the incorrect label of the input. In this way, the attack is called the fault sneaking attack [3][4][5][6].

Adversarial training is one of the most effective defense methods against adversarial examples [7][8][9][10][11]. In adversarial training, the attacked network is trained with adversarial perturbations added to the training samples. The adversarial samples are then used to evaluate the robustness of the attacked networks. In the testing phase, the network is tested on the same test set as the training set, and its accuracy is evaluated by the adversary. The adversary aims to fool the network to make incorrect predictions on the test samples. In other words, the attacker aims to make the network give wrong predictions on test samples, while the network's accuracy is still high on the testing set. In practice, adversarial defense methods can be used to defend against various types of attacks, such as FGSM and PGD attacks.

 proposed the Fast-Gradient Sign Method for adversarial attack. The FGSM iteratively updates the sign of the gradients of the loss function with respect to the perturbed input. The sign function is defined as:

\[\mathcal{L}_{\infty}(\mathbf{x})\geq\mathbb{E}_{x}^{\theta}(x)=\log\frac{1}{n}\sum_{i=1}^{n}x^{i}+\lambda_{i}-\theta_{i}\log(x)^{i}\geq 0\] (1)where \(n\) is the number of input samples, \(\lambda\) is a hyperparameter, and \(\lambda\in[0,1]\) denotes the hyperparameters. The goal of the attack algorithm is to maximize the loss of the model on the input data, while minimizing the loss on the output data. In our work, we use the \(\ell_{2}\) norm as the adversary's norm, because it is the most common norm in the real world.

 first proposed the fast-gradient sign method to generate adversarial images. In their method, the sign function of the gradient is computed as:where \(\gamma\) is chosen from a Gaussian distribution. The objective function is to minimize the following objective function:where"," **Parameter perturbation attacks.** Parameter perturbation attacks were given under different names such as fault injection attack, fault sneaking attack, stealth attack, and weight corruption. The fault injection attack ([4]) was first proposed by Liu et al., and was further studied in ([1]; [6]). In ([1]), the first physical fault injection attack on DNNs was given, by using the laser injection technique on embedded systems. In ([5]; ; [3]), some security boundaries for parameter perturbations were given. In ([2]; 2021), the stealth attack was proposed to make the attacked DNN output a desired label for a given input image.

The adversarial parameter attack has the following advantages compared to previous works. First, by keeping the accuracy and reducing the robustness, the adversarial parameter attack is more difficult to be recognized. Second, we prove the existence of adversarial parameters under reasonable assumptions, while most previous works rely on experimental results.

**Algorithms to train robust DNNs.** Many methods were proposed to train more robust DNNs to defend adversarial samples (). The adversarial training proposed by ([7]) is considered to be one of the most effective methods to train robust networks, and is used to compute adversarial parameters in this paper. Methods to train DNNs that are more robust against parameter perturbation attacks were also proposed ([4]; [6]; ). In (;a;b), the adversarial weight perturbation was proposed which was a generalization of the adversarial training by considering both the adversarial examples and the adversarial parameters, and hence led to more robust networks.

**Theory of adversarial examples.** The existence of adversarial examples was usually demonstrated with numerical experiments, and mathematical guaranteed results were desirable. Along this line of research, it was proved that a well-learned DNN always has adversarial examples for certain classification functions and data distributions ([8]). In ([2]; 2021), it was proved that there exist attacked DNNs that give a desired label for any sample.

Theories for certified robustness of DNNs were given in several aspects. In (; ; [11]), some security boundaries of adversaries were given. In ([9]), the randomized smoothing method was proposed and security boundaries of adversaries were given. Lower bounds on stability in terms of the classification function were also given in ([10]; [2]). However, these safety bounds are usually very small when the depth of the DNN is large. In (), the information-theoretically safe bias classifier was introduced by making the gradient of the DNN random. In this paper, we show that by making small perturbations to the parameters, the DNN will have adversarial examples with a high probability for any input.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"," **Parameter perturbation attack.** Parameter perturbations are a safety threat to deep learning, where small parameter perturbed inputs are made to the parameters of the network, and the attacked network will give wrong or desired labels of the adversary to specified inputs. In [1], the authors proposed to attack DNNs by injecting fault injection attacks on the activation functions of the DNN, such as ReLu, softmax, sigmoid, tanh, and tanh. In this paper, we focus on the adversarial parameter attack, which is more stealthy than the fault injection attack.

**Adversarial example attacks.** The adversarial example attack is a type of the parameter perturbant attack, in which the attacker tries to perturb the network parameters such that the network outputs the wrong label to a specified input pattern. The adversary is able to modify the parameters in a way that the accuracy of the attacked model decreases, but the network performs normally on standard samples. In the literature, there are many different types of adversarial attack methods, such that we can divide them into two categories: adversarial examples attacks and stealth attacks.


 proposed the adversarially robust neural network attack (A-A-N) in [2], which is a special case of the adversary example attack. In A-A, the adversary tries to modify some parameters in the network to make the network output the incorrect label of a specific input pattern, and then the network is evaluated on a validation set. In contrast, the stealth attack (STEVE) [2] is a more general version of the A-N attack, where the adversary can modify the parameter of the model to make it perform as normal on the validation set, but its accuracy will not decrease much. In addition, the authors in [3] proposed to use the gradient descent attack (GDA) [4] to make small perturbed parameters to make DNN outputs the incorrect labels of a specified pattern. In spite of the effectiveness of A-VAE attack, it is not a certified defense against adversarial attacks. The authors of [5] proposed a certified robustness bound for weight perturbated neural networks, which guarantees that the perturbed neural networks will not make erroneous outputs as desired by the adversary. However, the certified defense is not guaranteed to be robust to adversarial and stealth attack attacks. In our work, we prove the existence of nearly perfect adversarial parameters for the commonly used networks trained with various"," Many adversarial attack settings involve modifying the agent's observation (; ; [4]; [1]; ; [6]; [5]; [2]; [7]; ; ; [3]; ). Modifying the observation is problematic, as most RL algorithms train over a set of observations (; [11]; ; [9]; [10]; ; ; ; [8]; ). This paper focuses on a setting where an adversary only can append an arbitrary message to the Victim's observation, while the rest of the system is frozen (; [12]; ). Adversarial perturbations introduced in the observation have limited non-linearity ([5]), and many are unable to trigger changes in the underlying dynamics (; ). [7] show that such perturbations can cause a Victim's action to be drastically different from the original. [1] show that adversaries can successfully implement a random policy against DeepMimic, an approximate environment model. Here we focus on a white-box setting, allowing the Adversary to observe the Victim's state and parameters. Our setting also allows for a larger attacker, which gives it access to a much broader repertoire of append-only append-only attack strategies. The Appendix shows how a subtle modification of these attacks causes them to fail.

Multi-agent environments (; ; ; [13]; ; ; [14]; [15]; [16]) have also been studied. [14] consider an environment where agents can choose from a limited set of actions to send to other agents to effect their training, though the Adversary has no control over the Victim's policy. The setting in [13] is an environment with two agents, with a limited communication channel to exchange messages between them. [15] use the communication channel to share a finite set of messages between agents. Finally, in [16], each agent can shape the learning process of the other agents with a communication channel. In all of these works, the adversarial target is the agent's parameters (and not the Victim's observations), or the message passed to another agent (rather than the Victim's actions). This paper focuses on a scenario where an Adversary has unrestricted access to a Victim's observation, and no access to the Victim's parameters. Appendix D provides more details about existing multi-agent work.

",,"<Related work>
Adversarial attacks on deep neural networks have become a significant concern in the field of deep learning. Several research endeavors have focused on exploring different types of attacks and their implications on network robustness and security. Huang et al. [1] investigate the practical implications of physical fault injection attacks on deep neural networks, focusing on the manipulation of activation functions to achieve misclassification. This work highlights the potential catastrophic consequences of malicious faults in safety-critical applications, shedding light on the need for robust defenses against adversarial attacks.

Building on the concept of adversarial attacks, Li and Vorobeychik [2] introduce a formal theoretical framework for analyzing both adversarial examples and stealth attacks in AI systems. The framework provides insights into the susceptibility of AI decision-making spaces and the potential for attacks based on perturbations to the AI system itself. This broader perspective on adversarial attacks contributes to a deeper understanding of the underlying vulnerabilities and measures for enhancing network resilience.

Considering the vulnerability of model parameters, Shen et al. [3] propose an indicator to assess the robustness of neural network parameters through parameter corruption. Their work emphasizes the importance of understanding the vulnerability of network parameters and offers methods to improve model robustness using adversarial corruption-resistant training. Additionally, the study by Zheng et al. [4] explores fault injection attacks on deep neural networks, presenting two approaches for modifying network parameters to achieve misclassification. Their findings demonstrate the effectiveness of fault injection attacks and underscore the need for defenses against parameter-level vulnerabilities.

In the pursuit of certifying model robustness against weight perturbations, Wang et al. [5] introduce an efficient approach to compute certified robustness bounds of weight perturbations in neural networks. This work not only emphasizes the sensitivity of neural networks to weight perturbations but also sheds light on the connection between weight quantization and model robustness. Furthermore, the study by Zhang et al. [6] proposes a fault sneaking attack framework, wherein the adversary aims to misclassify input images into target labels by modifying DNN parameters. Their approach, based on the alternating direction method of multipliers (ADMM), showcases the development of a stealthy framework for misleading deep neural networks while maintaining overall test accuracy.

Finally, robust optimization techniques for enhancing adversarial robustness are discussed in the work by Wong et al. [7]. They present a principled approach to improving network resistance to adversarial attacks, offering security guarantees and methods for training networks with heightened resilience. This research demonstrates the potential for achieving greater adversarial robustness through robust optimization, addressing the inherent weaknesses of deep learning models against adversarial examples."
5173,5173," **Document Layout Analysis Datasets.** Most existing datasets for document layout analysis are based on PDF documents. The first real-world dataset for layout analysis is DocBank [3]. However, it contains only 1,000 documents and lacks annotations. To address this issue, PubLayNet [2] and Real-Doc [1] are proposed to provide annotations for more than 10,000 real documents. However, these datasets only contain PDF documents and lack annotations for modern documents. Recently, DocLaynet [4] and DocLayNet++  are proposed for large-scale document-layout segmentation datasets. These datasets are annotated by human annotators on scanned, photographed, and PDF documents, respectively. Although these datasets contain annotations for various types of documents, they are still limited in terms of annotation types and sizes. In addition, they do not have annotations for multi-layout and multi-language documents. In contrast, M\({}^{6}\)Doc contains annotations for both multi-Layout, multi-type, and multiple annotation categories.

**Document Segmentation.** Traditional document segmentation methods [5][6][7][8][9][10][11][12][13][14][15][16] are mainly based on hand-crafted features and heuristic rules, which may not be robust to various document types and layouts. Recent deep learning-based methods [9][16][13] have achieved state-of-the-art performance on various document analysis tasks, including document layout segmentation, table detection, and document semantic segmentation. In this paper, we mainly focus on the layout analysis task and propose a novel transformer-based model for this task.

 is the first public document layout dataset. It consists of 1,073 documents with 1,062 manually annotated pages. It is a small-scale dataset that contains only PDF documents for training and evaluation. The dataset is not large enough to train deep learning models and is not well-suited for evaluating the performance of layout analysis methods. The datasets in [1][4] are also small in size and have limited annotation types. To the best of our knowledge, \(M^{6}\)([4]) is the largest and most diverse dataset for modern document layout detection and table detection.

 and  are the two most similar datasets to ours. They also provide annotations on scanned and photographed documents, however, their datasets are small in scale and lack annotation types, which are important for evaluating layout analysis models. Moreover, the datasets in these two datasets are not designed for document understanding, which is a crucial prerequisite for document retrieval and conversion.

 provides a large-size dataset for document object detection (DOD). However, their dataset is only for academic documents and does not contain annotations of modern documents, such as scientific articles, textbooks, books, test papers, magazines, newspapers, and notes.

 also provides a dataset for table detection (TableNet), but it only contains 2,000 tables and 2,500 images. The tables in TableNet are designed for academic images, while the images in our dataset are for scanned and"," A variety of modern layout analysis datasets have been created in recent years. In 2009, Antonacopoulos et al. [1] presented the PRImA dataset, which was the first commonly used real-world dataset with 305 images of magazines and scientific articles. In 2019, Zhong et al. [2] published the PubLayNet dataset, which contains over 360,000 page samples annotated with typical document layout elements such as text, heading, list, graphic, and table. Annotations were automatically generated by matching PDFs and XML formats of articles from the PubMed Central Open Access subset. In 2020, researchers at Microsoft Research Asia built the DocBank dataset [3], which contains 500,000 document pages and fine-grained token-level annotations for document layout analysis. It was developedbased on a large number of PDF files of papers compiled by the LaTeX tool. Unlike the conventional manual annotating process, they approach obtaining high-quality annotations using a weakly supervised approach in a simple and efficient manner. In 2022, IBM researchers presented the DocLayNet dataset [4], which contains 80,863 manually annotated pages. It contains six document types (technical manuals, annual company reports, legal text, and government tenders), 11 categories of annotations, and four languages (English documents close to 95%). A few pages in the DocLayNet dataset have multiple manual annotations, which allows for experiments in annotation uncertainty and quality control analysis.

However, the predominant document format for large datasets is PDF, not scanned and photographed images as in real-world scenarios. Only a few public datasets include real-world data. The variety of layouts in current public datasets is still very limited and is not conducive to the development of logical layout analysis. Currently, 95% of the publicly available datasets are English documents, which are largely unsuitable for Asian language documents. To this end, we propose the \(M^{6}Doc\) dataset to facilitate the development of layout analysis.

Earlier layout analysis methods [5][6][7][8] used rule-based and heuristic algorithms, so they were limited to applications on certain simple types of documents, and the generalization performance of such methods was poor. However, with the development of deep learning, DLA methods based on deep learning have been developed to tackle challenging tasks. Mainstream approaches include object detection-based models [9][10][11], segmentation-based models [12][13], and multi-modal methods [14][15]. For example, Li et al. [10] considered DLA as an object detection task and added a domain adaptation module to study cross-domain document object detection tasks. Lee et al. [12] used segmentation methods to solve DLA problems and introduced trainable multiplication layer techniques for improving the accuracy of object boundary detection to improve the performance of pixel-level segmentation networks. Zhang et al.  proposed a unified framework for multi-modal layout analysis by introducing semantic information in a new semantic branch of Mask R-CNN [16] and a module for modeling element relationships. Behind their success, large datasets are required for training and evaluating the models.

However, the lack of a multi-format, multi-type, multi-language, and multi-label categorized logical layout analysis dataset makes it difficult for current methods to obtain good results in real-world and other language scenarios. Moreover, a data format that links visual and textual features has not yet been established for multi-modal tasks.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"," **Document Layout Datasets.** Most public datasets for document layout analysis are either PDF-based [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17] or are based on PDF documents. For example, PubLayNet [2] contains over 1 million PDF documents from PubMed Central [1] and is the largest publicly available dataset for document analysis. However, it only provides bounding box annotations for PDF documents and does not provide annotations for the layout of the document images. The layout annotations are manually annotated and are limited to a small number of classes. DocBank [3] contains 500K document pages annotated with fine-grained annotations. It is a benchmark dataset that contains 500,000 document pages with annotation labels for document layouts. However it is not large enough to support training and evaluation of deep learning models.

DocLayNet is a large-scale dataset for modern document layout. It contains 80863 manually annotating pages from diverse data sources to represent a wide variability in layouts. It provides 11 distinct classes of annotations for each PDF page. However the annotation labels are manually created using a semi-automated tool and the document layout annotations have been limited to the page and region level. The \(M^{6}\)Doc dataset is much larger than DocLayNet and DocBank. It has a much larger number of annotation labels and annotated pages with more types of annotations. The annotation labels in \(M\({}^{6}\)-Doc are more diverse and more detailed. The annotations are also more detailed and more accurate. The \(\mathcalculus\)-Doc dataset [4] is a dataset for layout analysis of scientific articles. It consists of 8,080 manually annotator-annotated pages and contains 11 different layouts. The layouts are manually labeled using the PAGE format [1], which is a more detailed representation of both complex and simple layouts. In addition, it contains a subset of double- and triple-labeled pages to determine the inter-annotator agreement. However this dataset is limited to scientific articles and contains only 11 layouts.

 proposed a dataset called DocBank, which contains 500k document pages. It also contains 11 distinct layouts with annotations for 11 different classes. However DocBank is not a large scale dataset. The dataset is also limited to PDF documents only. The datasets in [4][1] contain only PDF documents, which are not suitable"," **NBV.** Existing methods for Next Best View (NBV) in robotics [1][2][3][4][5][6][7][8][9] mainly rely on expensive 3D supervision from depth sensors. In particular, methods relying on the classic NBV approach [10][5][11] need the cost function \(\mathcal{L}_{\text{NBV}}:=\|H-N\|_{\infty}\) where \(H\) and \(N\) are the set of pixels of the scene to be explored (H) and the next potential exploration scene (N) respectively and typically are evaluated through exhaustive search. While in autonomous driving NBV methods are often constrained by a ""safety first"" criterion [12][13][14][15][16][17], in our setup, the viewpoint is moved from a quadcopter (Figure 3) such that a distance criterion is not possible. [18][19][20][21][22][23] learn an NBV exploration heuristic with camera motion, egomotion and/or optical flow estimation as the supervision. [19] additionally predicts depth with scene priors and learned semantic segmentation [24]. [25] does a similar job but on multi-modal input. [26] extends [24] by predicting an additional global depth. [27] and [28] make use of SfM with dense point clouds to generate the exploration map. [29] relies on polarimetry. [30] proposes to predict depth and gradients with a CNN as the supervision of the NBV. [31] extends [30] by using the gradients and the self-supervision from SfM as the supervision. [32][33][34] use temporal cues, while [35] uses the NBV exploration cost \(\mathcal{L}_{\text{NBV}}\) as a supervision. [36] extend this approach by also using other cues. However, in this work, we learn the NBV predictor from RGB only, making our method scalable to outdoor scenes.

Other monocular NBV methods include [37][38][39]. In particular, [38] uses past and future frames as self-supervision, and [39] adds object pose information as well. [37] leverages SfM supervision in an end-to-end fashion. [40] proposes to estimate depth, ego-motion and visual odometry, using a recurrent network with a SfM-like cost as supervision. Temporal features are used in [41] to evaluate the potential exploration cost \(\mathcal{L}_{\text{NBV}}\). Finally, in [42][43]",,"<>
Layout analysis of document images is a crucial component in the field of document understanding and information retrieval [1]. Existing datasets such as PubLayNet and DocBank have been instrumental in advancing document layout analysis by providing a large number of document images annotated with layout elements [2, 3]. However, these datasets primarily focus on specific types of documents, such as scientific articles and books, and lack diversity in terms of document types and layouts. The proposed \(M^{6}\)Doc dataset addresses these limitations by introducing a comprehensive dataset with diverse properties, including multi-format, multi-type, multi-layout, multi-language, and multi-annotation category features [4]. This dataset is expected to facilitate the development and evaluation of models for document layout analysis that can generalize well to real-world scenarios.

In addition to dataset advancements, recent research has focused on developing novel methodologies for document layout analysis. For example, TransDLANet, a transformer-based document layout analysis method, has been proposed to leverage an adaptive element matching mechanism and construct a segmentation branch for precise document image instance segmentation [4]. Additionally, other approaches such as the use of CNN-based methods for fast and accurate document layout analysis have shown promise, featuring improved performance and reduced computational costs [9, 12, 13]. Moreover, the concept of multi-task layout analysis using fully convolutional networks has emerged as an effective strategy for solving multiple layout-related problems simultaneously [13]. Furthermore, the growing interest in end-to-end approaches for table detection and structure recognition in document images has led to the development of models like CascadeTabNet, which achieved competitive accuracy results across different datasets [11].

The advancement of document layout analysis is also closely linked to the development of image analysis techniques such as Mask R-CNN, a framework that efficiently detects objects in an image while simultaneously generating high-quality segmentation masks for each instance [16]. These developments highlight the continuous efforts in enhancing the accuracy, efficiency, and generalization capabilities of document layout analysis methods. Moreover, the availability of diverse benchmark datasets and the ongoing exploration of novel methodologies ensure a promising trajectory for the field of document layout analysis."
2938,2938," Camouflaged object detection (COD) aims to identify camouflaged objects visually blended into the surrounding background. Existing approaches to this problem can be roughly divided into two categories. The first category of methods [1][2][3][4] focuses on detecting the camouflaged object in a coarse-to-fine manner. For example, [1] first detects the target object in the first stage and then refines the detection results in the second stage. [2] utilizes the gradient information of the input image to guide the object detection. [3] proposes a segment, magnify and repeat (SMSR) framework to refine the object boundaries. However, these methods ignore the ambiguous boundary problem.

The second category of approaches [5][6][7][8][9][10] aims to improve the image quality of the reconstructed images. For instance, [5] proposes the focal frequency loss to reduce the gap between the real and generated images. [6] proposes to use the wavelet transform to transfer the style information from the source image to the target image. [7] proposes an adaptive wavelet distillation method to enhance the interpretability and computational efficiency of deep neural networks. [9] uses the implicit differential equation (ODE) for image dehazing. Different from these methods, our FEDER model decomposes the features into different frequency bands and focuses on the most informative ones to mine subtle cues that differentiate foreground and background.

 proposes to learn an auxiliary edge reconstruction task alongside the COD task. It uses the gradient of the loss function to generate the edges of the target objects. Differently, our model uses the ODE to learn the edge of the foreground object, which is more interpretable and effective.

 also proposes an auxiliary loss function for COD, which can be viewed as a special case of our loss function. The difference between our model and theirs is that our model learns the edge map in the frequency domain instead of the image domain.

 first proposes an edge-aware loss function, and then uses it to train a segmentation network. In this work, we propose a novel loss function and a guidance-based feature aggregation module to solve the ambiguity problem of COD. Moreover, we design a novel feature attention module to mine the salient features.

 is a multi-task learning framework that jointly learns the classification and segmentation tasks. The main idea of this framework is to jointly learn the classification task and the segmentation task in a unified framework. In our model, the auxiliary loss functions are also used to train the feature attention and the edge reconstruction modules, which are more effective and effective than the existing methods.

 designs an auxiliary task to train an object segmentation model. It learns to predict the contours of the object and the contour masks of the background. The contour mask is then used to supervise the training of a camouflaged detection model. In contrast, our proposed edge reconstruction module learns to reconstruct the exact edge map of the camouflage object.

 introduces a new loss function that can be applied to the edge prediction task. The loss function is based on the difference between the gradient"," **Camouflaged object detection.** Unlike existing object detection tasks, camouflaged object detection (COD) poses new challenges for mining subtle discriminative features under complex camouflage strategies [1]. Early techniques utilized the hand-crafted operators for COD [2], which were only applicable to camouflaged scenarios with simple backgrounds. Recent research has leveraged the huge capacity of deep learning to detect camouflaged objects in a learning manner [1][3][4]. Inspired by the hunting process of predators, SINet [1] designed a bio-inspired network to gradually search and locate the camouflaged object. PFNet [4] proposed the position module and focus module to imitate human identification with the distraction mining strategy. By simulating human behaviors in understanding complex scenarios, SegMaR [3] integrated segment, magnify and reiterate in a coarse-to-fine manner using the multi-stage strategy. However, these COD solutions mainly focus on mimicking biovision systems, which can be easily confused by complex camouflaged strategies and struggle to excavate the subtle discriminative features, thus failing to handle the IS and ED challenges (see Fig. 1). Unlike these human perception-oriented techniques, we first propose to address the COD task from a decomposition perspective by decomposing the extracted features into different frequency bands with learnable wavelets and filtering out the most informative bands to excavate those inconspicuous discriminative features, thus remedying the human visual deficiency and solving the IS challenge. To handle the ED challenge, we propose learning an auxiliary edge reconstruction task along with the COD task to facilitate the generation of precise segmentation results with clear object boundaries.

**Deep wavelet decomposition.** Deep wavelet decomposition is an effective tool to decompose image/feature into various frequency components and has gained immense popularity in many domains, such as image restoration [5]and style transfer [6]. To handle the IS challenge, we introduce deep wavelet decomposition into the COD task. Furthermore, to better accommodate the COD data, we employ the learnable wavelets for deep adaptive feature decomposition, whose coefficients are updated following AWD [7].

**ODE-inspired network.** Researchers have established a relationship between ODE and neural networks.  first analyzed ResNet from the perspective of discrete ODE and [8] further extended ResNet to an ODE-inspired network architecture with a more accurate transmission. Since then, ODE-inspired networks are widely utilized in many fields, such as image dehazing [9] and machine translation [10]. In this paper, to accommodate the fine-grained property of the edge, we propose an ODE-inspired edge reconstruction module with the second-order Runge-Kutta and a weighted gate mechanism, aiming to generate more accurate boundaries. Furthermore, we apply the Hamiltonian system to our OER module to ensure the stability of edge reconstruction.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"," Camouflaged object detection (COD) aims to identify camouflaged objects visually blended into their surroundings. Existing approaches to this problem have developed various techniques to mimic the human visual system. Fan _et al_. [1] first proposed the COD task and proposed a comprehensive study on this problem. They proposed a method based on the 3D convexity operator [2] to detect camouflaged targets in the complex background. Fan [1], in [1][3], proposed a single-stage detection framework, which first detects the camouflaged object in the background and then refines the object mask in the ambiguous regions. Fan  further proposed a multi-stage detector, which uses a two-stage method to refine the object masks and refine the bounding boxes in a coarse-to-fine manner. Fan and Fan [4] proposed a position-and-focus network (PFNet), which mimics the process of predation in nature. PFNet contains two key modules, _i.e_., the positioning module and the focus module, to mimic human predation for progressively refining the coarse prediction via focusing on the ambiguous region. Fan proposed a novel distraction mining strategy for the distraction discovery and removal, to benefit the performance of estimation. Fan developed a simple yet effective framework, _SINet_, which adopts a search identification network (SIN) for COD, and an iterative refinement network, _e.g_., SegR, to progressively magnify and re-iterate the mask in a region of interest [3]. Fan proposed SINet, which is a simple but effective framework that can achieve state-of-the-art performance on all datasets tested. Fan also proposed a focal frequency loss (FFL) [5] to narrow the gap between the real and generated images, which has been widely used in image reconstruction and synthesis [6][7][8][9][10].


In this paper, we propose the FEature decomposition and edge reconstruction (FEDER) model to address COD. The FEDER model addresses the intrinsic similarity of foreground and background by decomposing the features into different frequency bands, and then focuses on the most informative bands to mine subtle cues that differentiate foreground from background. To combat the ambiguous boundary problem, we design an edge reconstruction module that generates exact edges.

FEDer is inspired by the recent work of [10], which proposed the ODE Transformer model for sequence generation. The O"," Multi-label learning can be seen as a regression task where the label vector \(\mathbf{y}\in\mathbb{R}^{m}\) is mapped to the image feature vector \(\mathbf{x}\in\mathbb{R}^{d}\) through a function \(f(\mathbf{x},\mathbf{y})\). In general, a mapping function can be learned through a supervised method [1][2][3] or an unsupervised method [4]. On the other hand, the semi-supervised method for multi-label learning also exists [5][6]. In our approach, we assume the function \(f\) is fully known and thus fully supervised.

Many works have been focused on how to treat unobserved labels. Some approaches set all unobserved labels to a negative label [7][8][9] and optimize the model on the basis of the real positive label, although this approach may result in the memorization effect [10][11]. When unobserved labels are assigned with random noise, this method does not cause memorization effect because there are no real positive labels, but this results in sub-optimal performance [12]. For other approaches, they treat unobserved labels either as a real positive label [13] or as a negative one [14], which can reduce the model's memorization effect. [15] propose a selective loss, which requires additional model parameters. In our approach, we simply set the scores of unobserved labels to be the same as those of other unobserved labels.

As we described in the introduction, our work is motivated by the finding in [10], and we fix the loss functions used in [10] throughout this work.

In another research area called ""weakly supervised learning"" [16][17][18][19][20][21][22][23][24][25][26], the model only receives image-level labels from a partial set. Compared to multi-label learning, these methods are more complicated in terms of weakly supervised object localization [17][24] and semantic segmentation [23][25]. Recently, the works on class activation maps (CAM) [17] are made for weakly supervised object localization. In particular, CAM-G [19] visualizes a model's prediction on regions of the image, for which it has learned to associate the label, by simply taking the result of averaging channel-wise gradients. However, there is a phenomenon that the generated CAM masks are not on the regions of actual target objects in the image [18][19]. In this work, we focus on the CAM-G approach and tackle this problem.

",,"<Related work>
Camouflaged object detection (COD) is a challenging task in computer vision, given the difficulty in identifying objects visually blended into the background [1]. Existing approaches have focused on developing techniques to mimic the human visual system, such as the Search Identification Network (SINet) that outperforms state-of-the-art object detection baselines by addressing the intrinsic similarity between the target object and the background [1]. Additionally, the Segment, Magnify and Reiterate (SegMaR) framework utilizes a multi-stage detection fashion to integrate segmentation, magnification, and reiteration, showing remarkable improvements in detecting small camouflaged objects over other methods [3].

Distinct from standard object detection, camouflaged object segmentation (COS) also demands effective and efficient techniques, as demonstrated by the Positioning and Focus Network (PFNet) that mimics the process of predation in nature to identify camouflaged objects assimilated into their surroundings [4]. Furthermore, the proposed FEature Decomposition and Edge Reconstruction (FEDER) model addresses the challenges of COD by decomposing features into frequency bands using learnable wavelets and learning an auxiliary edge reconstruction task alongside the COD task [Target Paper]. These approaches collectively address the ambiguity and intrinsic similarity challenges in detecting camouflaged objects by leveraging innovative techniques inspired by natural processes, frequency decomposition, and attentive multi-stage detection.

Apart from the specific problem of COD, the use of wavelet transforms in computer vision has been leveraged in other tasks, such as photorealistic style transfer and adaptive wavelet distillation for interpretability and computational efficiency [6][7]. Moreover, the reinterpretation of deep residual networks as ordinary differential equations has led to the development of reversible architectures that allow deeper networks to be trained using modest computational resources [8]. Additionally, the connection between residual networks and numerical ODE methods has also inspired architectures like the ODE Transformer, showing improvements in model performance over strong baselines in large-scale machine translation and other tasks [10].

While acknowledging the unique challenges and context-specific nature of camouflaged object detection, these related works demonstrate the wide applicability and potential beneficial impact of innovative techniques inspired by mathematical, computational, and natural concepts in various computer vision and machine learning tasks. Collectively, they contribute to advancing the state-of-the-art in subtle, complex, and visually challenging detection and segmentation tasks, including COD."
3389,3389," Adversarial attacks for semantic segmentation.Adversus adversarial attacks have been extensively studied in the past few years. [2] proposed the Fast Gradient Sign Method (FGSM) to generate adversarial examples for image classification. [7] proposed a variant of FGSM based on the Basic Iterative Method (BIM) and [1] proposed an iterative variant of the Iterative Gradient Descent (IGD) algorithm. [3] proposed to use the Fast Fourier Transform (FGT) for generating adversarial perturbations and [10] proposed using the Fast Iterative Iterative Closest Point (FIT) algorithm for generating the adversarial noise. [11] introduced the primal-dual primal gradient descent (PDGD) algorithm to generate the perturbation. [6] presented an attack method based on iterative FGSM.

Proximal Splitting.The idea of proximal splitting was first introduced by [13] and has been widely used in the optimization community. In this paper, we introduce the idea of applying it to the problem of adversarial image segmentation, and show that it can significantly improve the robustness of deep neural networks.

 and  proposed to apply it to image classification and object detection tasks, respectively.

 proposed a gradient-based adversarial attack for object detection. They use the gradient of the loss function with respect to the output of the classifier. They also propose to use it for adversarial example generation.

 introduced a gradient based attack for image-level classification. They used the gradient based method.

 used the Fast-Gradient Iterative Descentering (FGD) method. They applied FGSM to the image classification task.




"," While the literature on minimal adversarial attacks for classification is vast, the research on attacks for segmentation is much less developed. The main work on adversarial attacks for segmentation is done by Xie _et al._[1]. It proposes a simple algorithm to generate adversarial perturbations for dense prediction tasks, including object detection and segmentation, called the Dense Adversary Generation (DAG) attack. In this attack, the rescaled gradient of the loss w.r.t. the input is added to the current perturbation, until the stopping criterion is reached, _i.e_. a given percentage of pixels is adversarial. In each iteration, the total loss is the sum of the losses over pixels that are not adversarial. This can be seen as a form of greedy algorithm. See Appendix A for the complete algorithm of the DAG attack and a discussion on the stopping criterion used. In practice, this attack is quite efficient, however, it simply accumulates gradients until the stopping criterion is reached. Therefore, it does not minimize the norm considered. Cisse _et al._ propose the Houdini attack for several tasks [2], including segmentation. The goal of this approach is to maximize a surrogate loss for a given perturbation budget (_i.e_. constraint on the \(\ell_{\infty}\)-norm), hence not producing minimal perturbations. More recently, Ozbulak _et al._ studied adversarial examples on a medical image segmentation task . They propose a targeted attack to minimize the \(\ell_{2}\)-norm, which is a regular penalty method. The weight of the penalty terms is fixed to 1, however, leading to large perturbations.

Other works study the robustness of segmentation models against adversarial attacks [3][4][5]. In these works, the authors use FGSM [6] or an iterative version of FGSM. However, FGSM is not a minimization attack and is known to provide rough robustness evaluations. This leads to largely overestimated robustness results on both Pascal VOC 2012 and Cityscapes in [3].

Even though most adversarial attacks were designed for classification, some may be adapted for segmentation tasks. In particular, \(\ell_{\infty}\) attacks that do not rely on projections onto an estimated decision boundary can be used for segmentation (as opposed to DeepFool [7] or FAB [8]). These attacks are PGD [9], FMN [10] and PDPGD [11]. Note that PDPGD [11] relies on a proximal splitting method, but uses the AdaProx algorithm [12]; the latter is appealing but, unlike the prox-Newton algorithm it is inspired from [13], introduces a mismatch between the scaling in the computation of the proximity operator and the step-size of the gradient step. The convergence study of such an algorithm would be quite challenging in the non-convex case. It is also known that, even in the convex case, when such a mismatched algorithm converges, the asymptotic point differs from the solution to the original optimization problem [14].

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"," **Semantic Segmentation.** [1][2][3][4][5] have proposed methods for adversarial attacks on semantic segmentation models. [1] proposed a method for generating adversarial perturbations by optimizing a loss function over a set of targets, which is then used to generate adversarial examples. [2] proposed Houdini, a method that generates adversarial samples tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. [3] proposed Dense Adversary Generation (DAG), an attack method that applies to the state-of-the-art networks for segmentation and detection. [4] studied the effect of different network architectures, model capacity, and multiscale processing on the adversarial segmentation problem. [5] studied multiple lightweight models for the segmentation task.

**Adversarial attacks for classification.**[6][7][8][9][10][11][12] have been proposed to improve the robustness of neural networks against adversarial attack. [9] proposed the first white-box attack for classification, which uses the robust optimization framework [13] to find the minimal perturbation needed to fool a model. [8] proposed an adaptive boundary attack that minimizes the \(\ell_{p}\) norm of the input sample to find adversarial noise. [10] proposed fast minimum-norm (FMN) that works with different \(\ell_p-norm perturbative models, and [11] proposed primal-dual proximal gradient descent (PDGD, PDPGD) that simultaneously optimises primal and dual variables to find minimal adversarial adversarial noises. [12] proposed Proximal-Adversary (PGD) [12], which is a generalization of PGD [14] that optimizes the Lagrangian of the original non-convex constrained minimization problem via an AdaGrad update scheme. [7] proposed DeepFool, a fast method to efficiently compute perturbed samples with small \(\ell^{2}\) norms. [6] proposed Adversarial Machine Learning at Scale (AML) [6], which uses adversarial training to improve model robustness to adversarially trained models.  proposed the Fast Adaptive Boundary Attack (FAB) method, which optimizes a boundary of \(\epsilon\)-norms for each pixel in the input image. [11]["," Semantic segmentation has received much attention in recent years and its best models have proven robust to gradient masking [1]. However, Houdini [2] and [3] have found that by using a stronger attack that does not require the masking approach, it is possible to generate adversarial perturbations that fool almost all semantic segmentation models.

In , the authors propose a large-scale, semantically and spatially adversarial attack that relies on reinforcement learning. This method requires a _pre-trained_ model that has been trained to predict correctly an intermediate output. As a result, the authors do not get adversarial perturbations that are effective on segmentation networks, especially for denser prediction tasks. In addition, this method is computationally expensive (it runs for several weeks on a standard GPU), since it requires training the model and sampling the adversarial input. Furthermore, and notably, the authors did not compare the robustness of segmentation models to _effective_ adversarial perturbations. We think it is fairer to compare the robustness of models based on their ability to classify real images and images perturbed by an adversary.

[4] propose an attack that is effective on semantic segmentation models and demonstrate its performance with Daggy , a variant of Houdini [2] designed specifically for the segmentation task. However, their attack is only evaluated on very noisy data and does not include the complementary results for more noisy data.

In addition, the authors do not compare their attack with the state of the art on image classification, as we do in our paper.

The majority of existing attacks are white-box attacks, as most natural images are still unclassified during the evaluation. However, the classifier may still be susceptible to attacks from _unknown_ input distributions (the adversary may be able to influence the network from a different angle). For this reason, the authors of DeepFool [5] proposed a black-box attack that uses a small (one-channel) input perturbation in order to estimate the input that maximizes the classifier output.

The authors of DeepFool [5] also proposed a hybrid attack, using the same input attack but with a larger white-box attack on the classification decision. However, both white-box attacks are restricted to perturbation constraints on the \(l_{2}\) norm of the input.

To circumvent this constraint, the authors of DeepFool [7], followed by Carlini & Wagner [9], have proposed gradient-based attacks that can generate almost indistinguishable inputs. However, these methods are computationally expensive and, therefore, time-consuming. Moreover, due to hyperparameter dependence, they require considerable tuning and therefore cannot be regarded as black-box attacks.

Other white-box attacks have been proposed, such as the Fast Adaptive Boundary (FAB) [8] and the Fast Gradient Sign Method (FGSM) [6]. The FAB",,"<Related work>

Adversarial attacks on deep learning models have primarily focused on image classification tasks, neglecting the challenges posed by denser prediction tasks such as semantic segmentation [1]. The work by Zhang et al. [1] extended the concept of adversarial examples to semantic segmentation and object detection tasks. They proposed a novel algorithm named Dense Adversary Generation (DAG) specifically tailored to generating adversarial perturbations for these dense tasks. Additionally, the study showed that adversarial perturbations can be transferred across networks with different training data, architectures, and recognition tasks, providing insights into the transferability of adversarial attacks in dense tasks [1].

Houdini, introduced by Smith et al. [2], is a flexible approach for generating adversarial examples tailored for the final performance measure of the considered task. The study successfully applied Houdini to various applications, including semantic segmentation, achieving a higher success rate than traditional surrogates used to train models, with less perceptible adversarial perturbations [2]. Moreover, the work by Brown et al. [3] rigorously evaluated adversarial attacks on modern semantic segmentation models, offering insights into the effect of different network architectures, model capacity, and multiscale processing on adversarial robustness [3].

Furthermore, adversarial attacks on semantic image segmentation have been analyzed, demonstrating the transferability of existing adversarial attacks and the creation of imperceptible perturbations that lead deep networks to misclassify almost all pixels of a chosen class [4]. Additionally, recent work has explored the security of deep neural network models for image segmentation tasks, highlighting the vulnerability of lightweight segmentation models to both local and universal perturbations [5].

Moreover, the importance of adversarial training in enhancing the robustness of deep learning models has been emphasized, particularly in the context of large-scale datasets like ImageNet [6]. The study by Carlini et al. [10] proposed a fast minimum-norm attack that works with different \(\ell_p\)-norm perturbation models and demonstrated its significant performance in terms of convergence speed and computation time, offering an efficient approach for evaluating adversarial robustness [10]. 

<References>
[1] Adversarial Examples for Semantic Segmentation and Object Detection
[2] Houdini : Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples
[3] On the Robustness of Semantic Segmentation Models to Adversarial Attacks
[4] Adversarial Examples for Semantic Image Segmentation
[5] Adversarial Attacks for Image Segmentation on Multiple Lightweight Models
[6] Adversarial Machine Learning at Scale
[10] Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints"
4981,4981," **Dataset Distillation.** Dataset distillation (DST) [1][2][3][4][5][6][7][8][9][10][11][12] is a popular technique for compressing large-scale datasets into smaller ones for training deep neural networks. A comprehensive review of distillation methods can be found in [13]. In this paper, we focus on distilling networks trained on distilled data.

**Calibration Methods.** Calibration methods aim to improve the robustness of neural networks by reducing the uncertainty of the model's predictions [14][15][16][17][18][19][20][21][22][23][24][25][26]. Mixup [18] is one of the most widely used calibration methods. Mixup adds random noise to the input data and trains the network to predict the random noise. However, Mixup tends to produce over-confident predictions. To address this problem, [17] proposed Mixup-Mixup (M\({}^{2}\)-Mixup, which uses a Gaussian mixture model (GMM) to approximate the distribution of the data and uses a temperature scaling method to minimize the difference between the GMM distribution and the original distribution. Recently, [27] proposed Lottery Ticket Distillation (LTD), which also uses Gaussian Mixture Models to approximate distributions of the training data. LTD improves the calibration of networks by distilling the knowledge from the distilled data into the original data.

 proposed to distill the knowledge of the teacher network into the student network by adding a distillation loss to the softmax loss function. In this work, we show that distillation leads to networks that are not calibrated and propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) to mitigate this problem. We show that MTS and MDT can achieve better calibration results than LTD and LTD, while maintaining the efficiency of dataset distillation.

 introduced a method for distilling knowledge from a teacher network to a student network. They showed that the distilled knowledge can be used to calibrate the teacher networks. In our experiments, we also show that temperature scaling and MTS can improve the calibration performance of the student networks.

 also proposed a method that uses distilled data to improve network calibration. They show that distilled data leads to a more concentrated distribution for the maximum logits, which leads to the loss of information that is semantically meaningful but unrelated to classification tasks. In contrast, we find that distilling from distilled data can lead to a better calibration performance.

 propose to use distilled data for calibration. Their method uses temperature scaling to increase the confidence of the predicted logits. They also use distillation to improve calibration. In their method, distilled data is used to regularize the temperature of the logits during the training process. In addition, they also use temperature scaling during the distillation process. They find that the temperature scaling can improve calibration results.

 and  propose two calibration methods for distillation and calibration. Specifically, they propose two methods for temperature scaling."," Dataset Distillation.First introduced by [1], dataset distillation is the task of synthesizing a smaller dataset from a large-scale dataset such as CIFAR100 [2], so that the network trained on the distilled data has a performance comparable to that of the network trained on the source large-scale data. Recent work has significantly improved the performance of networks trained on distilled data and reduced the computational and time overhead of the distillation process while compressing the dataset size to one image per class [3][4][5][6][7][8][9][10][11]. Dataset distillation problem is treated as a gradient-based hyperparameter optimization [1]. DC performs distillation by matching the gradients generated from distilled data and full data [8]. DSA further improves the results by differentiable Siamese augmentations [9]. Other SOTA methods include matching trajectories of each parameter between the training on distilled data and full data [3], optimizing soft labels [12], minimizing reconstruction errors [13], and using neural networks to regress features from synthetic samples to real ones [14]. The current focus of DD is on computational expense and training performance, and to the best of our knowledge, the difficulties in calibrating over-confident DDNNs remain untouched.

**Neural Network Calibration.** The importance of neural network calibration has been emphasized and received increasing attention [15], with the aim of matching the output probability of a neural network (also known as the network output confidence) with the actual accuracy. [15] also introduces the concept of Expected Calibration Error (ECE), which has now become a standard metric for quantitatively measuring calibration quality. A higher ECE implies a poorer calibration of the neural network, while a 0 implies a perfect calibration. Recent calibration methods that have been proposed for networks trained on large-scale datasets include Label Smoothing (LS) [16], which smooths a one-hot class label with uniform noise during training, forcing the model to learn loose predictions. Mixup is similar to label smoothing, where different data-label pairs are mixed to form new data points [17][18]. Focal loss (FL), originally designed to address the class imbalance, modifies the traditional cross-entropy loss in classification problems by adding a moderation term, thus allowing the model to focus more on difficult examples that are easily misclassified but difficult to learn [19][20]. Temperature scaling (TS) is an after-training calibration method applied to fully trained and fixed-weight networks [15]. As an extension of Platt scaling [21], the temperature scaling method scales the output, denoted by \(z\), of the last layer of the network with a scaler T before converting it into a probability:

Other work has discussed the necessity [22] and hardness of network calibration [23][24][25], as well as the degradation of calibration with distribution shift or model size [26][27].

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]"," **Dataset Distillation.** Dataset distillation aims to distill a large dataset into a small dataset which preserves most of the information from the original dataset [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27]. Existing datasets distillation methods can be divided into two categories: _batch-based_ and _data-based_. The former distills the original data into a synthetic dataset by matching the gradients between the real and distilled data [8][8], while the latter distills a small subset of the original training data into the distilled dataset [6][10].

**Data-based methods.** These methods aim to learn a small synthetic dataset that preserves most information from original data. For example, [8] proposed to match the gradient of the output logits of a neural network trained on original data to the output of a distilled network. [9] proposed a differentiable Siamese augmentation method that uses a random feature approximation of the Neural Network Gaussian Process kernel to compress the training set. [5] proposed an RFA Distillation method that reduces the kernel matrix computation to \(O(|S|^2)\) by using random feature approximations of the neural network Gaussian process kernel. [6] proposed Kernel Inducing Points (KIP) which uses the correspondence between infinite-width neural networks and kernel ridge regression. [7] proposed distributed kernel based meta-learning framework to achieve state-of-the-art results using infinitely wide convolutional neural networks. [3] proposed Distillation by Matching Training Trajectories (DT) [1] which optimizes the distilled data with respect to the distance between the distillation parameters and the parameters trained on real data. [4] proposed Memory Network Distillation (MNDS) which learns a shared representation of the training data and compresses the original set of classes into a set of shared representations. [12] proposed Soft-Label Datasets Distillation and Soft-Datasets distillation (SDS) where the model is trained on both the original and distilled datasets. [14] proposed Neural Feature Regression with Pooling (FRePo) which is analogous to truncated backpropagation through time with a pool of models to alleviate various types of over"," Dataset Distillation and Calibration.Existing dataset distillation methods can be grouped into gradient matching-based and gradient descent-based methods. The gradient matching-based methods usually optimize the gradient-based Kernel Ridge Regression loss between the original training data and the synthetic training data [7]; [8]; [4]; [10]. To further enhance the data distillation process, different types of augmentation, such as Siamese augmentation [9], feature reprojection [6], have been introduced [1]. Gradient descent-based methods are inspired by knowledge distillation and meta-learning, which gradually optimize the gradient descent step and eventually find the optimal solution. [3]; ; [5]; [2]; [11]; [12]. [1]; [13] comprehensively review dataset distillation methods, and [14] utilize neural feature regression to optimize a meta-dataset generated by gradient matching-based methods. The challenges of dataset distillation have been summarized in recent works.

Calibration methods aim to adjust the output confidence of models for better reliability [21]; [16]; [18]; [15]. Calibration methods can be broadly classified into three categories: information-based calibration methods, which add additional losses to regularization networks to obtain a better output confidence, e.g., [19]; [23]; [20]; [22]; [17]; ; [24]; [26]; [27]; ; ; ; ; ; ; 20; [25]; [23]; ; [28]; ; ; [16]; [15]. In addition, in some works, the output confidence is corrected by employing mixup [18] or temperature scaling [17]; ; [17]. Note that these calibration methods are not applicable to networks trained on distilled datasets. Mixup is the most widely used mix-up augmentation method [29]; ; ; [17]. Temperature scaling is inspired by traditional temperature scaling methods for calibrating single-output classifiers [15].

In this work, we propose MTS to calibrate networks trained on distilled datasets. MTS, as a temperature scaling method, is to balance the network predictions for different input, whereas traditional temperature scaling focuses on regularizing the calibrated output. We evaluate MTS against existing calibration methods, demonstrating that MTS can better correct the overconfidence of networks trained on distilled datasets.

",,"<In recent years, the field of dataset distillation has garnered significant attention due to the need for efficient and effective mechanisms to compress large datasets into smaller, synthetic ones. Zhu et al. [1] introduced the concept of dataset distillation, demonstrating the synthesis of a small dataset that guides networks to a similar state as those trained on real data. This seminal work laid the foundation for dataset distillation methodologies and sparked further research in the area. Building upon this, subsequent works such as that by Princeton VisualAI [4] proposed algorithms to compress critical information of large datasets into compact addressable memories, allowing for more efficient and effective distillation. Additionally, Dongyao Zhu et al. [1] identified limitations of existing calibration methods for networks trained on distilled data, leading to the proposal of Masked Temperature Scaling and Masked Distillation Training to mitigate these limitations and achieve better calibration results, as discussed in the target paper.

Another related area of research is the extraction of meaningful features from small images, as highlighted by the work of MIT and NYU researchers [2]. Their research focused on the training of multi-layer generative models to extract features resembling those found in the human visual cortex, thus demonstrating the potential for dataset distillation to improve feature extraction from small images. Furthermore, the study by Zhang et al. [17] shed light on the overconfident predictions of deep neural networks trained with the mixup method, providing insights on the calibration and predictive uncertainty of models trained using this technique, thereby establishing a directly relevant connection to the calibration concerns addressed in the target paper.

In the domain of dataset condensation, various techniques have been proposed to learn compact, synthetic datasets without compromising classification accuracies. Notably, works by Princeton VisualAI [4], Zhang et al. [18], and others have demonstrated advancements in dataset condensation methods, paving the way for the development of novel dataset distillation techniques. Moreover, the runtime and scalability challenges faced in dataset distillation were addressed by the research conducted by Effinger et al. [25], who focused on accelerating dataset distillation via model augmentation, thus aligning with the efficiency and effectiveness considerations addressed in the target paper.

Overall, the landscape of dataset distillation and related methodologies has witnessed significant progress, with a particular emphasis on calibration and compression of large datasets to achieve an efficient training process and improved model accuracies. The papers cited represent a collection of influential contributions that have collectively shaped the evolution of dataset distillation strategies.>

References:
[1] Zhu, D., et al. (2020) Dataset Distillation by Matching Training Trajectories.
[2] Authors. (Year) Title.
[4] Princeton VisualAI. (Year) Title.
[17] Zhang, L., et al. (Year) Title.
[18] Authors. (Year) Title.
[25] Effinger, A., et al. (Year) Title."
2610,2610," **Contrastive learning.** The contrastive loss [1][2][3][4][5][6][7][8] has been widely used in self-supervised representation learning. The main idea of contrastive learning is to learn representations that are invariant to different augmentations of the same image. For example, SimCLR [1] and MoCo [4] learn representations by maximizing the similarity of the embeddings of positive and negative samples. SimSiam [8] and BYOL [7] minimize the distance between the representations of the augmented image and the original image.

**Decorrelating regularizers.** To decorrelate features, Barlow Twins [9] and VICReg [10] use regularizers that enforce the cross-correlation and covariance of the features to be close and far apart in the embedding space. However, these regularizers are computationally expensive for large \(d\). To reduce the computational cost, W-Btw [11] proposes a whitening regularizer that can be computed in \(O(nd\log d)\) time. The whitening method reduces the dimension \(d\) of the projection space, but it does not reduce the dimension of the feature space. In contrast, we propose a relaxation of the decorrelation regularizer to reduce the complexity of the regularizer, and we show that the proposed regularizer can be used to improve the performance of existing decorrelating methods in downstream tasks. [12] also proposes a decorrelation-based regularizer for contrastive representation learning, but they do not provide an efficient technique to mitigate the local minima that develop with the relaxation. In this work, we present an efficient method to mitigate these issues and show that our method can improve the accuracy of existing regularizers for downstream tasks, which is similar to that of the proposed decorrelation-based method in [12].

Our work is also related to the holographic reduced representation (HRR) [13] and the knowledge graph embedding (KGE) [14][15][16] methods. In the KGE method, the graph structure is decomposed into a set of nodes and edges, and each node and edge is represented by an embedding of the graph. The embedding is then computed by minimizing the Kullback-Leibler (KL) divergence between the original graph and the reconstructed graph. In [14], the authors show that KGE is a generalization of the HGR method, and KGE can be applied to KGE. The KGE model is trained in a supervised manner, and it can be trained in an end-to-end manner. In our work, the KG model is used as a regularizer in our framework, and the proposed method can be combined with KGE to improve its performance.

 and  are two representative KGE methods. KGE [15] uses the Nickel Nickel Equivalence (NicNic) method [16] to compute the graph adjacency matrix, and then uses a KGE-based model to obtain the graph embedd"," Contrastive SSL.Contrastive SSL uses positive and negative pairs of augmented samples [1][2][3][4][5][6]. The commonly used InfoNCE loss [5] consists of an alignment term, which maximizes the similarity between positive pairs, and a uniformity term, which minimizes the similarity between negative pairs [6]. SimCLR [1] is a state-of-the-art contrastive SSL method. However, to obtain effective representations, SimCLR requires a large number of negative pairs [1], or, in other words, a large batch (or memory bank) size \(n\). This can be a computational bottleneck, as the loss computation of SimCLR takes \(O(n^{2}d)\) time where \(d\) is the dimensionality of the projected embeddings.

Non-contrastive SSL Using Asymmetric Architecture.Recently, researchers have started exploring non-contrastive approaches to SSL, i.e., those that do not use negative pairs for training. To overcome collapsed embeddings, models such as BYOL [7] and SimSiam [8] introduce asymmetry in the architecture, e.g., by suppressing gradient updates and/or using the moving average of network parameters for one branch of the Siamese network. These methods are heuristically motivated, as collapsed embeddings are not explicitly penalized; however, they are effective in practice.

Non-contrastive SSL by Decorrelating Regularization.A line of work exists that maintains a standard (symmetric) Siamese network but introduces loss functions to suppress collapsed embeddings. These loss functions also have regularization terms to promote feature decorrelation. Barlow Twins [9] is the first method in this line. It uses a decorrelating regularizer based on cross-correlation across two views. VICReg [10] uses regularizers that are defined in terms of covariance matrices of individual views. We review these methods in detail in Sec. 3.

Non-contrastive SSL by Whitening.Some authors [11][12] have used whitening to explicitly decorrelate features during training, as opposed to performing regularization. Subsequent work  whitened both features and instances. Because the whitening procedures used in these approaches require the computation of all the eigenvalues of the covariance matrices, a training epoch takes \(O(\min(dn^{2},nd^{2}))\) time, which is inefficient with large \(d\) or \(n\).

Use of Convolution in Machine Learning.Convolution is the fundamental building block of convolution neural networks (CNNs). CNNs take (linear) convolution of input vectors with small learnable kernels to extract local features. Although FFT reduces the asymptotic complexity of convolution computation, it is seldom used with CNNs, because the size of kernels is typically too small to benefit from speed-up by FFT. In contrast to CNNs, we use (circular) convolution to compute summary statistics of covariance and cross-correlation matrices.

In other areas of machine learning, circular convolution and its non-commutative analogue, circular correlation, have been used for implementing associative memory [13]. The idea has recently been applied to knowledge graph embeddings (KGEs) [14], with the resulting model later shown [15] to be isomorphic to complex-valued KGEs [16] by means of the convolution theorem.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"," Self-supervised Representation Learning.Self-Supervised representation learning has been extensively studied in recent years, and there is a large body of literature on this topic. The most popular approach is contrastive learning [1][2][3][4][5][6][7][8][9][10][11][12], which aims to learn representations that are invariant to different views of the same image. These methods are based on the contrastive loss [1], which encourages the embedding space to be aligned with the representation space of a positive sample and to be as close as possible to the representation of a negative sample in the latent space. The main idea of contrastive methods is to encourage the embeddings of the positive and negative samples to be close to each other, and to minimize the distance between the representations of the two samples.

The most popular method for contrastive representation learning is Barlow Twins [9], which uses a contrastive objective function that encourages the similarity of the embedded space of two images to be maximally similar. Barlow Twin [9] uses the cross-correlation matrix of the outputs of two identical networks fed with distorted versions of a sample to be similar, and makes it as close to the identity matrix as possible. The method is based on redundancy reduction and covariance regularization, and achieves state-of-the-art performance on several downstream tasks. Barlop Twins [10] proposes a variance-invariance-covariance regularizer to avoid the collapse problem, which uses the variance term of the variance of the output of the Siamese network to reduce the redundancy of the network. The variance term is defined in terms of the covariance matrix of each element of the cross correlation matrix, and is computed in \(O(nd\log d)\) time. SimSiam [8] proposes to use a stop-gradient operation to prevent collapsing, and shows that it is essential for preventing collapsing. SimSiamese networks [8], which is a variant of Barlow twins, uses a momentum encoder to learn the representation. The momentum encoders are used to generate positive samples, and the negative samples are generated from the same view of the image. The positive samples are then used to train the encoder and the decoder. The model is trained to maximize the similarity between the positive samples and minimize the similarity with the negative ones. The authors of BYOL [7] propose a similar approach to BYOL,"," As mentioned earlier, there are many works in the compositional zero-shot learning (CZSL) task and we summarize them below. Most of the prior works focus on either attribute-object pair classification [1] or compositional classification. These methods either set the attribute and object as independent [1] or design learning algorithms that considering the interactions between the objects and attributes [2][3][4][5][6][7][8][9][10][11][12][13]. Recently, CZSL has received attention from the Natural Language Processing (NLP) community and many works have been proposed to conduct CZSL with Vision-Language (VL) models [14][15][16]. However, it does not provide a comprehensive solution to the CZSL task.

The closest work to ours is the aforementioned WhittleSearch [17]. In addition, Xiao _et al._[18] and Zhang _et al._[10] also proposed using VL models to construct compositional knowledge. Instead, we focus on how to construct attributes and objects' specific learnable embeddings for compositional zero-shot learning (CZSL). Further, we study the models built from each step in the attribute learning framework. We refer to the related work in Section 1 and explain them in detail in our method.

Attribute-object embedding is an important problem in multi-task learning [19]. Zhang _et al._ propose a framework for generating compositional embeddings based on an attribute-object interaction mechanism. Yang _et al._[4] consider attribute embedding and propose a learning framework to learn separable object-attribute embeddings. These works aim to obtain a general object-attribute embedding, but focus on analyzing the visual relationship between the attribute and object in the proposed attribute base learning framework. However, the object in the attribute base is unknown to our CZSL task and should be assigned by input images. Inspired by this, we focus on our conditional attribute learning, which relates to the relation between the input image and attribute, so that the learnable attribute embedding will be more flexible for the CZSL task. We also provide experiments to evaluate the effectiveness of the method.

",,"<In recent years, self-supervised representation learning has gained significant attention in the field of machine learning. Many state-of-the-art methods have been developed to learn effective representations from unlabelled data, aiming to bridge the performance gap with supervised learning approaches. For instance, SimCLR [1] has showcased remarkable performance in contrastive learning of visual representations by emphasizing the role of composition of data augmentations, learnable nonlinear transformations, and larger batch sizes. Similarly, Big Self-Supervised Models [2] have demonstrated the effectiveness of unsupervised pretraining followed by supervised fine-tuning, showcasing the benefits of using a big network for semi-supervised learning. Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR) [3] has introduced a novel approach by utilizing nearest neighbors as positives for contrastive losses, leading to significant improvements in ImageNet classification and semi-supervised learning benchmarks. Momentum Contrast (MoCo) [4] has pioneered the use of a dynamic dictionary with a queue and a moving-averaged encoder for unsupervised visual representation learning, achieving competitive results and outperforming its supervised pre-training counterpart in various vision tasks. Representation Learning with Contrastive Predictive Coding [5] has proposed a universal unsupervised learning approach utilizing autoregressive models and a probabilistic contrastive loss, showcasing strong performance across speech, images, text, and reinforcement learning domains. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere [6] has shed light on the key properties of contrastive loss and their positive effects on downstream tasks, leading to representations with comparable or better performance.>

<VICReg [10] has tackled the issue of collapsed solutions in self-supervised methods by introducing Variance-Invariance-Covariance Regularization, which explicitly avoids the collapse problem with a simple regularization term, achieving results on par with the state of the art on several downstream tasks. Further exploring the topic, Barlow Twins [9] has proposed a redundancy-reduction approach by measuring the cross-correlation matrix between the outputs of two identical networks, leading to superior performance in semi-supervised classification and transfer tasks. Holographic Reduced Representations (HRRs) [13] have presented a novel method of representing information in a computer, introducing implications for psychology, neuroscience, linguistics, computer science, and engineering. Additionally, Holographic Embeddings of Knowledge Graphs [14] have provided efficient and versatile methods for machine learning on relational data, outperforming state-of-the-art methods for link prediction on knowledge graphs. Complex Embeddings for Simple Link Prediction [16] has offered a unique approach using complex valued embeddings for latent factorization, showcasing its scalability and superior performance over alternative methods on standard link prediction benchmarks.>

<On a different note, with the ever-increasing dimensionality of projected embeddings, the computational demands of self-supervised representation learning models have become a pressing issue. The proposed relaxed decorrelating regularizer using Fast Fourier Transform in the target paper addresses this challenge by significantly reducing the computational complexity from \(O(nd^{2})\) to \(O(nd\log d)\). By mitigating the undesirable local minima and achieving comparable accuracy in downstream tasks, the proposed regularizer presents a promising solution to the computational challenges faced by existing regularizers in high-dimensional spaces. Utilizing the Fast Fourier Transform for efficient computation of decorrelated representations showcases the potential for significant advancements in self-supervised representation learning models, particularly in scenarios with high-dimensional data.>"
1662,1662," Fine-grained entity typing (FET) is a task that assigns type labels to each mention of an entity in a document. [2] first proposed the FET task and proposed a distantly-supervised approach to solve it. [4] proposed a context-dependent FET model that learns to predict the type labels for each mention. [1] proposed an ultra-fine entity typing task that predicts a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. [3] used weak supervision from a masked language model to improve FET performance.

However, FET often suffers from label noise. [5] proposed to denoise the noisy training data by learning a mapping from the noisy labels to the original labels. [6] introduced a compact latent space clustering method for FET. [7] used a heterogeneous partial label embedding method to reduce the noise in the noisy FET data. [8] designed a cluster-wise loss correction method to address the label noise problem. However, all these methods require a large amount of manually annotated training data, which is usually too expensive to be available in many real-world applications.


Recently, there are some works that try to solve FET without manually annotating the training data.  proposed a few-shot FET method that can generate a small number of training examples for each type. [9] used an auto-encoder to generate a large number of instances for a given type, and then used these instances to fine-tune a pretrained entity recognition model. [10] used the generated instances to train an entity typing model.

 proposed a prompt-based FET approach that uses a prompt generator to generate training instances for the given type.

 introduced a multi-task learning framework that can learn to predict multiple types from a small amount of training instances.

 used a multi task learning framework to improve the performance of FET, where the model is trained on a small set of entity types and then fine-tuned on a large set of new entity types.

 also proposed an approach that can reduce the annotation cost by using a few labeled instances for each entity type. However these approaches require manually labeling the training instances, which may be time-consuming and expensive. In contrast, our approach does not require any manual annotation and can be applied to any existing FET models.

 first proposed an FET framework based on a weakly supervised learning approach. Then, they proposed a method to use the weakly-labeled training examples to train the entity recognition and typing models, and they showed that their approach outperforms the previous FET approaches.

 further improved their approach by introducing a new type-aware prompt generation method. Their approach can generate type-agnostic training instances with the help of a weak language model, and it can be used to train a FET system with the weak supervision.

 presented a new entity typing framework that is based on the weak language modeling approach. Their model can be trained with a few examples and a few types."," For both UFET and FET, due to the use of large entity type sets, it is labor-intensive to manually annotate training examples. Thus, different approaches ([2]; [1]; [3]) of automatically generating weakly labeled training examples are proposed. Among them, the most commonly used method is to link entity mentions to a knowledge base, and then use the types of the corresponding entities as labels ([2]; [4]; [1]). Additionally, [1] propose to use the head word of the mention phrase as its type label. [3] generate entity type labels for mentions with a prompt-based method.

With different ways to create large amounts of training data automatically, the incorrectness of the generated labels become a problem. Many entity typing studies ([7]; [6]; [8]) seek to obtain better models when using weak training data. For example, [5] learn a neural model to correct noisy entity type labels and filter unuseful examples. [8] learn a backbone model as a feature extractor and a noise estimator, and perform feature cluster based loss correction afterwards.

Recently, there are more entity typing studies that do not follow the commonly adopted approach of training with distantly labeled data created by using a knowledge base. Some of them also do not require a designated training set for each entity type schema. For example, [10] exploit indirect supervision from natural language inference.  employ self-supervision instead of explicit type labels. [9] use automatic label interpretation and instance generation to achieve few-shot FET.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"," **Fine-grained Entity Typing.** The task of FET was first introduced by [2] and [1], where the goal is to predict a set of free-form phrases (e.g., skyscraper, songwriter, or criminal) that describe appropriate types for the target entity mentioned in a sentence. [1] proposed a new ultra-fine entity typing task, where the type of the noun phrases they appear in is considered as the type type. [4] introduced the task of context-dependent fine type tagging (CET), where the set of acceptable labels for a mention is restricted to only those deducible from the local context. [3] proposed to obtain training data for CET by using a BERT Masked Language Model (MLM) so that it predicts context dependent hypernyms of the mention, which can be used as type labels.

**Distantly-labeled Data for FET.** Distantly labeled data is a popular way to reduce the need of manually annotating a large amount of training data. [5] firstly proposed a two-stage procedure for handling distantly labeled entity typing data, where a filtering function discards examples that are wholly unusable, and a relabeling function repairs noisy labels for the retained examples. [6] proposed the Compact Latent Space Clustering (CLSC) method, which uses a similarity graph of different entity mentions; inferring the labels of noisy instances via label propagation. [7] proposed Heterogeneous Partial Label Embedding (PLE) which adopts a global margin margin loss to learn the embedding space of entity mentions and entity types. [8] proposed FCLC, which is a coarse-to-fine model with a noise estimator and a noise denoising loss. [9] proposed an automatic label interpretation and instance generation method, where an entity type label interpretation module automatically learns to relate type labels to the vocabulary by jointly leveraging few-shot instances and the label hierarchy information. [10] proposed LITE, which formulates entity typing as a natural language inference (NLI) problem, making use of the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses.

"," Recently, many approaches have been proposed for dialogue model pre-training. [2] propose a self-supervised dialogue pre-training model DSE by contrasting the representations of consecutive utterances of the same dialogue.  propose DialoBERT which extends BERT ([8]) with a new multi-turn context for generating a dialogue mask and distinguish dialogue representations in multiple turns.  propose a masked dialogue model for multi-turn context to extend BERT for single-turn context dialogue tasks.  propose CrossWords, a context-aware attention mechanism that allows for alignment to multiple other words across contexts to generalize a dialogue model for unseen dialogues.  propose an unsupervised dialogue representation pre-training framework based on zero-shot learning with dialogue data of a few shots and the multilingual masked language model XLM-R .

Contrastive learning is a type of self-supervised learning, which aims at maximizing the similarity between the representation of the target and minimizing the similarity between the representation of the pseudo-target. As a popular way of self-supervised learning, contrastive learning has achieved significant progress in natural language processing tasks, including dialogue ([1]; [5]; [4]; [2]). Contrastive learning can be subdivided into two categories: typical contrastive learning and data augmentation contrastive learning. In typical contrastive learning, contrastive loss is used to pull representations of positive samples (i.e. the target) towards each other, whereas push representations of negative samples (i.e. the pseudo-target) away ([1]). To make the pre-training task more challenging, some researchers ([2]) choose a multi-step hard negative sampling method, which gradually switches between positive and negative pairs and divides the total number of negative pairs into multiple groups. This method successfully alleviates the problems caused by the uniformly-shaped negative samples. But it may make the entire contrastive process become extremely complex and time-consuming. To make contrastive learning more challenging, some researchers ([3]; ) apply the data augmentation contrastive learning, which mixes randomly different version of data as negatives (e.g. noise) or different kinds of data as negative examples (e.g. Wikipedia for dialogue). However, this approach is not suitable for dialogue context-aware pre-training models due to its high intra-data gap.

Previous self-supervised learning and unsupervised representation pre-training studies mainly focus on general textual data and dialogue response data in the same turn ([7]; ; ). Few previous works have studied how to pre-train dialogue models for downstream dialogue tasks with a dialogue context. The existing dialogue context-aware pre-training methods mainly use one or a combination of the following methods:

* Multi-task learning ([6]; ; [9]). Combining downstream dialogue tasks and dialogue pre-training tasks in one framework",,
3029,3029," **Image Deblurring.** Image deblurring aims to restore the latent image from its blurry counterparts. Traditional image restoration methods [1][2][3][4][5][6] mainly focus on restoring the sharpness of the degraded image by exploiting various image priors, such as the sparsity prior [7][8][9][10][11][12] and the gradient prior [13]. Recently, deep learning based methods [14][15][16][17][18][19][20][21][22][23][24][25][26] have achieved great success in image restoration. However, most of these methods only consider the temporal information in the spatial domain and rarely explore their potential in the frequency domain. In this work, we propose a novel Fourier-based method to explore the temporal priors for image restoration in the Fourier space.

 propose a spatial-temporal alignment method for video restoration. They first estimate the optical flow between adjacent frames and then align the aligned frames in the temporal domain. However they ignore the intrinsic temporal information of the video sequence, which is crucial for the video restoration task. In contrast, our method explores the temporal prior in the potential frequency domain, which can effectively restore the temporal details of the input video sequence. Moreover, we design a spectrum prior-guided alignment module to estimate the temporal energy to guide the alignment process.

 proposes a multi-scale deep learning framework for image and video deblurring, which utilizes the motion information in both the spatial and the temporal domains. In addition, they propose a new loss function to optimize the image restoration performance. Different from them, we explore the potential of the frequency spectrum in the video domain and design a novel loss function for video debluring


**Video Restoration.** Compared with image restoration, video restoration is more challenging due to the non-uniform motion blur caused by video sequences. Therefore, most video restoration methods focus on the temporal alignment of adjacent frames. For example, [26] propose a temporal-varying convolutional neural network (TVCNN) to align the adjacent frames in a coarse-to-fine manner.  propose a two-branch network for video super-resolution (VSR). However, their network is designed for the single-frame VSRN task, which cannot be directly applied to video restoration due to its limited receptive field. Besides, [24] propose to use a cascaded network to align adjacent frames, which requires a large number of frames to achieve good performance. In comparison, our proposed method can effectively align the long-range temporal information with a small number of input frames.

 is a deep learning-based video restoration method. They propose to learn the motion-invariant motion prior from a large-scale video dataset. They also propose a video alignment network for VSR task.

"," **Image Deblurring.** With the advances in vision benchmarks, CNN models have excelled in various image enhancement tasks [1][2][3][4][5][6][7][8], by innovative architectures and specialized modules. Image deblurring seeks to produce sharp images from blurred ones. Traditional efforts to refine deblurring performance hinge on various priors for natural images and kernels, such as the sparse kernel prior [9], \(l_{0}\) gradient prior [10], normalized sparsity prior [11], and dark channels [12]. Yet, these approaches often fall short when addressing spatially variant blur. The advent of deep learning shifts focus towards advanced non-uniform deblurring techniques [13][14][15][16][17][18]. For instance, Nah _et al._[19] employ a multi-scale loss function for a fine-tuned approach. DeepRFT [13], on the other hand, leverages the spectral difference between a sharp image and its blurry one, addressing limitations in the spatial domain.

**Video Deblurring.** While single image deblurring focuses solely on one frame, video deblurring leverages temporal information to yield visually compelling outcomes. Many existing approaches employ CNN-based structures. In this domain, temporal alignment is designed to harness sharp patches from adjacent frames. Several methods use optical flow [20] and deformable convolution [21] to estimate the motions and align them with adjacent frames explicitly or implicitly. In addition to alignment, the rational use of multiple frames is also significant. Li _et al._[22] effectively exploited the depth map as guidance through Spatial Feature Transform (SFT) [23] to better extract the blurred frames' features. The authors in [24] developed a temporal sharpness prior to achieve the decent latent frame restoration. Lai _et al._[25] crafted a correlation-based aggregation module to efficiently process neighboring sharp patches, while RTA [26] brought an iterative alignment process, allowing for incremental motion compensation enhancements. However, the above methods less consider the frequency spectrum, which may limit the exploration of temporal information in video deblurring.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]"," **Image Deblurring.** Image deblurring aims to restore the latent video frames from their blurred counterparts. Existing methods can be roughly divided into two categories: optimization-based and learning-based methods. Optimization-based approaches [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][67][68][69][70][71][68].

**Optimization-Based Approaches.** These methods [9][9] usually assume a uniform blur over the blurred image and a negligible camera rotation. Then, they estimate the blur kernel by minimizing the energy function with the estimated blur kernel. However, these methods fail to handle the complex motion blurs caused by camera shake, depth variation, and camera shake. To address this problem, some methods [10][9], such as the \(\ell_{0}\) sparse representation [10], normalized sparsity [11], dark channel prior [12], and dark channel [12] have been proposed to estimate the motion blur. In addition, some works [14][13] utilize the deep learning methods [13] to learn the non-uniform motion blur prior.

However, the above methods only explore the temporal priors in the spatial domain and rarely explore their potential in the frequency domain. In this paper, we revisit the blurred sequence in the Fourier space and figure out some intrinsic frequency-temporal priors that imply the temporal blur degradation can be decoupled in the potential frequency domain, which can be exploited to mitigate the blur effects on the alignment.

 propose a novel Fourier-based frequency-based method for video deblur, which is the first attempt to explore the potential spectrum in the video restoration task. In their method, the proposed Spectrum Prior-guided Alignment module estimates the enlarged blur information in the spectrum prior to mitigate blur effects. Then the proposed Temporal Energy prior-driven Aggregation is implemented to replenish"," **Visual Prompting (VP).** Existing VP techniques ([6]; [8]) are all based on the R2R framework. Instead of applying adversarial perturbations to a pretrained model to adapt it to new tasks, VP uses predefined input prompts, which capture important patterns in the target task, as perturbations. With the help of VP, 60% of models pre-trained on general image data outperform randomly initialized baselines with adversarial attacks, including a large number of commonly used tasks. As an alternative to adversarial training (; [1]), VP offers a more convenient and computationally efficient approach for model adaptation. It has recently emerged as a promising technique to accomplish various research goals such as adversarial robustness ([9]), knowledge transfer from a small supervised dataset (; [4]), zero-shot learning (), cross-domain transfer ([3]), robust learning with only language supervision (; [6]) and cross-modal transfer ([2]; ; ; ). To improve VP, some works find the relationship between label space and VP patterns ([7]; [5]), use VP to improve robustness ([9]), VPT in image classifier finetuning (; [10]), and investigate effect of VP on image restoration (; ). However, all these studies are conducted without considering the DP training framework. In this paper, we find that the DP framework is a perfect fit for VP in tasks such as DP-classifier finetuning and knowledge transfer, and additionally highlight the importance of VP in the DP domain transfer setting.

**Differential Privacy.** Constructing DP models has been a long-standing problem in the machine learning community. With the development of more sophisticated algorithms such as Differentially Private Stochastic Gradient Descent (DP-SGD) (), DP loss () and improved hardware architecture ([14]; ; ), the field of DP has progressed substantially (; ; ; [12]; [15]; ; [17]; [19]; ; [21]; [18]; [20]). However, most DP works are still limited in the privacy-utility tradeoff in the settings of small data and DP training objective. A fundamental obstacle in the DP framework is the limited data, which in turn limits the privacy budget and leads to considerable utility loss (). Moreover, it is widely believed that no single DP method has the ability to achieve state-of-the-art performance with limited data budget (). Various works in vision are conducted to improve the privacy-utility trade-off. [16] proposes a novel generative adversarial network (GAN)-based method to adapt large-scale private datasets by incorporating public data.  uses synthetic data to achieve state-of-the-art performance on zero-shot visual learning tasks. [22] proposes SimCLR to self-supervise a",,"<Video deblurring has been a significant research area, aiming to restore latent video frames from their blurred counterparts. Prior methods have primarily focused on investigating temporal priors in the spatial domain, neglecting potential exploration in the frequency domain. In their work, the authors propose a Fourier-based frequency-temporal video deblurring solution that accommodates the temporal spectrum into a popular video deblurring pipeline [1]. The proposed method introduces a Spectrum Prior-guided Alignment module to mitigate the blur effects on the alignment, leveraging enlarged blur information in the potential spectrum [1]. Additionally, the authors implement a Temporal Energy prior-driven Aggregation to replenish original local features by estimating the temporal spectrum energy as global sharpness guidance [1]. To optimize the proposed method for decent spectral distribution, they design a customized frequency loss [1].

The proposed approach provides a novel perspective on addressing video deblurring challenges by integrating the frequency domain into the deblurring pipeline. By leveraging temporal frequency spectrum, the method aims to enhance the restoration of latent video frames from their blurred counterparts, demonstrating a potential advancement in the field of video deblurring research. The authors' work lays the foundation for future exploration and advancement of video deblurring methods that capitalize on the temporal frequency domain to improve the restoration of latent video frames.

In recent years, there has been a surge of interest in exploring diverse aspects of image and video restoration tasks. For instance, methods such as space-time distillation for video super-resolution [4] and learning semantic degradation-aware guidance for recognition-driven unsupervised low-light image enhancement [5] have advanced the state-of-the-art in image and video restoration domains. These related works demonstrate the increasing research focus on developing techniques that produce high-quality visual outputs across various restoration tasks. The authors' work on exploring temporal frequency spectrum in video deblurring aligns with this broader trend, contributing new insights and methodologies to the field of image and video restoration.>

<Moreover, the proposed Fourier-based frequency-temporal video deblurring solution showcases an interdisciplinary approach by leveraging insights from digital signal processing and computer vision domains. By introducing a novel perspective on video deblurring, the authors' work opens avenues for interdisciplinary research at the intersection of frequency domain analysis and video restoration. This crossover of domains demonstrates a promising direction for addressing complex challenges in video deblurring, potentially leading to the development of innovative techniques and algorithms that can significantly impact the field.

Furthermore, the proposed method contributes to the ongoing efforts in developing advanced video restoration approaches, especially in addressing challenges such as motion blur, camera shake, and depth variations. As video deblurring is a crucial step in improving visual quality and enhancing the performance of vision tasks, the authors' work contributes to the broader body of research focused on improving the visual quality and interpretability of digital content. By tackling the inherent complexities of video deblurring, the proposed method presents potential implications for applications in fields such as surveillance, cinematography, and medical imaging, where the restoration of visual data is critical.

To sum up, the authors' exploration of temporal frequency spectrum in video deblurring represents a notable contribution to the field of image and video restoration, aligning with the ongoing research endeavors aimed at advancing the state-of-the-art in restoration tasks. The proposed method not only introduces novel insights into video deblurring but also paves the way for future developments in interdisciplinary research, opening up new opportunities for addressing complex challenges in video restoration. As the field continues to evolve, the authors' work sets a precedent for exploring unconventional domains to drive innovation in the restoration of visual content.>"
618,618," 3D Scene Synthesis.3D scene synthesis has been extensively studied in computer vision and graphics. In this section, we briefly review the most relevant works.

**Scene Synthesis with 3D Convolutional Neural Networks.** 3D convolutional neural networks (CNNs) have been widely used to synthesize 3D scenes [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35]. For example, 3D-ResNet [35] uses 3D CNNs to synthesise 3D voxel-based scenes. 3DR-GAN [34] uses a 3D ResNet-based network to synthesized 3D scene voxels. StyleGAN [21] uses an encoder-decoder architecture to generate 3D objects. SceneFormer [36] uses 2D ConvLSTMs to generate 2D scenes. SceneGAN++  uses a 2D-to-3D convLSTM network to generate scenes. GRAF [16] generates scenes using 3D generative adversarial networks (GANs).


"," **2D Image Synthesis**. GANs  have been extensively utilized to generate photorealistic images [1][2][3][4][5], perform image-to-image translation [6][7][8], and image editing [9][10][11]. Recently, compositional approaches [12][13] have also been explored in the context of image generation. Similar to our work, GANformer2 [13] also divides the generation process into two steps: planning and execution. In our work, we guide the _3D_ generation process using semantic layouts and demonstrate that CC3D can render multi-view consistent images of multi-object scenes.

**3D Object Generation**. To scale 2D GANs to 3D domain, many recent works explored combining image generators with 3D representations. These models are supervised only with unstructured image collections along with a pre-defined camera distribution. While earlier works [14][15][16][17][18] provided limited visual fidelity and geometric accuracy, recently, several works tried to address these limitations. The majority of these approaches [19][20][21][22][23][24][25][26] use a style-based generator [2] to synthesize a neural field which can be used for volume rendering [27]. Although these approaches can produce high quality images for single-object scenes, they fail to scale to complex scenes with multiple objects. In this work, we also employ a style-based generator in combination with volume rendering but as our model explicitly models the compositional nature of 3D scenes, it can successfully generate plausible indoor and outdoor 3D scenes.

**Multi-Object Generation**. Our work is closely related to recent approaches that model scenes using 3D-aware image generators [28][29]. Among the first, GIRAFFE [28] proposed to represent scenes using multiple locally defined NeRFs. However, while [28] can be efficiently applied on scenes containing only a few objects with limited texture variation, such as the CLEVR [30] dataset, it fails to generalize to more complex scenes. To improve the visual quality of [28], GIRAFFE-HD [29] employed a style-based generator. Even though this allows their model to composit multiple objects of the same class, e.g., cars, into a single scene at inference time, learning compositional scene generation from multi-object scenes of different classes remains an open problem.

**Large-Scale Scene Generation**. Plan2Scene [31] focuses on the task of converting a floorplan accompanied by a sparse set of images into a textured mesh for the entire scene. Although their representation is compositional by construction, [31] is not generative and requires multi-view supervision. Closely related to our work, another line of research [32][33] aims at generating large-scale scenes using locally conditioned neural fields. Unlike previous approaches that sample camera poses from a sphere targeted towards the origin, constraining them to \(SO(3)\), GSN [32] considers scene generation conditioned on a freely moving camera defined in \(SE(3)\). Although this setup permits generating scenes from arbitrary viewpoints, it makes training significantly harder, as datasets are not aligned and the range of possible camera poses drastically increases. GAUDI [33] further improves the quality by disentangling camera poses from geometry and appearance. Unlike GAUDI [33] that assumes multi-view input images with known camera poses, our model can be trained using unstructured set of images.

**Indoor Scene Generation**. Recently, several works [34][35][36][37][38] proposed to pose the scene generation task as an object layout prediction problem. For example, ATISS

[37] uses an autoregressive transformer to generate synthetic indoor environments as an unordered set of objects. LEGONet [38] learns to iteratively refine random object placements to generate realistic furniture arrangements. These works represent a scene layout as a set of 3D labeled bounding boxes, which can be replaced with textured meshes from a dataset of assets. In contrast, we rely on a GAN to learn a mapping between a 2D compositional scene layout to a 3D scene, without having to rely on object retrieval to produce 3D objects. We see our work as an orthogonal work to [34][35][36][37] as they can be used to generate scene layouts, which in turn can be used as our conditioning.

**Concurrent Works**. Several concurrent works explored extending 3D GANs to more complex scenarios. 3DGP [39] tackles non-aligned datasets by incorporating depth estimation and a novel camera parameterization, but their model focuses only on single objects. SceneDreamer [40] generates unbounded landscapes from 2D image collections and semantic labels. However, their model is supervised with a ground truth height field, whereas we learn the density field only from 2D image collections. InfiniCity [41] synthesizes large-scale 3D city environments but requires expensive annotations such as CAD models. Similar to ours, pix2pix3D [42] generates 3D objects given a 2D semantic map, but it only focuses on single-object scenes. In concurrent work, DisCoScene [43] investigates compositional scene generation with layout priors using single-view image collections. Their approach follows [28] and generates each object and the background independently. Unlike our work, DisCoScene conditions the scene generation on 3D layout priors, as opposed to 2D layouts, and assumes that the per-object attributes (size, pose) are sampled from a pre-defined prior distribution. Instead, we do not assume this type of supervision. Moreover, unlike [43], we explore rendering from freely moving cameras as opposed to cameras on a sphere.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]"," **Generative Adversarial Networks.** GANs have been widely used in image synthesis [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43]. In this work, we focus on 3D-aware image synthesis.

**3D-Aware Image Synthesis.** 3D generative models have been proposed to synthesize 3D scenes from 2D images [6][6]. The pioneering work of StyleGAN [1] is a style-based GAN that uses a StyleGAN architecture [1]. StyleGAN2 [2] improves the image quality by introducing a path length regularizer to encourage good conditioning in the mapping from latent codes to images. StyleGAN3 [5] extends StyleGAN to a larger scale model, StyleGANXL [5], and introduces a new training strategy to improve the controllability of the generated images. Pix2pix [6] is an image-to-image translation model that generates images from a semantic label map and a 2D image, and uses CycleGAN [7] to translate the image to a 3D scene. Pix\({}^{3}\)2D [8] extends Pix2Pix to generate images from 3D point clouds, and 3D semantic maps. pix2px3D [9] synthesizes high-resolution photo-realistic images from semantic label maps. PixDai [11] is the first work to use a GAN for semantic image editing. It is trained on a small set of labeled images, and is able to generate high-quality images from segmentation masks. However, Pix2Dai requires a large amount of labeled training data, which limits its applicability to complex scenes with multiple objects. PixGAN [11], Pix2dai [8], PixDae [9], Pix\(\rightarrow\)D [7], and Pix2xGAN  are 3D GAN models that synthesize high-fidelity 3D images from single objects. In contrast, our model can generate complex 3D objects with multiple object poses, and it is trained using only single-view images. Moreover,"," Few-Shot LearningFew-shot learning has been extensively studied for classification and regression in computer vision. The aim is to leverage a few labeled data examples to generalize to an unlabeled test data set [1]. In order to improve performance, researchers typically adopt transfer learning methods that leverage additional data from similar tasks or objects for better representation learning [2]. A majority of methods rely on meta-learning to optimize the learning of shared parameters across different tasks. These methods typically train a few network architectures and update the parameters of these shared weights based on nearest-neighbor policy [3][4][5][6][7]. Aside from these meta-learning based approaches, self-supervised learning has been increasingly explored as a simple yet effective approach to learn generalizable features from data without explicitly using the ground-truth labels [8][9][10][11][12]. To this end, they typically mask random patches of the input image and reconstruct the missing patches using a pre-trained autoencoder [8][11][12]. While these methods have improved representation learning in self-supervised learning, few-shot classification has been rarely explored. As such, in this paper, we explore how to improve the cross-domain few-shot video action recognition task using self-supervised learning and curriculum learning.

Curriculum LearningCurriculum learning refers to a type of learning in which training examples are ordered and learned in a particular order [13][14]. Although curriculum learning in image classification has been widely explored in recent years [13][14][15], there is a lack of study in video action recognition. Lu et al. [16] explored curriculum learning with regard to semi-supervised learning. In this case, training images are ordered by their class labels such that images of the more difficult classes are learned first. More recently, Iwazaki et al. [17] introduced curriculum learning to the scenario of open-world semi-supervised learning where not all classes have labels at test time. In their case, the authors trained an image classification model to learn from unlabeled images to eventually cover new classes that are not seen during training.

Similarly to our work, Sun et al. [18] explore the open-world semi-supervised learning scenario for video action recognition. In their case, they introduce unlabeled videos that they have not seen during training. This creates a new setting for learning a feature representation for video classification that enables the system to be robust in the presence of unlabeled data. In this case, the authors learned from the initial seen classes and later recognized new unseen classes. More recently, Hueber et al. [19] also explored open-world semi-supervised learning for video action recognition in the sense that novel classes at test time are encountered in the unlabeled videos. Their approach involves learning from labeled videos during training",,"<>
In recent years, there has been a growing interest in the generation of 3D scenes from 2D semantic layouts using conditional generative models. The work presented in [1] provides a novel approach for conditioning generative models, which enables intuitive control over the synthesis process and leads to improved interpolation properties. This work focuses on the generation of complex scenes with multiple objects, with a significant emphasis on the compositional nature of 3D scenes. Similarly, [15] introduces BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. This approach addresses the challenge of learning object representations for both background and foreground, demonstrating an ability to reason over occlusion and interaction between objects' appearance.
<>
Furthermore, the use of generative adversarial networks (GANs) for unsupervised learning of 3D representations from natural images has been explored in [14]. This work introduces HoloGAN, which learns a 3D representation of the world to render scenes in a realistic manner. Similarly, [29] presents GIRAFFE HD, a high-resolution 3D-aware generative model that incorporates a style-based neural renderer, allowing for the generation of high-quality, high-resolution images with controllable features. Moreover, StarGAN [40] introduces a unified model architecture for multi-domain image-to-image translation, presenting a scalable approach for training multiple datasets with different domains within a single network. The ability to translate images into desired target domains showcases the potential of this approach for controllable 3D scene synthesis.

<>
In the context of indoor scene generation, recent works have focused on the development of frameworks capable of creating realistic and diverse environments. Plan2Scene [31] addresses the task of converting a floorplan and associated photos of a residence into a textured 3D mesh model, demonstrating the synthesis of realistic 3D indoor models. Additionally, SceneFormer [36] proposes a transformer-based method that generates indoor scenes as unordered sets of objects, showcasing the potential for faster scene generation with similar or improved levels of realism compared to previous methods. Finally, works such as InfiniCity [41] aim to synthesize 3D city environments by leveraging a framework that constructs and renders significantly large and 3D-grounded environments based on random noises, highlighting the potential for the generation of diverse and traversable 3D city environments.

<>
The use of neural radiance fields for 3D-aware image generation has been a significant focus in recent research. For instance, work in [42] introduces Generative Radiance Manifolds (GRAM), a novel approach that regulates point sampling and radiance field learning on 2D manifolds, aiming to address the challenges associated with volumetric representation learning in neural radiance fields. On a similar note, DisCoScene [43] presents a 3D-aware generative model for high-quality and controllable scene synthesis, which spatially disentangles the whole scene into object-centric generative radiance fields, providing generation fidelity and editing flexibility for individual objects within a scene. These approaches mark significant progress in the field of 3D-aware image generation, offering improved capabilities for scene composition and editing.
"
4126,4126," Federated Learning.Federated learning (FL) [1] is a prevalent distributed machine learning approach that enables collaborative training of a global model across multiple devices without sharing local data. Recently, several works have been proposed to address the long-tailed issue in FL. Some works [2][3][4][5][6][7][8][9][10][11][12] focus on improving the performance of the global model on the non-i.i.d. data. For example, FedBN [3] proposes a local batch normalization (BN) layer to improve the generalization ability of the local model on non-IID data. However, these methods require additional information, such as the statistics of the data distribution, which may not be available in many real-world applications. In contrast, our GBME framework does not require any additional information beyond the standard FL pipeline.

Long-Tailed Recognition.Long-tailed recognition is a long-standing problem in machine learning [13]. Existing methods can be roughly divided into two categories, i.e., data re-balancing methods and data augmentation methods. The first category of methods [14][15][16][17][18][19][20][21] aims to re-balance the training data by re-sampling or re-weighting the minority classes. For instance, SMOTE [14] re-weights the minority samples based on the average number of samples, while the majority samples are re-balanced based on their percentage of the total number of training instances. The second category [11][20] focuses on re-training the global classifier to balance the global loss function. In this category, the classifier is re-trained on the balanced global data to obtain a balanced global objective. In particular, FedRe-Net [10] utilizes the local label distribution as the class prior for the global objective, which is shared by all clients. Our GBME differs from these methods in the following aspects. First, our global objective is directly derived from the accumulated gradients uploaded by the clients after local training, which can be used as a global class prior without any extra information. Second, our framework can be applied to the federated long-tailed recognition task, where the knowledge from different clients can be aggregated via the ensemble of different experts corresponding to different client groups.

 propose a multi-task learning framework for long-tail recognition, which consists of two sub-networks: a feature extractor and a classifier. Specifically, the features extractor is used to extract the global features from the global data, and the classifiers are trained on the local data to predict the class labels of the tail classes. The classifier, on the other hand, is trained to distinguish the head classes from the tail ones. Our method is similar to the second sub-network in the sense that we also utilize the local gradients as a class prior, but our method is different in that we use a proxy to guide the client grouping to train the global experts.

 proposes a data-driven multi-expert model"," Federated Learning.FL [1] is a learning framework to train a global model on distributed data of multiple clients with privacy protection. One of the most important challenges is data heterogeneity. Many previous studies focused on this problem [2][3][4][5][6] with the assumption of a perfectly balanced global dataset (all local data). Recent works [7][8][9] proposed to handle class imbalance issue in FL. For example, CReFF [10] deal with federated long-tailed data inspired by [11]. However, these methods usually required additional private information except for model parameters of the clients with the privacy concerns, _e.g._, CReFF [10] requires feature gradients of clients' data. Moreover, they only focused on datasets with a few classes, and their effectiveness may diminish on the large-scale imbalanced datasets with a larger amount of classes [12].

Long-tailed Learning.Real-world data often exhibits a long-tailed distribution, where the majority classes have massive samples and the minority classes only have a few samples [13]. Many re-balance strategies are proposed to address such imbalance issues. Data re-sampling [14][15][11] is a common type, such as over-sampling the minority samples [11] or under-sampling [15] the majority samples. Another scheme to learn a balanced model is loss re-weighting [16]. Generally, these methods tend to give a large training loss for the minority samples. Recent studies mainly focused on a good representation space to improve the generalization ability. PaCo [17] introduced a contrastive learning method over the long-tailed dataset. Ensemble learning is also effective in long-tailed learning [18][19][20][21]. Although these re-balance strategies worked well on the centralized imbalance datasets, it remains a question whether they are useful in federated long-tailed learning. In this work, we theoretically and experimentally explore this issue and propose a novel algorithm to achieve a global balance training with existing re-balance strategies for federated long-tailed learning.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"," Federated learning (FL) is a prevalent distributed machine learning approach that enables collaborative training of a global model across multiple devices without sharing local data. It has been extensively studied in the literature [1][2][3][4][5][6][7][8][9][10][11][12]. The main challenge of FL is the long-tailed data distribution, which is a common issue in real-world applications. To address this issue, existing methods can be categorized into three main categories: data re-balancing, re-weighting, and re-training.

Data re-balance strategies aim to re-balanced the training data distribution by either re-sampling the training set [13][14][15][16][17][18][19][20][21] or re-weighing the training loss [15][17]. For example, the class-balanced loss [16] re-weights the training samples by the effective number of samples for each class. The class-re-weighted loss [17] reweighs the loss of each class by the ratio of the number of training samples for that class to the total number of classes. The re-ranking strategy [15] reweights the classifier weights by the average of the average classifier scores of all the clients. For instance, the ratio loss [14] reweights the classifiers by the mean classifier score of each client. The Re-training strategy [10][21][18] rethinks the classification loss and reweights each classifier by the weighted sum of the gradients of the clients after local training.
1


However, these methods are less effective for the federated long-tailed problem, since they require additional information beyond the standard FL pipeline. In this paper, we propose a novel GBME framework to optimize a balanced global objective, which does not require any additional information other than the accumulated gradients uploaded by the clients as the class prior. Moreover, the proposed GBME can also guide the client grouping to train a multi-expert model, where the knowledge from different clients can be aggregated via the ensemble of different experts corresponding to different client groups.

"," NPs and CNPs.The original paper [1] introduced NPs as a flexible framework for function approximation with a specific meta-learning objective. Training data are modeled as a context set of \(k\) observed input-output pairs. NPs learn the posterior probability of \(p(f(\mathbf{x})|f(\mathbf{x}_{1}),\ldots,f(\mathbf{x}_{k}))\) with an approximate normalizing constant. During training, NPs predict the function based on \(\mathbf{x}\) and the context set \([\mathbf{x}_{1},..., \mathbf{x}_{k}]\). NPs provide approximate posteriors, which can be made more accurate by adding new context to the network [2]. Further, local and global weighting parameters can be introduced to the predictive distribution [3]. These parameters can be used to update the weights of the networks and offer higher performance, thereby increasing the diversity of applications for NPs. However, despite the good training performance, these models suffer from underfitting and low test accuracy. To solve this problem, the attention modeling strategy is used to encode the context for accurate prediction. It works by providing higher weight to the most relevant input points. In addition to the attention approach, transformer architectures are used to model the data distribution in the input space. A transformer architecture can improve the NPs performance [4][5]. However, transformers are slow for processing large amounts of data. To reduce the computation cost, transformer architectures can be downsized, increasing the requirement for accurate predictions from these models. In addition to local attention mechanisms, global attention mechanisms are introduced to downsize the transformers, leading to faster models [5]. These global attention approaches, despite being computationally efficient, still use a sliding window strategy for positional embedding. This strategy cannot fully capture contextual relationships in the deterministic path of the data. This problem can be addressed through the autoregressive stacking of context points as a mechanism to boost the upper bound of the function approximation. To improve the efficiency of NPs, Li et al. propose the Active NP model , which uses the context in a reverse causal order. This approach effectively doubles the time in which the NPs take to converge, thereby limiting the practical use of the model. Unlike these methods, we have proposed the use of autoregressive stacking to improve NPs performance.

Sequential Data Models.Learning to predict sequences is a challenging problem for modern neural networks. The sequence data is typically modeled as a Markov process. Such approaches have been successfully applied in image generation  and text generation  tasks. However, Markov models cannot model time-dependent long-range dependencies due to the requirements of conditional independence. Long-range relationships in the data cannot be modeled using Markov models. One way to address this issue is by introducing recurrent neural networks (RNNs) [6][7], which use a mechanism for processing data",,"<In the domain of federated learning, several approaches have been proposed to address the challenge of long-tailed data distribution. SCAFFOLD [2] introduces a control variates-based algorithm to correct for 'client-drift' in federated averaging, showcasing significant reductions in communication rounds and resilience to data heterogeneity. Similarly, the FedBN framework [3] leverages local batch normalization to mitigate feature shift non-iid, outperforming classical FedAvg and state-of-the-art methods for non-iid data. In a similar vein, Self-Balancing Federated Learning with Global Imbalanced Data in Mobile Systems (Astraea) [7] presents a self-balancing framework that incorporates Z-score-based data augmentation and mediator-based multi-client rescheduling to mitigate imbalances and improve accuracy in federated learning applications. Additionally, Federated Learning with Class Imbalance Reduction [8] introduces an estimation scheme to detect class distribution and proposes a multi-arm bandit based algorithm to select client sets with minimal class imbalance, resulting in improved convergence performance of the global model. Moreover, Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features (CReFF) [10] addresses the long-tailed data challenge by re-training classifiers on federated features, demonstrating promising results compared to existing FL methods.>

<The issue of long-tailed data distribution has also been extensively studied in the context of deep learning and computer vision. For instance, Decoupling Representation and Classifier for Long-Tailed Recognition [11] separates representation learning and classification, offering new insights into handling class imbalance in long-tailed recognition tasks. Moreover, Long-tailed Recognition by Routing Diverse Distribution-Aware Experts [19] introduces RIDE, a framework that reduces bias and variance of long-tailed classifiers using a distribution-aware diversity loss and expert routing module, achieving significant performance gains in long-tailed data. Additionally, ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot [21] presents a one-stage long-tailed recognition approach, ACE, that outperforms existing methods in terms of accuracy and addresses the trade-off between head and tail performance. These works contribute significantly to the understanding and mitigation of long-tailed data challenges in the context of federated learning and deep learning applications.>

<Moreover, traditional computational techniques for imbalanced datasets can be adapted to federated learning setups. SMOTE: Synthetic Minority Over-sampling Technique [14] proposes oversampling the minority class and under-sampling the majority class, which has shown to achieve better classifier performance compared to varying loss ratios in certain classifiers. Additionally, Learning From Imbalanced Data [15] discusses existing learner challenges, methods, and evaluation metrics identified when dealing with imbalanced datasets, providing valuable insights into adaptation for federated learning settings. Furthermore, Class-Balanced Loss Based on Effective Number of Samples [16] introduces a theoretical framework to measure data imbalance and proposes a class-balanced loss based on the effective number of samples, resulting in improved classifier performance on long-tailed datasets.>"
2667,2667," **Correctness of AD for non-differentiable functions.** There is a large body of work on the correctness of AD in the context of functions that are not differentiable. For example, [2] prove that AD is always correct for functions with non-linear activation functions. [3] extend this result to higher-order functions, and [1] prove the same result for functions whose activation functions are continuous. However, none of these results apply to functions with machine-representable numbers, which is the setting we consider in this paper.

**Correctness for AD in programming languages.** Automatic differentiation has been studied in the programming language of lambda calculus (; [5]; [7]; [8]). In particular,  prove that the \(\mathcal{O}(n^{2})\) norm of the gradients of a function \(f(\mathbf{x}\in\mathbb{R}^{n\times n}\) is the same as the norm of its activation function, and prove that \(\mathrm{L}_{\text{log}(f(\cdot)\)=\frac{1}{n}\sum_{i=1}^{N}\log(f(x)\) for all \(n\), where \(n\) is a number of functions. In this paper, we focus on the case where \(f\) is an activation function.

 prove that for functions \(\mathbf{\theta}(x,y)\) that are continuous, the \(\theta\) can be a function of the form \(\bm{x}=\bm{y}\), and \(\bm{\bm{\Theta}(\bm{z}=y\), which is a non-convex function. The \(\mathtt{log}\)-norm of the gradient of the function is defined aswhere \(\mathtilde{\bm{L}}(x)=\frac{\frac{2}{n\log(y|x)\|_{i,j}+\bm{\tilde{j}|_{j}^{2}\bm{j}\),\),\[\begin{split}\&\mathrm{\mathtt{\log}\|\mathtt\|\cdot\| \bm{\phi}\|_{2}^{-1}-\mathrt{L}\|^{n}\end{split}}\] (1)


"," The correctness of AD has been extensively studied, especially in the past few years. When a program uses only differentiable functions, AD is shown to compute its standard derivative at all real-valued inputs ([5]; ; [4]; [6]; ; [3]; [1]; ; ). In contrast, when a program uses non-differentiable functions, the program itself can be non-differentiable, and AD can return a value different from its standard derivative, at some real-valued inputs. Nevertheless, for a large class of programs, such inputs are shown to be in a Lebesgue measure-zero subset of the real-valued input domain (; ; [2]; ). All these works consider the case when inputs to AD are real-valued, while our work focuses on the case when the inputs are machine-representable.

The Clarke subdifferential and its connection to AD have been studied for decades. Some classes of functions (e.g., subdifferentially regular or strictly differentiable) are shown to admit exact chain rules for the Clarke subdifferential (e.g., Theorems 2.3.9, 2.3.10, and 2.6.6 of [7] and Theorem 10.6 of ), and this implies that AD always computes a Clarke subderivative for a certain class of programs. However, this class of programs is restrictive, excluding even simple neural networks (e.g., \((1-\mathrm{ReLU}(x))^{2}\)) (). In contrast, our Theorem 3.6 shows that AD always computes a Clarke subderivative of neural networks with bias parameters. For piecewise differentiable functions, the Clarke subdifferential can be expressed in terms of the standard derivatives of underlying differentiable functions (e.g., Proposition 4.3.1 of ), but this result is not directly related to AD.

A variety of algorithms (other than AD) have been proposed to compute a Clarke subgradient of a scalar program, correctly and efficiently. For a large class of programs \(f:\mathbb{R}^{n}\to\mathbb{R}\) and an input \(x\in\mathbb{R}^{n}\), the algorithm by  computes a Clarke subgradient of \(f\) at \(x\) in time \(\mathcal{O}(T)\) almost surely, while the algorithms by [8];  compute the quantity in time \(\mathcal{O}(nT)\) deterministically, where \(T\) denotes time to evaluate \(f(x)\). Our Theorem 3.6 provides a relevant result as described above, but we point out that our work is about analyzing the correctness of vanilla (forward/reverse-mode) AD, not about proposing a new algorithm.

Recently, [9] empirically studied how the choice of \(D^{\mathsf{Ab}}\mathrm{ReLU}(0)\) changes the output of AD and the training of neural networks. In contrast, our work theoretically studies the correctness of AD. Further connections between this and our work are discussed in SS6.

","[1, 2, 3, 4, 5, 6, 7, 8, 9]"," **Correctness of AD for non-differentiable functions.** Several recent works have studied the correctness of AD over non-reals functions. [1] prove that AD over linear functions is always correct. [2] prove the same result for deep learning functions, but only for the ReLU function. [3] prove AD over the reals is correct when the function is a linear function, and [4] prove it for the RPN function. However, these works assume that the function has a linear form, which does not hold in practice. [5] and [6] show that AD in reverse mode is correct for ReLU functions, and they prove that it is also correct for other types of functions.

**Correctness for AD over machine-representable numbers.** There is a large body of work on the correctness for AD on machine-represented numbers. [7] prove a tight bound on the number of non-zero derivatives in the non-normalized case of AD. [8] prove an upper bound on this number. [9] prove upper bounds on the size of the nonlinear set of AD on neural networks with bias parameters. [10] show upper bounds for the size and the bias of the incorrect set for neural networks.

"," MMCL has achieved great success on various vision-language tasks. Following the pre-text work of OpenAI [1] on CLIP, other pre-training techniques also follow the idea of learning bidirectional text-image pairs.  uses paired text and image pairs with cross-attention and mixture of experts. [2] uses a two-stream architecture for both text and image modalities to pre-train the Vision-Language model (ViLBERT). [3] incorporates cross-attention modules with contrastive learning for better alignment between modalities. Recently, [4] uses a large-scale image-text pre-training dataset FILIP300M for finer-level alignment of visual-language pairs.

Despite the empirical success of MMCL, the theoretical understanding of this method remains elusive. Different from [1], we unify MMCL and contrastive learning by showing that MMCL implicitly performs SSCL with (pseudo) positive pairs induced by text pairs. As a consequence, we leverage the work of contrastive learning to obtain guarantees on the downstream accuracy of MMCL. As the first work of this line, we show that text pairs induced by negative pairs can provably lead to better visual representations for downstream tasks.

SSCL have been widely used in vision-language tasks to learn visual-language representations without supervision. For example,  learns a joint visual-language model with cross-attention and contrastive learning.  proposes an efficient and effective unsupervised task, vCLS, and then utilizes it for better language representation. Recently,  proposes the intersection constraint between the region and text similarity for the generation of pseudo-positive pairs, which achieves strong generalization results. These methods can be seen as contrastive learning in image domain, and some of them use text pairs to generate pseudo-positive pairs.

For the connection of contrastive learning with information theory, [8] uses information bottleneck to learn deep representations of semantic embeddings. [7] analyzes contrastive learning from the perspective of representation learning in latent class models. [6] studies the relationship between the representations of different samples and achieves new bounds on the downstream performance of contrastive learning. Inspired by [5], we unify MMCL and SSCL and provide theoretical analysis for MMCL on the basis of [7] and [6].

Recently, there are also some works on theoretical justifications of multi-modal learning.  proves that the dimensionality reduction of multi-modal data is equivalent to multi-dimensional sparse coding. [9] proves that multi-modal learning can reduce the population risk more than learning with unimodal data.

",,"<Automatic differentiation (AD) has become an essential tool in machine learning for computing gradients of differentiable functions, enabling efficient training of neural networks [2]. While previous work has focused on the correctness of AD over the real number space, the paper ""On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters"" tackles the important question of the correctness of AD when the parameter space of a neural network consists solely of machine-representable numbers [1]. Notably, the authors analyze two sets of parameters on which AD can be incorrect: the incorrect set on which the network is differentiable but AD does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. The paper provides rigorous proofs regarding the correctness of AD in these scenarios, including bounding the size of the non-differentiable set and proving that AD always computes a Clarke subderivative even on the non-differentiable set.

The paper makes significant contributions to the understanding of the correctness of AD in the context of machine-representable parameters for neural networks. It not only delves into the theoretical aspects but also establishes practical implications for efficient implementation of AD systems in the context of non-differentiable functions encountered in deep learning applications [2]. By addressing the issues of correctness and bounds on the non-differentiable set, this work lays a solid foundation for future research and development in algorithmic differentiation for neural networks. The results presented in this paper extend the scope of previous AD correctness studies, which have predominantly focused on differentiable functions, and provide valuable insights for the practical implementation of AD in machine learning systems.

The findings in this paper also align well with previous research in the field of automatic differentiation, in particular, the work on proving the correctness of autodiff systems applied to non-differentiable functions [2]. It builds on the formalization of AD into forward-mode and reverse-mode AD, shedding light on the correctness of AD when dealing with non-differentiable functions encountered in machine learning applications. Additionally, the paper is relevant to the broader research on generalizing automatic differentiation to higher-order functions, as it contributes to the understanding and formalization of AD systems, particularly in the context of neural network training and optimization [3].


The formalization of AD into forward-mode and reverse-mode AD, as well as the study of correct application of AD to non-differentiable functions [2], provides a strong basis for the current paper's investigation into the correctness of AD when working with machine-representable parameters in neural networks. By examining classical AD algorithms such as backpropagation, the work presented in the current paper serves to simplify and streamline the understanding of automatic differentiation, delineating the concerns of differentiation from those of efficiency, with potential implications for improved AD implementations [4]. The rigorous proofs and analyses of AD correctness and the bounds on the non-differentiable set in the context of machine-representable parameters contribute significantly to the existing body of work on automatic differentiation, providing essential insights for advancing the practical implementation and robustness of AD systems for neural network training and optimization. In doing so, the paper not only contributes to the formal understanding of AD but also offers practical implications for the field of machine learning as a whole.>



"
3759,3759," **Navigation.** Navigation has been extensively studied in robotics and computer vision [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23]. In this work, we focus on the task of multi-object navigation, where the goal is to navigate to an object of interest in a novel environment. In this section, we briefly review the most relevant literature.

**Implicit Representations.** Implicit representations have been widely used in computer vision to represent 3D scenes [24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43]. These representations can be broadly classified into two categories: topological representations and semantic representations. Topological representations represent the 3D structure of the scene as a set of signed distance functions (SDFs) or occupancy fields (e.g. signed distance fields). Semantic representations encode the semantic information of the environment, such as the semantic labels of objects. In contrast, semantic representations are more expressive and can be learned from data. For example, in [35], semantic labels are learned from a large-scale dataset. In [36], the agent is trained to predict the semantic label of an object in an image. The semantic representations can also be learned in an end-to-end manner. In our work, the semantic representation is learned in a self-based manner, while the semantic-based representation is learnt in a fully supervised manner.

 propose to use a graph neural network to model the semantic structure of a scene. In their work, they use an attention-based neural network [19] to encode the semantics of an image, while in our work we use a Transformer-based architecture [19].

Our work is different from previous works in that we propose to learn the semantic and semantic embeddings in a unified latent space. We also propose a novel read-write mechanism to read the semantic representations from the latent space to the image space.

.

 proposes to learn a mapping from the image to a semantic embedding space. However, they do not explicitly model the topology of the image, which is important for navigation.

 uses a topological representation to represent the geometry of an environment, while our work uses a semantic representation to map the scene to the semantic space. Our work differs from theirs in the following aspects: (i) they use a topology-free representation, while we use an explicit semantic representation. (ii) their model is learned offline, whereas our model is trained online. (iii) They use a fixed topology while our model learns a dynamic topology. (iv) Their model is offline while ours is learned online.

 use an external memory to store semantic information. Our model uses a learned semantic representation and a learned navigation policy.

 also uses a neural network. We use a differentiable map representation. We compare our model to theirs."," **Visual Navigation --** is a rich problem that involves perception, mapping and decision making, with required capacities being highly dependent on the specific task. A summary of reasoning in navigation has been given in [1], differentiating, for instance, between waypoint navigation (_Pointgoal_) [1] or finding objects of semantic categories (_ObjectGoal_) [1]. More recent tasks have been explicitely designed to evaluate and encourage mapping objects of interest during navigation itself [2][3]. They are of sequential nature and use external objects, which are not part of the scanned 3D scenes but randomly placed. In this work we address _Multi-Object Navigation (MultiON)_[3].

**Mapping and Representations --** Classical methods often rely on SLAM [4][5] which has been proposed in different variants (2D or 3D metric, topological, eventually with semantics) and observations (LIDAR, visual etc.). The objective is to integrate observations and odometry estimates over a trajectory and reconstruct the scene. Differentiable variants have been proposed recently [6][7]. Mapping can also be discovered through interactions by a blind agent [8]. Visual Navigation can also be framed as an end-to-end learning problem, where representations are learned automatically from different signals, in particular RL. Memory can take the form of vectorial representations in recurrent units [9][10][11], with hybrid variants including mapping [12][13][14]. Recent work tends to augment agents with more structured memories. Examples are spatial metric tensors, which can contain occupancy [15], semantics [16] or be fully latent, effectively corresponding to inductive biases of the neural agents [17][18]. Other alternatives are topological maps [12] or self-attention and transformers [19] adapted to navigation [20][21][22][23].

**Implicit representations --** were initially targeting 3D reconstruction [24][25][26]. The core idea is to replace the need for discretizing 3D space into voxels [27], 3D points [28] or meshes [29], by an implicit representation of the 3D structure of the scene through the parameters of a learned neural network. Recent work [30] achieved state-of-the-art performance on novel view synthesis with neural implicit representations. The NeRF paper introduced a differentiable volume rendering loss allowing to supervise 3D scene reconstruction from only 2D supervision . For a more detailed overview of recent advances in the rapidly growing field, we refer the reader to [31].

**Implicit representations in robotics --** are a recent phenomenon, used to represent density [32] or to perform visuomotor control [33]. Related to goal-oriented navigation, some work targets SLAM with neural implicit representations [34], follow-up adding semantics [35], learned from sparse semantic annotations of the scene. [36] is also built on top of [34] and allows a user to interactively provide semantic annotation for the implicit representation to be trained on in real time. [37] proposes a hierarchical implicit representation of a scene to scale to larger environments and obtain a more detailed reconstruction. [38] combines feature-based SLAM and NeRF. Our work goes beyond implicit SLAM and does not stop at reconstructing a scene. We not only build implicit representations dynamically during the episode, we also use them in a down-stream navigation task _without_ requiring any initial rollout for pre-training or building a representation. We also combine two different implicit representations targeting semantics vs. scene structure.

**Analyzing the neural network function space --** implicit representations are instances of function spaces, which are represented through their trainable parameters. Previous work performed analyses by predicting accuracy from network weights [39][40] or the generality gap between train and test performance from hidden activations [41][42]. A direction pioneered by Hypernetworks  directly predict the network weights. Recently, [43] generate the weights of a CNN from support samples in the context of few-shot learning. More related to our work, [44] learns to predict the weights of an implicit representation based on external factors in the context of spatio-temporal dynamics encoding. In this work, we learn a direct mapping between an implicit representation, represented by its weights, to an actionable embedding summarizing the scene globally.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]"," **Navigation.** Navigation is a fundamental problem in robotics and computer vision, and has been studied extensively in the past decades [1][2][3]. In this work, we focus on the task of multi-object navigation, which requires the agent to navigate to an instance of a previously seen object in an unseen environment. This task is more challenging due to the lack of visual information and the need for high-level reasoning. In the literature, there is a large body of work on SLAM-based navigation [4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23], where the goal of navigation is to estimate a map of the environment and navigate to the location indicated by a previously visited object. In this paper, we study multi-Object Navigation (MOTN) [3], which requires navigation to an episode-specific sequence of objects in a realistic environment.


**Neural Implicit Representations.** Neural implicit representations have been widely used in computer vision and machine learning to model 3D shapes [24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44]. These representations are typically defined as a neural network that maps the input space to a latent space, where the latent space is represented as a continuous vector or occupancy vector, and the output space as a signed distance function. These representations have shown promising results in various tasks such as 3D reconstruction [24], 3D shape and image synthesis [25][24][26], and 3D object reconstruction [28][35]. In the context of MOTN, we are interested in learning a neural implicit representation that maps a query object to its previous location in the scene. This is achieved by training an agent with Reinforcement Learning (RL) [2] to predict the location of the queried object in a given episode. The Semantic Finder [1] is trained with a variety of different tasks, including object localization, mapping, and semantic map retrieval. The Occupancy and Exploration Implicit representation is learned with a novel global read mechanism, which directly maps from function space to embedding space.

"," Blind image deblurring has been an active research topic in recent years. Traditional methods take low-level priors such as sparsity [1][2][3], patch prior [4], intensity prior [4] into consideration to estimate the kernel of blur process in an image, and then apply conventional deconvolution methods to denoise the degraded image. Recent researches have demonstrated that learning-based methods have outperformed traditional methods. Rather than utilizing priors, they use synthesized blur images to learn the kernel, and then adopt efficient learning methods such as Markov Random Field (MRF) [5][6], Convolutional Neural Network (CNN) [7][8], and Recurrent Neural Network (RNN) [9][10][11] to denoise the degraded images. Considering that blurry images usually contain rich global structures which include low-frequency and high-frequency information, our work focuses on jointly exploring the global information to improve the performance of deblurring.

Image deblurring using multi-scale strategy has been widely adopted by recent researches. A typical scheme is utilizing multi-scale image pairs to restore images in a coarse-to-fine manner [8][12][13][14][15]. Liu _et al_. [8] employ a deep learning based method to restore the dynamic scenes, and then upsample the restored image to a higher resolution to generate the multi-scale image pairs for the coarse-to-fine scheme. Zhang _et al_.  and Wei _et al_. [13] utilize a bi-directional strategy to generate multi-scale image pairs, and then adopt a ResNet [7] to denoise images. However, this scheme usually brings high computational costs since a dense and highly connected ResNet is utilized to model the multi-scale images. To address this issue, several works [12][16][17][14] have demonstrated that a simple structure such as an encoder-decoder can also achieve good results. Our work differs from these multi-scale approaches, as we design a multi-scale Residual Low-Pass Filter (RLPF) module to explore the low-frequency content, which can capture the global information.

Recent years have witnessed the rapid development of transformer-based models on computer vision tasks [18][19][20][21][22][23]. They have shown their potential in capturing long-range context information. Thus, they have been widely utilized in deblurring methods [17][21]. DeblurGAN [17] applies a CNN-based encoder-decoder architecture to synthesize blurry images and then uses a conditional GAN to supervise the deblurring network. RESiD  extends the image deblurring architecture with",,"<Understanding and mapping new environments are essential for autonomously navigating agents, and extensive research has been conducted in this domain. Traditional robotics often relies on Simultaneous Localization And Mapping (SLAM) approaches [4], which maintain topological or metric representations of the environment. In contrast, recent works have focused on end-to-end learning of navigation using neural networks with inductive biases, resulting in the use of neural implicit representations as a memory source [1][6]. These implicit representations are dynamically learned during each episode and capture information about the environment, such as the position of queried objects and obstacle information. This approach has been explored in the context of multi-object navigation, showing promising results when leveraged by agents trained with Reinforcement Learning (RL) [3][8]. The use of neural implicit representations as a memory source has demonstrated high impact in enabling agents to navigate and make decisions based on dynamic scene information.

Several other works have also delved into related areas. Deep reinforcement learning has been employed to train agents capable of complex reasoning and navigation tasks, simplifying the training process by decoupling from the requirement of high fidelity photographic observations [2][10]. Additionally, research has focused on benchmarking semantic map memory using multi-object navigation tasks to investigate the impact of maps on navigation tasks of varying complexity [9][13]. Furthermore, the combination of modular designs and hybrid algorithms, which combine classical planning with machine learning, has been explored to leverage the complementary nature of the policies and the complex regularities between scene structure and planning performance [14][15]. Other works have investigated the prediction of neural network accuracy from weights and the use of heavy-tailed universality to predict trends in test accuracies for large pre-trained deep neural networks, offering insights into understanding network training and performance [39][40].>

<Moreover, the exploration of implicit representations extends to space and time, with research on predicting generalization gap in deep networks using margin distributions and task and architecture-independent generalization gap predictors [41][42]. Additionally, the application of hypertransformer, a model generation approach for supervised and semi-supervised few-shot learning, has shown promising results in generating weights of a convolutional neural network directly from support samples, effectively decoupling the complexity of the large task space from individual tasks [43]. Furthermore, the development of Neural Implicit Flow, a mesh-agnostic dimensionality reduction paradigm of spatio-temporal data, demonstrates its potential to efficiently represent the complexity associated with spatio-temporal data using a low-dimensional subspace [44].

In conclusion, the use of neural implicit representations as a memory source has garnered significant attention in the field of autonomous navigation, enabling agents to capture and leverage dynamic scene information for effective decision-making. The exploration of related methodologies, from deep reinforcement learning to model generation and dimensionality reduction paradigms, contributes to the advancement of techniques for enabling autonomous navigation in complex environments. The integration of these diverse approaches provides a comprehensive understanding of the current landscape of research in this domain.>"
3729,3729," **Contrastive self-supervised learning.** Contrastive predictive coding (CPC) [1] and its variants [2][3][4][5] are the most popular SSL methods that learn representations by contrasting positive and negative samples in the embedding space. The idea is to learn representations that are invariant to the transformations of the input data by pulling together similar samples and pushing apart dissimilar samples. This idea has been extended to the spectral domain [2] and to the spatial domain [6][7][8][9][10]. In this paper, we focus on the contrastive setting.

**Equivariant SSL.** The idea of learning equivariant representations with SSL was first introduced in [11] where a rotation predictor is trained to predict the rotation of an image. The rotation prediction is then used to train a classifier that predicts the rotation order of the rotated image. In [12][13][14][15], invariant representations are learned by decomposing the data into a set of invariant subspaces and then using a clustering algorithm to assign each subspace to one of them. In contrast to these methods, our method does not require any transformations to be applied to the input images and can be trained in an end-to-end manner.

 proposed a method to learn invariant embeddings by solving a convex optimization problem. The objective is to minimize the Euclidean distance between the latent representations of the transformed images and the original images. In this work, we show that this can be achieved with a hypernetwork-based predictor without compromising on equivariance.

 introduced a method for learning invariant features by solving an optimization problem based on the symmetric group theory. The invariance of the learned representations is obtained by minimizing the distance between two sets of transformations. However, this method requires a large number of transformations to achieve invariance. In our method, we do not need any transformations and can learn representations with no possible collapse to invariance by splitting the representations into two parts.

 introduce a method that learns invariant latent representations by minimizing a loss function that encourages the representations to be invariant under certain transformations. In their method, the invariance is achieved by maximizing the mutual information between the representations and the transformations. This loss function is defined aswhere \(\mathcal{L}_{1}\) and \(\mathbb{E}_{2}\) are the Euclidian distance and the Mahalanobis distance, respectively. The main difference between their method and SIE is that SIE does not impose any constraints on the transformation of the representations. In addition, their method is trained on a synthetic dataset which is limited to a small number of object categories. In comparison, we introduce a dataset consisting of more than 55 classes and more than 2.5 million images where we have full control over the transformations applied on the objects.

 propose a method based on a variational auto-encoder (VAE) framework to learn a representation that is invariant and equivariANT to transformations. The method uses a two-stage training procedure where the latent representation is first"," Invariant Self-supervised learningTwo main families of methods can be distinguished: contrastive and non-contrastive. Contrastive methods (; [3]; ; ; ) mostly rely on the InfoNCE criterion ([1]) except for ([2]) which uses squared similarities between the embedding. A clustering variant of contrastive learning has also emerged (; ; ) and can be thought of as contrastive methods, but between cluster centroids instead of samples. Non-contrastive methods ([4]; ; ; [8]; [7]; [6]; ) aim at bringing together embeddings of positive samples, similar to contrastive learning. However, a key difference with contrastive methods lies in how those methods prevent a representational collapse. In the former, the criterion explicitly pushes away negative samples, i.e., all samples that are not positive, from each other. In the latter, the criterion considers the embeddings as a whole and encourages information content maximization to avoid collapse, e.g., by regularizing the empirical covariance matrix of the embeddings. While we study methods from both families in our experiments, they have been shown to lead to very similar representations ([5]).

Introducing equivariance in invariant self-supervised learningWhile most of the aforementioned works focus on learning representations that are invariant to augmentations, some works have instead tried to learn representations where information about certain transformations is preserved. This can be done by predicting the augmentation parameters ([9]; ; [11]), or by introducing other transformations such as image rotations (). Preserving the augmentations' strength in the representations can also be used to learn less invariant representations ([10]). These methods offer no guarantees on the existence of a mapping between transformed representations in latent space, nor ways to prove its existence or lack thereof. As such these methods cannot be considered to truly be equivariant.

Equivariant representation learningPrevious works have explored equivariant representation learning using autoencoders, such as transforming autoencoders (), Homeomorphic VAEs ([12]) or ([14]). Recent works such as EquiMod () or SEN ([15]) have also included a predictor that enables the steering of representations in latent space, without requiring reconstruction. These methods form the basis for our comparisons. In ([13]), representations are split in class and pose, i.e. invariant and equivariant, and assumes a simple equivariant latent space where the group action is the same as in the underlying data, e.g. 3 dimensions to represent pose. This assumes prior knowledge on the group of transformations, and can prove limited when the transformations cause a loss of information. Transformations are also assumed to be small, similarly as for SEN. We aim at deriving a more general predictor architecture with no such priors. In (), equivariant representations are learned with no knowledge of the group element associated with the transformation, but by having pairs of samples where the same transformation was applied.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"," **Self-supervised representation learning.** Recent progress in SSL has been driven by the contrastive learning paradigm [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16]. The main idea of contrastive methods is to learn representations by pushing positive samples from the same class closer to each other while pushing samples from different classes away from each other [1]. This idea has been shown to be equivalent to non-contrastive methods [5] and to be provable in certain regimes [2]. Theoretical results have also shown that the performance gap between contrastive and non-Contrastive SSL methods can be closed [5]. However, these results are based on the assumption that the positive and negative pairs are independent, which may not hold in practice. To address this issue, BYOL [4] and its variants [4][7] use a slow-moving average of the online network to generate negative samples. SimCLR [1] uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. MoCo [3] proposes a dynamic dictionary with a queue and a moving-averaged encoder to build a large and consistent dictionary on-the-fly. Sim-CLS [2] introduces spectral contrastive losses which perform spectral decomposition on the population augmentation graph on data.

1.



\({}^{2}\)[2] proposes Spectral Contrastive Loss which performs spectral augmentation decomposition in the spectral domain and provides provable guarantees for linear probe evaluation. SimCluster [6] is a general purpose clustering method which uses a domain-specific constraint to ensure the identification of the manifolds and learns to parameterize each manifold as a linear subspace in a feature space. NMCE [6], a general-purpose method for general purpose manifold clustering, uses Maximum Coding Rate Reduction (MCR) objective to learn a meaningful and interpretable feature space and achieves state-of-the art performance on various datasets.
 is an extension of NMCE to the SSL framework [6]. NMCE uses the maximum coding rate reduction objective to encourage the representations to be equivariant to a set of transformations defined a priori. This allows the learned representations to retain information about task-relevant transformations. However, the invariance assumption often limits the expressive power of the representations and model"," **Graph Contrastive Learning.** As a method with plenty of potential, graph contrastive learning (GCL) has gained considerable attention. Existing methods put forward a set of data augmentations, then generate the contrastive views from the original graph and the data augmentations. For example, _Gromov-like_ leads to the contrary of each feature along the degree as the edge, _parameter-based_ introduces the randomness of the augmentation parameters to the contrastive model, and _instance-based_ only considers the contrast between the two instances. However, the above data augmentations could not guarantee the uniqueness and efficiency of these views, leading to the failure of the model. Recently, _learnable data augmentations_([4]; [5]; [6]; [1]) are proposed to discover the correspondence between the views. The neural networks are used to learn the data augmentation process of the target instance to generate the views.

**Structural Entropy.** Structural entropy ([8]) proposes a general definition of the entropy of the statistical model (a graph or a sequence) by counting the cardinality of the statistical model. This definition extends the existing definition of entropy and has been applied in various tasks such as graph mining, pooling, and node classification (; [9]). Furthermore, it has been shown that the entropy provides a mathematical explanation for the discriminating ability of graph theory (; [10]; [11]).

",,"<>
Recent advancements in self-supervised learning have paved the way for the development of invariant or equivariant representations. Notably, Representation Learning with Contrastive Predictive Coding [1] proposes a universal unsupervised learning approach that extracts useful representations from high-dimensional data by predicting the future in latent space. The method uses a probabilistic contrastive loss to induce the latent space to capture information maximally useful for predicting future samples. Furthermore, Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss [2] presents a loss function performing spectral decomposition on the population augmentation graph, ensuring features with provable accuracy guarantees under linear probe evaluation.

Another significant contribution is Momentum Contrast for Unsupervised Visual Representation Learning [3], introducing MoCo as a dynamic dictionary-building approach to facilitate contrastive unsupervised learning. Furthermore, Bootstrap Your Own Latent [4] introduces an innovative approach to self-supervised image representation learning, relying on two neural networks, online and target networks, and achieves a state-of-the-art performance. Additionally, On the duality between contrastive and non-contrastive self-supervised learning [5] examines the theoretical similarities between these two learning families and shows how they can be related algebraically and be equivalent under limited assumptions.

Moreover, Neural Manifold Clustering and Embedding [6] underscores the potential of deep neural networks in achieving manifold clustering under highly nonlinear settings. Whitening for Self-Supervised Representation Learning [7] introduces a new loss function based on whitening of latent-space features, accelerating self-supervised training and yielding more effective representations for downstream tasks. Furthermore, Barlow Twins: Self-Supervised Learning via Redundancy Reduction [8] proposes an objective function that naturally avoids collapse without requiring large batches or asymmetry between network twins.

In addition, Self-Supervised Learning Through Efference Copies [9] derives a theoretical framework, Self-supervision Through Efference Copies (S-TEC), based on the biological principle of embodied learning, extending the understanding of SSL methods such as SimCLR and BYOL. Lastly, What Should Be Equivariant In Self-Supervised Learning [10] presents a novel SSL framework that encourages feature representations to preserve the order of transformation scale in the embedding space for some transformations while maintaining invariance to others, demonstrating its effectiveness on various datasets.

Overall, these comprehensive advancements in self-supervised learning have greatly enhanced the understanding and applicability of invariant and equivariant representations, providing valuable insights for the development of more diverse representations suitable for a wide range of tasks. These contributions have expanded the scope of self-supervised learning, opening new possibilities for leveraging unlabeled data in feature learning."
2876,2876," Vision-and-Language Navigation (VLN) is a task that requires an agent to navigate in an environment given a natural language instruction. Previous work on VLN can be divided into two main categories. The first category focuses on training a model to generalize to unseen environments. The second category aims to train a model that can perform well on a specific task. For example, in the Room-to-Room (R2R) dataset [1], the agent is required to navigate to a specific room. The R2R dataset consists of simulated 3D environments [2][3][4] where the agent has access to a depth sensor and a camera.

[5] focuses on evaluating the generalization ability of a model on a particular task. This includes evaluating the ability of the model to:"," **Vision-and-Language Navigation (VLN).** Since its introduction in [1], many variants of the Vision-and-Language Navigation (VLN) task have been proposed including those in continuous simulators . We refer the reader to [2] for a comprehensive survey. In this work, we examine agents in the Room-Across-Room (RxR) dataset [3] which extends the original VLN task to a multilingual setting with longer, more complex trajectories and pose traces which provide temporal alignment between instruction words and visual observations. There has also been significant modelling work to develop instruction-following agents [4][5][6][7][8][4][9] and we examine three recent models in our analysis [6][7][9].

The RxR task is situated in the Matterport3D [10] environments which provide an interface for agents to move through the environment along a graph of panoramic viewpoints taken in real environments. Matterport3D also provides region annotations for room type which we utilize in our experiments. We also leverage annotations from the REVERIE [11] dataset which extends VLN settings with an additional goal of identifying an object described by a referring expression. Specifically, using the annotations from REVERIEv1 to identify visible objects at each viewpoint.

**Evaluating VLN Agents.** In standard settings, VLN agents are evaluated by metrics that focus on either the agent reaching the goal efficiently (Success weighted by inverse Path Length [12]) or by their trajectory's alignment with the ground truth path (Normalized Dynamic Time Warping [1]). These metrics focus on the agent's performance in aggregate and do not examine agent performance on the level of sub-instruction or skills.

Some works have examined VLN agent behavior more closely by masking or replacing portions of the instructions [13][14] and observing the resulting change to overall task metrics like those described above. Zhu _et al_. [14] find that VLN agents still achieve relatively high success rates even when all references to visual objects are masked from instructions. These findings cast doubt on the vision-language alignment ability of these agents. [13] also perform masking experiments but come to different conclusions, with some models relying more heavily on nouns than directional words. In both works, agent performance is measured on an episodic level that relies on a sequence of agent decisions; however, single errors in a trajectory may compound and result in misestimating the impact of masked terms. In contrast to these works, we present a skill-based analysis of VLN agents by constructing specific intervention episodes wherein the appropriate next action is known.

**Behavioral Analysis of AI Models.** Recent work in natural language processes has applied behavioral analysis to examine specific skill capabilities. Like us, Riberio _et al_. [15] develop an intervention paradigm wherein dataset examples are modified in ways such that the desired change in model behavior is knowable. These examples and their associated skills are collected into a ""checklist"" that can be used to evaluate models. Likewise, our work can be construed as generating a checklist of skills for VLN. Yang _et al_. [16] follow a similar paradigm and develop a method to automatically generate test cases using a large language models [17].

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"," **Vision-and-Language Navigation (VLN).** VLN is a task that aims to ground natural language instructions to actions in photorealistic environments [1][2][3][4][5][6][7][8][9][10][11][12][13][14]. Most of the previous work has focused on the Room-to-Room (R2R) and Room-Across-Room [1] benchmarks [1], where the goal is to learn to navigate from human-annotated instructions in a simulated environment. The R2R dataset consists of 2,500 panoramic panoramas captured in real-world buildings and contains instructions in English, Hindi, and Telugu. The Room-Cross-Room dataset [3] consists of instructions in multiple languages, including Hindi, English, and Spanish, and is larger than the R2L dataset [1]. The Room Across-Room task is a multilingual task that requires agents to navigate between rooms in a multi-room environment with multiple rooms and multiple rooms. The task is more challenging than the Room to Room (RxR) task, as it requires the agent to ground instructions to specific actions in the environment. In this work, we focus on the more challenging Room-To-Room setting, where the agent must ground instructions about stopping, turning, and moving towards specified objects or rooms.




"," Backdoor AttackThe classic backdoor attack [1] requires a special trigger pattern to be input to the backdoored model to trigger the backdoor behavior. However, backdoor attackers can also mislead the model into the backdoor state without the trigger pattern inputs. For example, graph neural network backdoor attack [2], poisoned contrastive learning [4], and federated learning attack [3] do not require any extra triggers. Many later works focused on how to construct clean models without backdoor behaviors. We categorize them into three parts: (1) injecting latent backdoor patterns into clean model [5]; (2) controlling the model behavior by manipulating the training dataset [6]; (3) excluding the backdoor behaviors of the model with post-hoc methods [7]. More discussions on backdoor attacks can be found in [1].

Backdoor DefenseAlthough it is difficult to detect backdoor attackers, a variety of defense methods have been proposed, such as clean label attack [8][9][10][11][12], steganography based [13][14], dynamic backdoor [15][16], etc. The detection methods can be categorized into (1) outlier detection based [17][18][19][20][21][22][23][24][25][26][27], (2) anomaly detection based [28][29][30], and (3) perturbation based [15].

Causality and Representation LearningRecent years have seen remarkable progress in causal inference, which makes causal inference a trendy and essential topic. In this paper, we adopt the mechanism of causality to analyze and understand the behavior of backdoored models. One line of causality research is based on causal graphs, such as counterfactual inference  and decision making . Another line of causal inference, first proposed by Pearl [31], is widely used for self-supervised learning . Causal inference also has applications in downstream tasks, including visual grounding [32], representation learning [33], and natural language processing [34][35]. Recently, researchers started to employ the causal inference technique to model neural network robustness [36][37][38]. Specifically, [36] tries to simulate adversarial data manipulation in the generation process, while [37] and [38] both regard backdoor as an instance of manipulation. Different from them, we model the backdoor backdoor effect by deconfounding the representation learning process.

",,"<Related work>

Vision-and-Language Navigation (VLN) has garnered significant attention from the natural language processing, computer vision, robotics, and machine learning communities [2]. One of the key components in VLN is grounding natural language instructions to actions in the surrounding environment. Recent work has focused on skill-specific analysis of VLN agents, examining how effectively they ground instructions about stopping, turning, and moving towards specified objects or rooms [1]. This approach involves generating skill-specific interventions and measuring changes in agent predictions, shedding light on the biases from training that have lasting effects on agent behavior [1]. The analysis of existing VLN agents also indicates that they are capable of grounding simple referring expressions, and this behavior is correlated with improvements in the overall VLN task performance [1, 2].

Several recent studies have emphasized the role of language in VLN and its impact on agent behavior. For instance, multilingual VLN datasets, such as the Room-Across-Room (RxR) dataset, have been introduced, featuring larger data and annotations, as well as highlighting the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities [3]. Novel data augmentation methods, such as Envedit, have been proposed to improve the generalization of VLN agents to new, unseen environments, demonstrating significant improvements in all metrics and achieving state-of-the-art results [4, 5]. Moreover, transformer-based models, such as Scene- and Object-Aware Transformer (SOAT), have been shown to substantially improve VLN performance by leveraging vision-and-language pretraining [6].

Additionally, recent advances in vision-and-language methods, such as CLIP (Contrastive Language-Image Pre-training) and large-scale pretraining techniques, have shown significant advantages in VLN tasks, achieving competitive or better results on diverse VLN tasks and setting new state-of-the-art results on various VLN benchmarks [7, 8]. Furthermore, the need to address the challenge of navigating unseen environments has led to the development of generalizable navigational agents using mixed imitation and reinforcement learning, fine-tuning with newly-introduced 'unseen' triplets, and semi-supervised learning via back-translation, resulting in substantial improvements in generalizability and performance on challenging VLN tasks [9, 10].

In addition to these advancements, recent work has also introduced benchmarking methodologies and diagnostic experiments to evaluate and understand the inner mechanics of VLN agents [12, 13, 14]. Behavioral testing methods, such as CheckList, have been proposed to comprehensively test the capabilities of NLP models, uncovering critical failures and bugs in commercial and state-of-the-art models [15]. Furthermore, capability-based testing frameworks, including TestAug, leverage advanced language models, such as GPT-3, to generate diverse test suites, which can be used to detect more errors compared to manually created counterparts, offering a scalable and effective approach for evaluating VLN models [16, 17].

<>"
1200,1200," **Semantic Segmentation.** Semantic segmentation is a fundamental task in computer vision. With the development of deep learning, many methods [1][2][3][4][5][6][7][8][9][10][11][12][13] have been proposed to improve the performance of semantic segmentation. In this work, we focus on the task of image segmentation, which has been studied for a long time.

**Continual Learning.** Continual learning (CL) aims to learn new tasks while retaining the knowledge learned from previous tasks [14][15][16][17][18][19][20][21][22][23]. Most of the existing CL methods focus on improving the generalization ability of the model to new tasks. For example, some methods [24][25][26][27] introduce a memory buffer to store a small number of exemplars from the previous tasks. Other methods [28][29][30][31][32][33] employ reinforcement learning to learn a policy to select the exemplars for replay. Different from these methods, we propose a novel memory sample selection mechanism based on a decision-making process and a dual-stage action space.

 propose a data augmentation strategy to alleviate the catastrophic forgetting problem in image classification. However, they do not consider the task-incremental setting. In contrast, our method can be applied to the image classification task, which is more challenging than image classification and requires more fine-grained understanding of the image.

 introduce a dynamic memory selection strategy for image classification tasks. They select exemplars based on the similarity between the current exemplars and the previous exemplars. Their method is effective, however, it is not suitable for the task with a large number of classes. In addition, their method is not end-to-end trainable.

 also propose a memory selection method for object detection task. They use an attention mechanism to select a few exemplars of the current class. They also use a reinforcement learning framework to learn the policy for selecting exemplars in a data-augmentation task.

 use a two-stage training strategy. They first train the model on the current task and then fine-tune it on the previous task. Their approach is effective but time-consuming.

 first train a model on a large-scale dataset. Then, they fine tune it on a smaller dataset. They then use the fine-tuned model on both datasets. They achieve the following results:

"," **Semantic Segmentation and Continual Semantic Segmentation.** Semantic segmentation is a basic task in computer vision and has achieved great success in recent years benefiting from the rapid development of deep-learned based algorithms such as encoder-decoder structure [1][2], dilated convolution [3][4][5], pyramid structure [5][6][7], attention mechanism [8][9][10] and transformers [11][12][13][14]. To meet the requirement in real applications where the new classes are incrementally added, continual learning has been proposed [15][16][17][15] and applied to the semantic segmentation task [18][19][15][16][20][21]. Among them, many works adopt replay-based methods, which show high effectiveness. [22][23] use a memory buffer to store replay exemplars, however, in which the samples are selected either randomly or according to heuristic rules. [20] derives richer replay exemplars through a generative adversarial network with high computation cost or web-crawled images requiring the extra data. Different from the above methods, with an RL-driven automatic memory selection policy and the gradient-based sample enhancement operation, our method can be very effective for CSS.

**Memory Sample Selection.** How to select the appropriate samples is a severe issue for replay-based continual learning methods. Most the previous selection methods rely on manually-designed strategies based on heuristic rules such as sample diversity [24][25][26], adversarial Shapley value [27] or feature matching [28]. In general, such hand-crafted methods lack effectiveness guarantees and are difficult to be optimal due to a complex interplay between factors that affect selection performance, as discussed in the Introduction. Our method explores a novel direction by enabling the selection policy to be automatically learned with a carefully-designed RL mechanism.

**Reinforcement Learning.** Reinforcement learning (RL) has achieved remarkable success in many decision-making tasks like game intelligence  and robot control [29][30]. It has also been employed to computer vision with various applications such as active learning [31], pose estimation [32], model compression [33] and person re-identification [34]. [35] uses RL for the exemplars length management, however, with the completely different working mechanism from ours. Instead of employing RL to control class-level memory length and then still needing a random selection process, our method is end-to-end and can directly select specific samples in one step fully automatically, showing significant effectiveness in semantic segmentation with the task-tailored state representations and a novel dual-stage action space.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]"," **Semantic Segmentation.** Semantic segmentation is a fundamental task in computer vision and has been extensively studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14]. Most of the existing semantic segmentation methods are based on fully convolutional networks (FCN) [1], which are designed to extract high-level semantic features from the input image and then refine them to obtain pixel-level predictions. For example, SegNet [1] and DeepLab [4] are the pioneering works in this direction. To further improve the segmentation performance, many works [15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30] have been proposed. For instance, SegFormer [11] and SegFormer++ [12] are two pioneering works that use a pure transformer encoder to encode an image as a sequence of patches. Segmenter [14] and SETR [13] extend the Transformer architecture to semantic segmentations by introducing a point-wise linear decoder or a mask trans-former decoder. In addition, PSPNet [6] and PSPNet++ [10] introduce a pyramid pooling module to capture multi-scale context information. Recently, some works [7][7] introduce low-level texture information to improve the performance of segmentation.
-STL-Net [7] introduces a statistical texture learning network to capture the global statistical texture information of the input images.
the performance of the image segmentation task.


**Continual learning.** Continual learning (CL) [31][32][33][34][35] aims to train a model to incrementally learn more and more tasks without storing any data from the previous tasks. CL methods can be divided into two categories: replay-based methods and distillation-based ones. The former one stores a small number of samples from previous tasks for replay, while the latter one keeps a small set of samples for continual learning. For the replay method, the memory buffer is constructed to store a small amount of samples of the previous classes for replay. The memory buffer can be constructed either randomly or based on a single-factor-driven hand-crafted strategy, which has no guarantee to be optimal. For memory sample selection, some methods [26][25] select the memory samples based on the constraint"," **Sign Language Recognition.** Current studies on SLR can be categorized into two types: isolated SLR and CSLR. For isolated SLR, researchers propose a variety of methods to utilize reliable hand pose as an intermediate representation. For instance, Zhong [1] and Huang [2] proposed an hand-prior model for word-level SLR by capturing the subtle pose changes between signs and surrounding characters. Liang [3] developed a hand-model-aware neural network to recognize a sign by leveraging hand pose via a graph convolutional network. On the other hand, Zhou _et al_.  and Wan _et al_.  found that 3D convolutions could effectively extract the appearance and hand movement. To speed up the training, these studies employed the sequence learning strategy based on Connectionist Temporal Classification (CTC) loss [4][5][6] or Hidden Markov Model (HMM) [7][8][9][10]. To improve the classification quality, researchers often modify CTC loss to better encourage the learning of temporal information [11][12]. To refine the generated feature, both Zhong [13] and Sun [14] proposed temporal lift pooling to increase the receptive fields of a network.

For CSLR, existing studies usually rely on frame-to-frame alignments to discover hand trajectories. For instance, Li _et al_. [15] proposed multi-stream CNN-HMMs to discover temporal alignments between sign recognition and text generation. Du _et al_. [16] fused various cues (_e.g_., the face and the hand) into a network. To improve the alignment quality, Li _et al_.  proposed an end-to-end trained alignment module. Gu _et al_. [17] proposed an algorithm to optimize the alignment maps for more precise sign recognition. Although the existing studies make great progress on the CSLR, the trajectories are only captured between adjacent frames and no body motion information across consecutive frames is well leveraged.

**Correlation Network.** Correlation network was first introduced in [18] to effectively perform the matching and regression of an object across frames.  used the correlation network to compute flow between two frames. Recently, the correlation network [19][20][21][22] has been widely used to compute flow between two consecutive frames, thereby effectively detecting the motion changes within a video. Motivated by the idea of correlation network, researchers [23][24] proposed correlative operators for representation learning. In particular, these correlation operators enable the network to fuse the feature maps of different layers, thereby utilizing cross-layer correlation information for a better representation learning. Furthermore, Sanchez [25] proposed to leverage a correlation operator to effectively fuse multi-modal features to recognize actions in videos. In this work, we introduce correlation network to tackle the CSLR problem. By",,"<In the field of continual semantic segmentation, there have been several approaches to alleviate the issue of catastrophic forgetting when introducing new classes for training. SegNet [1] introduced a deep fully convolutional neural network architecture for semantic pixel-wise segmentation. The architecture consists of an encoder network and a corresponding decoder network, allowing for pixel-wise classification. Similarly, BiSeNet [2] has focused on real-time semantic segmentation, proposing an efficient structure named Short-Term Dense Concatenate network (STDC network). These approaches have shown promising results in segmenting objects at multiple scales and incorporating image context at multiple scales. Additionally, DeepLab [3][4] has addressed the poor localization property of deep networks by combining responses at the final deep convolutional neural network layer with a fully connected Conditional Random Field. These methods have set new benchmarks for semantic image segmentation tasks.>

<Other significant contributions in the field include Rethinking Atrous Convolution for Semantic Image Segmentation [5], Pyramid Scene Parsing Network [6], Learning Statistical Texture for Semantic Segmentation [7], Dual Attention Network for Scene Segmentation [8], and ACFNet: Attentional Class Feature Network for Semantic Segmentation [9]. These works present novel techniques such as employing atrous convolution to explicitly adjust filter's field-of-view, utilizing global context information for scene parsing, and proposing an attentional class feature module to calculate and combine different class centers for coarse-to-fine segmentation. The proposed methods have shown state-of-the-art performance on various datasets, demonstrating the effectiveness of their approaches.>

<Additionally, recent work has also addressed the issue of continual learning in the context of semantic segmentation. One approach involving continual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations [19] aims to reduce forgetting while improving the recognition of novel classes. Another approach, PLOP: Learning without Forgetting for Continual Semantic Segmentation [20], has focused on retaining the information about existing classes without storing their data and effectively learning new classes by relating them with the previously seen classes. Furthermore, Gradient-based Editing of Memory Examples for Online Task-Free Continual Learning [25] proposes a framework that edits stored examples in continuous input space via gradient updates, creating more challenging examples for replay to overcome catastrophic forgetting.>

<Continual learning methods have also been explored in other fields such as robotics and reinforcement learning. For instance, there have been studies on deep reinforcement learning for robot control [30], incremental learning techniques for semantic segmentation [21], and reinforcement learning-based network compression for deep learning models [33]. These works have shown promise in addressing the challenges of continual learning and memory management in various domains.>

<In the context of autonomous systems, there has been a growing focus on active learning and continual learning. Meta Agent Teaming Active Learning for Pose Estimation [31] introduces a novel framework for actively selecting and labeling informative images for effective learning. Similarly, Temporal Complementarity-Guided Reinforcement Learning for Image-to-Video Person Re-Identification [34] presents an approach that employs deep reinforcement learning to dynamically select suitable frames from gallery videos for person re-identification, balancing efficiency and accuracy. These active learning and reinforcement learning-based techniques have shown significant potential in improving learning efficiency and performance in dynamic environments.>

<To conclude, the fields of continual semantic segmentation, continual learning, and reinforcement learning have seen exciting developments in recent years. These advancements have addressed challenges such as catastrophic forgetting, memory management, and active learning, paving the way for more robust and efficient learning algorithms in various domains. The integration of deep reinforcement learning, active learning, and continual learning has shown promise in addressing the complex and evolving nature of real-world learning tasks.>"
3898,3898," **Sound source localization.** Sound source localization aims to localize the corresponding sound source in a video. It has been widely studied in the past few years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22]. In this section, we briefly review the related works.

**Sound source separation.** The goal of sound source separation is to separate the sound from the visual input. Early works [23][24][25] rely on hand-crafted features to separate sound. Recently, deep learning-based methods have been proposed to learn sound separation models. For example, Song _et al._[25] propose a joint statistical model to learn the separation function. However, these methods are limited to a fixed number of sound sources, and they cannot learn compact class-aware representations for individual sound sources. In contrast, our method can localize multiple sound sources simultaneously by learning category-wise audio-visual features from the input audio mixture and image simultaneously. In addition, we propose a novel framework to learn category-aware semantic features for each source, which can be used as a guidance for sound source localization in an end-to-end manner. **Audio-visual clustering.** Recently, some works [26][27][28][29][30] propose to cluster the audio and visual features to learn more discriminative representations for sound separation. For instance, Wang _et. al._ propose to learn a clustering function to cluster audio and image features. Different from these methods, our proposed framework can learn class-wise semantic representations for each sound source.

 propose a self-supervised method to learn audio representations by clustering the visual features. They also propose a contrastive learning framework to improve the performance of the clustering task. In this paper, we introduce a new framework to jointly learn the audio representations and the corresponding visual features for the sound separation task.

 proposes to learn visual representations and audio representations in an unsupervised manner. They first cluster the visual and audio features, and then use the learned visual representations to generate the corresponding audio representations. Our method differs from them in two aspects: (1) Our method can learn the visual representations for multiple sources simultaneously, and (2) Our framework can be trained in a fully supervised manner.

 firstly propose to use visual features as a supervisory signal to train the localization model. They propose a two-stream network, which firstly generates visual features and then uses them to generate audio features. The visual features are concatenated with the audio features to generate a unified representation. The audio features are then used as the supervision signal for the localization network. In our work, we also use the concatenation method to generate visual features, which is different from their method.

 also propose to fuse the audio information. They concatenate the visual information and audio information to generate an audio representation. They use the audio representation and the visual representation to train a localization model simultaneously. They train the two networks separately."," **Audio-Visual Joint Learning.** Audio-visual joint learning has been addressed in many previous works [1][2][3][4][5][6][7][8][9][10] to learn the audio-visual correlation between two distinct modalities from videos. Such cross-modal alignments are beneficial for many audio-visual tasks, such as audio-event localization [11], audio-visual spatialization [12][6][13], audio-visual navigation [14][15] and audio-visual parsing [16][17]. In this work, our main focus is to learn compact audio-visual representations for localizing individual sources on images from sound mixtures, which is more demanding than the tasks aforementioned above.

**Audio-Visual Source Separation.** Audio-visual source separation aims to separate individual sound sources from the audio mixture given the image with sources on it. In recent years, researchers [4][18][19][20] have tried to explore diverse pipelines to learn discriminative visual representations from images for source separation. Zhao _et al._ first proposed a ""Mix-and-Separate"" network to capture the alignment between pixels and the spectral components of audio for the reconstruction of each input source spectrogram. With the benefit of visual cues, MP-Net [20] utilized a recursive MinusPlus Net to separate all salient sounds from the mixture. Tian _et al._[18] used a cyclic co-learning framework with sounding object visual grounding to separate visual sound sources. More recently, more types of modalities have been explored to boost the performance of audio-visual source separation, such as motion in SoM [10], gesture composed of pose and keypoints in MG [21], and spatio-temporal visual scene graphs in AVSGS [22]. Different from them, we do not need to recover the audio spectrogram of individual sources from the mixture. Instead, we leverage the category-aware representations of individual sources to localize the corresponding regions for each source, where learnable audio-visual class tokens are applied as the desirable guidance.

**Visual Sound Source Localization.** Visual sound source localization is a typical and challenging problem that predicts the location of individual sound sources in a video. Early works [23][24][25] applied traditional machine learning approaches, such as statistical models [25] and canonical correlation analysis [24] to learn low-level alignment between audio and visual representations. With the success of deep neural nets, recent researchers [26][5][27][28][9][29] explored many architectures to learn the audio-visual correspondence for localizing single-source sounds. Attention10k [9] localized a sound source in the image using a two-stream architecture with an attention mechanism. Hard sample mining was introduced in LVS [26] to optimize a differentiable threshold-based contrastive loss for predicting discriminative audio-visual correspondence maps. More recently, a multiple-instance contrastive learning framework was proposed in EZVSL [28] to align regions with the most corresponding audio without negative regions involved.

Due to the natural mixed property of sounds in our environment, recent works [30][31] also have explored different frameworks to localize multiple sources on frames from a sound mixture simultaneously. DSOL [31] utilized a two-stage training framework to deal with silence for category-aware sound source localization. More recently, a contrastive random walk model was trained in Mix-and-Localize [30] to link each audio node with an image node using a transition probability of audio-visual similarity. While those single-source and multi-source approaches achieve promising performance in sound localization, they can only handle a fixed number of sources and they cannot learn discriminative class-aware representations for individual sources. In contrast, we develop a fully novel framework to aggregate compact category-wise audio and visual source representations with explicit learnable source class tokens. To the best of our knowledge, we are the first to leverage an explicit grouping mechanism for sound source localization. Our experiments in Section 4.2 also demonstrate the effectiveness of AVGN in both single-source and multi-source localization.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]"," **Sound source localization.** Sound source localization is a typical and challenging task that predicts the location of sound sources in a video. It has been extensively studied in recent years, and there are two main categories of approaches: single-source methods and multi-source ones. Single-source approaches [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31] have been proposed to solve this task. For example, Song _et al_. [9] proposed a two-stream network structure that handles each modality, with attention mechanism, for sound source localization and separation. In [10], the authors proposed a motion-based system that explicitly captures such motion cues for sound separation and separation, and a curriculum learning scheme that exploits the inherent coherence of audio-visual signals from a large amount of unlabeled videos. In, the authors introduced a novel audio source localization task that localizes the sound source in a given video, and they proposed an end-to-end learnable model called Deep Dense Trajectory (DDT), which exploits the co-occurrence of audio and visual signals from large quantities of videos. However, these methods mainly focused on the audio-source localization task, and did not consider the mixed property of multiple sources in the original space, which is the focus of our work.

. In this paper, we propose a novel AVGN framework that can learn category-wise semantic features for each source from the input audio mixture and image to localize multiple sound sources simultaneously. Moreover, our AVGN can learn compact class-aware source features for individual sources, which can be used as guidance for localizing the corresponding visual regions.

"," Many research works have been done in the field of action recognition and video domain adaptation. We provide a brief overview on the following works: (1) deep learning-based action recognition [1][2][3][4][5][6][7][8]; (2) R-CNN-based action recognition [9][10]; (3) Transformer-based action recognition [11][12][13]; (4) Domain Adaptation in Action Recognition [14][15][16][17][18]; and (5) CLIP-based work on video domain adaptation [19][20].

_Action recognition_ The emergence of deep learning has led to a rapid progress in action recognition. Deep convolutional neural networks [1][2] have completely replaced previous pipelines, using previous hand-crafted features. Following this, 3D convolution networks [6][8] were proposed to better capture the temporal information in videos. Subsequently, two-stream networks [9][10] were developed for more efficient spatiotemporal representations. With the advent of Transformer networks [11][12][13], an additional representation and connection to the background video feature stream has improved action recognition. Overall, deep networks have led to improvements in classification accuracy [1][2][1][9][8][20], especially when combining 2D and 3D networks [6][8]. Recently, [18] has released a large-scale video dataset [18].

_Domain adaptation for action recognition_ The works in this category focus on the task of transferring the knowledge of source domain to the target domain, for example, large-scale UCF101 to small-scale Kinetics. _Transfer learning_  is used in the majority of works for UDA-action recognition, for example [15], where information from the source is projected to the target using adversarial learning, _i.e_., forcing the model to learn features that are representative of the target. Alternatively, [14] uses self-supervised domain alignment. The main drawback of these methods is that they assume the target domain to be _closed set_, _i.e_., target contains only classes that belong to the source.

To resolve this limitation, [17] proposes a new framework for the Open Set UDA, termed OI-ODDA. It models the problem as an inverse zero-sum auction problem, where the bidder tries to buy the instances by offering high enough price while the seller tries to sell the instance by choosing high enough price. However, its bidding process is time-consuming. In contrast to the work in [17], we propose a zero-shot approach for detecting target-private instances using the CLIP.

_CLIP-based work on video domain adaptation_ This works [21][19][20] focus on adapting the target domain. In",,"<The field of audio-visual source localization has seen significant progress in recent years, with several approaches leveraging the complementary information from both auditory and visual data to localize sound sources in videos. Look, Listen and Learn [1] introduced the novel ""Audio-Visual Correspondence"" learning task and demonstrated state-of-the-art performance in sound classification by training visual and audio networks from scratch using unlabelled videos. SoundNet [2] proposed a method to learn acoustic representations using unlabeled video data, achieving significant improvements in acoustic scene/object classification. Both these works highlight the importance of exploiting the natural synchronization between vision and sound for learning effective audio-visual representations.

Music Gesture for Visual Sound Separation [3] presented a structured representation called ""Music Gesture"" to model body and finger movements of musicians during a performance, which led to notable improvements in music source separation tasks. Audio-Visual Sound Separation Via Hidden Markov Models [4] proposed a method to exploit audio-visual cues for speech separation under non-stationary noise conditions and demonstrated promising improvements in machine intelligibility. These works showcase the potential of leveraging structured representations and hidden models to enhance audio-visual source separation tasks while overcoming challenging environmental conditions.

Deep Multimodal Clustering for Unsupervised Audiovisual Learning [5] proposed a novel unsupervised audiovisual learning model, Deep Multimodal Clustering (DMC), which effectively captures multiple audiovisual correspondences and outperforms human performance in certain tasks. Learning Representations from Audio-Visual Spatial Alignment [6] introduced a self-supervised pretext task for learning representations from audio-visual content using a transformer architecture to combine representations from multiple viewpoints, highlighting the importance of leveraging spatial cues in audio-visual representation learning.

Robust Audio-Visual Instance Discrimination [7] presented a self-supervised learning method for audio and video representations, addressing the challenges of faulty positives and negatives within self-supervised contrastive learning. Additionally, Audio-Visual Instance Discrimination with Cross-Modal Agreement [8] introduced a method that explores cross-modal agreement, providing gains in downstream classification tasks. These approaches contribute to the robust learning of audio-visual representations and instance discrimination for various tasks.>

<Impressive advancements have been made in the field of audio-visual source localization with the development of novel learning paradigms and multimodal fusion techniques. Learning to Localize Sound Source in Visual Scenes [9] presented a two-stream network structure with an attention mechanism for sound source localization, showcasing the potential for both unsupervised and semi-supervised learning settings. The Sound of Motions [10] proposed a motion-based system that explicitly captures motion cues for sound localization and separation, achieving improved performance in separating musical instrument sounds and addressing challenging duet scenarios.

Dual Attention Matching for Audio-Visual Event Localization [11] introduced a dual attention matching module for audio-visual event localization, emphasizing the importance of global feature representation and local temporal information modeling. 2.5D Visual Sound [12] proposed a method to convert mono audio into binaural sound by leveraging visual frames, demonstrating the ability to obtain spatialized sound from single-channel audio. These approaches showcase the potential of leveraging multi-modal fusion and attention mechanisms for effective audio-visual event localization and spatial sound processing.

Self-Supervised Generation of Spatial Audio for 360 Video [13] introduced an approach to convert mono audio into spatial audio using end-to-end trainable neural networks, demonstrating the potential for spatial sound localization based on synchronized 360° video and audio data. Learning to Set Waypoints for Audio-Visual Navigation [14] presented a reinforcement learning approach for audio-visual navigation, incorporating waypoints and acoustic memory for improving the understanding of complex 3D environments. These works highlight the significance of developing advanced navigation and spatialization techniques for audio-visual source localization and event parsing in immersive environments.>

<SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning [15] introduced a platform for on-the-fly geometry-based audio rendering in 3D environments, providing high-fidelity acoustic simulations for audio-visual research tasks. This platform facilitates research in audio-visual navigation, mapping, source localization, and acoustic matching by enabling continuous spatial sampling and simulation of novel environments. Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing [16] proposed a method to leverage additional cross-video and cross-modality supervisory signals for weakly-supervised video parsing, demonstrating the advantages of exploiting event co-occurrence across audio, visual, and audio-visual streams.

Exploring Heterogeneous Clues for Weakly-Supervised Audio-Visual Video Parsing [17] investigated the weakly-supervised audio-visual video parsing task and explored event co-occurrence across different videos and modalities to facilitate training with only video-level annotations. These approaches emphasize the importance of leveraging additional signals and heterogeneous clues for weakly-supervised audio-visual event parsing and classification in videos.>

<Recursive Visual Sound Separation Using Minus-Plus Net [20] proposed a novel framework, Minus-Plus Network (MP-Net), for visual sound separation, which recursively separates sounds in the order of average energy, leading to state-of-the-art results across datasets with different types and numbers of sounds. Music Gesture for Visual Sound Separation [21] introduced a keypoint-based structured representation to explicitly model body and finger movements of musicians during music performances, achieving strong improvements in musical instrument source separation tasks. Visual Scene Graphs for Audio Source Separation [22] presented the Audio Visual Scene Graph Segmenter (AVSGS), a deep learning model that segments the visual graph into subgraphs associated with distinct sounds, demonstrating the potential for joint audio-visual segmentation and source separation.

Audio Vision: Using Audio-Visual Synchrony to Locate Sounds [23] explored the use of audio-visual synchrony to locate sound sources, leveraging a system that searches for regions of the visual landscape correlated with acoustic signals. Pixels that sound [24] introduced a stable and robust algorithm based on canonical correlation analysis for localizing visual events associated with sound sources, demonstrating high spatial resolution and effectively detecting dynamic pixels correlated with sound. These works showcase the significance of leveraging structured representations, recursive separation methods, and audio-visual synchrony for effective visual sound localization and separation in complex environments.>

<The literature on audio-visual source localization and separation has advanced with the development of self-supervised learning, multimodal fusion, and structured representation techniques. A Closer Look at Weakly-Supervised Audio-Visual Source Localization [27] extended popular benchmarks to address the limitations of existing evaluation protocols, aiming to capture the model's performance in scenarios with no visible sound sources. Localizing Visual Sounds the Easy Way [28] introduced EZ-VSL, a simple yet effective framework for audio-visual source localization without relying on the construction of positive and negative regions during training, achieving state-of-the-art performance on popular benchmarks.

Learning Sound Localization Better from Semantically Similar Samples [29] proposed leveraging hard positives in contrastive learning by incorporating their response maps directly into the training objective, demonstrating favorable performance on challenging datasets. Mix and Localize: Localizing Sound Sources in Mixtures [30] presented a method for simultaneous localization of multiple sound sources within a visual scene, demonstrating the potential for joint audio-visual association and multi-source localization. These works contribute to the advancement of techniques for enhancing robustness, addressing challenging scenarios, and achieving state-of-the-art performance in the domain of audio-visual source localization and separation.>"
1271,1271," **Visual SLAM.** Visual SLAM [1][2][3][4][5][6][7][8][9][10][11][12] aims to localize a robot in an environment with a pre-built map. The goal of map sparsification is to acquire a subset of the original map to reduce the memory footprint and improve the robustness of the localization system. In this section, we briefly review the related works.

**Mapping and localization.** A map is a 3D point cloud representation of the environment. A map can be represented as a set of 2D descriptors [13][14][15][16][17][18] or 3D points [1]. The former one is more compact than the latter one, but the former is more memory-expensive. The latter one is the most memory-efficient, but is less accurate. In the following, we review the most related works on map compression and localization, and then discuss the differences between our method and the previous ones.

 first compresses the map into a sparse set of descriptors and then compresses it back to the original point cloud. They use a quadratic term in mixed-integer programming to enforce a uniform distribution of selected descriptors, which requires high memory capacity and heavy computation. In contrast, our method compresses a map in a linear form and uses a discretized grid to represent the map. Furthermore, we introduce a space constraint term based on 2D discretization, which reduces the influence of different spatial distributions between the map and query sequences.

[1] first compress the map to a coarse-to-fine format and then convert it to a fine-grained format. However, their method requires a large number of hand-crafted features, which is not suitable for large-scale map compression. In addition, they also need to convert the map back to a point cloud, which increases the memory consumption significantly.  first compress a map into an unordered list of points and then process it with a graph convolutional neural network (GCN) to convert it back into a map. Their method is more efficient than [1], but it still needs to process the entire map.  further compresses an entire map and then processes it with GCN to obtain the final compressed map. In their method, the map is first decomposed into a coarse grid and then processed with a GCN, which also requires a high memory consumption. In comparison, our map is compressed by discretizing it into 2D and 3D grids, which can be processed in a much faster way.

 is the first work to introduce the idea of map compression for visual SLAM, and it compresses maps into a compact set of keypoints. They first convert the whole map to 2D keypoints and then apply a graph-based GCN on it to obtain a compact map. They also use a fixed number of points to represent each keypoint, which limits the efficiency of their method.

 compresses keypoints into a fixed-size grid and processes it using GCN. They then process"," Map compression or sparsification has received many research attention in SLAM and structure from motion (SfM) because of the burden of maintaining and utilizing a large map. Different methods are also proposed to solve this problem. In [1][2][3], the descriptors are quantized to compact forms to reduce memory consumption. Mera-Trujillo et al. [4] propose a constrained quadratic program method and design an efficient solver to select landmarks. Conteras et al. [5] develop a map compression method based on traveled trajectories, and they also propose an online system performing SLAM and map compression simultaneously [6]. Some SLAM systems [7][8][9] are also designed to select and build a compact map online, but much more landmarks are still remained compared with the offline methods. Recently, some learning-based map sparsification methods have also appeared and achieved good results [10], but they require high-cost GPUs to train and struggle to be applied in different unseen scenes.

Map sparsification is more commonly regarded as a \(K\)-cover problem. Since \(K\)-cover is an NP-hard problem, Li et al.  propose to use a greedy algorithm to select the landmark observed by the largest number of not-yet-fully covered frames. Cao et al. [11] also use a greedy algorithm to select landmarks, considering both the coverage and distinctiveness. In [12], the original \(K\)-cover method is improved by predicting the parameter \(K\). Similarly, the landmarks are scored and sampled based on the observation count, covariance, and descriptor stability in [13]. Moreover, the authors come up with more scoring factors to rank landmarks for selecting in [14]. These methods can provide compact maps for localization, but depend on a careful design of selecting methods and cannot get the optimal solution.

Another \(K\)-cover based map sparsification solution is to be formulated as a mixed-integer programming problem, which is introduced in detail in [15]. The linear formulation does not consider the distribution of the selected landmarks but can be solved efficiently, and therefore it is adopted in some later works [14][16]. The mixed-integer quadratic programming formulation encourages the uniformly dis tributed landmarks but requires a high capacity of memory and heavy computation [15] and becomes impractical for large maps. To improve the computation efficiency, Dymczyk et al. [17] have to divide the whole map into several small parts and perform map sparsification separately. Camposeco et al. [18] keep the linear form of map sparsification but divide each image into \(q\) uniformly-sized cells and let each cell be covered by \(K/q\) landmarks. This method seems reasonable, but we find in the experiments that the number of corresponding selected landmarks in some images is much less than \(K\) actually, and thus the localization results get worse. Similarly, the proposed method also makes use of discretized images, but the image is still considered as a whole and thus avoids the above problems.

All of the above methods are based on the existing association between landmarks and images, thus landmarks are selected only to ensure localization performance of these images. For an arbitrary query image with a different viewing perspective, localization may fail unexpectedly, especially when the compression ratio is high. To solve this problem, the proposed method considers landmark visibility in the 3D space and adds constraints to preserve enough visible landmarks for the whole space in compact maps.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"," **Map Sparsification.** Map sparsification aims to select a subset of the original map to reduce the memory footprint and improve the localization performance of the map. Existing methods can be divided into two categories: feature-based methods [1][2][3][4][5][6][7][8][9][10] and feature-free methods [11][12][13][14][15][16][17][18].

**Feature-based Methods.** Feature-based map sparsifiers select a small subset of landmarks that are most relevant to the mapping and query sequences. The selection criteria can be categorized into two types: feature selection and feature matching. Feature selection methods select landmarks that contribute the most to the map and query sequence. The feature selection criteria is usually formulated as a k-means optimization problem [7][7] or a random sampling problem [8][5]. The feature matching methods select the landmarks that have the highest matching accuracy to the query sequence, which are more suitable for real-time applications. For example, [8] uses the Max-logDet matrix revealing metric to select good feature matching candidates. [7] uses a probabilistic model to select features that are of high estimation utility for localization in the SLAM/SfM estimation process. [5] uses traveled trajectory analysis to select landmarks. [9] proposes an information-based reduced landmark SLAM algorithm to select landmark candidates with minimal estimation error. [10] proposes a heterogeneous GNN to select sparse points for long-term visual localization. However, feature selection methods require high memory capacity and heavy computation.




"," Previous methods [1][2] utilize learnable gating functions or multiple view heads to obtain better performance in multimodal fusion. These methods attempt to align the multimodal information in the same space, but usually rely on tuning each modality separately. We focus on pre-training methods, where the method of fusing multiple modalities is not important since all modalities are learned from scratch.

Prompt-based fine-tuning approaches [3][4][5] have recently attracted a lot of attention. Existing methods aim to add prompt vectors for fine-tuning, which can be easily finetuned on a new dataset. Note that our proposed approach is in the general category of prompt-based methods.

Multimodal fusion approaches can be divided into three categories. The first category is based on multi-task learning [6][7][8][9][10], where modality-specific networks are used in addition to the common network. The second category is parameter-efficient [11][12][13][14][15][16] approaches, which reduce the memory usage of models by adding extra heads for new modalities to the pre-trained models. The third category of methods, such as CLIP [17] and PaLM [18], does not directly fine-tune the original parameters of pre-trained models. Instead, they are trained on downstream tasks with masked language modeling and few-shot learning strategies, and are expected to fine-tune on a new dataset when required. In this paper, we adopt a parameter-efficient multimodal fusion approach.

**Prompt-based approaches.** Prompt-based approaches [19][20][21][22] are recently popular for large-scale pre-trained models. As an extension of this, Visual Prompting approaches [23][24][25] allow users to obtain high-quality downstream results without fine-tuning pre-trained models with a few samples. They are usually based on prompt tuning [26][27][28][21]. These approaches seek to learn how to ask the pre-trained models to output the target labels in an unsupervised way by adjusting the input text prompts. In particular, a textual prompt that is only 5-10 tokens long is sufficient to learn the optimal task-specific prompt that can allow the models to obtain satisfactory results. This procedure can be made even more effective by learning the distribution of prompts [26]. Our work is also built on prompt tuning [28] and we introduce a novel approach to fusing unimodal transformers based on prompts. We design prompt-based approaches that are applicable to a large variety of multimodal models.

",,"<Recent years have seen a surge in research focused on map sparsification for efficient visual-based localization. Akien et al. [1] presented a strategy for structure-based localization using fine vocabularies, and their method showed robust performance while reducing memory footprint. In contrast, Wang et al. [2] proposed a cascaded parallel filtering method for memory-efficient image-based localization, which leverages feature, visibility, and geometry information to filter wrong matches. This approach achieved competitive localization performances while addressing memory consumption and match ambiguity. In another study, Li et al. [12] formulated 3D point cloud simplification as a weighted K-Cover problem and proposed an adaptive exponential weight function to select a subset of points for efficient localization. They demonstrated that their approach outperformed state-of-the-art methods for image-based localization application>.

<In addition, researchers have explored the concept of map compression in the context of visual SLAM and mobile robotics. For instance, Smith et al. [15] introduced a method for accelerating the matching process and reducing memory footprint using mixed-integer quadratic programming. Similarly, Zhang et al. [16] addressed efficient map compression for collaborative visual SLAM by applying feature coding and a minimum spanning tree to connect observations of map points. Their approach outperformed existing techniques and contributed to efficient long-term operation in large-scale environments. Moreover, Yang et al. [17] developed a map compression algorithm that approaches data reduction as a constrained optimization problem, resulting in improved scalability for virtually unlimited dataset sizes. Both their proposed hybrid compression algorithm and summary map representation showcased enhanced localization performance under stringent memory constraints [18]>.

<Furthermore, novel algorithms have been proposed to address the challenge of efficiently selecting a reduced number of landmarks and poses for robot localization and mapping. Wu et al. [9] presented an information-based approach to reduce the number of landmarks and poses in a SLAM estimate without compromising trajectory accuracy. Their results demonstrated a reduction in the number of landmarks and poses while maintaining minimal estimation error. These advancements in efficient map sparsification and compression techniques are vital for enabling effective robot navigation and localization in large-scale, memory-constrained environments.>"
4879,4879," **Semantic Segmentation.** Semantic segmentation is a fundamental task in computer vision. Recent years have witnessed remarkable progress in this field [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58]. In this paper, we focus on the long-tailed semantic segmentation task.

**Re-balancing.** The goal of this task is to improve the generalizability of the model to different data distributions. To achieve this goal, several methods have been proposed, such as re-sampling [59], re-weighting [60] and re-training [61][62][63][64][65][66]. In particular, the re-parameterization of the loss function has been widely studied in the literature. For example, the Re-weighted Softmax loss [65] is proposed to reduce the distributional shift between the training and testing phases. The Re-Weighted Mean Squared Error (RSE) loss [62] is a variant of the softmax loss, which has been shown to be effective in improving the generalization ability of the network. In this work, we propose to re-balance the feature distribution by introducing a category-wise distributionalignment loss.




\[\begin{tabular}{c,c,d,c} \hline\hline \multirow{hline{c,e,d} \multicol{c}{c}{e,c}\multirow}{hline {c}{d,e} \cline{c} & hline{d}{c-e,hline}{c+1}{c} \\hline{\multicol} \frac{1}{2}{c}\frac{2}{h-1}{4}{c}}\multirow {c,h-e}{c},c-c}\multicol {\frac{h-c}{1}{3}{c}}} \clines{c-d}{h,c}} \hlines{c,-c}{2,c}{5}{c.c}\\hline"," **Semantic segmentation.** Network architecture for semantic segmentation has evolved for years, from CNNs [1][2][3] to Transformers [4][5][6][7][1]. Another line of research works focuses on enhancing the extracted representations like integrating attention mechanisms [8][9][10][11] or context representations [12][13][14][15] into segmentation models. BLV is complementary to these various frameworks and improves several state-of-the-art methods consistently.

**Semi-supervised semantic segmentation.** To alleviate the heavy need for large-scale annotated data, semi-supervised semantic segmentation has become a research hotspot. There are two typical frameworks for this task: consistency regularization [16][17][18] and self-training [19][20][21][22]. Consistency regularization applies various perturbations [17] on training data and forces consistent predictions between the perturbed and the unperturbed input [18]. Self-training [23][24][20][23][25][13][26] uses the predictions from the pre-trained model as the ""ground-truth"" of the unlabeled data and then trains a semantic segmentation model in a fully-supervised manner. These two frameworks have no specialized operations for long-tail data. To this end, we provide a concise and generic approach that can be integrated into any framework.

**Unsupervised domain adaptive semantic segmentation.** UDA semantic segmentation aims at learning segmentation model that transfer knowledge from labeled source domain to unlabeled target domain. Early methods for UDA segmentation focus on enabling the model to extract to domain-invariant features. They align the cross-domain feature distribution at image level [27][28][28], feature level [29][30][31][32] and output level [33][32][34] via image style transfer [35][36][37][38][39], image feature domain discriminator [40][41][42][40] or well-designed metrics [43][44][45]. Follow-up study [46][47] suggests that the self-training-based pipeline leads to more consistent improvement. Recently, DAFormer [48] and HRDA [49] provide a self-training-based Transformer architecture together with many efficient training strategies, which can achieve consistent improvement over other competitors. BLV can be simply integrated into existing pipelines, and consistently improve their performance.

**Long-tail learning.** Since the long-tail phenomenon is common  in deep learning, the performance of the model tends to be dominated by the head category, while the learning of the tail category is severely underdeveloped. One intuitive solution to alleviate unbalanced data distribution is data processing, which typically consists of three ways: over-sampling [50][51][52], under-sampling [53][54] and data augmentation [14]. Various methods have been proposed to alleviate the long-tail phenomenon in semantic segmentation, which can be mainly divided into three settings: fully supervised [55][56], semi-supervised [57][58][59], and UDA [60][61][62]. It is noteworthy that existing methods are usually limited to a specific setting and lack generalizability.

**Noise-based augmentation.** To improve model robustness and avoid over-fitting, augmenting data with noise [63][64][65] at image level or feature level is widely applied to model training. Techniques [66][67] like Dropout [68], color jittering [69], gaussian noise, are the most common methods and proved to be simple yet efficient, but they might also introducing task-agnostic bias . Another line of research aims to optimize the noise added in extracted features to ""fool"" the model [70]. The optimized noise is defined as adversarial examples, which are commonly recognized as the hard sample for the model. Methods like _M2m_[71] and _AdvProp_[72] utilize adversarial examples to augment the training data and significantly improve model robustness. Prior arts focus on improving the robustness yet ignoring the prevalence of long-tail data, whereas our BLV can alleviate the feature squeeze caused by long-tail data effectively.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]"," **Semantic Segmentation.** Semantic segmentation is a fundamental task in computer vision and has been extensively studied in recent years [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72]. In this paper, we focus on the long-tailed semantic segmentation task.

**Unsupervised domain adaptation (UDA).** UDA for semantic segmentations aims to transfer the knowledge from a label-rich source domain to an unlabeled target domain. Existing UDA methods can be roughly divided into two categories: (1) _Style transfer_ methods [28][28] transfer the style of source domain images to the target domain style, such as using style transfer [28] or domain flow [27] to transfer source images into different styles in the intermediate domains [27]. (2) _Class-level alignment_ methods align the source and target domains at the class-level. For example, [38][37] and [39] align the low-frequency and high-frequency feature spaces, respectively. For instance, [37] aligns the texture of the source domain image to the texture in the target one. [40] and  align the patch-level feature space. (3) _Category-level alignments_ align the feature space of different categories. For examples, [39][41] align features of stuff and things categories. [42][47] align class-wise feature spaces. [44][46] align feature spaces at the decision boundary. [45][44] align domain-specific features. [46][45] align intermediate feature spaces in the feature level. [47] and DAN [45] adapt the feature spaces of different classes in the classification and segmentation layers respectively. [48] align both the feature-level and category-level space. [61][42] align"," **Posterior approximation.** Modern approaches to Bayesian learning approximates the posterior distribution either by a low-dimensional representation, e.g. a Gaussian distribution over network weights ([3]; [4]; [9]), a flat distribution over network weights ([1]; [7]; ), or a restricted set of network architectures ([2]; [8]). When facing infinite-dimensional problems, one needs to use approximate Bayesian inference. This is achieved by variational Bayesian inference (VBI), which approximates the intractable integral in the Bayesian evidence by a tractable distribution by using the variational principle. The Kullback-Leibler (KL) divergence between the true posterior and the approximating distribution can be used to quantify how well the distribution approximates the true posterior ([9]; [6]; ; [5]). Unfortunately, the KL-divergence does not always lead to better approximations ([10]). For this reason, prior works like ([8]) and ([11]) proposed other loss functions to encourage the diversity of the latent variables to approximate the posterior.

In contrast, we use stochastic ensembles to approximate the posterior. This yields more accurate results without relying on penalty functions or assumption on the true posterior distribution. Furthermore, ensembles are not restricted to use specific distributions for modeling the latent variables, and have proved useful for uncertainty quantification ([12]; ; [16]; [13]).

**Deep ensembles.** The core idea of deep ensembles is to randomly initialize different networks and treat them as different views of the true posterior. Training them independently and then averaging the predictions as the final output can result in a large ensemble. Most approaches focus on the neural network architecture and the learning methods to find the best configuration of networks. [14] proposed to train networks with different architectures. [15] proposed to stack a series of neural networks with multiple layers and decouple the tasks of network architecture selection and weight initializations. While the combination of different architectures works well for the architecture selection part, the combination of multiple independent networks are difficult to train and introduce additional overfitting. Additionally, we need to select the number of networks, which limits the number of model diversities we can achieve.

To improve the scalability and efficiency of deep ensembles, recent work focused on the initialization or training methods. Some methods proposed different initialization methods to make different networks of the same architecture more diverse ([20]; [19]; [24]; [25]). Other methods proposed better optimization strategies to encourage different networks to represent different models and alleviate the catastrophic forgetting of more complicated models ([23]; [26]; [22]; [21]). There are also efficient deep ensembles proposed by leveraging insights from lifelong learning ([27]; [28]; [29]; [23]; [31]; [20]).

However, the main difference of our stochastic",,"<<In recent years, deep learning methods have shown remarkable success in the field of semantic segmentation. However, an inherent challenge in semantic segmentation is the long-tailed distribution of data, where certain classes have a significantly smaller number of samples compared to others. This imbalance can lead to features of the tail classes being squeezed into a narrow area in the feature space, impacting the overall performance of semantic segmentation models [1] [2]. To address this issue, various approaches have been proposed in the literature. For instance, some studies have focused on introducing category-wise variation into network predictions during training, leading to a balanced feature representation by assigning smaller variations to head classes and larger variations to tail classes [3] [4].

Additionally, the use of consistency regularization and pseudo-labeling has been explored for semi-supervised semantic segmentation, aiming to leverage the unlabeled target domain data for model adaptation. These approaches have shown promise in handling class imbalance and improving model performance on under-represented categories [5] [6]. Furthermore, noise injection methods have been investigated as a means of enhancing the generalization capability and robustness of deep neural networks to tackle class imbalance and overfitting [7] [8]. Another line of work has focused on re-distributing biased pseudo-labels to address the imbalance in category representations for semi-supervised semantic segmentation [9] [10].

Moreover, techniques such as adversarial training and domain adaptation have been proposed to mitigate domain shift issues and improve model generalization performance, especially in the presence of imbalanced data distributions [11] [12]. Furthermore, transformation-based augmentation methods, such as additive Gaussian noise and data perturbations, have been employed to enhance the robustness of deep neural networks to different corruptions and improve overall performance on imbalanced data [13] [14].

In addition to architectural modifications and augmentation techniques, the use of adversarial examples has also been explored to improve the robustness and generalization capability of deep neural networks, especially in the presence of imbalanced datasets [15] [16]. The adoption of major-to-minor translation techniques for imbalanced classification has demonstrated potential to enable models to learn more generalizable features of minority classes by leveraging majority information [17] [18].>>

Please note that the provided related work represents a subset of the available literature on the topic of long-tailed semantic segmentation and related methods."
3612,3612," **Cooperative Learning in Neural Sequence Models.** Our work is related to the cooperative learning in neural sequence models (NMS) () and language generative adversarial networks (L-GANs) ([1]), where the generator and discriminator are trained to cooperate to produce output sequences during training. In particular, our CoRe framework is inspired by the idea of L-GAN, where the generators and discriminators are trained in a cooperative manner to improve the performance of each other. However, our work differs from these works in the following aspects: (1) Our CoRe is a PLM-based method, which can be applied to solving MWPs, while L-GANs are not; (2) Our work focuses on generating reasoning paths instead of generating a single answer, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator.

**Reasoning-Induced PLMs.** Recently, there has been a surge of interest in applying PLMs to solve complex reasoning tasks, such as the math word problem (MWPs) ([2]; [4]; [6]; [5]; [3]; ). [2] and [4] proposed to train transformers to generate a sequence of reasoning steps before producing the final answer. [5] proposed a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting with a diverse set of reasoning paths. [6] proposed zero-shot prompting to generate reasoning paths with exemplars. [3] proposed dual-system, neuro-symbolic reasoning, which consists of an immediate reaction system (system 1) and a delicate reasoning systems (system 2). However, these methods only focus on improving the generalization ability of existing PLMs, while our work aims at improving the reasoning ability of PLMs for solving complex MWPs.

 and  proposed to use reinforcement learning to train a sequence-to-sequence model to generate logical reasoning paths, which is similar to our work in the sense that the generator is trained to produce reasoning paths that are consistent with the ground-truth. In contrast, we propose a cooperative reasoning-induced PLM for solving math word problems, where system 1 is responsible for generating the reasoning paths and system 2 serves as the verifier.

 proposed a reinforcement learning-based approach for generating logical paths in the context of machine translation, which was later extended to the language modeling task by. However, their approach is not suitable for solving the mathematics word problem, as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. In this work, we develop a new approach to solve the mathword problem, which uses a dual reasoning framework with system 1 as the generator, system 2 as the system 2, and verifiers as the evaluators for the evaluation.

 first proposed a neural sequence model for solving arithmetic tasks. Their approach consists of a generator and a discriminator, which are trained with reinforcement learning (RL) to generate the logical paths and a verifier to evaluate the correctness of the generated logical paths. Our approach differs from theirs"," Dual-process theory (; ) argues there are two cognitive systems underpinning human reasoning: System 1 and System 2. The purpose of clarifying these systems is that they have the potential to help us construct artificial intelligence systems that benefit from human flexibility and methodical generalization.

Dual process system model guidance is not new. [3] simulated Systems 1 and 2 to improve consistency and coherence of neural networks. Similar to several studies ; [2]; [1], in addition to System 1 for the generation, we develop a distinct model as System 2, called Verifier. The Verifier checks the feasibility and correctness of the generator's content and collaboratively solves the reasoning task together.

Many works exploit the multi-step reasoning ability of language models.  showed that training a verifier to score the solutions generated by a fine-tuned GPT-3 could improve the performance compared to solely fine-tuning a GPT-3. [4] discovered that asking the language model to write the intermediate process could achieve better results on various NLP tasks. Likewise, Chain-of-Thought (CoT) prompts () prepended exemplars with intermediate reasoning steps as prompts and achieved SoTA on several reasoning benchmarks by using large-scale PLMs. [5] further boosted CoT's performance by sampling a bunch of possible solutions and then obtained the final answer by majority voting. DIVERSE ([2]) proved diverse CoT prompts and an extra verifier were both helpful for PLMs to solve reasoning problems. [6] found that by simply adding ""Let's think step by step"" after the question. PLMs could successfully step by step solve the problems, called Zero-shot-CoT.

These above methods rely on extremely large language models, resulting in high computational cost and time-consuming. Moreover, several works (; [6]) point out that neither CoT nor Zero-shot-CoT is helpful to smaller models. While our method does not necessarily require extremely large PLMs and can work with models with different size scales, thus reducing computational cost and inference time. Our approach has competitive zero-shot performance thanks to the efficient and collaborative application of a dual-process system.

","[1, 2, 3, 4, 5, 6]"," **Cooperation in PLMs.** The cooperative reasoning-induced PLMs have been proposed to improve the performance of PLMs on math word problems. [1] propose to make the discriminator and generator cooperate to produce output sequences during training. [2] propose a diverse verifier to distinguish good answers from bad answers for a better weighted voting. [3] use a dual-system, neuro-symbolic reasoning module to improve coherence and consistency in neural sequence models by adding symbolic variations between generations. [5] propose self-consistency to replace the naive greedy decoding used in chain-of-thought prompting, which first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. [4] train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a ""scratchpad"", showing the results of intermediate computations.

**Zero-shot Learning.** [6] show that PLMs are decent zero-shot reasoners by simply adding""Let's think step by step"" before each answer. [6]; [2]; [3]; [1]; [5]; [4]; [6], which are the most related to our work, propose to add the ""Let's-think-step-by-step"" prompt to the question-answer pairs, and show that the PLMs can be good zero-step reasoners. However, their methods are designed to solve arithmetic and commonsense reasoning tasks, which are different from the math word problem.

-related tasks, and thus are not directly applicable to our problem. In addition, their approaches are not designed to be applied to the math problem, which is a more challenging task.

"," Diffusion-based Language ModelsDiffering from recent trends in autoregressive text generation, diffusion models [6]; [2]; [3]; [1]; [8] are well-established in the field of image synthesis [7]; [5], protein structure design [3], and language modeling [4]. For each task, diffusion models show good performance while suffering from scaling challenges. Our Ssd-LM is, to the best of our knowledge, the first diffusion language model that explicitly models discrete text with a simplex-based latent variable and receives syntax-guided text as input. Our design allows for local latent update in a semi-autoregressive way, facilitating decoding in both steps (forward and backward) using bidirectional contextual constraints.

Semi-Autoregressive Language ModelsNon-autoregressive language models [15]; [13]; [9] have been investigated as a means to improve the computational efficiency of sequence generation. [13] propose SAT to tackle the challenge of non-autoregressive language modeling in both training and decoding. The SAT maintains the autoregressive property in global (by assigning a large weight to the global start token) but relieves the cost in local by only using a single token as the local output at each time step. [15] propose the IRev model, which employs an iterative refinement mechanism to locally fine-tune the generated sequence at each time step. [12] propose the FastDseq model, which applies discrete latent variable to partially skip prediction for efficiency. [14] propose the Levenshtein Transformer, which allows insertion and deletion operations in addition to the typical unit selection. These models can be viewed as basic building blocks for Ssd-LM. In contrast, our Ssd-LM models each discrete token in a discrete latent variable for high quality language generation with limited computational cost, by proposing semi-autoregressive decoding.

Constrained Text GenerationIn the context of constrained text generation, early work primarily focuses on generation rejection. [10]; [18]; [17] employ Langevin dynamics in latent space to directly guide the autoregressive models for constrained text generation. [21]; [19] utilize latent space constraint for generating specific tokens.  re-weight the probability distribution over the latent space to mitigate spurious term. [20] utilize multiple discrete constraints in latent space. [16] propose conditioning language models with a post-processing step of constraint-guided sampling. [23] use graph grammar constraint and formulate it as a linear programming problem for constrained text generation. Most recently, [22]; [25] and [24] release pretrained conditional models that are designed to provide explicit control over text generation via prefix codes. However, these",,
3078,3078," **Incremental Learning.** Continual Learning (CL) is a long-standing problem in machine learning and computer vision [1]. The goal of continual learning is to learn new tasks while maintaining the knowledge learned from previous tasks. The main challenge in CL is to prevent catastrophic forgetting [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20]. In this work, we focus on the problem of Class Incremental Learning (CIL) where a model incrementally learns about new classes without using any past exemplars.

**Regularization-based Methods.** Regularization based methods alleviate catastrophic forgetting by penalizing changes in important parameters such that they do not change significantly when learning new tasks. For example, Elastic Weight Consolidation (EWC) [2] and Synaptic Intelligence (SI) [3] penalize changes in the weights that are important to old tasks by freezing or pruning the important parameters when learning a new task. Memory Aware Synapses (MAS) [4] and Model Consolidation [5] constrain the change of important parameters by limiting the update of the parameters that are not directly related to the new task by restricting the changes to the parameters of the old task. However, these methods do not work well in the non-exemplar class-incremental setting as they require storing exemplars from the old classes, which may not be feasible in real-world applications with limited memory or privacy constraints. To address this limitation, episodic memory based methods store a small subset of exemplars of old tasks in memory [11][13] or use generative models to generate samples of old classes [14][16] or synthesize samples of new classes [17][19] to alleviate forgetting. However these methods are not suitable for the NECIL setting where only a subset of old data is available for training the model. To overcome this limitation of storing exemplar data, data-free methods [21][22][23][24][25][26][27][28][29][30][31][32] have been proposed to address the class incremental learning problem. These methods either use self-supervised learning to generate pseudo exemplars for old tasks [22][21][23] or augment the feature representation of the model [24][26]. However, the performance of these methods heavily relies on the quality of the pseudo samples generated by the augmentations. In contrast, our method uses the neighborhood information in the feature space to enforce the separation between the neighboring classes and to generate new samples that are more likely to be confused with each other.

 proposed a method called Neural Gas to learn the topological relationships in feature space, identifying the neighbors that are most likely to get confused with other classes. Neural Gas [25] is a method that learns topologies for incremental networks by growing the topologies of the network. The topologies are then used to regularize the network and prevent forgetting of old knowledge. In our method, we also learn topologies, but unlike Neural Gas, we do not"," The techniques proposed to combat catastrophic forgetting can be broadly categorized into three [1]. (1) The regularisation-based methods that add an extra regularisation loss term either to penalize changes to the network parameters that are important for previous tasks [2][3] or to distill knowledge from previous tasks to the current task [4][5]. (2) The parameter isolation-based methods that assign each task with an isolated set of parameters to prevent task interference either by dynamically increasing the network capacity [6][7][8] or by masking previous task parameters in a fixed size network [9][10][11]. Although parameter isolation methods are effective in overcoming catastrophic forgetting, they experience either a linear increase in network parameters or a decrease in capacity per task as the number of tasks grows [12]. (3) The rehearsal-based methods that store a small subset of previous task data to either retrain [13] or constrain the optimisation [14][15][16] during the learning of new tasks in order to retain the discriminability between old and new classes. However, these methods also encounter pitfalls due to memory limitations, and other pragmatic concerns such as privacy or consent issues when storing samples. An alternative to rehearsal-based methods is ""pseudo-rehearsal"", which involves training a generative model to mimic past task distributions [17][18]. Despite the encouraging results, generative models are computationally expensive to train [19] and are also prone to catastrophic forgetting [20]. This motivated the development of NECIL strategies that neither depends on real nor fake past samples [21][22][23].

NECIL methods benefit from powerful feature extractors learning transferable features across tasks, as demonstrated by SDC [23], which showed that embedding networks suffer significantly less from catastrophic forgetting. PASS [21] also showed that self-supervised learning alleviates task-level overfitting. Furthermore, to maintain the decision boundaries of previously learned classes, PASS introduced a class-mean prototype augmentation technique based on Gaussian noise. While this technique aids in the retention of old information, it can be further improved by leveraging the knowledge of the distribution of classes in the feature space. Accordingly, IL2A [24] proposed storing covariance matrices to retain class variations, but this approach can be memory intensive. SSRE [22] proposed a dynamic structure reorganization strategy to retain and transfer knowledge between tasks along with a prototype selection mechanism that utilizes an up-sampling technique of non-augmented class-mean prototypes. Similar to these approaches, we also store the mean prototype, while proposing a new method to augment them. To this end, we use the topological connections derived from an NG-like vector quantization to generate prototypes that lie within the shared feature regions of the confusing classes which aid in establishing better class discrimination.

Vector Quantization (VQ), a technique used to discretize a continuous data space into a finite set of ""coding vectors"" (CVs) was popularised with the advents of Self-organizing Maps (SOMs) . In addition to quantizing the data manifold, a SOM captures a topological mapping from data to the CVs. Neural Gas (NG) networks [25][26], on the other hand, were introduced to address a shortcoming of the original SOM by allowing a generic graph structure rather than a fixed lattice structure. In NG, the CVs are adjusted to capture the data-dense regions, and the edges between these CVs are formed based on their proximity. These edges and the CVs form a graph that approximates the topology of the data manifold.

Coding vector-based learning can be traced back to the K-nearest neighbor (K-NN) algorithm [27]. For instance, Learning Vector Quantization (LVQ) was proposed to derive the CVs used in a 1-NN classifier [28][29]. Despite common roots, LVQ algorithms and unsupervised VQ algorithms such as SOM differ in their primary usages of CVs; the unsupervised algorithms attempt to obtain a set of CVs to best represent the data while LVQ algorithms attempt to reduce the misclassification rate by focusing on the decision boundaries between classes. These complementary properties allow us to combine unsupervised and supervised VQ methods  to obtain CVs to both reduce the misclassification rate and represent the data distribution .

Multiple studies explored the integration of the hierarchical feature-extracting capability of deep feature extractors with VQ  which were also later adapted to Continual Learning. TPCIL [30] proposed to retain the topology of the feature space to preserve old knowledge over the increments. IDLVQ [31] proposed to adapt a margin-based loss for the task of few-shot class incremental learning (FSCIL) - a special case of CIL, therefore not directly transferrable to CIL/NECIL - to create a large margin between classes to mitigate overlap. TOPIC [32] was also proposed for the FSCIL setting with the aim of preserving old knowledge by stabilizing a NG network. We highlight that changes to the topology are possible due to the inevitable feature drift occurring over incremental steps thus a method that uses both augmented prototypes and new data to update the topological graph between CVs is warrented.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]"," **Continual Learning.** The literature on continual learning can be divided into two main categories: exemplar based and non-exemplar based methods. We refer the reader to [1] for a comprehensive survey.

**Exemplar Based Methods.** These methods [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32] are the most relevant to our work. The idea of using exemplars is to store a small number of samples from previous tasks in memory. These samples are then used to train a model on a new task. The model is then trained on the new task data without storing any samples from the old tasks. This approach is known as _data-free class incremental learning_ (DFCIL) [1].

In DFCIL, the model is trained on synthetic data generated by inverting a frozen copy of the learner's classification model [12][15]. Then, it is used to perform distillation on the synthetic data and use the old data to train the new model. This is done by distilling the old knowledge from the frozen model to the new one. The main drawback of this approach is that it requires storing all the data from the previous tasks, which may not be feasible in real-world applications where memory constraints or privacy constraints exist. To address this issue, some methods [13][12] store a subset of old data in a memory buffer and use them to train an incremental model. However, this approach requires storing the whole dataset, which is not feasible in many real-life applications.


"," The goal of HSI generation is to generate realistic interactions between a human and an object in a 3D scene, which has been studied for many years in various research fields, including computer vision, computer graphics, and robotics. In this section, we briefly introduce relevant HSI generation methods and review related 3D VA work.

**HSI Generation.** Human-object interactions have been broadly studied in computer vision, but they are generally focused on object detection and segmentation [1][2][3][4]. Besides these approaches, recent approaches tackle human-object interaction generation from 2D [5] to 3D scene and objects [6][7][8]. Specifically, Pang _et al._[9] and Kuchman _et al._ propose to generate interactions by inferring contact regions with body meshes and geometric constraints of objects in a 3D scene. Furthermore, Zhou _et al._[10] represent objects in a 3D scene as key points. They propose a novel iterative optimization and planning framework called SceneDiffuser to generate realistic HSI. Then, Li _et al._[7] propose a scene-scene refinement module to iteratively refine the predicted 3D human pose and scene layout. More recently, several generative methods have been proposed to generate diverse interactions. Abeck _et al._[11] propose a new HSI generation framework by optimizing a scene-oriented distribution to obtain a variety of HSI with action information. Also, Yao _et al._ propose PHOUS, a framework that leverages multiple levels of interactive details from both the scene-object-human-action and scene-object level. Also, Chen _et al._[12] propose to generate diverse HSI controlled by semantic scene structure. However, these methods are limited by their process of inferring a body-centric interaction from the scene, which naturally makes these methods unable to control the interactions. In contrast, we propose Narrator, a novel relationship reasoning-based generative approach, which is equipped with human-scene relationships and natural control.

**3D VA Generation.** Several VA generation methods have been proposed in recent years [13][14][15][16][17][18][19][20]. For example, Dong _et al._[13] propose a conditional generative model to generate an action by incorporating temporal information. Later, Yang _et al._ propose to generate diverse actions for humans from a video. Chen _et al._ propose to translate language inputs into diverse human motions [14], [18] propose to synthesize a diverse 3D human motion from a textual description, [15] propose a text-driven motion recommendation and neural mesh stylization system. But these methods cannot generate controllable HSI from a textual description.

As for 3D scene understanding tasks, there are also many related methods including visual ground",,"<Continual learning, also known as lifelong learning, addresses the challenge of acquiring new knowledge without forgetting previously learned tasks. This problem is particularly pronounced in deep neural networks, which often suffer from catastrophic forgetting [2]. This issue has prompted the development of a broad range of methods and frameworks to mitigate catastrophic forgetting, with a particular focus on Non-Exemplar based Class Incremental Learning (NECIL), where the model incrementally learns new classes without using past exemplars [1][4]. NECIL methods aim to overcome the lack of old data by avoiding the need to store exemplars, but they face challenges in discriminating between old and new classes, leading to overlap in feature representations [1][4].

Methods such as Neural Gas have inspired the proposal of NAPA-VQ, a framework that aims to reduce class overlap in NECIL by learning topological relationships in the feature space and identifying neighboring classes most likely to be confused with each other [1][15]. In this approach, neighborhood information is utilized to enforce strong separation between neighboring classes as well as to generate old class representative prototypes that aid in obtaining a discriminative decision boundary between old and new classes. Through the use of vector quantization, NAPA-VQ addresses the issue of feature overlap in NECIL, thereby contributing to the continual learning literature by providing a framework for enhanced discrimination between old and new classes in the absence of past exemplars.

Other approaches, such as Progressive Neural Networks and Dynamically Expandable Networks, aim to address the challenge of continual learning by leveraging transfer and preventing catastrophic forgetting, demonstrating their potential for achieving human-level intelligence [6][7]. These methods leverage lateral connections to previously learned features and dynamically adjust network capacity during online training, showcasing their ability to adapt to changing domains and prevent semantic drift. Additionally, recent work suggests that task-based hard attention mechanisms and generative replay frameworks could offer effective solutions for reducing catastrophic forgetting [10][17].

In summary, the development of novel frameworks and methodologies for continual learning, such as NAPA-VQ and others, demonstrates the ongoing efforts to address the critical challenge of catastrophic forgetting in deep neural networks [1][6]. These approaches represent important advancements in the field and contribute to the potential for achieving lifelong learning capabilities in artificial intelligence systems."">

"
5409,5409," **Optical Flow.** Traditional methods [1][2][3][4][5] estimate optical flow by solving a quadratic optimization problem. Recently, deep learning based methods [6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23] have achieved remarkable progress in optical flow estimation. These methods can be roughly divided into two categories, _i.e._, global matching methods and local matching methods.

**Global Matching Methods.** Global matching methods aim to find dense correspondences between all pixels in the input image. The pioneering work, _e.g._, FlowNet [6], employs a two-stream network to extract feature maps and regress the optical flow. To handle occlusions, GlobalFlow [11] introduces a global motion aggregation module to refine the estimated flow. However, the global matching method is sensitive to the initial estimated flow, which leads to sub-optimal results. To address this issue, recent methods [16][18] propose to use a kernel-based attention mechanism [14][20] to capture long-range dependencies in the feature maps. In particular, SuperGlue [18] proposes a graph neural network (GNN) to learn the feature correlations between different feature maps, which is further enhanced by a graph convolutional network (GCN) in [15]. However, these methods still suffer from the problem of feature misalignment. To solve this problem, some methods [21][23][22] introduce the attention mechanism into the matching process. For example, ASpanFormer [20] proposes an adaptive span transformer (ASTR) to capture the long-term dependencies between feature maps in an end-to-end manner, while FlowFormer [13] introduces an attention-based self-attention mechanism to model the spatial-temporal dependencies between features. In this paper, we propose a QuadTree attention module [24] to better capture the spatial and temporal dependencies among the features. Our method is also related to GCNN-based methods [25][26][27][28][29][30] in the sense that we also use CNNs to extract features and regress optical flow from the extracted features. Different from these methods, our method uses a pre-trained GIM model to learn more discriminative feature representations for optical flow regression.

 is a recent work that proposes to use GIM as a pretraining task for CNN-based optical flow models. The key difference between our method and theirs is that our method is designed to improve the performance of optical flow model by pre-training on large-scale real-world data, while theirs focuses on improving the performance on synthetic data. In addition, our model is more robust to occlusion and has a better generalization ability.

 proposes to jointly learn the geometric and appearance features of the scene and the object. In contrast, we focus on the geometric features of objects and scenes with consistent displacements.

 designs a new geometric matching task. It aims to learn a more robust"," **Optical Flow Estimation.** Traditionally, optical flow estimation [1][2][3][4][5] is treated as an energy minimization problem. Nowadays, deep models [6][7][8][9] formulate the optical flow estimation as a regression problem with an end-to-end trainable Convolutional Neural Network. More recently, deep learning based iterative refinement for optical flow [7][10][8][11][12] has resulted in a big breakthrough. RAFT  constructs a 4D multi-scale correlation volume and utilizes a convolutional GRU block as an update operator. And GMA [11] further proposes a global motion aggregation module to tackle the occlusion problem. Besides, recent methods [13][14][15][16][17] mainly focus on the recurrent decoder of RAFT. These methods are orthogonal to ours as we mainly focus on GIM pretraining of feature encoder and provide a rethinking for optical flow from the geometric matching consistent scene perspective.

**Geometric Image Matching.** Geometric image matching [18][19] tends to find correspondence among images with different views. Different from optical flow estimation, image matching usually assumes that the scene is static and the geometric displacements are due to the changeof viewpoints (camera pose). Among recent detector-free matching methods [20][21][22][23][24], Tang _et al_. [24] propose QuadTree attention to capture both fine-level details and long-range dependencies, which outperforms the Linear attention [25] used in [23]. In this work, we also employ a QuadTree attention-based network as our feature extractor to learn feature correlation between two-view images. Benefited from the matching pre-training, we can get much better flow estimation within non-occluded regions. Furthermore, global motion aggregation module [11] can help propagate the accurate flow within non-occluded regions to more challenging occluded ones, which can boost the performance of optical flow estimation a lot.

Besides, there is also a line of works [5][10][26][27][28] which try to reduce the gap between optical flow estimation and GIM. GMFlow [5] formulates the optical flow estimation as a dense global matching problem, while GMFlowNet [10] introduces the matching loss and matching initialization into the optical flow. On the other hand, Depthstill [27] constructs matching pairs based on estimated depth for direct optical flow training and achieves superior generalization to unseen real data. We should clarify that our work is different from them because MatchFlow focuses on the GIM based pre-training for better feature representations from the perspective of curriculum learning [29]. Moreover, MatchFlow outperforms these competitors with superior generalization on Sintel.

**Curriculum Learning**. Large datasets are one of the driving forces of deep learning. However, due to it's difficult to provide reliable ground truth optical flow labels, there is limited target data. Thus optical flow models typically rely on the pre-training on the synthetic FlyingChair [6] and FlyingThings3D [30] dataset with the curriculum learning [29]. In contrast, we give a rethinking to this pipeline and propose using GIM as a pre-training task for optical flow.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]"," **Optical Flow Estimation.** Optical flow estimation is a fundamental problem in computer vision and has been extensively studied in the past decades. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30]. In this section, we briefly review recent deep learning based optical flow methods.

**Optical flow estimation.** The pioneering work RAFT [3] firstly proposed to estimate optical flow from a pair of images using hand-crafted features. RAFT is a direct regression method, which regresses the optical flow field from the features of the first image to the second image by minimizing the energy function. RAFI  further improves the performance of RAFT by introducing a matching-optimization step to handle large displacements. However, RAFT suffers from the problem of occlusion and large displacement. To address this problem, many methods [9][8] have been proposed to improve the accuracy of optical flow estimation, such as PWC-Net [8], PWCFlow [8] and FlowNet2 [9]. However, these methods still suffer from the challenge of large displacement and occlusions. To tackle these problems, some methods [10][13] have proposed to use global matching to handle occluded regions. For example, GMFlowNet [14] and KPA-Flow [14], which use kernel patch attention to explicitly take the local context relations into consideration, and achieve better performance than RAFT and RAFI, respectively. Recently, some works [15][12] have also proposed to utilize graph neural networks to explicitly reason about the underlying 3D structure of the 3D scene. For instance, AGFlow [15] and CRAFT [12] propose to use graph neural network to explicitly model the scene structure. SKFlow [16] proposes to use super kernel to alleviate the impact of occlude areas. SK-Flow  proposes to utilize the convolutional neural network (CNN) to warp the feature maps and then use the warped features to estimate the flow field. SK FlowNet [17] proposes a high-resolution optical flow method to reduce the computation time.


"," Transformer.In computer vision, there are two parallel developments that are directly related to this work: Transformers for image recognition [1][2][3][4][5][6] and Transformers for instance segmentation [7][8][9][10]. For image recognition, one line of work [5][6] searches for a large attention map by encoding a large batch, and then decoding the same encoded image again. Another line of work [1][2] only uses a small attention map and starts the decoding process at the same time as the encoding. The proposed method is related to the second line of work. For instance segmentation, to aggregate the instance information from different video frames, there are several works using multi-stage encoder-decoder transformers, such as DETR [7], UP-DETR [7], and Deformable DETR [8]. Some works use transformer for instance segmentation. Although these works do not directly address our problem, we believe that our work may benefit from the research progress of transformers in image recognition and instance segmentation.

Learning representation for tracking.For tracking, to learn the representation, some works [11][12] use transformer encoder-decoder architectures [13] to learn the feature representation for the whole tracking pipeline. However, these works only rely on a single template-search pair as the training data. Some works [14][15][16] adopt the masked image reconstruction to learn the representations for different tasks. Unfortunately, in most tracking works, the representations are not considered at all. Instead, the network is pre-trained on general images, and then transferred for tracking. There are also many trackers that use backbone models with pre-trained weights. However, such a manner often incurs severe performance drops compared to the model that is directly fine-tuned for the task.

More specifically for the representation learning in tracking, Siamese trackers [17][18][19] use the correlation scores to establish the correlation between the template and search region features. Some Siamese trackers use multiple parallel streams [20][21], or different branches [22][23][24] for different purposes. However, their improvements are rather small and they are often evaluated by additional metrics. The proposed method has a similar design of network, but it adopts a different training strategy. The results of Siamese trackers indicate that they can perform well if we train the model with adequate data. However, we argue that many previous Siamese trackers [21][22] suffer from the lack of discriminative learning. By this representation learning method, we obtain much better performance, as shown in this paper.

",,"<Related Work>

The field of optical flow estimation has seen significant advancements in recent years, particularly with the integration of deep learning techniques. A framework for robust estimation of optical flow [1] demonstrated the importance of addressing violations of brightness constancy and spatial smoothness assumptions. Another study proposed a framework based on robust estimation to address violations caused by multiple motions, such as transparency, depth discontinuities, and independently moving objects [2]. Furthermore, research on determining optical flow presented a method for finding the optical flow pattern, assuming that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image [3].

Recent approaches have leveraged deep learning for optical flow estimation, such as FlowNet [6], which demonstrated the use of convolutional neural networks (CNNs) for solving the optical flow estimation problem as a supervised learning task. Additionally, LiteFlowNet [7] presented a lightweight CNN for optical flow estimation, achieving competitive accuracy at faster running speeds. Another notable model, PWC-Net [8], used pyramidal processing, warping, and cost volume to design a compact yet effective CNN model for optical flow.

Moreover, recent approaches have focused on reimagining the optical flow estimation process through novel architectures. For example, GMFlow presented a learning-based matching-optimization framework, allowing for efficient global matching calculations and patch-based overlapping attention to improve matching quality [10]. Similarly, another study introduced CRAFT, a Cross-Attentional Flow Transformer [12], addressing the limitations of traditional correlation volume computation to revitalize the correlation volume computation. Furthermore, FlowFormer introduced a transformer-based neural network architecture for learning optical flow, achieving impressive performance and strong generalization [13].

As the field progresses, there has been a growing emphasis on complexity reduction and long-range dependency handling. Notably, a study introduced QuadTree Attention for Vision Transformers, reducing the computational complexity from quadratic to linear and achieving state-of-the-art performance in various vision tasks [24]. Additionally, research on Linear Transformers addressed the computational complexity issue of transformers, reducing the complexity from O(N^2) to O(N) and achieving similar performance to vanilla transformers, but with significantly enhanced speed [25].

Overall, these advancements in optical flow estimation underscore the growing significance of deep learning and novel architectural approaches in reshaping the way researchers address longstanding challenges in the field. These approaches show promise in achieving state-of-the-art performance, computational efficiency, and robustness in handling large displacements and complex scenarios.

<>"
366,366," **One-class Classification.** One-class classification (OC) [1] has been widely used in anomaly detection [2][3][4][5][6][7][8][9][10][11][12][13]. In OCC, the model is trained only on normal data and is expected to classify as abnormal anything outside the training distribution. In contrast, in our work, we consider both normal and abnormal data.

**Motion Generation.** Motion generation has been studied extensively in computer vision [14][15][16][17][18][19][20][21][22][23][24][25][26]. Most of these methods are based on Generative Adversarial Networks (GANs)  or Variational Auto-Encoders (VAEs). In particular, [22][25] use VAEs to generate human poses conditioned on the past motion. However, these methods do not consider the multimodality of human motions. In this work, instead of using VAEs or VAEs, we use diffusion probabilistic models to generate multimodal future human poses.

 proposed a generative model for video anomaly detection (VAD). However, their model is designed for image-based anomaly detection and cannot be directly applied to video-based OCC. Our model is based on the diffusion model and can be applied to both image and video domains.

 introduced a VAE-based model for action recognition. Their model uses a VAEs-based encoder-decoder architecture to encode the past and future frames of an action sequence and a decoder-based decoder to synthesize the future frame of the action sequence. The decoder is trained by maximizing the reconstruction error of the current frame and the previous frame. In our model, we leverage the improved mode coverage capabilities of diffusion models to improve the generation quality of the future frames.

 also proposed an action-conditioned VAE model for anomaly detection. They use the VAE to model the distribution of future frames and use the decoder as a discriminator to distinguish between the normal frames and the abnormal ones. In their model, the latent representations of the past frames are assumed to be Gaussian and the latent representation of the next frame is modeled as a Gaussian distribution. The latent representation is learned by minimizing the KL divergence between the two latent representations. Our work differs from theirs in two aspects. First, we do not use the adversarial loss in their model. Second, we propose a novel conditioning model to generate future frames conditioned on past motions.

 and  are the two state-of-the-art methods for action classification and anomaly detection, respectively. Their models are designed for the action classification task and the anomaly detection task respectively. They assume that the action of interest is the same for all action instances. In the case of action recognition, the anomaly is the difference between the action instances of the same action. In VAD, we assume that both normality and abnormality exist in the training data.

 is an anomaly detection method for video. It is a two-stage approach. It first generates a set of normal"," Previous work relates to ours from two main perspectives: Video Anomaly Detection methods (see Sec. 2.1), and diffusion models for motion synthesis (see Sec. 2.2).

Pioneer works analyze the trajectory of the agents in the frames to discriminate those distant from normality . Within recent literature, two major trends can be identified: latent- and reconstruction-based methods. VAD techniques also vary based on the type of input data they use, such as videos or human skeletal pose motions. MoCoDAD, as all the VAD works presented in this section, adheres to the OCC protocol, which simulates the scarcity of anomalies in real-world scenarios [1].

**Latent-based VAD** methods identify abnormality according to a score extracted from a learned latent space whereby normality is supposedly mapped into a constrained volume, and anomalies are those latents lying outside, with a largerscore (see [2][3][4] for an overview of latent-based AD). Sabokrou et al. [5] propose a two-staged cascade of deep neural networks. First, they employ a stack of autoencoders that detects points of interest (POIs) while excluding irrelevant patches (e.g., background). Second, they identify anomalies by densely extracting and modeling discriminative patches at POIs. Notice that this work constrains normality to belong to a single mode and anomalies outside, thus, addressing the openset'ness of anomalies, but it hampers the multimodal and diversity  aspect of normal motions. Contrarily, our work considers the multimodality of normal and abnormal motions.

Notably, Nguyen et al. [6] propose an image-based technique exploring multimodal anomaly detection via multi-headed VAEs. However, considering a fixed number of modes for reality amends multimodality only partially, as it misses to unleash its openset'ness. Differently, we adopt diffusion models for their improved mode coverage and generate multiple futures, not being constrained on a fixed number of heads (see Sec. 5).

**Reconstruction-based VAD** methods consider the original metric space of the input and leverage reconstruction as the proxy task to derive an anomaly score. These models are trained to encode and reconstruct the input from normal events, producing larger errors on anomalies not seen during training. [7][8] use sequences of frames and feed them to convolutional autoencoders. Gong et al. [9] ""memorize"" the most representative normal poses to discriminate new input samples. Liu et al. [10] tackle intensity and gradient loss, optical flow, and adversarial training. Luo et al. [11] use stacked RNNs with temporally-coherent sparse coding enforcing similar neighboring frames to be encoded with similar reconstruction coefficients. Barbalau et al. [12] builds upon [13] and integrates the reconstruction of the input frames, via multi-headed attention, into a multi-task learning framework. Besides [13][10], all works rely on a single reconstruction proxy task via non-variational architectures that learn discrete manifolds. However, normality and abnormality are multimodal and diverse, making it hard for these techniques to have an exact match (reconstruction) over the GT. Additionally, GANs used in [10] suffer from mode collapse [14] lacking to represent the multimodality of reality. Similarly to [6][12] can represent only a fixed number of modalities, which does represent the openset'ness of reality. MoCoDAD is a reconstruction-based approach and leverages diffusion processes [15] to account for the openset'ness of normalcy and anomalies in terms of pertinence to the GT.

**Skeleton-based VAD** methods exploit compact spatio-temporal skeletal representations of human motion instead of raw video frames. Morais et al. [16] use two GRU autoencoder branches to account for the global and local decomposition of the skeleton in a particular frame. Luo et al.  exploit stacked layers of ST-GCN [17] to accumulate joint information over the spatio-temporal dimensions of the frame and predict joints in the future. However, [17] uses a fixed adjacency matrix, depicting joint connections, for all ST-GCN layers, which hinders the exploration of intra-frame and intra-joint relationships, two factors that play a crucial role in improving the encoding of spatio-temporal features [18]. Markovitz et al. [19] utilize the encoder of an ST-GCN autoencoder to embed space-time skeletons into a latent vector. This vector is then fed to an end-to-end trainable deep-embedded clustering procedure which produces \(k\) clusters representing the multimodality of normalcy and anomalies. Flaborea et al. [20] propose COSKAD and force the normal instances into the same latent region driving the distances to a common center. MoCoDAD is also a skeleton-based approach that mitigates the choice of _k a priori_ to cover the multimodality of reality.

Diffusion models have marked a revolution in generative tasks such as image and video synthesis [21][22][23], but they have not been employed for VAD. Saadatnejad et al. [21] propose a two-step framework based on temporal cascaded diffusion (TCD). First, they denoise imperfect observation sequences and, then, improve the predictions of the (frozen) model on repaired frames. Tevet et al. [22] use a transformer encoder to learn arbitrary length motions [24][25] coherent with a particular conditioning signal \(c\). They experiment with constrained synthesis where \(c\) is a text prompt (i.e., text-to-motion) or a specific action class (i.e., action-to-motion) and unconstrained synthesis where \(c\) is not specified. Chen et al. [23] design a transformer-based VAE [25] to learn a representative latent space for human motion sequences. They apply a diffusion model in this latent space to generate vivid motion sequences while obeying specific conditions similar to [22]. Differently, MoCoDAD is a diffusion-based model that uses conditioning over a portion of the input (e.g., previous frames condition the generation of future ones).

Wyatt et al. [26] propose AnoDDPM, a diffusion model on images, which does not require the entire Markov chain (noise/denoise) to take place. They use decaying octaves of simplex noising functions to distinguish the corruption rate of low-frequency components from high-frequency ones. However, they add and remove noise without conditioning, identifying anomalies when noise removal diverges from the input. Differently, MoCoDAD is based on generating and comparing multimodal motions against the GT in terms of pertinence. Our proposed model is the first to exploit the multimodal generative and improved mode-coverage capabilities of diffusive techniques, via forecasting tasks, further to being first in adopting them for detecting video anomalies. Hence, to transfer DDPMs from video-basedAnomaly Detection to skeleton-based VAD we rely on a U-Net-shaped stack of STS-GCN [18] layers, which includes the spatio-temporal aspects of joints in sequences of human poses.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]"," **Anomaly Detection.** Anomaly detection has been studied extensively in the literature [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26]. In this section, we focus on the most relevant works to ours.

**Anomaly detection in video.** Most of the existing methods for video anomaly detection are based on the assumption that normal\({}^{1}\) motions are present in the training data. For example, [6] proposed a multi-hypotheses autoencoder to learn the data distribution of the foreground more efficiently. [7] proposed to learn a generative model for regular motion patterns using multiple sources with very limited supervision. [8] proposed an auto-encoder-based method for anomaly detection. [9] proposed the memory-augmented deep autoencoders (MemAE) to learn prototypical elements of the normal data. [13] proposed multi-task learning to jointly learn multiple proxy tasks: three self-supervised and one based on knowledge distillation. [12] proposed SSMTL++ to leverage multi-head self-attention modules to capture multi-scale sequential patterns. [11] proposed Temporally coherent sparse coding (TSC) to enforce similar neighbouring frames be encoded with similar reconstruction coefficients. [10] introduced a future frame prediction method to leverage the difference between a predicted future frame and its ground truth frame. [19] proposed graph-based pose clustering to detect anomalies. [4] proposed OC4Seq to detect events in discrete event sequences. [20] proposed COSK-Kinematic-Kep model to detect human anomalies in video sequences. However, all these methods assume that normality and abnormality share the same openset'ness property. In contrast, our method assumes that both normal and abnormal motions are multimodal.

 is the first work to propose a model for VAD, which leverages the improved mode coverage capabilities of diffusion probabilistic models to generate plausible future human poses.

"," For OCC problems, the common formulation is the ""one-class SVM problem"" [1], i.e. learning the representative feature space of one class by constructing a binary classifier for that class. One-class classifiers can be further divided into _density-based_ (DB) approaches, which aim to identify the rare point where the conditional distribution diverges from that of the majority class, and _dispersion-based_ (DB) approaches, which learn models to detect anomalous instances that deviate from the typical samples in some pre-defined metric (e.g. maximum likelihood distance).

For anomaly detection in videos, standard DB approaches (e.g. SVM) are hard to optimize. To overcome this difficulty, _density-aware classifiers_ [2] are usually designed, which consider not only the common feature vector but also the distribution from which the normal sample is drawn. _Dispersion-based_ approaches [3][4][5] utilize a distribution of normal samples at inference time. In contrast to the former, the latter can model the distribution of normal samples at test time and do not need any additional samples from the majority class, which makes them more appealing.

Some _motion-based_ techniques [6][7] for video anomaly detection focus on preserving the key motion patterns of normal data. Several of these techniques model human poses. Some model the temporal evolution of a human pose [7], others estimate the inherent structure of a single pose [6]. Others focus on trajectory and pose features extracted from optical flow [8]. Yet others assume the knowledge of future frames at inference time, but without a mode collapse [9][10]. Our _generative_ approach in contrast differs in that we do not focus on the temporal evolution of the motion.

Sparse coding based [11] and novelty-based approaches [12][13] for video anomaly detection have been proposed. However, they use multiple object classifiers to make predictions and handle the temporal-continuity in videos which require a more sophisticated anomaly detector than the ones introduced in [11][12][13]. These methods require an object detector and an object classifier as a pre-processing step. In contrast, our method generates the future human poses directly and does not require any object detectors or classifiers.

Contrary to the one-class classifier approaches, Generative Adversarial Networks (GANs) [14] as well as Variational Autoencoders (VAEs) [15] have become prominent in anomaly detection. GANs and VAEs have been widely used for video anomaly detection since they do not require additional samples of the majority class. However, this limitation is removed by the one-class classifier approaches and the consequent shot change problem between the majority and anomaly class are addressed by the _dispersion-based_ approaches, which learn from the majority data",,"\<Related Work\>
This paper introduces a novel approach for video anomaly detection using a multimodal motion conditioned diffusion model based on skeletal representations[1]. The proposed model leverages state-of-the-art diffusion probabilistic models to generate multimodal future human poses through a conditioning on the past motion of people. This enables the generation of different but plausible future motions, enhancing the model's capability to capture the variations in human actions[1]. The approach is validated on multiple benchmarks, including UBnormal, HR-UBnormal, HR-STC, and HR-Avenue, where it surpasses state-of-the-art results through extensive experiments[1].

The paper is related to existing literature on anomaly detection, specifically in the context of video data. It builds upon the use of generative models for anomaly detection, aligning with recent advances such as Deep Support Vector Data Description[2] and multi-scale one-class recurrent neural networks[4]. Moreover, the proposed method resonates with the concept of one-class classification[1], which is relevant for anomaly detection where training data is limited to normal instances. This aligns with the paper's focus on modeling both normality and abnormality as multimodal and leveraging generative models to capture the complexity of human actions[1].

In addition, the paper's use of diffusion probabilistic models for anomaly detection is related to the study on estimating the support of high-dimensional distributions using kernel expansion and regularization techniques[3]. The proposed multimodal motion conditioned diffusion model aims to address the challenges of anomaly detection in video data by explicitly considering the aggregation of future motion modes for anomaly detection, aligning with the objective of learning the data distribution of the foreground more efficiently[6]. The model's use of skeletal representations and its focus on capturing regular motion patterns also align with previous works on learning temporal regularity in video sequences and spatio-temporal autoencoders for video anomaly detection[7][8].

Overall, the proposed multimodal motion conditioned diffusion model for skeleton-based video anomaly detection presents a novel and promising approach for addressing the challenges of anomaly detection in video data, aligning with recent advances in generative models and one-class classification while leveraging state-of-the-art diffusion probabilistic models. The extensive validation on established benchmarks further strengthens the significance and potential impact of the proposed approach."
3975,3975," In recent years, CNN-based methods have achieved great success in SISR. SRCNN [1] is one of the pioneering works in this field, which uses a three-layer convolutional neural network (CNN) to learn the mapping from LR images to HR images. EDSR [2] further improves the performance by introducing a residual block and a deeper network structure. SRMDN [3] proposes a dilated convolution based multi-scale information learning inception module to exploit the redundancy in natural images. RDN [4] introduces a densely connected residual block to enhance the feature extraction ability of SR models. Recently, transformer-based models [5][6][7][8] have been proposed to improve the performance of image super-resolution. Inspired by the success of transformers in natural language processing (NLP) tasks, researchers have also applied transformers to image restoration tasks [6][5][8]. However, these models are not suitable for real-time applications on mobile devices due to the large number of parameters and high inference latency.

To address this issue, many lightweight SR models [9][10][11][12][13] are proposed. MemSR [11] and ESRGAN [12] use skip connections to reduce the number of FLOPs and train the model in an end-to-end manner. RFSN [10] and IMSDN [9] propose to use the information multi-distillation network (IMDN) to capture the long-range dependencies between LR images and HR images, which reduces the model size and improves the inference speed. However, they still contain time-consuming operators, such as convolution and ReLU, which increase inference latency, limiting their real-world applications. To address this problem, ESRNet [13] proposes an anchor normalization layer (ANet) to replace the convolution operation with ReLU and achieves the state-of-the-art performance in terms of inference speed and reconstruction quality. In this paper, we propose an efficient SR model ETDS based on Equivalent Transformation and Dual Stream Network Construction (ETDS), which is more suitable for mobile devices.

 propose a novel network structure based on the dual stream network structure, which is the most similar to our proposed model. The dual stream structure consists of two parallel streams, one for the LR image and the other for the HR image. The main difference between our model and theirs is that our dual stream consists of a convolution layer and a ReLU layer, which can be viewed as a special case of the dual-stream structure in our model. In addition, our model is designed for efficient SR, which has not been explored in previous lightweight SR methods.

 proposes a novel model for efficient image SR based on a novel dual-branch structure. They propose a dual-path network structure that consists of an encoder and a decoder, where the encoder is a Transformer [7] and the decoder is an attention-based Transformer. The encoder extracts features from the input LR images, and then uses the features extracted from the"," Due to the rapid development of convolutional neural networks (CNN), CNN-based methods [1][2][3][4] have become mainstream methods for SR tasks. SRCNN [1] pioneered the application of convolutional neural networks on the SR task, surpassing the performance of traditional methods. In EDSR [2], a very deep network was utilized and the batch normalization layers in the residual block were removed. SwinIR [5] first attempted to apply the Swin-Transformer [6] on the SR task, showing the potential of Transformer-based networks [7]. To enlarge the receptive field, the hybrid attention block (HAB) was proposed by HAT [8], which achieved state-of-the-art performance. However, these models have high requirements for memory and computational resources which are not easily attainable in real-world applications.

To realize lightweight SR for GPU servers, many approaches tried to reduce the number of parameters and FLOPs. CARN  attempted to apply group convolutions. IMDN [9] employed a progressive refinement module to improve the information extraction ability and reduced the number of layers and channels. For further improvement in the efficiency of feature extraction, in RFDN , \(1\times 1\) convolutions were utilized to replace the \(3\times 3\) convolutions on the split channel of the IMDB [9] module. RLFN [10] replaced the progressive refinement module with a simpler residual module, which further improved the running speed and achieved great performance. MemSR [11] optimized the network in terms of memory by removing the residual structure in the model and proposed a novel knowledge distillation method to improve the performance.

Due to the particularity of the mobile platforms, most models for GPU servers are not directly applicable to mobile devices whose applications require to be carried out in a timely fashion with restricted computational resources. To solve this problem, current methods mainly optimized the network from three aspects, _i.e._, neat network topology, computation reduction and operator substitution. ECBSR , which proposed an Edge-oriented Convolutional Block (ECB) to achieve better performance, used relatively neat topology for low inference latency on mobile devices and introduced reparameterization techniques to achieve computation reduction for SR task. Following , RepSR [12] optimized the performance and training efficiency of the reparameterization module in ECBSR to achieve further computation reduction. ABPN [13] first attempted to apply operator substitution and used the faster _repeat_ operator instead of nearest neighbor interpolation in global residual connections. Inspired by the reparameterization technique, we propose ET, which substitutes more types of operators to achieve low inference latency. Our proposed ETDS optimized the network from all three aspects, where ET optimized the network by achieving operator substitution and computation reduction while the dual stream network adopted a neat topology.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]"," **Lightweight Super-Resolution.** Recently, many lightweight super-resolution models have been proposed to reduce the model size and inference latency on mobile devices [1][2][3][4][5][6][7][8][9][10][11][12][13]. EDSR [1] is the first to use convolutional neural network (CNN) for SR. It consists of three convolution layers and two ReLU layers, which are used to learn the mapping between LR and HR images. EDSSR is further improved by removing redundant modules in conventional residual networks [2] and introducing dilated convolution [3] to learn multi-scale information. RDN [4] proposes a residual dense block (RDB) to extract abundant local features via dense connected convolution and introduces a contiguous memory (CM) mechanism to fuse local and global hierarchical features in a holistic way. However, these models still contain time-consuming operators that increase inference latency, limiting their real-world applications on mobile device. To address this issue, we propose a novel model for lightweight SR based on Equivalent Transformation (ET) and dual stream network construction.

**Transformer for Vision.** Transformer [7] was first proposed for machine translation and has been widely used in many computer vision tasks [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, among others [7]. Recently, Transformer has also been applied to low-level vision tasks. SwinIR [5] is a lightweight model for image restoration based on the Swin transformer [6]. It consists three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. The main idea is to use Swin transform [6] as the backbone network for image SR. SwINet [8] is another lightweight model based on Transformer. It combines channel attention and window attention to capture more spatial information for better reconstruction. HAT [8], a hybrid attention transformer, combines both channel and window attentions to capture global statistics and strong local fitting capability for better feature extraction ability. SwiIR [9] proposes an adaptive cropping strategy to super-resolve block-wise image patches using the same well-trained model. It also proposes a lightweight information multi-distillation network (IMDN) to super resolve block-level image patches"," **Low-dimensional representations of 3D scenes.** Given the potential of SLAM systems, various methods have been proposed in the past years. Here, we focus on two broad categories of methods: dense and sparse visual SLAM.

In terms of visual SLAM, sparse methods typically estimate the camera pose by tracking sparse features [1][2][3], while dense methods estimate the camera pose and scene geometry simultaneously using dense point clouds or voxel grids [4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19]. Dense SLAM is sub-optimal due to its coarse representation of 3D scene geometry. Hence, recent deep learning based SLAM systems aim to reconstruct the 3D scene with the ground-truth geometry [20]. This facilitates the estimation of the camera pose in RGB space using an optical-flow or a point cloud representation of the scene. However, 3D reconstruction with a point cloud representation is inaccurate due to the lack of fine-grained geometric details. To address this, recent methods show promising results in reconstructing scenes with geometry details [21][22][23][24][25][26][27][28][29].

Some recent studies [30][31][32][33][34][35][36] use neural implicit functions, _e.g_., neural volumetric rendering to estimate the scene geometry and perform camera pose estimation using a point cloud representation. Other studies [37][38][39][40][41] use neural implicit functions to estimate camera pose and geometric details from depth sensors and perform dense RGB-D reconstruction. However, they lack reconstruction of the photometric details, since their method estimate geometric details using a RGB value. Some recent studies [42][43] show promising results in dense RGB-D reconstruction with an explicit scene representation, _i.e_., volumetric grid and occupancy grid, by using depth and color maps as inputs. However, they lack efficient parallelizable processing, and expensive memory usage, making it unsuitable for real-time processing.

**Learning an implicit representation of 3D scenes.** Recently, some studies [44][45][46] propose to estimate an implicit scene representation that can be used for simultaneous localization and mapping. However, they assume that a known global camera pose exists at the beginning of their method and the global camera pose is updated iteratively. Therefore, they are not suitable for online SLAM using only sparse observations.

There are also some studies that propose to use neural implicit functions for efficient visual SLAM systems [47]. However, they require training the implicit functions for each target scene for each sequence of R",,"<The field of single-image super-resolution (SISR) has seen a surge in lightweight and efficient models due to the increasing demand for real-time super-resolution networks on mobile devices. The work by ECNUSR [1] presents an efficient SISR model based on Equivalent Transformation and Dual Stream network construction (ETDS), which focuses on transforming time-consuming operators into more time-friendly operations for mobile devices. The proposed dual stream network aims to alleviate redundant parameters and enhance feature extraction ability, resulting in an efficient SR model for mobile devices. The approach showcases superior inference speed and reconstruction quality compared to previous lightweight SR methods. Additionally, the work draws inspiration from the success of deep convolutional networks (CNN) [2], residual learning techniques [2], dilated convolution based multi-scale information learning inception module [3], residual dense network [4], and Swin Transformer [5]. These existing works have explored different network structures, training methods, and feature learning mechanisms to achieve state-of-the-art results in image super-resolution tasks.

Furthermore, the proposed ETDS model can be aligned with recent advancements in vision Transformer frameworks. The hierarchical and efficient architecture of Swin Transformer [6] and the attention mechanisms proposed in the original Transformer model [7] provide insights into the potential enhancements for super-resolution tasks. The study of Activating More Pixels in Image Super-Resolution Transformer [8] introduces the Hybrid Attention Transformer (HAT), which combines channel attention and window-based self-attention schemes. This approach could provide valuable insights for further improving the feature aggregation and utilization in SISR models. Additionally, the approach of Information Multi-distillation Network [9] and Residual Local Feature Network for Efficient Super-Resolution [10] focuses on addressing the balance between model performance and inference time, which aligns with the goal of developing efficient SR models for mobile devices as demonstrated in the ETDS model. Moreover, the proposed work can benefit from the techniques presented in MemSR [11] and RepSR [12], which aim to reduce the memory footprint and improve the efficiency of SISR models using memory-efficient training and re-parameterization techniques.

In summary, the related work encompasses a wide range of techniques and methodologies, including deep CNN architectures, residual learning, multi-scale information learning, attention mechanisms, and memory-efficient training strategies. The approaches collectively provide valuable insights and building blocks for the development of efficient and lightweight super-resolution models beneficial for mobile devices <[2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]>.</>"
3605,3605," Text-based Visual Question Answering (VQA) and Text Captioning (TC) have been extensively studied in the computer vision and natural language processing (NLP communities. Early work on text-based VQA focused on detecting and classifying text in images [1][2][3][4][5][6][7][8][9][10][11]. More recently, the community has shifted its focus to tackling the task of text-captioning [12][13][14], where the goal is to generate a natural language description for a given image. In this work, we focus on the text-text understanding task, which requires the model to recognize and reason about text in the image.

Our work is inspired by the recent advances in V&L pre-training [15][16] and the recent success of the Transformer-based models in the NLP community. We propose a novel pre-trained model for text-vision tasks, which is trained end-to-end on a large-scale image-text dataset. Our model is trained on a variety of pre-text tasks, including: (i) text classification, (ii) text detection, (iii) text-question answering, and (iv) image captioning.



"," **Scene-Text Understanding**. Most early STU works [1][2][3][4][5] have merely focused on Optical Character Recognition (OCR). We instead focus on scene-text understanding (STU) in the context of V&L tasks: VQA [6][7] and image captioning . The most common approach for these STU tasks is to fuse pre-extracted object detection features with off-the-shelf OCR signals as additional input [6][8][9][10][11][12][13]. These works often focus on specific challenges in downstream STU tasks, including dealing with noisy OCR signals, enabling the generation of rare words, or incorporating geometric information of OCR texts. In contrast, our work focuses on pre-training general-purpose STU models and shows the effectiveness of our objectives on multiple downstream STU tasks (SS3.1).

**V&L Pre-Training for STU**. One line of works incorporates OCR signals explicitly for pre-training [14][9][15]. TAP proposes an objective to learn the relative spatial position of two OCR texts. LOGOS [15] localizes a region that

The other line of works is OCR-free. Recently, extremely large image-text models have shown promising results on STU tasks, despite having no explicit STU objectives (_e.g_., GIT2 [16], Flamingo [17]). However, it would require an analysis of their private data and a prohibitive amount of resources to pinpoint what contributes to such strong results. Our study offers a complementary perspective to this OCR-free approach by pushing the limit of the OCR-heavy approach further than before and conducting more thorough experiments at a smaller scale.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"," Scene text understanding has been studied extensively in the literature. Early work on scene text understanding focused on character-based methods [1][2][3][4][5], which relied on hand-crafted features and hand-engineered rules for character recognition. Recently, deep learning methods have been applied to scene text recognition [6][7][8][9][10][11][12][13][14][15][16][17], where the goal is to learn a model that can recognize and reason about scene text in an image.

Text-based V&L tasks such as Text-VQA [7][6][8] and Text-Caps [7] require the model to understand the visual context of the question and the text in the image. These tasks are more challenging than STU, as they require the models to understand and reason over the visual and textual context of an image, rather than just recognize the text. In this work, we focus on the Text-Text VQA task, where the model is required to answer questions that require understanding the scene texts in the images. The text in STV-QA is usually extracted from OCR tokens [7], and the model must reason about the relationship between the OCR token and the image text in order to answer the question correctly. The model is typically trained on a large-scale image-text dataset, and the question-text pairs are generated by OCR systems. In contrast, Text-VCQA focuses on the question answering task, which is more challenging as the model needs to understand both the image context and the scene text. The models in ST-TextVQAA [7, ST-Vqa, and TextVCQAA  are based on vision-language pre-training, where they are trained to recognize the image and OCR texts, and then use the extracted OCR text to answer a question. ST-VCG [13] is a recent work that uses a transformer-based encoder-decoder architecture for STU. It uses a scene graph as the input of the encoder and a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modality interactions. In addition, it uses an additional OCR-based scene text encoder to learn the relationship among the objects in the scene, the ORC tokens and the questions in STQA.


In contrast to STV and ST"," We will review two aspects of human pose estimation: 1) the pose estimation methods and 2) the domain adaptation methods.

Pose Estimation and Representation Learning.Pose estimation methods have been mainly designed for detection-based [1][2][3][4][5] or regression-based [6][7][8][9][10] techniques. In this paper, we focus on regression-based methods since they are more robust against pose perturbations [7]. A major drawback of these regression-based methods is their sensitivity to pose labels. To alleviate the issue, Haque et al. [10] propose to disentangle the keypoint predictions and associate them with a segmentation-like field. In contrast to this work, we improve upon their domain adaptation methods and study the source-free scenario.

Domain Adaptation.Different methods have been introduced for domain adaptation (; [11]), e.g., contrastive learning [12][13], progressive adaptation [14], adaptive neural networks [15][16], pixel-level adaptation [17], or domain-aware clustering . Most of the work focuses on the image classification task (e.g., [12][13][14][15][17]) and the other works are introduced for the object detection task [14]. Since our method is also related to these tasks, we review the recent work in these areas.

Prior-guided Source-free Domain Adaptation.Prior-guided Domain Adaptation (PDA) has recently been introduced [18][19][20][21][22] and these approaches address the unsupervised learning scenario where no access to the source data is allowed during adaptation. Initially, Kuang et al. [20] propose the Consistency with Nuclear-Norm Maximization (CoNMix) method for UDA. Their approach effectively predicts unlabeled data with the auxiliary and source models to enforce the consistency. Later, Luo et al. [19] propose a semi-supervised feature distillation framework that is inspired by adversarial learning [13] and it utilizes a kernel-based strategy to feature alignment. Tang et al. [18] introduce a powerful feature-transferable based method to learn task-specific features from the source to transfer knowledge to the target domain. Similarly, Kim et al.  propose to learn source-aware features that are transferable to the target domain for semi-supervised feature adaptation. Recently, Meng et al.  propose an unsupervised CoTSA method to learn a compact feature embedding space for instance level adaptation.

In the domain adaptation literature, it is rarely discussed how to address the limitation of existing PDA methods that utilize a single-source domain. Recently, Liu et al.  propose a multi-source PDA framework for keypoint regression. Similarly, Song et",,"<Target Paper: ""PreSTU: Pre-Training for Scene-Text Understanding"">

The task of scene-text understanding has gained significant attention in the field of computer vision and natural language processing. Several approaches have been proposed to address the challenges associated with recognizing and reasoning about text embedded in visual inputs. Choudhury et al. [1] introduced a framework for the recognition of natural scene text without requiring human-labelled data, achieving state-of-the-art performance on standard datasets. Similarly, Liu et al. [3] presented Rosetta, a scalable optical character recognition (OCR) system designed to process images uploaded daily at Facebook scale, demonstrating efficient detection and recognition of text in images. On the other hand, Wang et al. [9] proposed LaTr, a layout-aware transformer for scene-text visual question answering which showed improved robustness towards OCR errors and outperformed existing approaches.

Many approaches have focused on the joint task of text detection and recognition in natural scene images. For instance, Baek et al. [5] proposed FOTS, a fast oriented text spotting network which achieved significant improvements in text detection and recognition, outperforming state-of-the-art methods. In a similar context, He et al. [14] introduced Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks, explicitly incorporating scene text during pretraining, and achieving substantial performance gains on multiple tasks. In addition, Chen et al. [15] proposed LOGOS, a model leveraging scene text clustering and grounding tasks, which demonstrated superior performance on Text-VQA benchmarks without additional OCR annotation data.

Moreover, several works have addressed the challenge of text understanding in images in the context of visual question answering. For example, Xu et al. [6] introduced a novel ""TextVQA"" dataset and a model architecture called LoRRA, which excelled in reasoning about text in images and outperformed existing state-of-the-art VQA models. Similarly, Jain et al. [8] proposed a model based on a multimodal transformer architecture for the TextVQA task, achieving superior performance by enabling iterative answer decoding with a dynamic pointer network. Furthermore, Zhang et al. [13] presented SceneGATE, a scene graph-based co-attention network for TextVQA, which outperformed existing methods due to its attention to semantic relations among objects, OCR tokens, and question words.

In summary, the recognition and reasoning about text in images have been addressed by various research works using techniques such as scene text recognition, joint text detection and recognition, multimodal transformer architectures, and attention mechanisms. These approaches have demonstrated significant improvements in scene-text understanding, visual question answering, and image captioning tasks, highlighting the importance of effectively incorporating text information from visual inputs for downstream reasoning tasks. <1>

References:
[1] Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition
[3] Rosetta: Large Scale System for Text Detection and Recognition in Images
[5] FOTS: Fast Oriented Text Spotting with a Unified Network
[6] Towards VQA Models That Can Read
[8] Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA
[9] LaTr: Layout-Aware Transformer for Scene-Text VQA
[13] SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering
[14] TAP: Text-Aware Pre-training for Text-VQA and Text-Caption
[15] Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling"
4389,4389, **Binocular view.** The view from a binocular camera. The distance to the ground plane. The height of the sun. The sun's surface. The location of the horizon.," To the best of our knowledge, all currently available systems for tracking machine learning models (such as DVC7, MLFlow (), WandB8, Neptune9, and the Hugging Face Hub ()) track a model checkpoint as a single large file (i.e., a blob of data). As in Git LFS, these systems track large files by using Git to track metadata about the file while keeping the contents in a storage system external to the repository. Consequently, these systems cannot be used for collaborative model development since they cannot take advantage of recent research on communication-efficient training and model merging.

Like AdapterHub (), Git-Theta enables the sharing of cheaply-communicable/storable updates that allow existing pre-trained models to perform a new tasks. Unlike Git-Theta, AdapterHub does not track continual changes to a given model and does not include functionality for merging different versions or histories of a model.

An earlier related system, ModelHub ([1]), aims to track the full lifecycle of a model, including its architecture, hyperparameters, training details, metrics, and more. ModelHub also includes a query language to find specific models based on the large amount of metadata it stores. Like Git-Theta, ModelHub includes some functionality for minimizing storage costs, but it still treats the model as a single large tensor of values and does not support merging. Philosophically, the two systems also differ significantly--ModelHub aims to be a complete system for tracking experiments, models, code, and metadata, whereas Git-Theta is a lightweight Git extension that supports existing workflows and is easily extendible.

Since Git-Theta focuses on tracking changes to a model made by distributed contributors, it is complementary to systems that implement similar functionality for datasets such as Dolt10, Pachyderm11, and XetHub ([2]).

","[1, 2]"," **Git-based version control systems for machine learning.** The most widely used version control system for machine Learning is Git ([2]), which has been widely adopted in the machine learning community and has been used in a wide range of applications, such as data-driven machine learning (; [2]), model-based learning (), and deep learning pipelines ([1]). However, as we (and others) demonstrate, Git, as designed today, does not scale to the needs of ML dataset management. To address this issue, XetHub ([2]) is a system that allows users to edit model checkpoints in the Git repository, but it does not support fine-grained updates to model parameters. In addition, it is designed to work only with pre-trained models, and does not provide a way to integrate new models into the Git system.

**Git extensions for ML.** There have been several attempts to extend Git for the task of model version control. For example, [2] proposed a system for storing model versions of datasets, but they do not support updates to the model. [1] proposed ModelHub, which is a model versioning system for the deep learning domain. ModelHub uses a domain-specific language for searching through model space, but does not allow updates to models that are not part of the dataset.  proposed an extension to the original Git, which allows for updates to pre-defined model checkpoints. However, it only supports the use of a single checkpoint per model, and is not able to support updates that do not require a pre-specified model.
-specific search. In contrast, our work is the first attempt to extend the Git extension for the problem of continual improvement of machine learning models. We show how our extension can be used in this setting, and provide a detailed comparison with ModelHub.

"," Deep Learning in Graph.Many works [1][2][3] have studied graph kernel methods for deep neural networks. And Shuman and Fowlkes  also address the end-to-end deep learning for visualizing and analyzing graphs in graph visualization.

Dictionary Learning.Based on the dictionary representation, traditional machine learning algorithms can efficiently solve many real-world problems. For example, [4] extend kernel approaches to dictionary learning. And [5] propose the WL kernel, which maps the original graph to a sequence of graphs, then the sequence of graphs are compared to extract graph similarities. In recent years, there are a bunch of novel methods for dictionary learning [6][7]. One typical method is the Sparse Representation Learning (SRL) [8], which learns a dictionary from the unlabeled data as well. Recent works [9][10] mainly focus on the unsupervised SRL. Some works [11][12][13] address dictionary learning from the labeled data, but all of them focus on Euclidean data. Different from them, our method aims to learn the node representations and their graph embeddings in the generative view for the nonlinear graph data.

Graph Embedding Learning.Based on the RBM (Radial Basis Functional) [6], Superior Graph Embedding (SGE)  has recently been proposed to generate high-quality node embeddings. SGE uses two sets of basis functions to represent two graphs, one for node representations and another for graph structure, which needs to be learned together. SGE is not suitable for discrete and nonlinear data. On the other hand, [13] learns node embeddings based on Gromov Wasserstein (GW), but it needs additional information and time-consuming computations.

Graph Factorization.Graph Factorization (GF) [14] uses the graph structural similarity to predict the node structure. By learning the pairwise relations of nodes and the weights of each relation, GF learns node representations. GF needs to learn the structure and the number of nodes simultaneously, and it also uses the nonlinear graph data, which is different from our FGW Mixture Model.

",,"<The development of Git-Theta aligns with the growing need for collaborative and continual improvement in machine learning models. It addresses the challenges posed by the current centralized approach to model training and updates, and the lack of support for distributed collaboration in the machine learning domain. This parallels the work of Raffel (2023) who highlights the similarity between open-source software development and machine learning model development, pointing out the need for a version control system that enables distributed collaboration. By extending Git, the widely used version control software, Git-Theta provides a solution that allows for fine-grained tracking of changes to model parameters alongside code and other artifacts, as well as efficient updates, automatic model merges, and meaningful reporting about model version differences. This extension of Git to support machine learning model development fills an existing gap in version control systems, particularly in their treatment of model checkpoints as blobs of data, rather than leveraging the structure of checkpoints to enable efficient and meaningful updates [1].

ModelHub, as introduced by the work of the authors, also addresses the challenges in managing the deep learning lifecycle, particularly the versioning of machine learning models. While Git-Theta presents an extension of Git specifically tailored for machine learning models, ModelHub focuses on providing a comprehensive solution for deep learning lifecycle management, including a novel model versioning system and a domain-specific language for searching through model space, in addition to a hosted service. The emphasis on model versioning in ModelHub aligns with the objectives of Git-Theta, demonstrating a parallel effort in the domain of machine learning model management [2].

Furthermore, ""Git Is For Data"" proposes XetHub as a system that retains the user experience of Git while focusing on addressing the challenges posed by dataset management for machine learning. The paper acknowledges that existing solutions for ML pipeline reproducibility introduce friction and reduce flexibility, and proposes leveraging Git to provide both speed of iteration and reproducibility to source code. As with Git-Theta, the proposed system in ""Git Is For Data"" highlights the potential for adapting existing version control systems to address the specific challenges of machine learning development. While Git-Theta focuses on the development and management of machine learning models, XetHub targets the management of ML datasets, showcasing the broader relevance of adaptations to version control systems in the context of machine learning [3].>"
2376,2376," **End-to-end image translation.** Image translation is a task that translates an image containing text in the source language to the target language. Existing methods can be roughly divided into two categories: two-stage and one-stage. Two-stage methods [4]; [3] first extract visual features from the input image, and then translate the image using the encoder-decoder framework. One major challenge with these methods is the modality gap between visual text inputs and textual inputs/outputs of MT. To address this problem, [5]; [2]; [1] propose to align the visual features with the textual features of the MT model. However, the aligner and regularizer used in these methods are designed for MT-based models and cannot be directly applied to the end-toend models.

**Cross-modal pre-trained models.** Recently, there has been a surge of interest in cross-lingual pre-training for various NLP tasks, such as machine translation (MT) and speech processing (SPT). [7]; [6]; [8] propose a unified framework for speech/text representation learning by aligning the audio and text modalities.  propose a framework that aligns the visual and audio modalities with a shared encoder and a shared decoder. The alignment is done by concatenating the vision-text embeddings of the two modalities as the input of the shared decoders.  employ a self-supervised learning strategy with a masked language modeling task to pre-train the vision and text encoders for the image translation task.

 propose a multi-task learning framework that jointly trains the vision encoder with the MT encoder. Their model is composed of a vision-language encoder (VCE) and a language-language decoder (LED). The vision encodes the input images into a latent space, and the LED decodes the translated images into the latent space. The VCE encoder is trained by minimizing the cross-entropy loss between the visual encodings and the textual embedding of the image and the translated image.  introduce a cross-modality regularizer that enforces the aligned visual and textual modalities to be close to each other in the embedding space.  use the alignment and regularization to improve the performance of image translation models.

 introduce a framework for image translation that uses a shared image-text encoder to translate an image from one language to another. Their framework consists of an image translation network (ITN) and an image captioning network (ICN). The IN consists of a shared backbone network that translates the image from the source domain into the target domain, and a text encoder that maps the image to the text domain. The COCO-Net [4] is a two-step framework that uses an OCR-based encoder followed by an NMT-based decoder for the translation. The OCR encoder first extracts character-level features from an input image. The NMT decoder then translates the OCR features into a target language"," Recent years have witnessed increasing attention on multimodal machine translation (MMT) that translates a source sentence into the target language accompanied with an additional modality (). Given the additional modality and its relation to the source sentence, MMT can be roughly divided into image-guided translation (; ), video-guided translation (), speech translation ([2]; [1]), IT ([4]). Image-guided MMT aims to leverage visual context to aid textual machine translation ([3]; ). The significant difference between image-guided translation and image translation is that the latter embeds the source sentence in its visual modality in the image while the former has the image and the source sentence separated and the image is used to provide additional information for translating the source sentence.

In contrast to image-guided translation, IT has not yet been fully explored in the literature probably due the lack of publicly available datasets for IT. Both [4] and  propose end-to-end approaches to it. [4] uses a convolutional encoder to encode the image and Transformer decoder to generate target translation. The end-to-end IT model is able to locate characters in image, performs implicit tokenization on the source text, and then extracts latent semantic representations from them. This model can extract the latent token representations of image and text, and map into a shared space to implement the E2E IT. While they provide an initial definition of the IT task, they neither consider the modality gap nor verify the effect of the proposed models on real-world images.

For speech translation (ST), recent efforts have shifted towards end-to-end speech-to-text translation that directly translates a speech in the source language into a text in the target language ([6]; [7]). This is because end-to-end ST is of less error propagation and low latency compared with traditional cascaded ST (Inagumaet al., 2021; [1]). However, E2E ST suffers from the high cost of speech-to-text parallel data creation. Pre-training and multitask learning strategies have been explored to mitigate this data scarcity issue ([5]; [8]). In addition, similar to E2E IT, E2E ST is also confronted with the cross-modality issue, which can be mitigated by sharing the same semantic space for audio and text representations ([2]). Partially motivated by E2E ST, we propose an end-to-end framework for IT from the perspectives of pre-training with data of the MT task, sharing parameters across modalities, knowledge transfer via multitask learning, attempting to address the data scarcity and modality gap issues in IT.

","[1, 2, 3, 4, 5, 6, 7, 8]"," **End-to-end image translation.** End-to end image translation is a new task that directly translates an image containing text in the source language to the target language. Existing methods for image translation are mainly based on the encoder-decoder framework [4]; [5]; [3]; [2]; [1]; [7]; [8]. For example, [4] proposed a cascaded model that first translates an input image into a sequence of text, followed by a decoder that translates the sequence back to the input image. [5] proposed COnSecutive Transcription and Translation (COSTT), an integral approach for speech-to text translation that generates the source transcript and target translation text with a single decoder. [2] proposed to bridge the modality gap between speech and text translation by projecting audio and text features to a common semantic space. [3] proposed Visual Agreement Regularized Training (VAT) to make better use of visual information for multi-modal machine translation.

**Cross-lingual pre-trained models.** Recently, pre-training models have been widely used for cross-linguistic speech representation learning. For instance, [6] proposed XLS-R, a large-scale model that pre-trains a large number of models on more than half a million hours of speech audio in 128 languages. [7] proposed SpeechT5, a unified model that uses a shared encoder and decoder for speech/text representation learning, and [6]. [8] proposed the X-Mixup method, which adaptively calibrates the representation discrepancy and gives a compromised representation for target languages.

"," Commonsense knowledge is defined as ordinary everyday knowledge that is a priori more plausible to be true than false (; ). Given its utility in inferring new facts (; ), it has received much attention from both the language community and the AI community. Our work builds on previous efforts that develop and validate commonsense knowledge graphs for question answering ([5]; [1]; [3]), NLI benchmarks ([4]; ), chit-chat dialogue ([2]), and story generation ([9]; [10]).

Persona knowledge is a subset of commonsense knowledge that explicitly describes an agent's personal attributes, such as their name, profession, and interests. Common examples include work profiles (; ), cultural identities (), and athlete profiles (). Our commonsense knowledge graph is a subset of knowledge targeting persona agents. It contains common characteristics in both common knowledge and persona knowledge. Prior work has shown that leveraging persona information during inference can generate more engaging and consistent dialogue ([8]; [7]; [12]; [11]). Our work contributes to this research by constructing an entity-level, multilingual, and large-scale commonsense knowledge graph for persona agents and studying the challenges associated with generating persona consistent responses for dialogue systems.

",,
3737,3737," Controllable text generation has been studied in a wide range of tasks, such as dialogue generation [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 56, 57, 58, 59, 59].

[15, 15, 16][17][16][18][19][20][21][22][23][24][25][26][27][28][29][31][32][33][34][35][36, 37][43][56]. [15] and [16] use attribute-specific embeddings. [15][16] generate embedding vectors for each attribute. [14] generates embedding vector for each word. [10] generates attribute-aware embedding for each sentence. [12] generates a set of embedding codes for different attributes. [5] generates different embedding spaces for different sentences. [6] generates multiple embedding space for different attribute combinations. [8] generates several embedding representations for different outputs.  generates different outputs for different output spaces. [7] generates output representations for multiple outputs.

"," **Controllable Dialogue Generation** Currently, there have existed many studies on CDG ([6]; [1]; [2]). CTRL ([9]) used 55 kinds of attribute control codes to finetune an LM which is expensive and requires extensive annotated attribute labels. [8]; [4]; [5]; [10] addressed these limitations by employing an attribute discriminator to update the hidden activations or re-weight the next token distributions, resulting in a slow inference speed. Despite the progress, these models all focus on the single-attribute CDG where the attribute only contains coarse-grained discrete values, such as _happiness_ in emotion-controlled generation. It is also vital to explore multi-attribute CDG with multi-granularity attributes. Recently, some works ([7]; [3]) extend to multi-attribute controllable text generation by simply concatenating the prefixes trained for single attribute. However, they are only suitable for discrete attributes but not for fine-grained continuous attributes like personas ([2]). Besides, we find all these methods have a large performance drop from seen attribute values to unseen combinations. Therefore, in this paper, we are the first to explore the compositional generalization for multi-attribute CDG where a model could learn from seen attributes and generalize to out-of-distribution (OOD) combinations.

**Compositional Generalization in NLP** Compositional generalization has gradually attracted the interest of NLP researchers. The main application is in semantic parsing, involving grammar-based approaches ([14]), data augmentation strategies ([15]), disentangled representations ([13]), etc. Recently, a large-scale benchmark, STYLEPTB, is constructed to advance the development of compositional style transfer ([11]), and a template-based input representation is also performed on the data-to-text task ([12]). Overall, the application of compositional generalization in NLP tasks is not widespread and there is no related work on CDG at all.

**Prompt Learning** Prompt-based methods have achieved significant success in many NLP fields ([17]; [18]). [19] proposed the task-specific continuous prompts to finetune a NLG model. For controllable generation, [16]; [3]; [7] applied the prompt learning to represent each attribute value as an independent prefix. However, those methods are impractical for fine-grained attributes with a large value set. In contrast, we use the control codes to generate attribute-oriented prompts to guide the generation via a shared MLP layer.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"," **Controllable dialogue generation.** Controllable text generation aims to generate text with desired attributes, such as emotion, sentiment, and personality traits [2]; [1]; [6]; [3]; [2]. Existing works mainly focus on single-attribute controllable dialogue control. [4] propose a plug-and-play language model (PPLM) for controllability dialogue generation, which combines a pretrained language model with one or more attribute classifiers that guide text generation without any further training of the LM. [5] propose FUDGE, which learns an attribute predictor operating on a partial sequence and uses this predictor's outputs to adjust G's original probabilities. [3] propose Contrastive Prefix, which utilizes a set of small attribute-specific vectors, called prefixes, to steer natural language generation. [2] propose Personalizing Dialogue Agents (PDA), which is trained to make chit-chat more engaging by conditioning on profile information about the interlocutors. [6] propose Emotional Chatting Machine (ECM), which can generate appropriate responses not only in content and grammatical but also in emotion. [1] propose DailyDialog, a manually labeled multi-turn dialogue dataset, which is human-written and less noisy. [7] propose Tailor, which represents each attribute as a continuous vector and guides the generation of a fixed PLM switch to a pre-trained continuous vector. [8] propose GeDi, which uses a generative discriminator to guide generation from large LMs to make them safer and more controllible. [11] propose StylePTB, a large-scale dataset for fine-grained controlled text style transfer, which allows modeling of fine grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text.

**Compositional generalization.** There are several works on compositional generalizing to unseen compositions of seen components in semantic parsing [15]; [14]; [13]; [12]; [10]; [15]. [14] propose SpanBasedSP, which extends CKY-based span-based semantic parser to compose partial programs. [13] propose Disentangled Seq2Seq model, which encourages disentanglement by re-encoding source input at each time step. [15] propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional performance. [12] propose to use template-based"," **Learning uniform samples of Boolean functions.**[5] first demonstrated a bias towards low-entropy functions in _single-layer_ perceptrons. This property was found to persist even when there are _hundreds_ of neurons (in the hidden layer). [1] showed that this bias is likely a consequence of the _probabilistic complexities_ of the functions with high and low sensitivities respectively. We find that the bias is still present in the more complex Transformers. Additionally, we also find that the _probabilistic complexity_ remains largely unchanged across model architectures. This reinforces the idea that Transformers learn functions with this bias naturally. In contrast to Transformers, LSTMs learn uniform samples of functions from any sensitivity. Recent work ([4]; [2]) showed that the distribution over neural networks is closely related to a Gaussian Process (GP) ([4]). For a GP, we have a distribution over functions (the kernel) and the trainable hyperparameters.  show that maximizing the test performance on Boolean functions using standard optimization methods results in distributions over functions with large variances, rendering them less capable than simple linear models.  further prove that even with limited data, LSTMs can learn simple Boolean functions and also empirically show the Gaussian process' power in representing functions. To the best of our knowledge, none of the above works consider the dependence of the chosen kernel on sensitivity or inductive biases in the family of functions learned by the kernel. We provide new insights on Transformers' ability to learn a family of Boolean functions of sensitivity 2.

**Sparse vs dense functions.** Our results also indicate that Transformers exhibit a bias towards sparse functions. It is well known that random models trained on sparse input distributions exhibit low bias to small sensitivity functions.  provide a theoretical analysis of this phenomenon and show that the lower the dimension of the input space, the lesser the probability a model trained on sparse data will assign to high sensitivity functions. There is also evidence to suggest that these results hold even in the presence of non-uniform noise in the training data ([6]). It is important to note that the theory of sparsity-inducing noise distribution is applicable only when the noise follows a discrete distribution, unlike the Poisson noise distribution that we use. [3] prove that when the model and the data have the same distribution (with additional conditions) a probability of at most \(\frac{1}{n}\) of the high sensitivity functions being assigned small loss values under Stochastic Gradient Descent (SGD). A recent line of work has provided further theoretical analysis for sparse neural networks ([7]). Our work extends this line of research and provides empirical evidence to demonstrate this sparsity-inducing phenomenon in the case of Transformers. We provide theoretical and empirical evidence for Transformers' ability to generalize when trained on sparse Boolean functions and compare them with RNNs. In contrast to RNNs,",,
3895,3895," **Video captioning.** Video captioning aims to generate natural language descriptions according to the visual content of a video. Existing methods can be roughly divided into two categories: 1) Feature extraction and 2) Captioning model learning.

**Feature extraction.** Most existing methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] focus on feature extraction and captioning in the same pipeline. For example, Wang _et al._[1] propose a two-stream network to extract frame-level features and then generate captions with the extracted features. However, this pipeline is time-consuming due to the need to extract features for each frame separately. To alleviate this problem, some methods [2][4] propose to first extract frame features from the compressed video and then conduct the captioning process in the decoded video. For instance, Slow-I-Fast-P [2] proposes to use the I-frame features extracted from the CIFAR-10 dataset [1] as the initial frame features for the video captioning task. In addition, some works [3][5] introduce self-supervised learning for video representation learning in the compressed domain, which aims to improve the video representation by using the motion information of the compressed videos as the additional supervision signals. Although these methods have achieved promising results, they still need to sample frames from the original video during the inference process, which may result in redundant information in the sampled frames. In this work, we propose a simple yet effective model design to leverage the entire compressed video for captioning, which is more efficient in inference.



Compared to the existing methods, our model is simpler and more effective in utilizing the information from the whole compressed video without manual sampling.

 propose an end-to-end transformer-based framework for video caption generation. They propose to use an encoder-decoder structure to encode the video features and a decoder to generate the captions. The encoder is then used as the decoder of the caption generation model. The decoder is composed of a language decoder and a video encoder. The language encoder takes the input video features as input and generates the caption. The video decoder then generates the corresponding captions using the generated captions as the input. The model is trained in a supervised manner using the HowTo100M dataset [11], which consists of \(100\)M\) video clips with manually-annotated captions, and the caption generator is trained using the pre-trained language model as the language model.

 introduce a new pre-training method to learn the video-text embeddings from the large-scale unlabeled video datasets. They also propose a new loss function to train the video and text encoders jointly. The main difference between their model and ours is that our model does not require any extra training data.

 proposes a new video-captioning dataset named VideoBERT. It consists of short video clips and captions extracted from YouTube videos. The captions are automatically generated by"," **Compressed vision task**. The main idea of introducing compressed video into current computer vision tasks is to utilizing the motion vector and residual on the compressed domain to avoid fully decode all frames from the video and save the storage space at the same time. Early work mainly base on MPEG-4 video codec [1][2][3][4]. CoViAR [1] proposed a back-tracking technique to trace motion vectors back to I-frame, which works on MPEG-4. MM-ViT [4] proposed a multi-modal transformer to process the I-frame, motion vector, residual and audio in the compressed video. Since the MPEG-4 codec is outdated, other works, e.g., MVCGC [5] and ATTP , is designed to work on other coedcs like H.264 and H.265 to ensure generalizability. Comparing with MPEG-4, H.264 and H.265 allow a more flexible yet complicated compression, which makes it more challenging to learn from compressed domain. MVCGC [5] proposed a self-supervised method to learn video representations by utilizing the mutual information between RGB video frames and motion vectors. ATTP  designed a lightweight deep neural network to process the compressed video and achieve real time action recognition on embedded AI devices. Similarly, our work

**Video captioning.** Video captioning aims to convert the content of videos into natural language descriptions, which requires the model to understand the objects in the video and the behavior of the objects. Some works focus on the design of the model structure. These methods usually extract features offline, and then models use these features to generate captions by designing different network architectures. HMN [6] proposed a hierarchical modular network that serves as a strong video encoder, which bridges videos and languages. ORG-TRL [7] proposes an object relational graph based encoder, which captures more detailed interaction features to enrich visual representation. SGN [8] designed a semantic grouping network to group video frames with discriminating word phrases of partially decoded caption. Some works explore additional information to help the model generate more accurate video captions. TextKG [9] propose a two-stream network capable of knowledge-assisted video description using knowledge graphs. Univl [10] learns powerful vision-and-language representations by pre-training the models on large-scale datasets, _e.g_., HowTo100M [11] and WebVid-2M [12]. Some other works focus more on end-to-end video captioning generation. SwinBERT [13] proposed an end-to-end transformer-based model, which takes video frame patches directly as inputs and then uses VidSwin to extract visual features. MV-GPT [14] designed an encoder-decoder model end-to-end to generate the video caption from video frames and transcribed speech directly. We propose an end-to-end video captioning model based on the compressed domain without decoding video frames and extracting features offline, which not only accelerates the generation of captions, but also performs favorably against the state-of-the-art methods.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"," **Video Captioning.** Existing video captioning approaches can be roughly divided into two categories: (1) methods based on feature extraction and (2) methods that use the compressed video for captioning.

**Feature extraction and feature extraction.** Feature extraction methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14] extract video features from the video and then use them to train a captioning model. For example, [1] proposed to learn a deep network directly on compressed video by using the I-frames and optical flow as input. [2] proposed a Slow-I-Fast-P (SIFP) architecture that consists of the slow I pathway receiving a sparse sampling I-frame clip and the fast P pathway receiving dense optical flow clip. [4] proposed the Multi-Modal Video Transformer (MM-ViT) that exploits all modalities, i.e., motion vectors, residuals, and audio waveform, for video action recognition. [5] proposed Compressed Video Contrastive Learning (CVCL) to simultaneously learn from compressed videos and capture mutual information between two input streams. [8] proposed Semantic Grouping Network (SGN) to group video frames with discriminating word phrases of the partially decoded caption and then decode those semantically aligned groups in predicting the next word. [7] proposed an object relational graph based encoder to enrich visual representation and an effective training strategy to make full use of the abundant visual representation. [6] designed a hierarchical modular network to bridge video representations and linguistic semantics from three levels before generating captions. [9] designed an external stream to absorb additional knowledge, which models the interactions between the additional knowledge and the built-in information of videos, e. [14] proposed MV-GPT to leverage both text source and video source to generate multimodal captions, which can be effectively used for video caption generation tasks. [10] proposed UniViLM to learn video-text joint, conditioned masked language model (CMLM) and conditioned masked frame model (CMFM) for both video and text captioning tasks, which achieved state-of-the-art results on MSVD and MSR-VTT datasets.


"," There are a number of approaches that regularize the model by regularizing the softmax values [1][2][3][4][5][6], the temperature-scaled probability [7][5][8], the entropy of the probability [7][3][9][10][11], and pairwise softmax values [3][4][12]. Among them, we will focus on label smoothing [7][13][14][12] and focal loss [7][9] since these approaches can be regarded as regularization in terms of network calibration and have been widely adopted in the community.

The aim of label smoothing is to rescale the model predictions, preserving the order in the probability distribution [13]. In particular, all the elements of the predictions are multiplied by a scalar \(\epsilon\), yielding the soft prediction values. As discussed in Sec. 1, we study the label smoothing approaches that exploit the entropies as regularization terms, namely Focal Loss [7] and entropy-based label smoothing approaches [13].

Concurrent with our work, Sue _et al_. [12] present a similar perspective on label smoothing. However, their approach differs from ours in several ways. The most important difference is that we focus on the impact of \(\epsilon\), _i.e_. the regularization strength, and discuss its pros and cons.

The author [15] proposed a confidence loss that regularizes the model's true class probability (TCP) rather than the maximum class probability (MCP). In this work, we study the calibration impact of label smoothing from the TCP perspective.

In general, label smoothing approaches and the focal loss approaches generate higher accuracy but worse calibration, _i.e_., worse calibration for the model [16][3]. It has also been observed that overconfident predictions are a symptom of optimizing a loss that is not well-calibrated [16][17][18]. Specifically, models trained on a loss that is unsuitable for the dataset result in poorly-calibrated networks, making the predictions overconfident.

",,"<The field of video captioning has seen significant developments in recent years, with researchers continuously exploring novel approaches to improve the accuracy and efficiency of caption generation. One approach that has gained attention is video captioning in the compressed domain. The work by [1] introduces the concept of training deep networks directly on compressed videos, leveraging their higher information density and ease of processing compared to raw videos. Similarly, [2] proposes a Slow-I-Fast-P neural network model for compressed video action recognition, eliminating the reliance on traditional optical flows calculated from raw videos. These studies highlight the benefits of working with compressed videos for more efficient and accurate video captioning.

An alternative perspective on video representation learning in the compressed domain is presented in [3], where a method is developed to decouple motion supervision from context bias through a pretext task using key frames and motion vectors in compressed videos. This explicit decoupling enhances the quality of learned video representation, leading to improved performance in video retrieval and action recognition. Building on this, [4] introduces a Multi-Modal Video Transformer (MM-ViT) designed to exclusively operate in the compressed video domain, yielding state-of-the-art results in efficiency and accuracy for video action recognition. These studies collectively underscore the potential of leveraging compressed video data for effective video captioning and representation learning.

Furthermore, the work by [5] introduces a novel Motion Vector-based Cross Guidance Contrastive Learning approach, leveraging compressed videos to simultaneously achieve storage and computation efficiency, while capturing mutual information between input streams. This approach sets new benchmarks in downstream tasks while significantly improving efficiency. These studies demonstrate the promise of using compressed videos for self-supervised video representation learning, paving the way for more storage and computation-efficient video captioning approaches.

In the context of video captioning, the integration of visual representation with linguistic semantics is crucial. Recent approaches such as the Hierarchical Modular Network proposed by [6] and the Object Relational Graph architecture by [7] demonstrate effective methods for bridging the gap between video representations and linguistic semantics, resulting in improved performance on benchmark datasets. Additionally, work by [8] introduces a Semantic Grouping Network, leveraging continuous feedback from decoded words and a contrastive attention loss for accurate alignment between word phrases and video frames, thus achieving state-of-the-art performance in video caption generation.

The advancements in video-language pre-training models have also contributed significantly to multimodal understanding and generation. The Unified Video and Language pre-training model, UniVL, proposed by [10], has shown promising results for both multimodal understanding and generation tasks, offering significant improvements in downstream tasks. These pre-training models have the potential to enhance video captioning tasks by leveraging multimodal representations.

Finally, the use of attention mechanisms in end-to-end transformer-based models, as seen in the SwinBERT model introduced by [13], has proven to significantly improve video captioning performance. This approach enables adaptive learning of sparse attention masks, resulting in substantial performance improvements across various video captioning datasets. The advancements in end-to-end generative pretraining, as exemplified by MV-GPT in [14], offer new avenues for learning from unlabelled videos and generating sentences, further enhancing the capabilities of multimodal video captioning.>

In summary, the advances in video captioning and representation learning in the compressed domain, the development of effective self-supervised learning approaches, and the integration of visual and linguistic semantics through hierarchical and relational networks have collectively pushed the boundaries of video captioning research. Additionally, the emergence of unified video and language pre-training models and the integration of attention mechanisms and end-to-end generative pretraining frameworks signify promising directions for advancing multimodal video captioning tasks."
4506,4506," **Point Cloud Completion.** Point cloud completion is a fundamental problem in 3D geometry processing. Traditional methods [1][2][3] focus on reconstructing a complete surface from a partial point cloud. Recently, deep learning-based methods [4][5][6][7][8][9][10][11][12][13] have been proposed to solve this problem. PCN [4] is the first work to propose an end-to-end deep learning framework for point cloud completion. It uses a convolutional neural network (CNN) to directly predict the complete point cloud from the partial one. However, it requires a large number of complete point clouds as training data. To overcome this limitation, some methods [5][7] propose to use a folding network [5] to deform the incomplete point cloud into a complete one. FoldingNet first predicts a set of folding matrices and then uses a decoder to reconstruct the point cloud with the deformed matrices. To further improve the completion performance, Zhou _et al_. [8] propose a geometry-aware transformer (GeoT) to learn a mapping from the incomplete to the completed point cloud, which is then used to predict the missing regions.

**Self-Supervised Learning.** Self-supervised learning (SSL) [14][15][16][17][18] aims to learn from unlabeled data without using any ground truth labels. It has been widely used in various computer vision tasks, such as image classification, object detection, semantic segmentation, _etc_. Recently, SSL has also been applied to 3D shape completion [19][4][4]. In [4], a generative adversarial network (GAN) is used to generate complete and partial point clouds for training, and the generated point clouds are used to supervise the training of the completion network. In [9], a GAN-based framework is proposed to generate partial-complete shape pairs for training. In addition, Cycle4Completion [11] proposes a cycle-transformation-based method to generate missing regions from incomplete point clouds. In this work, we focus on learning from only a single partial object per category.

 propose a self-supervision framework for shape completion. They propose to complete a shape by learning a latent code for each part of the partial object, and then reconstruct the complete shape from the latent code. In contrast, our method learns from a category-specific dataset to complete the partial shape of an object without any supervision.

 proposes to learn an auto-encoder from a large-scale synthetic dataset. It first predicts the latent codes of the complete object and then fills the missing parts with the learned latent codes. In our method, we learn from a much smaller dataset consisting of only one partial object for learning.

 is a recent work that learns to predict missing patches from partial images. It learns to complete an object by reconstructing the missing part of an image from its corresponding partial image. Different from them, we aim to learn the missing region from a single incomplete shape.

 also proposes to reconstruct an object from its partial"," **Supervised Point Cloud Completion.** Earlier efforts to address point cloud completion can be divided into surface reconstruction and template matching. Surface reconstruction methods [1][2] attempt to restore missing regions by fitting existing points to an implicit surface based on geometric cues, and then resample new points from the estimated surface. On the other hand, template matching techniques [3] retrieve a template shape from a database and deform it to fit the target shape. However, surface reconstruction-based methods are able to fill holes on the surface but are limited in handling severe geometric incompleteness, while template matching methods are computationally expensive and rely on the availability of a sufficient number of example shapes. Starting with the pioneering work PCN [4], deep learning-based methods [5][6][7][8] have gained significant attention in point cloud completion. However, the supervised training approach requires paired ground truth, which is difficult to obtain for real-world scans. As a result, these methods are often trained on synthetic datasets, which leads to impressive results on synthetic data but may not generalize well to real-world scans [9].

**Unpaired and Weakly-Supervised Completion.** To address the issue of data acquisition, Chen _et al._[10] proposed the first method, Pcl2Pcl, that can be trained without paired partial and complete point sets. This was achieved through a generative adversarial network , where the generator transforms a partial shape latent encoding into a representation indistinguishable from the latent variable obtained from real complete shapes by the discriminator. Following Pcl2Pcl, many methods [11][9][12][13] have been proposed to produce more accurate results. Nevertheless, complete shape repositories are still required, and combining unaligned real-world partial scans with complete shapes from other sources may result in poor outcomes due to alignment errors. Different from prior approaches, Gu _et al._ tackle the problem of point cloud completion by using unaligned real-world partial point clouds as their data source. The network is trained with multi-view geometric constraints as weak supervision cues. However, these methods require scans from multiple viewing angles, which are not always feasible to obtain.

**Self-Supervised Learning.** To mitigate the cost of dataset collection and annotation, self-supervised learning [14] have been proposed. For example, DINO [15] demonstrated improved classification performance using only self-supervised training, without any labels. Self-supervised learning has also gained popularity in point cloud studies. Building upon the work of He _et al._[16], Liu _et al._[17] proposed a self-supervised mask discrimination framework for pretraining transformers. For point cloud upsampling, SSPU-Net  leverages the consistency between input sparse and generated dense point clouds to train the network using only sparse clouds. Concurrently with our research, Hong _et al._[18] proposed a related point cloud completion scheme, but used the same data for training and testing to enable an adaptive closed-loop [19] optimization. In contrast, our approach uses distinct test samples.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"," **Point Cloud Completion.** Point cloud completion aims to recover the complete shape of an object from its partial observation. Existing point cloud completion methods can be roughly divided into two categories, _i.e._, deterministic methods and generative methods.

**Deterministic methods.** These methods directly predict the complete shapes from partial point clouds. Poisson surface reconstruction [1][2] is one of the earliest works to solve this problem. However, it requires complete point clouds as input. PCN [3][4] is the first work to use deep neural networks to complete point cloud objects. FoldingNet [5] proposes a novel folding-based decoder to deform a canonical 2D grid onto the underlying 3D object surface of a partial point cloud, achieving low reconstruction errors even for objects with delicate structures. SnowflakeNet [6] models the generation of complete shapes as the snowflake-like growth of points in 3D space, where the child points are progressively generated by splitting their parent points after each SPD layer. VRC-Net [7] introduces a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds, and proposes a dual path architecture to learn a point VAE. PoinTr [8] adopts a geometry-aware block to model the local geometric relationships explicitly. However these methods require either complete or partial point-cloud pairs for training, which limits their generalization ability to real-world point cloud data. In this work, we propose a self-supervised framework that completes point cloud using only a single partial cloud as input, which is much more robust to the domain gap between synthetic data and real data, and can generalize better to real data. Our work is also related to unsupervised methods [9][10][11][12][13], which do not require any complete or paired data for training. However they are still limited to the synthetic data due to the lack of domain-specific knowledge. In contrast, our method is able to complete complete pointclouds from a single incomplete point cloud without any additional supervision. Our method is also closely related to the unsupervision based methods [14][15][16][17][18]. However, our work is different from these methods in that we do not need any complete data for learning and we only need a single point cloud per object for training instead of multiple partial clouds per object, which makes our method more robust and generalizable to real world point cloud.
"," **Classical Image Stitching.** Early stitching schemes only involve geometric features (mainly correspondences [1][2][3][4][5][6][7]). The introduction of modern feature extraction algorithms like DL-Hough  and LSD [8] further improved the performance of traditional stitching schemes [9][2][10][7][11][12].

**Semantic-based Image Stitching.** A straightforward strategy is to use semantic features to improve the traditional stitching schemes [13][14][15][16][17][18][19]. To achieve this goal, several semantic segmentation algorithms are used to segment feature regions for stitching. Xu et al.  adopted Adaptive k-means to segment feature regions, and Chellappa et al. [13] utilized the Spectral clustering to handle semantic scene prior. Deep learning is the dominant segmentation technique. Dosovitskiy et al. [18] proposed a network to segment all features. In a word, the methods could achieve good stitching quality but required tedious manual efforts to determine the input features.

**Deep Image Stitching.** Based on the recent success of deep learning in many fields, deep stitching schemes have been proposed. Wu et al. [20] proposed an intermediate frame approach and a new convolutional neural network (CNN) to map each frame into intermediate frames. Wang et al. [21] proposed a method to estimate multiple homographies for stitching. Chen et al. [22] combined an image alignment loss with a distortion loss to produce seamless images. Both Wu et al. [20] and Chen et al. [22] used deep features for the local alignment to achieve better image stitching performance. However, their local alignment relied on the traditional homography transformation, which cannot handle large-parallax cases. To overcome this problem, Park et al.  proposed a unified deep fusion framework. They proposed an additional segmentation loss to jointly optimize the optical flow and camera calibration. Moreover, a new deep network was adopted to estimate the camera parameters for calibration. But their methods cannot handle scenes with large parallax and image resolution variation.

Several recent methods [23][24] have also been proposed to solve this problem. Wang et al. [23] proposed a weakly-supervised stitching network. They first trained the stitching network using real-world panoramic data. Then they pre-defined a virtual model using a simulated dataset with random numbers to replace the original dataset. Wang et al. [24] proposed an unsupervised deep image stitching framework. They proposed two loss functions (i.e., the flow-based loss and the reprojection-based loss) to jointly optimize two modules (i.e., an affine transformation warp model",,"<<Related work>>
Point cloud completion has been an active research area in computer vision and robotics. Several approaches have been proposed to address the problem of completing the shape of a point cloud based on partial observations. In particular, existing methods often rely on supervised learning, requiring either complete point clouds or multiple partial observations of the same object for training [1][2][3]. However, the approach presented in the target paper, P2C, introduces a novel self-supervised framework for point cloud completion, which is a departure from the traditional reliance on complete or multiple partial point clouds for training. P2C achieves this by grouping incomplete point clouds into local patches and predicting masked patches using prior information from different partial objects. This innovative approach eliminates the need for multiple observations or complete point clouds as ground truth, instead learning structural cues from a category-specific dataset to complete partial point clouds of objects.

Furthermore, prior work in this domain includes methods such as PCN [4], which operates directly on raw point clouds without structural assumptions, and FoldingNet [5], which proposes a novel end-to-end deep auto-encoder for unsupervised learning challenges on point clouds. SnowflakeNet [6] and VRC-Net [7] leverage specific architectural designs to address the issue of revealing fine local geometric details. PoinTr [8] reformulates point cloud completion as a set-to-set translation problem using a transformer encoder-decoder architecture. Additionally, ShapeInversion [9] introduces Generative Adversarial Network (GAN) inversion to shape completion, while Cycle4Completion [11] proposes a bidirectional geometry correspondence learning approach.

In addition to these works, approaches such as Learning a Structured Latent Space [12], Energy-Based Residual Latent Transport [13], and Masked Discrimination for Self-Supervised Learning on Point Clouds [17] have focused on unsupervised and generative modeling based on latent space transformation for point cloud completion. Additionally, innovative self-supervised learning approaches in other domains, such as multimodal representation learning [14] and Vision Transformer (ViT) properties [15][16], may provide insights that could be leveraged for self-supervised point cloud completion.

These existing approaches reflect the wide range of techniques and methods utilized to tackle point cloud completion, including self-supervised and unsupervised learning, generative modeling, and transformer-based architectures. The target paper P2C contributes to this body of work by introducing a self-supervised framework that addresses the completion of point cloud objects using training samples consisting of only a single incomplete point cloud per object, thus advancing the field of point cloud completion through innovative training methodologies and architectural designs [18]. <</Related work>>"
1505,1505," **Motion segmentation.** Motion segmentation is the task of predicting the motion of a single object in a video given a single image. It has been extensively studied in computer vision [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27]. In this paper, we focus on the problem of motion segmentation in videos.

**Depth estimation.** The goal of depth estimation is to predict the pixel-wise depth of an image given an input image. In this task, the input image is usually a single RGB image. Previous works [28][29][30][31] estimate the depth of the image by minimizing the photometric error between the input and the output image. For example, [28] estimates the depth by minimizing a photometric loss. [29] estimates depth by maximizing the difference between the predicted depth and the ground-truth depth. [30] predicts the object motion by minimizing an energy function. [31] estimates a motion field.  estimates the motion field by minimizing photometric errors. [22] predicts a motion vector. [23] estimates an object motion vector by minimizing their energy function by maximizing their photometric difference. [26] estimates motion vectors. [27] estimates object motion vectors by estimating object motion fields."," Basis Learning.Early work showed that optical flow estimation due to camera motion can be constrained using a subspace formulation for flow [1]. Basis learning has been used as a regularization in low-level vision, unifying tasks such as depth, flow, and segmentation [2]. PCAFlow [3] builds a higher dimensional flow subspace from movies to represent flow as a weighted sum of flow bases. Recent work [4] learns the coefficients to combine eight pre-defined flow bases for homography estimation.

Motion as Input.Most of the work in motion segmentation focuses on the single-object case. While earlier work uses traditional methods to cluster pixels into similar motion groups [5][6][7], later methods train deep neural networks which take flow as input and predict segmentation as output [8][9]. Another work [10] uses the distinctiveness of motion in the case of foreground objects by proposing an adversarial setting to predict motion from context. Segmenting objects in camouflaged settings can be achieved by modeling background motion to remove its effect and highlight the moving foreground object [11]. Recent work uses consistency between two flow fields computed under different frame gaps for self-supervision [12].

The most relevant to our work is OCLR [13] which extends motion segmentation to multiple objects by relating motion extracted from multiple frames using a transformer in a layered representation. In this work, we show that better results can be achieved on real data even from a single image by modeling pixel-wise geometry.

Motion for Supervision.While using motion only as input works well where appearance fails, e.g. the camouflage datasets, RGB carries important information that might be missing in flow. DyStaB [14] trains a dynamic model by exploiting motion for temporal consistency and then uses it to bootstrap a static model which takes a single image as input. A single image network is used to predict a segmentation in [15] and then the motion of each segment is predicted with a two-frame motion network. While image warping loss is used in [15] for self-supervision, recent work [16][17] uses flow reconstruction loss by assuming the availability of flow at train time only. GWM [16] segments foreground objects by fitting an approximate motion model to each segment and then merging them using spectral clustering. The follow-up work [17] extends it to multiple objects by predicting probable motion patterns for each segment with a distribution. We also reconstruct flow for supervision but differently, we account for 3D to remove the ambiguity in reconstructing motion from a single image.

The most relevant to our work is the previous work that uses flow as a source of supervision for depth [18] or segmentation [16][19]. In this work, we model both depth and segmentation with supervision from motion.

Multi-Object Scene Decomposition.Our work is also related to scene decomposition approaches which are mostly evaluated on synthetic datasets. The earlier image-based decomposition approaches such as MONet [20] and IODINE [21] use a sequential VAE structure where the decomposition at a step can affect the remaining parts to be explained in the next step. GENESIS [22] follows an object-centric approach by accounting for component interactions, which is extended to more realistic scenarios with an autoregressive prior in the follow-up work [23]. Slot Attention [24] uses an iterative attention mechanism to decompose the image into a set of slot representations. A hierarchical VAE is used in [25] to extract symmetric and disentangled representations.

There are also video-based approaches to multi-object scene decomposition. SCALOR [26] focuses on scaling generative approaches to crowded scenes in terms of object density. SIMONe [27] learns a factorized latent space to separate object semantics that is constant in the sequence from the background which changes at each frame according to camera motion. SAVi [28] extends Slot Attention [24]to videos and SAVi++ [29] extends it to real-world driving scenarios with sparse depth supervision.

Self-Supervised Monocular Depth Estimation.Zhou et al. [30] train a pose network to estimate the pose between the frames in a sequence and jointly train it with the depth network. Godard et al. [31] improves the results with a better loss function and other design choices. Guizilini et al. [32] learn detail-preserving representations using 3D packing and unpacking blocks. Given instance segmentation masks, a line of work [33][34] models the motion of objects in the scene in addition to the camera motion to go beyond the static-scene assumption. While the object masks are supervised using ground truth masks in , the masks are learned without supervision as an auxiliary output in [35] for better depth estimation. While they require multiple frames during inference, our approach can estimate masks from a single image. Additionally, our method does not use camera intrinsics.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]"," **Motion segmentation.** Motion segmentation has been extensively studied in computer vision for decades [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35]. In this work, we focus on unsupervised motion segmentation, which is a more challenging task due to the inherent ambiguity in predicting motion from a single image.

**Unsupervised Motion Segmentation**. Early work in un supervised motion segmentations [8][10] use optical flow as the motion cue to segment moving objects. However, optical flow is not robust to motion discontinuities and may not accurately represent the motion of objects. To address this issue, recent work [10][15] propose to use motion cues from optical flow to guide the segmentation of moving objects in a video. In particular, [15] uses optical flow and appearance cues to segment the moving objects and [10] uses motion cues to guide appearance cues. In addition to optical flow, [12] proposes a self-supervised method to segment optical flow frames into primary objects and the background, which achieves state-of-the-art results on camouflage dataset. [13] proposes an object-centric segmentation model with a depth-ordered layer representation. [14] proposes to segment objects by bootstrapping and [17] proposes the motion-aware self-training method to learn motion cues. [16] proposes Guess What Moves, which predicts the motion patterns that are likely to correspond to objects. [11] combines the strengths of motion-based and appearance-based segmentation by combining the strengths from both of them. [18] predicts a subspace of optical flow for each training example, which encompasses the variety of possible camera and object movement. [15][17] predict the optical flow of each object in the image and use it to guide their segmentation models. [17][11] use the motion cues of optical flows to segment multiple moving objects, which are then combined with appearance cues from CNNs to obtain a generic objectness prior for capturing the full extent of objects in the scene. [12][14] propose the motion grouping method which uses optical flows as motion cues for segmenting moving objects without using any manual annotations. [35] proposes"," Scene flow estimation can be broadly classified into two categories: classical methods and deep learning-based methods. The classical methods generally focus on detecting keypoints from images and then determining their correspondences with a structured similarity search approach [1]. These methods are mainly limited by the lack of 3D point cloud data. Since deep learning-based methods have access to 3D point cloud data, they have achieved a tremendous progress [2][3][4][5][6][7]. However, these approaches fail to meet the performance requirements when dealing with large datasets or complicated scenarios. Some of these approaches apply recurrent architecture to accumulate scene flow features iteratively [8][9][10]. These approaches are also inspired by classical iterative methods [11][12] that use optimization to refine scene flow. Compared with the iterative method, the recurrent architecture offers a unified paradigm that can be combined with other deep learning based scene flow approaches [13]. These deep learning based approaches are also combined with classical approaches [10][14]. PWC-Net [14] employs classical approaches to extract intermediate representations and then fuses them into a predicted 3D flow. Due to the inefficient cost volume, PWC-Net requires the input scenes to be cropped and upsampled. This process introduces an additional computational cost that limits the practical application of PWC-Net. LiteFlowNet [15] introduces a lightweight architecture that does not depend on a pyramidal structure to generate the cost volume. Instead, it generates a cost volume by warping a 3D feature map from input to target frames. LiteFlowNet achieves performance comparable to FlowNet2.0 [13] with half the parameter number. RCP [11] proposes to represent a 3D correlation volume by capturing the all-to-all pair correlations between two point clouds in a 3D point cloud domain. Point features extracted by a fully convolutional network are used to represent the 3D flow features. RCP [11] shows superior performance compared with PWC-Net and LiteFlowNet on FlyingThings3D. However, RCP is still limited by the fixed all-to-all pairwise correlation.

There are also several studies focusing on combining traditional approach and deep learning based approaches [16][17][18]. Zhou et al. [16] proposes a recurrent framework for scene flow estimation. The scene flow features are point-wisely optimized and then globally regularized by RCP [11]. FlowStep3D [12] proposes a recurrent architecture that updates one step of unrolled optical flow algorithm. PV-RAFT [11] introduces a point-voxel correlation volume that exploits both local and long-range correlations between two point clouds. Compared with the existing deep learning based approaches, PV-RAFT focuses",,"<>
Recent advancements in unsupervised multi-object segmentation have shown remarkable progress in predicting motion from a single image, despite the inherent ambiguity associated with predicting motion without the next image [1]. By considering the scene structure and the movement of objects within it, the set of potential motions for an image can be confined to a low-dimensional space. This approach models pixel-wise geometry and object motion to alleviate the ambiguity in reconstructing flow from a single image [18]. The proposed method divides the image into coherently moving regions and utilizes depth to construct flow bases that best explain the observed flow in each region. 

The challenges of estimating optical flow accurately and efficiently have been addressed through the use of a sparse-to-dense approach, where a set of sparse matches is used to regress to dense optical flow, demonstrating robust estimation and significantly faster results than popular methods [3]. Object discovery in videos has been approached as foreground motion clustering, involving a novel pixel-trajectory recurrent neural network that learns feature embeddings of foreground pixel trajectories linked across time for state-of-the-art performance [7]. Additionally, learning motion patterns in videos can be achieved through a fully convolutional network, coupled with objectness map and conditional random field, to denote independent motion of each pixel [9].

Furthermore, object segmentation methods have evolved to combine motion cues with appearance cues for spatio-temporal grouping, leveraging motion cues from optical flow as a bottom-up signal for object separation [8]. A focus on segmenting moving objects via a minimum cost multicut formulation introduces the optimization of a cluster assignment, the number of clusters, and varying cluster sizes without relying on spectral clustering, yielding state-of-the-art results on benchmark datasets [5]. Object trajectory segmentation has been addressed via a hierarchical variational approach, where sparse trajectory clusters obtain dense segmentations through a hierarchical, nonlinear diffusion process, increasing segment density from 3% to 100% and enhancing average precision of labels [6].

Finally, self-supervised monocular depth estimation methods, particularly those aiming to predict scene depth without direct supervision, have shown significant developments. Research focuses on leveraging geometric and motion cues for depth estimation, where end-to-end joint training frameworks have emerged to model the 6-DoF motion of multiple dynamic objects, ego-motion, and depth [34]. Additionally, self-supervised monocular depth estimation is being approached with the goal of jointly estimating depth and segmenting moving objects from monocular video without the use of ground-truth labels [35]. Such advancements in self-supervised learning have expanded the scope of monocular scene decomposition and depth estimation, serving as a testament to its potential for real-world applications."
3901,3901," **Deep Learning on Point Clouds.** PointNet [7] is the first work to apply deep neural networks directly on point clouds. Since then, a lot of works have been proposed to improve the performance of point cloud tasks, such as 3D object detection [6], 3D point cloud segmentation [3], video prediction [1], _etc_. Among them, PointCNN [4] and PointConv [11] are two representative works that directly apply convolutional neural networks (CNNs) on unordered point sets. However, these methods are limited by the permutation invariance of point sets, which makes it difficult to apply CNNs to point clouds with irregular permutations. To address this problem, PointNet++ [8] introduces a hierarchical structure to learn permutation-invariant features. KPConv[9] and PU-Net [10] further propose to learn point-wise features and point-to-edge features, respectively, to improve performance.

**Learning from Protein Structures.** Learning from protein structures has been widely studied in recent years [11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27]. For example, Qi _et al._[27] propose a self-supervised learning framework to learn from protein structure, which learns a probabilistic embedding space for protein sequences. In this paper, we focus on learning from point clouds, which are more challenging than protein sequences due to the irregular permutation and permutation of points in point clouds and the non-coding nature of proteins. We propose a PointListNet that learns from point sets with irregular point-feeding orders, which is orthogonal to the existing methods. Our method can be applied to both protein sequences and point clouds to further improve performance on both tasks, as shown in Sec. 4.3.1 and Sec. 5.2, respectively. Our work is also related to the Transformer-based methods [28][29][30][31][32][33][34][35][36][37][38] for natural language processing.

 propose a transformer-based architecture for sequence modeling. The core idea of the transformer is to learn a set of self-attention vectors for each element in the input sequence, and then apply a max-pooling operation to aggregate information from all positions in the sequence. In contrast, we propose a non-parametric distance-based attention mechanism to model the relation between two points in the point cloud, which has not been explored in previous works.

 introduce a transformer architecture for point cloud object detection. The key idea of this work is to use the attention mechanism of the vanilla Transformer to capture the spatial and temporal relationships between points in a point cloud. The main difference between our work and theirs is that we propose to use PointListNets to learn the relation among points, instead of applying the vanilla attention mechanism.

 also propose an attention-based network for 3D scene understanding. Their method is based on the PointNet architecture. Their network is"," **Deep Learning on 3D Point Sets.** Deep learning on point sets/clouds has been widely investigated in several problems, including shape classification, object part segmentation, scene semantic segmentation, reconstruction and object detection [1][2][3][4][5][6][7][8][9][10][11][12]. Most recent works aim at directly manipulating 3D points without transforming coordinates into regular voxel grids. Since a point cloud is essentially a set of unordered points and invariant to permutations of its points, deep learning on point clouds mainly focuses on designing effective operations that do not rely on point orders. Because point cloud methods do not involve sequence modeling, directly applying them to 3D point lists, _e.g._, proteins, may lead to inferior accuracy.

**Deep Learning on Proteins.** Proteins exhibit multi-level structures. Deep-learning-based methods for protein representation learning mainly focus on the 1D primary and the 3D tertiary structures understanding. The primary structure refers to the sequence of amino acids in the polypeptide chain. The tertiary structure refers to the three-dimensional structure created by a single protein molecule (a single polypeptide chain). For the primary structure, because acids in polypeptide chains can be seen as words in sentences, approaches for natural language processing can be used for sequence-based protein representation learning [13][14][15][16][17][18][16]. For the tertiary structure, the 3D geometric information of amino acids or atoms is used to enhance protein representation [19][20][21][22][23][24][25][26][27]. Different from these methods, we propose a Transformer-style method to model primary and tertiary structures for proteins. Moreover, we employ different approaches to capture the 1D and 3D structures.

**Transformer.** Impressive progress has been made on natural language processing due to the success of Transformernetworks [28][29][30][31]. In computer vision, the community has used self-attention or Transformer to model images in a non-local manner [32][33][34]. In particular, Zhao _et al_. proposed a Point Transformer [35] to model point clouds. Fan _et al_. proposed a P4Transformer [36] for point cloud video understanding. Lai _et al_. proposed a Stratified Transformer [3] for point cloud segmentation. Feng _et al_. proposed a Structure Embedding Transformer (SEFormer) [37] for 3D object detection. Wang _et al_. proposed a Relation-Enhanced Transformer [38] for text-based point cloud localization. Inspired by these methods, we propose a Transformer-style PointListNet for 3D point list modeling. Different from these methods, we replace learning-based self-attention with rule-based distance-attention, thus more efficient to achieve the correlation among microparticles. Moreover, we integrate relative structure modeling into Transformer and employ regular and irregular methods to capture the sequence and geometry structures, respectively.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]"," **Deep Learning on 3D Point Clouds.** Deep learning on point clouds has made tremendous progress in recent years. PointNet [7] is the first attempt to directly learn from point clouds. It directly applies a PointNet-style network on raw point clouds and uses a simple linear transformation on inputs to generate values, which does not explicitly model the relative relation between points. PointConv [11] extends PointNet to handle 3D point clouds by learning convolution kernels as nonlinear functions of the local coordinates of 3D points. KPConv and its variants [9]; [4]; [12]; [10]; [11]; [9] further extend KP to handle point clouds with deformable convolutions. [12] proposes a graph convolution operation to model the local structures of point clouds, which is differentiable and can be plugged into existing architectures. [10] proposes to learn multi-level features per point and expand the point set via a multi-branch convolution unit implicitly in feature space. PST-Transformer [1] and VoteNet [6] use self-attention on point features to preserve the temporal structure of point cloud videos. [3] proposes the key sampling strategy to capture long-range contexts in point cloud segmentation. [2] designs a point tube covering a local range along spatial and temporal dimensions to capture the local structure of each spatial-temporal dimension. [5] introduces a pure residual MLP network, which integrates no sophisticated local geometrical extractors using convolution, graph or attention mechanisms.
-MLP [5], PointNet++ [8], PointMLP[5], and PointNetVoting [6], propose to learn local features with distance-based attention.


**Protein-based learning.** Protein-based protein learning has been studied for a long time [13]; [19]; [18]; [15]; [21]; [22]; [23]; [24]; [25]; [20]; [26]; [27]; [14]. [19] proposes an EnzyNet-based model to predict the protein function from sequence. [17] proposes UDSMProt to learn a task-agnostic representation from unlabeled protein sequences and transfer it to specific tasks by finetuning. [16] proposes TAPE, a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. [18] proposes DeepGO to predict protein functions from sequence and"," **Balancing Modality.** Some existing methods, such as Improved Backbone [1], On-the-fly Gradient Modulation [2], and Grounding Tricks  achieve the joint training of multimodal learning with balanced data for all modalities.  augments the modalities with high feature entropy into a high-entropy modality. Then, a bi-modal loss is minimized. However, all these methods need extra supervision to train an even data distribution across modalities, and the mixing strategy should be carefully designed. Furthermore, due to the aforementioned limitations, we ignore those MML methods which involve one or two modalities.

**Prototypical Knowledge in Others.** Prototypical network [3] is proposed in the scenario of unimodal learning, to address the problem that a few classes dominate the classification. With the exploration of prototypical knowledge in other areas, it has been proved to be effective in few-shot learning (FSL) [4], graph representation learning , and attribute networks [5]. Recently, prototypical knowledge is also explored in the multi-domain learning scenario [6][7][8][9][10][11], where it is successfully applied to the downstream multi-domain few-shot tasks. These works are all about learning and embedding prototypical information of different classes.

**Prototypical Learning for Others.** Different from the MML scenarios, there are some works which focus on learning the prototypical information in unimodal learning [12][13]. Prototypical Contrastive Learning (PCL) [13] trains a prototypical classifier as a discriminative classifier to learn unimodal representations. Then, it achieves state-of-the-art performance on multiple unsupervised representation learning benchmarks. However, in MML, the aforementioned methods are not effective.

",,"<Related Work>
Point cloud understanding and deep learning have seen significant advancements in recent years, with several methods designed to handle irregular and unordered 3D point cloud data. As demonstrated in [4], the PointCNN framework presents a general approach for feature learning from point clouds, addressing the challenges posed by irregular and unordered data structures through the $\mathcal{X}$-transformation. Similarly, PointNet and PointNet++ (referenced in [7] and [8] respectively) provide pioneering efforts in developing neural network architectures specifically tailored for processing point sets, allowing for efficient and effective learning on irregular and unstructured data formats. Additionally, the work presented in [9] introduces the Kernel Point Convolution (KPConv), a robust and flexible point convolution mechanism, demonstrating superior performance on classification and segmentation tasks for point cloud datasets.

Furthermore, the emergence of transformer-based architectures such as BERT, XLNet, and Vision Transformer (ViT) (referenced in [29], [30], and [32] respectively) signifies the efficacy of attention mechanisms in handling diverse data types beyond text, including images and sequences. These advancements in machine learning architectures have opened new avenues for leveraging attention mechanisms in 3D point cloud data processing. Along similar lines, the work in [38] focuses on applying a Relation-Enhanced Transformer (RET) to cross-modal localization, emphasizing the importance of capturing relations among textual hints in a complex spatial environment.

Extending the application of transformers to 3D data, recent works such as Point Transformer ([35]) and SEFormer ([37]) bring attention mechanisms into the context of irregular and sparse 3D data, demonstrating their potential to capture structural and spatial information effectively. Moreover, the non-local operations presented in [33] demonstrate the benefits of capturing long-range dependencies, suggesting the broader applicability of non-local neural networks in various computer vision applications, including point cloud processing.

In summary, while the majority of existing works focus on applying deep learning techniques to point cloud data processing, recent advancements in transformer-based architectures and non-local neural networks offer promising directions for handling irregular and unstructured 3D data formats effectively.

<>"
1765,1765," Domain generalization (DG) aims to generalize beyond the training distribution. Existing DG methods can be roughly divided into two categories. The first category aims to learn domain-invariant representations (DARs) by minimizing the divergence between the distributions of the source and target domains. For example, [1] proposed to minimize the distance between the mean and covariance of the embeddings of the target and source domains. [7] proposed a cross-domain gradient reversal layer (CDR). [6] proposed an image style transformation (IST) layer. The second category focuses on minimizing the distributional uncertainty (DU) between the training and target distributions. For instance, [13] proposed the Maximum Mean Discrepancy (MMD) loss. [8] used the Jensen-Shannon divergence as the uncertainty measure. [14] used a KL divergence measure to measure the uncertainty. Recently, [9] introduced an uncertainty model to estimate the uncertainty of the training distributions.

Distributionally robust optimization (DRO) is proposed to improve the robustness of the model to distributional shifts. [17] proposed DRO to learn a distributionally robust neural network. [18] used DRO for robust supervised learning. [19] proposed another DRO based on the second-order derivative of the loss function. [21] proposed unsupervised domain adversarial training (UDA) for DG.

 proposed a DRO-based method for generalization. However, the performance of these methods is limited by the large uncertainty.

 introduced a new loss function for uncertainty estimation. It is based on a multi-layer perceptron (MLP). The performance of UDA is improved by UDA.  proposed a similar loss function to UDA, but it is not applicable to deep neural networks.

 presented a method for learning a generative adversarial network (GAN). It is similar to DRO, but the performance improvement is limited.

 also proposed a distributional adversarial learning (DAL) based method. It improves the model's robustness against distributional shift. It also improves the generalization performance."," Domain generalization aims to learn more generalized knowledge from existing multiple source domains and finally test on the unknown target domain. Over the years, great efforts have been made in many directions, such as Invariant Representation ([13]; [3]; [5]; [11]), Causal ([1]; ; [14]), and Optimization ([2]; [10]; [8]; ). To generalize well on the unknown target domains, previous works introduce a domain generation strategy, enhancing the performance of DNNs by generating new domains. [7] perturbs the input samples along the direction of the most significant domain change while maintaining semantics.  trains a domain transformation model to transform images to unseen domains by fooling a domain classifier. [6]; [4] simply use a style transfer like AdaIN () to argument data in style aspects to optimize the model.  train a data generator to generate new domains using optimal transport to measure the distribution divergence. Zhou et al. (2021; 20) achieves style augmentation in the feature level by mixing the CNN feature map's mean and std between instances of different domains. [9] focuses on addressing the uncertain nature of domain shifts by modeling feature statistics as uncertain distributions, which is also achieved through the use of AdaIN, where non-semantic factors are replaced with randomly chosen values from the modeled distributions. [12] address the problem of domain shift by developing two simple and efficient normalization methods that can reduce the non-semantic domain shift between different distributions, while  jointly learns semantic and variation encoders to disentangle the semantic and non-semantic factors. Our approach explores the non-semantic factor to create augmented samples, which to some extent, is similar to approaches of data augmentation.

Distributionally robust optimization is a promising approach to tackle distribution discrepancy by exploring unknown domains in a fixed uncertainty set ([17]). DRO has developed plenty of approaches with different methods to measuring distribution discrepancy, such as Wasserstein distance ([16]; [22]), \(f\)-divergence (; [20]; ; [19]) and maximum mean discrepancy (MMD) (). Unfortunately, employing DRO to DG has shown limited performance improvement in practice (). ([18]; [21]; ) have pointed out that in order to capture the unknown target domain, the uncertainty set is often overwhelmingly large, leading the learned model to make decisions with fairly low confidence in DRO. [15];  focuses on the low confidence problem, and use a Wasserstein distance is employed to determine the uncertainty set.  uses data geometry to construct more reasonable and effective uncertainty sets, while  constructs the uncertainty using the data topology. Our approach MODE tackles the low confidence problem by performing distribution exploration in a specific uncertainty subset (non-semantic factor) and uses Wasserstein distance ([16]; [22]) to measure the distribution discrepancy in DG.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]"," **Domain generalization.** Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Existing DG methods can be divided into two categories: data augmentation and distributional robust optimization. Data augmentation methods aim to generate new domains by augmenting the training data with different styles [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22]. For example, [3] proposed to use domain-invariant representation learning (DRE) to learn domain invariant representations. [4] proposed a domain augmentation method to improve generalization performance. [2] proposed the principle of risk extrapolation (REx) to encourage robustness over affine combinations of training risks. [1] proposed an iterative algorithm called MatchDG to match inputs across domains by using a contrastive loss formulation adapted for multiple domains. [14] introduced a general structural causal model to formalize the DG problem and proposed a causal causal model that each input is constructed from a mix of causal factors (whose relationship with the label is invariant across domains) and non-causal factors (category-independent), and only the former cause the classification judgments. [10] used a functional modular probing method to analyze deep model structures under OOD setting and demonstrated that models with particular structure contain unbiased functional sub-networks. [6] used stylized images to correct the bias of CNNs and showed that it can achieve better generalization accuracy. [7] proposed cross-gradient training to train cross-domain classifiers on examples perturbed by loss gradients of each other's objectives. [11] proposed inter-domain gradient matching for DG by maximizing the inner product between gradients from different domains.  proposed a method to learn a domain-agnostic representation by minimizing the KL-divergence between the source and target domain. [8] proposed near-optimal linear regression under distribution shift by estimating the covariance of the target data distribution with the source data distribution. [13] used self-representative representation learning to estimate the risk of DG. [9] improved the network generalization ability by synthesizing feature statistics with a multivariate Gaussian distribution.

**Distributionally robust optimization (DRO).** DRO [17] is a robust alternative to empirical risk minimization. DRO optimizes"," We give a short discussion of a few closely related research areas. For a more comprehensive survey of the entire domain of uncertainty quantification, we refer to Li &  and Maurer & .

**Double descent.** A landmark paper was the _deep double descent_ paper by Belkin et al. (2018) that brought together the reams of existing literature into one coherent picture, thus launching the field of double descent. The effects of over-parametrization are not only an important question for machine learning, but also for Bayesian statistics and nonparametric estimation (see [21]; [20]; ; [11]; [12]; [6]; [2]; [18]; [10]; [19]; [15]; [14]; [1]; [23]; [25]; ; [17]; [16]; [13]; [18]; [14]; [17]; [2]; [23]; [25]; [9]; [12]; [13]; [19]; [17]; [14]; [2]; [23]; [25]; [10]; [21]; [28]; ; [22]; [12]; [17]; [14]; [10]; [18]; [13]; [19]; [14]; [2]; [12]; [15]; [19]; [2]; [13]; [23]; [25]; [9]; [18]; [14]; [17]; [12]; [10]; [18]; [13]; [19]; [14]; [10]; [18]; [12]; [10]; [18]; [13]; [19]; [2]; [13]; [2]; [23]; [25]; [9]; [18]; [10]; [12]; [2]; [13]; [19]; [14]; [17]; [2]; [10]; [18]; [13]; [19]; [2]; [10]; [12]; [14]; [13]; [19]; [2]; [13]; [2]; [10]; [18]; [13]; [19]; [14]; [2]; [13]; [19]; [2]; [1]; [2]; [19]; [22]; [1]; [19]; [12]; [14]; [10]; [18]; [13]; [2]; [13]; [2]; [22]; [1]; [12]; [13]; [19]; [2]; [1]; [2]; [13]; [2]; [22]; [1]; [1]; [2]; [2]; [",,"\<In recent years, domain generalization (DG) has garnered significant attention as it addresses the issue of distribution shift between training domains and unknown target domains. Various approaches have been proposed to tackle this challenge. For example, MatchDG [1] aims to identify the right conditions for invariance by proposing a causal interpretation of domain generalization. It defines domains as interventions under a data-generating process and uses a causal model to learn an invariant representation. Another approach, REx [2], introduces the principle of Risk Extrapolation to encourage robustness over affine combinations of training risks, enabling models to extrapolate beyond the observed distributions. This is particularly important in achieving strong out-of-distribution generalization. Additionally, methods like Domain Invariant Representation Learning [3] and Rethinking Domain Generalization Baselines [4] propose techniques to find and learn domain-invariant information to support model robustness across domains. These methods enforce the representation network to be invariant under all transformation functions among domains and integrate data variability to improve generalization, respectively.\<br>

Moreover, Bayesian frameworks for domain generalization have been studied to address the challenges of domain shift and the uncertainty caused by the inaccessibility of target domain data. UvA-DARE [5] addresses both challenges with a probabilistic framework based on variational Bayesian inference, incorporating uncertainty into neural network weights to explore domain-invariant learning. The Frustratingly Simple Domain Generalization via Image Stylization [6] presents an effective method to correct bias by augmenting the dataset with stylized images, thereby improving the generalization performance of CNNs. Another notable approach is CROSSGRAD [7], which utilizes domain-guided perturbation to provide consistently better generalization to unseen domains, compared to generic instance perturbation methods for learning a classifier that generalizes to new domains. Lastly, the work on Gradient Matching for Domain Generalization [11] proposes an inter-domain gradient matching objective to address distribution shifts by maximizing the inner product between gradients from different domains, demonstrating efficacy on diverse range of tasks capturing distribution shift.>

These approaches collectively offer a diverse set of solutions to the domain generalization problem. At their core, they emphasize the importance of addressing distribution shifts, uncertainty, and the need for robust representations and classifiers that can generalize effectively across unseen domains. The proposed methods leverage techniques such as causal interpretation, risk extrapolation, domain-invariant learning, image stylization, and gradient matching to advance the state-of-the-art in domain generalization. Overall, these works contribute significantly to the development of robust and generalizable models that are capable of handling distribution shift and uncertainty in real-world applications.\<br>

It is important to note that there is also extensive research focused on developing robust and certifiable approaches for distributional robust learning in the context of domain generalization. For instance, Estimating Generalization under Distribution Shifts via Domain-Invariant Representations [13] aims to better estimate a model's performance under distribution shift, without supervision, by using a set of domain-invariant predictors as a proxy for the unknown, true target labels. Additionally, modeling the Second Player in Distributionally Robust Optimization [19] proposes a relaxation of the KL-constrained inner maximization objective to make the DRO problem more amenable to gradient-based optimization of large-scale generative models. These efforts contribute to the ongoing exploration of novel frameworks and methodologies for addressing the challenges associated with distributional robustness in the domain generalization context.>"
1906,1906," Domain generalization (DG) aims to generalize a model trained on a source domain to a target domain. Existing DG methods can be divided into three categories. The first category is to learn domain-invariant features by minimizing the discrepancy between the source and target domains. For example, [8] and [12] proposed to minimize the Maximum Mean Discrepancy (MMD) between the feature distributions of the two domains. The second category is based on adversarial training. For instance, [11] proposed a domain adversarial loss to encourage the feature distribution of the target domain to be indistinguishable from that of the source domain. The third category is feature normalization methods. These methods aim to normalize the features of the training samples to be similar to those of the test samples, such as [22]; [17]; [14]; [21]; [18]; [19].

Test-time adaptation (TTA) is a closely related problem to DG. TTA aims to improve the model's generalization ability to unseen domains during the test phase. Previous works on TTA mainly focus on two aspects: (1) how to design robust loss functions for TTA; and (2) what to learn during TTA. For TTA, [5] proposed an entropy minimization-based loss function. [4] designed a test-time classifier adjustment module. [6] proposed self-supervised TTA to improve TTA performance.

Recently, several calibration methods have been proposed for deep neural networks. [30] proposed temperature scaling to calibrate the network's predictions. [34] proposed contrastive learning to learn robust representations. [29] proposed decoupling the network into two components. [35] proposed adversarial learning to enhance the robustness of the network.

 and  proposed to learn a robust feature extractor.

 proposed to distill the knowledge from a pre-trained model to a new model.

 designed a domain-agnostic feature extraction module.

 presented a domain generalization method.

 introduced a domain adaptation method. They focused on the domain adaptation problem. They designed a new loss function for domain adaptation.

 also proposed a new network architecture. They trained a classifier on top of the pre-learned features. They also trained the classifier with the test data.

 developed a domain classifier. They evaluated the performance of their method on several benchmark datasets."," **Test time adaptation** (TTA) [1][2][3][4] is proposed to learn the test distribution by leveraging unlabeled test images, which provide hints about distribution information. Test-time training [1] employs a manually designed self-supervised learning task to learn the test distribution, which requires altering the training stage and finetuning all the layers. To mitigate this issue, Tent [5] is proposed by only finetuning the batch normalization layers with an unsupervised entropy minimization loss. Following works try different unsupervised losses to help test time adaptation, such as consistency loss [2], contrastive loss [6] or log-likelihood ratio loss [7]. However, when applied to the test data with a large domain gap, these methods commonly fail due to the inaccurate estimation of statistics and produce small gradients. In contrast, our method can alleviate these issues and also succeed in the few-data scenarios.

**Domain generalization** (DG) has attracted significant attention recently for its ability to generalize to unseen domains by only learning from source domains [8][9]. To achieve the generalization ability, current methods primarily aim to learn invariant features across all domains [10][11][12][13], augment data [14][15][16][17] to learn diverse features or regularize network with training schemes or losses [18][19][20][21]. While these methods only consider the training stage, several methods alter model behaviors according to the test samples for better adaptation to the unseen domains. Instance Normalization [22] and AdaBN  are simple but effective modules that utilize test statistics to perform normalization. Du _et al._[23] generates accurate statistics for each test sample with a trained statistics prediction network. In addition to normalization, Pandey _et al._[24] train a generative network to generate the nearest neighbor for each test sample in the source latent space. ARM [25] applies a meta-learning training scheme to extract batch-specific features during training and test for adaptation. Despite their efforts to adapt the model at the test stage, they require modifying the training stage and cannot be applied to an already trained model. In contrast, we propose our DomainAdaptor, which can be employed in any trained model, making it more practical in the real world.

**Temperature scaling** has been studied in different fields. In knowledge distillation [26][27][28][29], it is used to soften the probability distribution over classes, providing additional class relationship information for the student model. In confidence calibration [30][31][32][33], the temperature is finetuned as a parameter to ensure the predicted class confidence accurately reflects the likelihood of its ground truth correctness. In self-supervised learning [34][35], the temperature is used in contrastive loss to penalize hard negative samples. Differently, in this work, we employ temperature to encourage effective learning by fully exploiting unlabeled test data.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]"," **Test-time adaptation.** Test-Time Training (TTT) [1][2][3][4][5][6][7] aims to adapt a trained model to test-time distribution shift by updating the model parameters before making a prediction. It has been widely studied in the literature [8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44]. In this paper, we focus on the test-test adaptation problem, which is more challenging than the standard TTA, since the test data distribution is unknown during the test.

**Domain Generalization.** Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains by taking advantage of multiple source domains. Existing DG methods can be divided into two categories: domain adaptation methods and domain generalization methods. Domain adaptation methods aim to learn domain-invariant features by minimizing the distance between the feature distributions of source and target domains. For example, [8] proposed to minimize the Maximum Mean Discrepancy (MMD) loss to align the distributions across domains. [10] proposed an entropy regularization term that measures the dependency between the learned features and the class labels. [11] extended the MMD loss to a domain adversarial loss. [12] proposed a learning-to-learn approach that learns an invariant transformation by adversarial learning. [18] proposed the meta-learning method that trains a meta-model to generalize to unseen domains by simulating the domain shift during training. [19] designed an episodic training procedure to train a single deep network by exposing it to domain shift. [20] proposed Feature-Critic networks that learn a domain-specific feature space that performs well on the target domain. [21] proposed solving a jigsaw puzzle to learn the spatial correlation between different domains.


"," Domain adaptation is a long-standing research direction, which aims to tackle the domain shift problem [1]. Domain adaptation has been applied to many downstream tasks, including image classification [2][1], object detection , segmentation , and text recognition [3]. Recently, there are some studies that focus on improving the performance of TTA. Besides robustness verification [4], test-time adaptation strategies can be divided into four major categories: entropy minimization [5][6][7], consistency regularization [8][9][10][11], style augmentation [12][13][14], and normalization [15][16]. For entropy minimization, empirical studies [5] validate that minimizing entropy can make the network adapt to the unseen domain. Consistency regularization is used to encourage the prediction of samples of different domains to match each other. Instead of the architecture, style augmentation is performed in different directions, including augmenting the source samples [15][14][13], augmenting the target samples [16], and fusing the information of target samples and other related domains [12]. Adjusting the normalization can also improve the TTA. MixBN  and AdMixBN [16] shift the statistics of the normalization layers for various tasks. GEM loss  is a general and effective loss function, and it has been used in other tasks [16][17]. For the multi-task learning setting, there are several works [18][19][20] that use MetaLearning for feature generalization. Most previous works aim to improve the performance of TTA by learning domain-invariant features, and our test-time adaptation method is different from them, which aims to improve the accuracy of classification by using the test statistics information in the normalization layer.

Unlike other related studies, our work focuses on the more challenging task of test-time adaptation in the CV domain, which aims to further extend the performance of TTA in unseen domains and push the boundary of this task. Our method first extends the normalization operation in the input feature and then adjusts the statistics of the normalization layer to adaptively fuse the feature statistics of the target sample and the test sample. Besides, we design a GEM loss that extends the Entropy Minimization loss to further boost the test-time adaptation performance. There are some works that are related to our test-time adaptation method. Test-time transformation [21], meta-normalization [22][23], and label-preserving target projections [24] are related to our test-time transformation. Knowledge distillation [25][26][27][28][29] is also used to distill the knowledge of the network to improve the performance of the network in the test data. It is different from our test-time adaptation, because our method is to modify the network during the test phase to improve the",,"<The problem of domain shift and adaptation has been widely studied in the context of machine learning and computer vision. Several approaches have been proposed to address the challenge of adapting a trained model to unseen domains during test time. Test-time adaptation methods have gained attention for their ability to improve the performance of predictive models when training and test data come from different distributions. Test-time Training with Self-Supervision for Generalization under Distribution Shifts[1] proposed a general approach for improving model performance under distribution shifts by turning an unlabeled test sample into a self-supervised learning problem. Similarly, Test time Adaptation through Perturbation Robustness[2] aimed to adapt the model at test-time to handle any domain shift by enforcing consistency of predictions of data sampled in the vicinity of test samples on the image manifold. Domain-agnostic Test-time Adaptation by Prototypical Training with Auxiliary Data[3] tackled the problem of test-time adaptation in scenarios where the test data distribution is unknown and varies abruptly, by performing prototypical training with auxiliary data. Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization[4] presented an algorithm for robustifying a model to unknown distribution shift by adjusting the trained linear classifier with pseudo-prototype representations derived from online unlabeled data augmented by the base classifier trained in the source domains. Tent: Fully Test-Time Adaptation by Entropy Minimization[5] proposed a method for fully test-time adaptation, achieving improved generalization error for image classification and source-free domain adaptation. TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?[6] explored the limitations and enhancements of self-supervised test-time training, providing insights into the conditions where this approach thrives or fails and introducing a test-time feature alignment strategy.>

<Domain Generalization via Invariant Feature Representation[8] investigated domain generalization by proposing Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that aims to learn an invariant transformation by minimizing the dissimilarity across domains. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations[9] established rigorous benchmarks for image classifier robustness and perturbation robustness. Integration of this work provides insights into the robustness of neural networks in real-world scenarios and sheds light on the generating perturbations. Domain Generalization via Entropy Regularization[10] focused on learning discriminative domain-invariant features through entropy regularization, aiming to improve the generalization capabilities of classifiers across multiple source domains. Domain Generalization with Adversarial Feature Learning[11] addressed learning a generalized feature representation for an unseen target domain by utilizing adversarial autoencoders, aligning feature distributions, and introducing adversarial feature learning. Representation via Representations: Domain Generalization via Adversarially Learned Invariant Representations[12] investigated the application of adversarial censoring techniques for learning invariant representations from multiple domains. By leveraging these approaches, this related work provides a comprehensive overview of domain generalization methods and their potential implications on model robustness in varied scenarios.>

<A Style and Semantic Memory Mechanism for Domain Generalization*[13] proposed a probabilistic graphical model for leveraging intra-domain style invariance to improve domain generalization and introduced a ""jury"" mechanism for learning semantic feature commonalities among domains. Domain Generalization with MixStyle[14] introduced MixStyle, a novel approach for synthesizing novel domains implicitly by mixing styles of training instances, aiming to increase the domain diversity of source domains and thereby improving generalizability. Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization[15] addressed generalizable cross-modality segmentation by leveraging dual-normalization models and a style-based selection scheme for improved segmentation performance. These approaches highlight the importance of leveraging style invariance and feature mixing for improving generalization capabilities. Adaptive Risk Minimization: Learning to Adapt to Domain Shift[25] proposed a framework for learning models that adapt at test time to domain shift using unlabeled test points. These works contribute to a deeper understanding of the mechanisms of domain adaptation and enhance the adaptability of models to varied test scenarios.>

<The problem of calibration and temperature scaling has also been extensively researched in the domain of machine learning and uncertainty estimation. Post-hoc Uncertainty Calibration for Domain Drift Scenarios[32] addressed the challenge of over-confident predictions under domain shift and introduced a simple strategy to apply perturbations to samples in the validation set before performing the post-hoc calibration step. Sample-dependent Adaptive Temperature Scaling for Improved Calibration[33] introduced a novel approach to adjust the confidences of predictions on individual inputs by predicting a different temperature value for each input, enabling a finer granularity in balancing confidence and accuracy. Understanding the Behaviour of Contrastive Loss[34] presented an in-depth exploration of the behaviours of unsupervised contrastive loss, shedding light on its role as a hardness-aware loss function and its relationship with temperature τ in controlling the strength of penalties on hard negative samples. Finally, Temperature Schedules for Self-Supervised Contrastive Methods on Long-Tail Data[35] provided insights into the role of the temperature parameter in contrastive loss and proposed a dynamic temperature schedule for improved learning on long-tail datasets. These studies contribute to a better understanding of calibration mechanisms and provide new insights into improving uncertainty estimation in machine learning models.>"
4817,4817," **Semantic Parsing with Zero-Shot Learning.** Zero-shot semantic parsing aims to learn a semantic parser from a small amount of annotated training data. Previous work on zero-shot learning can be divided into two categories: supervised and unsupervised learning. Supervised learning methods require a large amount of labeled data. For supervised learning methods, the annotated data can be obtained from a large-scale knowledge base, e.g., Wikipedia. For example, [18] and [13] use a knowledge base as a supervisory signal to train semantic parsers. Unsupervised semantic parsing methods do not require any labeled data, and can be further divided into three categories. The first category is to train a parser with only a few annotated examples. For instance, [11] and  use a language model as a teacher to guide the training of the student model. The second category is the reverse scenario, where only a small number of labeled examples are available for training. [6]; [1]; [17]; [2]; [12] use the retrieved examples as supervision. The third category is based on the retrieval-based meta-learning framework, where the semantic parser is trained to generate pseudo-labels for unlabeled examples.


Our work is related to the second category. Different from the previous work, we propose a self-training framework to learn from ambiguous supervisions. We first train a classifier for each classifier, and then re-train the classifier with the pseudo-labeled examples generated by the classifiers.

 and  propose a similar framework.



"," Retrieval in Seq2Seq TasksIn semantic parsing, many previous studies ([2]) have propose to employ paraphrase scores to retrieve or rerank MRs, which all follow the order of generating first and then scoring. [5] first generate a set of candidate MRs and choose the realization that best paraphrases the input. [7] propose a set of reranking scorer for neural semantic parsers. [3] combine a retrieval model and a meta-learner to employ the similar datapoints from the training data. [1] construct parallel sentence pairs through retrieval, and conduct unsupervised machine translation models. [8]; [4]; [6] enhance the representations of instances or the robustness of decoder by retrieval. Different from the common generate-then-score framework, the order of our RaAS framework is the reverse of them. We are the first to use retrieval results to obtain supervision for zero-shot semantic parsing.

Low Resource Semantic ParsingMany low resource semantic parsing methods have been proposed to reduce the demand for annotations([10]; [16]; [14]). Many weakly supervised learning are proposed ([18]; [9]; [17]), such as denotation-based learning ([13]; ), iterative searching ([19]). Semi-supervised semantic parsing is also proposed ([23]; ; [12]). One other strategy is to augment data. [21] construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase.  produce pseudo-labeled data. [15] create new ""recombinant"" training examples with SCFG. [11]; [22]; [20] explore the training / decoding methods of PLMs for low-resource semantic parsing. Different from previous work, our framework focuses on obtaining and facilitating supervision signals rather than model design or data synthesization.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"," **Retrieval-based Semantic Parsing.** Retrieval has been widely used in semantic parsing [9]; [13]; [6]; [8]; [3]; [10]; [5]; [2]; [1]; [7]; [4]; [11]; [14]; [12]; [15]; [17]; [20]; [21]; [16]; [18]; [19]; [22]; [23]; [9]. In this paper, we focus on the zero-shot semantic parsing setting, which is more challenging than the supervised setting. Previous work has explored the use of retrieval for semantic parsing. For example, [4] propose a nearest neighbor machine translation model to retrieve the nearest neighbor of an utterance in a retrieval database, and [1] use it to initialize unsupervised machine translation models. [3] propose to use retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment. [10] use retrieved data to learn a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision.


**Zero-shot Semantic parsing.** Zero-shot learning has been a long-standing research topic in the semantic parsing community. [2] propose cross-domain semantic parsing as a domain adaptation problem, where they first train a semantic parser on some source domains and then adapt it to the target domain. [14] propose zero-Shot semantic parsing (ZSS), which is a cross-shot approach to semantic parsing where the model is trained to translate the source domain utterance into the target language. [21] propose an iterative iterative search algorithm to iteratively search for the correct answer to a question. [20] propose prompt tuning, which uses a pre-trained language model as a teacher to guide the model to generate high-quality semantic parses. [15] propose data recombination, in which they induce a high-precision synchronous context-free grammar from the training data, and then train a sequence-to-sequence model with a novel attention-based copying mechanism on datapsoints sampled from this grammar. [22] propose synchronous semantic decoding (SSD), which synchronously generates canonical utterance and logical form for each utterance, and uses paraphrasing to generate the corresponding logical form. [17] propose multi-policy distillation, which first train domain-specific semantic parsers using"," Previous works on Logical Reasoning (LR) focus mainly on the meta-pretraining of language models to incorporate prior knowledge about logical structures through external graph structure representations, abstract logical rules, or meta-linguistic information. [8]; [2] use logical inference keywords to meta-structure logical knowledge graphs for question-answering. LogiQA ([8]) follows a meta-pretraining approach with meta-rules and walk2question to yield a model that can recognize multiple types of deduction in human written reasoning. [3] introduce graphical units (discourse units) to perform structured modeling of text. DAGN ([3]) proposes to use discourse information as a graph, encoded as elementary discourse units (EDUs) and discourse relations. [5] train a logic-based context-extender to extract logical units from unstructured text. Logiformer ([2]) employs meta-graph networks to model logical rules and reasoning units over text. [6] propose using fact-driven logic units to form context for reasoning. [1]; [9]; [7] create synthetic datasets with ground-truth answers to pretrain language models by extracting symbolic logical rules from natural text, along with word representations and graphical representations. [4] propose a meta-path guided contrastive learning method for pretraining, wherein contrastive learning is used to directly align source-to-target reasoning paths. Recently, [11] shows that knowledge required to solve in-distribution problems with full accuracy in a small dataset may not generalize to unseen domains.

Recent works also take a generative approach to produce proofs, which generally are of higher quality than those generated from rule-based synthetic datasets [10]; [12]. [13] train transformer language models for automatic theorem proving over hand-crafted rules.

Previous works on selective-masking pretraining use a subset of training data or task-specific in-domain data to perform masked language modeling for task-specific data. [15] performs masked language modeling on biomedical entity-related data to infuse it into a pretrained model to perform clinical natural language processing. [17] proposes using a task-guided pre-training approach using out-of-domain data to improve the model's performance on the downstream task. In our case, we focus on a specialized masking loss to train the model to detect logical units and solve context-based inference problems.

Recently,  introduces learning reasoning trajectories to connect the proposed lattice models between MC-QA pairs. [14] use the biased and noise-free splits of ReClor ([1]) to perform masked language modeling using both positive and negative examples and the resultant pretrained model is fine-tuned for downstream tasks. We extend this work to perform pretraining using a logically structured subset of Wikipedia.

",,
3087,3087," **Audio-Driven Face Synthesis.** Early works [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30] synthesize talking faces based on the audio input. For example, Song _et al_. [15] propose a few-shot learning framework for synthesizing talking faces conditioned on the input audio. They use a pre-trained generative adversarial network (GAN) to learn a mapping between the audio and the generated face. The generated face is then passed to a decoder to synthesize the final image. However, their method is limited to synthesizing a single identity-specific talking head.

**Audio-to-Visual Diffusion.** Recently, diffusion models [31][32][33][34][35][36][37][38][39][40] have been proposed to model the mapping between audio and visual signals. These models have shown promising results on image generation tasks. In this work, we use a diffusion model as the audio-visual encoder and a GAN as the visual decoder.

 uses an auto-regressive model to map the audio signal to the visual signal. In contrast, our model uses a probabilistic model to learn the mapping from the audio to the image. This allows the model to generate high-quality images.




\begin{tabular}{l c c c l l l c l \l c l c \h l \hline \l l \r l c c \l r c c r c l & c c & c \r r c  & c hl c \cline{2}{2}{3}{4}{5}{6}{7}{8}{9}{10}{11}{12}{13}{14}{15}{16}{17}{18}{19}{20}{21}{22}{23}{20} & c. \l xl c}{2-3}{5-6}{8-9}{12-10-11}{13-14-15}{15-15-16}{18-19}{19-20}{20-21}{20+15-18}{15+15+16}{16+20}{19+15}{18+19}{22-20} \l 3-5}{10-12}{16-15+18-15} \hr}{3-7}{12+10-15\l c_{2-11}}{14-12}}{15,19-15_}{15.25-16+15_{20-16}}{16_{18-12}\l c-13}{13_{15-14}}{18,15-13}}{20,18-16_}{16,15+20-18} \\ c.h-1}{13,15,16-18+15}}{17-15,18,19}{"," Audio-Visual Cross-modal LearningCross-modal representation learning is a long-standing research topic, ranging from speech enhancement [1], speech source separation [2] to synchronization [3][4] and other speech disentanglement [5][6][7]. Among them, EVP [6] used cross-modal supervision to disentangle speech content and emotion from the audio signal with landmark as the intermediate representation. Recently, CMC  discussed how multi-view ""modality"" can be jointly unified to boost intrinsic representation through contrastive learning, rather than predictive (or reconstruction) learning, it also demonstratedthat the more views, the better. Concurrently, MMV [8] introduced different modality embedding graphs for effective cross-modal representations, again through contrastive learning. More recently, HCMoCo [9] extended similar ideas with a hierarchical strategy to learn different levels of representations for human-centric perception tasks. Although impressive results were reported, it is still unclear how it performs on face analysis tasks, not mention to on synthesis tasks. Our work shares a similar spirit but a different purpose, where we first pre-train a non-identity visual representation which then helps learn a lip and non-lip space, the latter further serves as the upper-bound learning target for subsequent audio-to-visual diffusion prior.

Face Reenactment & Talking Head GenerationFace Reenactment is designed to transfer part or full facial motion from a driving source to the target video with good ID-preserved appearance and background. It can be further divided into two categories depending on whether the driving source is from video [10][11][12][13][14][15][16][17][18] or audio [19][6][20][21][22][23]. Among them, audio-driven face reenactment generally aims to edit the mouth regions of the target video in order to match the input audio while leaving other facial attributes mostly unchanged, _i.e._, pose. EVP [6] tries to infer the non-rigid facial expression in addition to lip motion from the audio input. A closely related line of work is the audio-driven talking-head generations [24][25][26][27] where only one target reference face is given, hence, other face attributes including pose, expression, blink and gaze have to be either explicitly given [28][29] or partly inferred through statistical methods [25][30][26][27]. Specifically, Lu _et al_.  employed an auto-regressive model while Min _et al_. [30] leveraged normalized flow prior to predicting a natural-looking pose sequence from the input audio, both showed encouraging results. Compared with them, we aim to infer more diverse facial motions including pose, expression, and even blink and gaze in a holistic manner through an audio-to-visual diffusion prior model.

Diffusion Generative ModelsThe diffusion model [31][32][33], which is a likelihood-based model consisting of cascading denoising autoencoders, has recently shown great success in numerous generative tasks with different modalities including image [34][35][36][37], audio [38], video [39], and motion [40]. To name a few, DDPM [32] explored the diffusion model for unconditional image generation. GLIDE [35] introduced text-conditional diffusion model and showed that classifier free guidance has better performance than CLIP [41] guidance. DALLE-2 [36] modified GLIDE to generate semantically consistent images conditioned on a CLIP image embedding, and proposed a diffusion prior that produces the image embedding given a text caption. MDM [40] utilized a classifier-free diffusion-based generative model for text-to-motion and action-to-motion tasks, allowing motion completion and editing as well.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]"," **Audio-driven face reenactment.** Early works on audio-driven talking head generation [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41]. These methods are mainly based on the idea that the audio-to-visual mapping can be learned from a large amount of unlabeled video data, which is usually obtained by leveraging the cross-modal self-supervised learning [5][8] or the pre-trained GAN [12][9] models. For example, [5] proposed to learn representations of speaker identity and speaker identity without access to manually annotated data. [8] proposed a multimodal versatile network that can ingest multiple modalities and enable downstream tasks in multiple modality. [9] leverages the multi-modality nature of human data to learn discriminative representations for human-centric perception. [12] proposed an approach to learn directions in the latent space of a GAN to control facial pose and expression variations. [10] introduced a method to generate dense 3D facial geometry from the source video and use it to drive the generation of the target video. [11] proposed the Dual-Generator network to generate large-pose face re-enactments. [13] proposed Face2Face to synthesize realistic-looking face images from a monocular video sequence. [14] used a generative adversarial network (GAN) to generate photo-realistic talking head videos.

**One-shot talking head video generation.** Unlike the above methods that require additional driving sources for controlled synthesis in a deterministic manner, we instead sample all holistic lip-irrelevant facial motions to semantically match the input audio while still maintaining both photo realism and overall naturalness. [30] proposed StyleTalker, a one-shot style-based talking head model that can synthesize a video of a person from a single reference image. StyleTalker is trained with a contrastive lip-sync discriminator and a sequential variational autoencoder to learn the latent motion space disentangled from the lip movements. [29] proposed Granularly Controlled Audio-Visual Talking Heads ("," A large body of work has been devoted to various speech enhancement tasks, which can be categorized into audio-video and audio-only cases. Most methods in the former case take both audio and video signals as inputs, and rely on a well-learnt A-V prior to align both modalities by mutual optimization [1][2][3]. In contrast, with the recent success of auto-regressive GANs, a new direction is to leverage only audio information by directly synthesizing the video as an auto-regressive sequence [4][5][6].

**Talking head generation.** As a typical application of the above, existing work can be roughly categorized into two groups. The first group optimizes an image (face) conditional generator directly on the input speech signal. This approach relies on pre-established joint A-V correlation models or A-V transformers [7][8][9][10][11][12]. The second group uses a two-stage approach, where the face image is first pre-trained to reenact from a set of target sequences [13][14][15][16][17][18][19]. More recently, many one-shot methods have also been proposed to generate talking heads from a single given audio, based on either pre-trained A-V models or audio-driven transformers. For the former, Tao _et al._[20] utilize an A-V prior to minimize a speaker embedding divergence loss. In contrast, Zhao _et al._[21] leverage both the A-V prior and audio-to-lips embeddings to generate facial animation. Guo _et al._[22] further resort to the video data of the target speaker to learn the A-V prior and utilize high-resolution audio to boost the visual quality. Similarly, Kellman _et al._[23] proposed an alternative A-V prior based on first-order conditions. For the latter, Jamal _et al._[24] employed a three-stream transformer to progressively generate the image and lip-tracking result in real-time. Prakash _et al._[25] leveraged the Berkeley Depth-Face dataset and dance gestures to disentangle the identity of the synthesized face. Cao _et al._[26] generated the target face by inferring the weight of each latent code in a joint A-V transformer. Zhang _et al._[27] enhanced the audio-based method [25] with a pre-trained StyleGAN . However, no method yet can solve all these desired tasks (e.g., realistic facial expression, lip-capture accuracy, audio-lips synchronization) well with one simple model.

**Prior-free talking head generation.** Some recent studies propose to address the challenge",,"<>
In the domain of audio-driven talking head generation, recent advances have focused on generating realistic facial animations synchronized with input audio. For instance, Visual Speech Enhancement [1] improved the performance of audio-visual neural network methods by training on videos with added background noise and achieved superior results on lipreading datasets. Similarly, Looking into Your Speech [2] proposed a cross-modal affinity network to improve speech separation in videos through global and locally-varying affinities between audio and visual streams. In a related domain, Perfect Match [3] introduced a strategy for learning cross-modal embeddings for audio-to-video synchronization, outperforming existing baselines. Moreover, VocaLiST [4] developed an audio-visual cross-modal transformer-based model that outperformed baseline models in audio-visual synchronization tasks, particularly in lip-voice synchronization for both speech and singing voice videos.

Another relevant area of research is the disentanglement of speaker identity from audio-visual data, as demonstrated by Disentangled Speech Embeddings Using Cross-Modal Self-Supervision [5]. The paper proposed a self-supervised learning approach that employs natural cross-modal synchrony between faces and audio to leverage the representations of linguistic content and speaker identity. Furthermore, methods such as Audio-Driven Emotional Video Portraits [6] and Animating Face using Disentangled Audio Representations [7] focused on disentangling audio sequences into various factors such as phonetic content, emotional tone, and background noise, leading to improved robustness and accuracy in talking head generation.

Moreover, Self-Supervised MultiModal Versatile Networks [8] and Versatile Multi-Modal Pre-Training for Human-Centric Perception [9] presented approaches that learn representations using self-supervision by leveraging multiple modalities present in videos, including vision, audio, and language. These methods excel in learning versatile representations for downstream tasks such as video-text, image, and audio tasks. Furthermore, Depth-Aware Generative Adversarial Network for Talking Head Video Generation [10] highlighted the importance of dense 3D facial geometry for accurate talking head generation and proposed a self-supervised learning method to recover dense 3D facial geometry from face videos without costly 3D annotation data.

In addition, recent work in face reenactment and animation has focused on generating expressive and controllable facial animation from audio and textual inputs. For example, Emotional Video Portraits [6] and Disentangled Audio Representations [7] introduced techniques to synthesize high-quality video portraits with vivid emotional dynamics driven by audio and bridging the gap between deduced emotional landmarks and natural head poses of target videos. Similarly, Animating Face using Disentangled Audio Representations [7] proposed an explicit audio representation learning framework that significantly improved mouth movement accuracy in the presence of noise and emotional variations. Moreover, Dual-Generator Face Reenactment [11] and Finding Directions in GAN's Latent Space for Neural Face Reenactment [12] presented novel methods for large-pose face reenactment and neural face reenactment based on learning directions in GAN's latent space for controlling facial pose and expression. Furthering the capabilities of face reenactment, Face2Face [13] demonstrated real-time facial reenactment using monocular videos, extending the possibilities beyond static images and facili"
727,727," Vision-Language Models.Vision-Language models have been used for zero-shot image recognition [1][2][3][4][5][6][7][8], semantic segmentation [9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49]. In this work, we focus on the task of zero- and open-vocabulary segmentation, which is a more challenging task than image classification.

Robustness to Spurious Correlations.Previous work has shown that models trained on large-scale image datasets can be robust to spurious correlations [50][51][52][53][54][55][56][57][58]. However, these models have not yet been shown to generalize well to other datasets. In this paper, we investigate the robustness of vision-language models to such spurious correlations.

 and  show that contrastive learning improves robustness to such correlations. In particular, they show that a model trained on ImageNet can generalize to ImageNet-pretrained language models and ImageNet pretrained vision models. However, they do not show that this generalization ability can be transferred to other vision and language models. In contrast, we show that such generalization can be achieved with a minimal set of modifications.




"," **Vision-language models for grounding.** Contrastive language image pre-training [1] (CLIP) led to a range of follow up work performing open-vocabulary detection [2][3][4][5][6] or segmentation [7][8][9]. While these methods leverage dense human annotations for training, an alternate line of works [10][11][12][13] attempt to learn alignment between regions of images and language with only image level noisy captions for supervision. Their weak supervision allows better scalability (to more data) leading to learning more generic and transferable representations. In fact, multiple such works [10][11][14][8] perform zero-shot semantic segmentation. However, unlike [10][11] geared to segment a fixed count of foreground objects, our proposed CLIPpy can better segment arbitrary object counts and background classes. In contrast to  using generic image level features, CLIPpy explicitly learns local features during training. Moreover, CLIPpy requires no dense human annotations or task-specific fine-tuning in contrast to [14][8]. We also highlight how [10][11][14] perform grouping independent of language at inference - however CLIPpy can group conditioned on language, capturing variable object boundaries for different language prompts.

Multiple **contemporary works** also explore similar directions as CLIPpy, leveraging pre-trained vision-language models for various grouping tasks under weak supervision (no pixel level annotation) [15][16][17][18][19][20]. Combining self-supervised methods that emerge grouping [21] with CLIP models [1] for cross-modal alignment is explored in [15] gaining notable improvements at object boundaries. A clustering mechanism containing learnable centres similar to [10] is combined with reconstruction and super-pixel alignment losses to achieve grouping in [16]. Learning decoder networks over a frozen CLIP backbone [1] with text to image patch similarity losses are explored in [17][18] resulting in similar grouping behaviour. In contrast to these methods utilizing contrastive vision language training to emerge grouping, recent works [19][20] also showcase how text-to-image generative models (particularly Stable Diffusion [22]) can be leveraged to perform visual grouping.

**Zero-shot semantic segmentation.** A form of top-down grouping, this relatively new task [23][24][25][26][27][28][29][30] attempts to segment unseen classes, usually after a supervised training phase often involving dense annotation based supervision. Following two early representative works [24][25], most later approaches [28][31][32][33][30][34] formulate the task as a pixel-level zero-shot classification problem with a closed set vocabulary. While CLIPpy follows a similar pixel based formulation, in contrast, our method requires no dense human annotations for supervision, no task specific fine-tuning, and is open-vocabulary. Recent work [14][4] also explores region-level classification leveraging pre-trained CLIP models [1], but unlike CLIPpy perform grouping independent of language during inference.

**Unsupervised segmentation**. Analogous to bottom-up grouping, these works perform class-agnostic segmentation within the visual modality with no explicit language alignment [21][35][36][37][38]. This topic has a long, rich history in human visual perception [39] and computer vision [40], and has been explored as means of generalizing to new visual domains [41]. It is this goal that most closely inspires our work. Early efforts group pixels based on known spatially-local affinities [42][43][44], with subsequent methods leading to region proposal networks for object detection  and advances in semantic segmentation [45]. Recent methods employ self-supervision to learn perceptual grouping [46][35] or object-centric groupings [47][48][49][50][51]. Our proposed CLIPpy demonstrates competitive performance, but additionally aligns groups to the language modality explicitly.

**Learning robust visual representations**. For a long time, ImageNet [52] accuracy was believed to provide a reasonable proxy for quality of learned visual representations [53][54]. However, recent work highlights notable deficiencies in such learned representations [55][56][57] including sensitivity to low level textures, failure for domain shifts, and reliance on spurious correlations. These failures inspired a large literature to mitigate learning spurious correlations [58][59][60] by focusing on new optimization techniques. Progress on this issue may address parallel issues in fairness [61]. Resulting methods have largely focused on synthetic data, re-balancing data, and shaping learned embeddings [59]. Nonetheless, theoretical results suggest pessimistic bounds unless additional structure informs the problem (see refs. in [58]). Therein, the structured output predictions of proposed CLIPpy provide another promising solution.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]"," Vision-Language Models.Vision-language pre-training (VLP) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51]. In this work we focus on CLIP-based models [1], which have recently shown promising results in zero-shot image recognition [1] and zero- and few-shot semantic segmentation [25][24]. CLIP models have been shown to learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases [1]. However, these models are limited in their ability to understand where objects reside within an image and group together visually related parts of the imagery. In this paper, we address this limitation by proposing a minimal set of modifications to the CLIP model that allows it to uniquely learn both semantic and spatial information.

Perceptual Grouping in Vision-language models.The idea of perceptual grouping has been explored in the context of zero- or few- shot semantic segmentations [24][24] and open-vocabulary segmentation (OVS) [7][11], where the goal is to segment objects of arbitrary categories without any training data. In these cases, the model is trained on a fixed vocabulary of categories and is thus unable to generalize to unseen categories. In contrast, our work focuses on zero-and-few-shot segmentation, where the model must learn to segment arbitrary categories from a large amount of training data without any category-level annotations. In particular, we show that the learned representations are uniquely robust to spurious correlations in datasets designed to probe the causal behavior of vision models. We also show that our model is able to learn to group together visual objects in an unsupervised manner, and that it is uniquely able to handle spurious correlations. We compare our model to state-of-the-art models in terms of both zero-show segmentation and robustness analyses. We find that our approach outperforms the state of the art in both zero and few shot segmentation. We further show that we are able to uniquely handle spurious correlation"," Articulated object understanding.Previous work has tackled articulated object understanding from various angles such as pose estimation [1][2][3][4][5], grasp planning [2][6], and shape analysis [7][8][9]. Several methods [2][10][11][12][13][4][6] predict motions by performing pixel-wise intersection operations between part masks and 3D shape. These methods require training on the motions of all the possible parts, which is expensive and computationally infeasible. In contrast, our method does not require part motion priors.

Modeling dynamic objects from first-person RGB videos has been studied in many recent works, such as Ditto [14], CARTO [15], and PARIS [16]. PARIS  also tackles the problem of simultaneously recovering the object model and motion using semantic labels, but the focus of their work is to design a self-supervised loss function for jointly learning object model and motion. In contrast, our work aims to predict motion parameters without 3D supervision, and can tackle objects with arbitrary articulation.

**Reconstruction and implicit representation.** Reconstructing 3D objects from images using methods such as classical shape regression and neural surface reconstruction has been an active research area. These methods have been used to tackle tasks such as 3D object reconstruction [17][18][19][20], dynamic articulated objects [21][22][23][24][25][26] and non-rigid objects [27][28]. However, for the problem of articulated object shape and appearance recovery from static multi-view images, existing work [27][28] either requires 3D supervision during training, or cannot generalize to unseen articulation states.

",,"<<Introduction>>
Vision-language models have shown promise in learning generic visual representations with rich semantic information that can be probed with natural language phrases [1]. However, understanding an image is not just about identifying the content within the image, but also about comprehending the spatial relationships and grouping visually related parts of the imagery. In this context, recent work has focused on evaluating how well vision-language models understand the spatial organization of objects within an image and the ability to group visually related parts of the image [2].

<<Connection to Contrastive Vision-Language Models>>
To address the limitations in spatial understanding and object grouping, recent research has emphasized the importance of modifying contemporary vision and language representation learning models based on contrastive losses [3]. These modifications aim to enable the models to uniquely learn both semantic and spatial information, facilitating a more comprehensive understanding of the visual content in images. The proposed minimal set of modifications seeks to enhance the performance of vision-language models in terms of zero-shot image recognition and unsupervised segmentation, thus addressing the limitations in capturing limited object localization information [4].

<<Transfer Learning from Natural Language Supervision>>
Additionally, with the rise of transfer learning from natural language supervision, methods leveraging raw text about images have been proposed as efficient ways to learn state-of-the-art image representations from scratch [5]. By predicting which caption corresponds to a given image, these pre-training tasks have demonstrated the potential to learn image representations that capture both semantic and spatial information. This is crucial in advancing the understanding of where objects reside within an image and grouping visually related parts of the imagery [6].

<<End-to-End Multi-Modal Understanding>>
Furthermore, the proposal of end-to-end modulated detectors conditioned on raw text queries has the potential to enhance the understanding of spatial relationships within images [7]. By detecting objects in an image conditioned on a raw text query, such models aim to capture the long tail of visual concepts expressed in free-form text. These approaches have implications for improving the spatial understanding and object grouping capabilities of vision-language models, addressing the need to capture both semantic and spatial information.

<<Conclusion>>
In summary, recent developments in perceptual grouping in contrastive vision-language models have focused on modifying existing models to uniquely learn both semantic and spatial information. These modifications have the potential to enhance the models' performance in terms of zero-shot image recognition, unsupervised segmentation, and robustness analyses. Furthermore, the integration of transfer learning from natural language supervision and end-to-end modulated detectors conditioned on raw text queries has contributed to advancements in addressing the challenges related to understanding where objects reside within an image and grouping visually related parts of the imagery.

[1] Learning transferable visual models from natural language supervision
[2] Zero-shot detection via vision and language knowledge distillation
[3] MDETR - modulated detection for end-to-end multi-modal understanding
[4] Adapting CLIP for phrase localization without further training
[5] Multi-grained vision language pre-training: aligning texts with visual concepts
[6] Coarse-to-fine vision-language pre-training with fusion in the backbone"
4063,4063," **Heatmap-based methods.** Heatmap is the most popular representation for human pose estimation. It represents the human body as a set of keypoints, where each keypoint is represented by a heatmap. The heatmap can be computed by a convolutional neural network (CNN) [1][2][3][4][5][6][7][8][9] or a regression network (RNN) [10][11][12][13][14][15]. In this paper, we adopt the heatmap representation as the base representation for our model.

**Heatmap Distillation.** The idea of heatmap distillation was first proposed in [16] to transfer the knowledge from a teacher model to a student model. In [16], the teacher model is trained by minimizing the KL-divergence between the logits of the output of the teacher and student models, while the student model is learned by minimizing a logit-based loss function. Recently, [17][18] proposed to distill knowledge from the teacher network to the student network. However, these methods do not consider the difference between the feature spaces of the two models. In this work, we propose a novel method to transfer knowledge between heatmap and regression models, which is inspired by the idea of knowledge distillation in image classification.

 distill the knowledge of heatmaps from the regression model to the feature space of the feature representation of the heatmaps. Different from the previous methods, our method distill both the distribution and confidence of the predicted heatmaps, which can be regarded as the guidance for the training of the student models.




\[\begin{tabular}{l c c c l l l r r r l r c r c l r g c r r c c r g r r \hline \multirow{2}{2}{3}{4}{5}{6}{7}{8}{9}{10}{11}{12}{13}{14}{15}{16} & \multicolumn{3}{8} \multicline{3-9}{8-10}{12-13}{13-14}{14-15}{15-16}{16}{17}{18}{19}{20-20}{21}{22-20} & \(2.5\times\) \(3.5\) \(4.7\) \(5.8\) \(8.7\times\(5.3\)\) \(9.8\(8.9\)\) and \(10.3\(9.7}\) \(11.7_%\) of the total mAP were achieved by the proposed method.

 and  are the two most related works to our method. In, the authors proposed to use heatmaps as the intermediate representation for the teacher models. The difference between our method and their method is that our method uses the predicted confidence maps as guidance, while their method only uses the confidence map as guidance. In addition, they use the confidence maps to train the teacher networks, while we use the predicted probability maps to guide the student networks.

"," Heatmap-based pose estimation [1][2][3][4][5] dominated the area of human pose estimation in terms of performance. Some studies [1][2][5] constructed novel networks to extract better features. While others [6][7][8][9] built upon an optimization perspective trying to mitigate quantization errors. In summary, heatmap-based methods made full use of the spatial information of the feature map and obtain a preferable performance. However, efficiency is still a certain drawback of heatmap-based methods.

For regression-based methods, Deeppose  is firstly proposed to regress the joint coordinates directly. CenterNet [10] and DirectPose [11] are proposed to accomplish multi-person human pose estimation in a _one-stage_ object detection framework, which directly regresses the joint coordinates instead of the bounding box. SPM [12] introduced the root joints to indicate different person instances and hierarchical rooted human body joints representations to better predict long-range displacements for some joints. Recently, RLE [13] introduced a flow model to capture the underlying output distribution and gets a satisfying performance. Although these methods have made great efforts to find the implicit relationship of keypoints, their performance improvement is still insufficient due to the lack of explicit guidance of heatmaps.

Transformer is proposed in  and achieves great success in Natural Language Processing (NLP). Recent studies in vision tasks used Transformer as an alternative backbone to CNN for its ability to capture global dependencies. In the area of 2D human pose estimation, many efforts [14][1][15][4][5] have been done to incorporate the Transformers. TFPose [15] first introduced Transformer to the pose estimation framework in a regression-based manner. PRTR [14] proposed a two-stage and end-to-end regression-based framework using cascade Transformers and achieves SOTA performance in regression-based methods. TransPose [5] and TokenPose [1] introduced Transformer for heatmap-based human pose estimation achieving comparable performance while being more lightweight. In our work, we introduce the transformer module to assist in finding potential relationships between keypoints.

Knowledge Distillation (KD) is formally proposed in [16], which aims to transfer the teacher's learned knowledge to the student model. In 2D human pose estimation, FPD [17] first used knowledge distillation classically based on the Hourglass network. OKDHP [18] introduced an online pose distillation approach that distills the pose structure knowledge in a one-stage manner. ViTPose [4] also implemented a large-to-small model knowledge distillation to prove its knowledge transferability. However, all previous distillation works on human pose estimation ignore the knowledge transferring between heatmap-based and regression-based methods. In this work, for the first time, we propose a heatmap-to-regression distillation framework to take benefits from both schemes.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"," **Human Pose Estimation.** Human pose estimation has been extensively studied in the past few years. Most of the existing methods are based on the two-stage pipeline, where the first stage is to generate a set of keypoints, and the second stage predicts the pose of each keypoint. The keypoint-based methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15] have been dominating the field of human pose estimation for a long time. For example, Chen _et al._[7] proposed a novel Non-Maximum Suppression (NMS) based method to predict the keypoints of each person in a single shot, which is the first end-to-end framework for human pose regression. CenterNet [10] and CenterNet++ [9] proposed to use the center point of the bounding box as the keypoint of the object, and then regress the pose for each object. However, these methods are still based on a two-step pipeline, which requires additional post-processing steps for keypoint prediction.

**Heatmap-based Pose Regression.** Heatmap-Based Pose Estimators [16][17][18] are more efficient than the regression-Based Methods [7][12] in terms of speed, but suffer from the misalignment issue of heatmaps. To address this issue, some methods [13][15][14] have proposed to directly regress the heatmaps from the input images. For instance, Liu _et.al_[14] proposed Cascade Transformers [14] to regress heatmaps directly from the feature maps of the input image, which achieves the state-of-the-art performance in human body pose estimation. In this paper, we propose a novel framework DistilPose, which bridges the gap between heatmap- and regression-based pose estimation methods by introducing tokenization and Simulated Heatmaps.

"," **3D Hand Pose Estimation.** 3D hand pose estimation has been well-developed from reconstruction of both static and dynamic hand 3D models. Based on hand scans and segmentations, Li _et al_. [1] propose a learning-based method for training 3D hand models. The 2D/3D hand pose detection methods [2][3][4] mainly generate the hand keypoint. However, 3D hand reconstruction by scanning/modeling still require human efforts. Due to the asymmetry and complex interactions between hands, it is difficult to capture the interactions from static images. So the existing 3D hand pose estimation methods mainly focus on the monocular hand model reconstruction. Yu _et al_. [5] propose a graph convolution network (GCN) to reconstruct a full 3D mesh of hand from single RGB images. A strong interaction among hand-keypoints is also crucial for reconstructing the hand pose from RGB images. Zhou _et al_. [6][7] solve the occlusion problems and interactions between hands by attending heatmaps to the right or left hand. Most recent methods for single-hand 3D pose reconstruction, use the 2D/3D pose keypoints jointly. Yang _et al_. [8] and Zhang _et al_. [9] propose a two-stage framework to predict the 3D pose of both hands. They first predict hand joints, and then regresses 3D keypoints based on joints. These methods only focus on single hand pose estimation and do not consider two-hand interactions. Motion estimation by 3D hand pose helps the pose prediction and shape synthesis from motion capture system [10][11][12]. In addition to these methods, Körbes _et al_. [13] propose a deep prior to generate 3D hand model from videos.

**3D Motion Prediction.** Previous works [14][15][16][17][18] for human motion prediction mostly focus on 3D pose prediction, and achieve remarkable performances. Motion prediction for two-hand gestures could be viewed as an extension of human motion prediction. The key points for human motion prediction are joints [14][16][15][17][18], skeletons [14][16], and trajectory [17][18]. The success of 3D pose prediction could lead to great performances for 3D gesture synthesis from human motion.

**Memory-Based Models.** Memory-based models [19][20][21][22] have emerged to boost the performance of video generation tasks. The representations of memory-augmented models store the previous information to help the prediction of the current frame. Traditional methods [19] store the memory based on the spatial locations to support the understanding of the long-range dependencies. Recently",,"<Human pose estimation is a fundamental task in computer vision, and recent advancements have seen the development of various methods to improve the accuracy and efficiency of pose estimation models. TokenPose [1] introduced a novel approach based on Token representation for human Pose estimation, explicitly embedding each keypoint as a token to simultaneously learn constraint relationships and appearance cues from images. This method demonstrated lightweight models achieving competitive results. ViTPose [4], on the other hand, explored the potential of plain vision transformers for pose estimation tasks, showcasing the surprising capabilities of simple structures such as vision transformers. It emphasized scalability, flexibility in training paradigm, and transferability of knowledge between models. These advancements in representation learning and transformer-based models have laid the groundwork for improving human pose estimation.

Another significant line of work focuses on understanding and improving the encoding and visualization of human pose. TransPose [5] proposed a model that introduced Transformer for human pose estimation, capturing long-range relationships efficiently and revealing the dependencies of predicted keypoints. Likewise, Unbiased Data Processing (UDP) [6] brought attention to the biased data processing in pose estimation, presenting a principled way to tackle the issue, thereby significantly boosting the performance of existing methods. The work by Objects as Points [10] proposed a different approach to object detection, modeling an object as a single point—the center of its bounding box. This innovation showcases a shift in the paradigm of pose estimation, suggesting alternative representations and methods for human pose detection.

Furthermore, research has been particularly focused on bridging the gap between heatmap-based and regression-based methods, similar to the goal of the target paper. DistilPose [Target Paper] has taken a step in this direction by proposing a novel framework that maximizes the transfer of knowledge from a heatmap-based teacher model to a regression-based student model, showcasing significant performance improvements while maintaining efficiency. This aligns with the broader trend in the field of human pose estimation, where researchers are exploring methods to leverage the strengths of different approaches to achieve more accurate and efficient pose estimation models.>

<In addition to advancing the model structures, recent studies have delved into improving the optimization and learning process of human pose estimation. For instance, Distribution-Aware Coordinate Representation (DARK) [9] investigated the design limitations of standard coordinate encoding and decoding methods, proposing a principled distribution-aware representation of keypoints that significantly boosted the performance of existing human pose estimation models on benchmark datasets. Similarly, The Devil Is in the Details [6] focused on addressing biased data processing in pose estimation, introducing a principled way of processing data to improve the performance of existing methods. By tackling the biases in data processing and refining the coordinate representations, these works have contributed to enhancing the accuracy and robustness of human pose estimation algorithms.

Efficient pose estimation methods have garnered significant attention as well. Fast Human Pose Estimation (FPD) [17] and Online Knowledge Distillation for Efficient Pose Estimation (OKDHP) [18] highlighted the practical importance of model cost-effectiveness and introduced strategies to train lightweight pose estimators by distilling knowledge from powerful teacher models. These pioneering methods directly addressed the efficiency problem by proposing learning strategies to train lightweight models with low computational cost, providing promising alternatives for practical use.>

<Finally, developments in end-to-end training paradigms and structured representations have also influenced the advancement of human pose estimation. DirectPose [11] proposed the first direct end-to-end multi-person pose estimation framework, eliminating the need for heuristic grouping in bottom-up methods or bounding box detection in top-down ones. Furthermore, Single-Stage Multi-Person Pose Machines (SPM) [12] introduced a novel structured pose representation that unified person instance and body joint position representations, allowing for more efficient multi-person pose estimation. By streamlining the pipeline and improving efficiency, these methods have made significant strides in enabling more effective and practical multi-person pose estimation approaches.>

<In conclusion, the related work highlights the various avenues of research in human pose estimation, including novel representation learning methods, optimization improvements, efficient model architectures, and advancements in end-to-end training paradigms. These efforts collectively contribute to the broader goal of enhancing the accuracy, efficiency, and practical applicability of human pose estimation algorithms, aligning with the motive of the target paper to bridge the gaps between heatmap-based and regression-based methods for more effective pose estimation.>"
2858,2858," **Neural Scene Representations.** Neural scene representations [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72][74][75][76]. Our work is most related to DeepVoxels [4] and Neural Volumes [4], which represent dynamic scenes as a set of voxel grids. Our work differs from these methods in that we represent a scene by a grid of learned features, which allows us to model the geometry of the scene in a more efficient way.

**Spatio-temporal representations.** Our work builds on top of the NeRF framework [1], which represents a scene as a function of a sequence of points in space and time. The core idea of NeRF is to learn a function that maps each point in space to a coordinate in time, which is then decoded into a volume representation. This approach is computationally expensive, requiring a large number of MLP evaluations for each scene. To reduce the number of evaluations, several works have been proposed to reduce the computation cost. For example, NeRFVoxel [4][4] divides a scene into grid-like voxels and computes features for each grid cell using a tensor representation. However, this approach is limited to static scenes and does not generalize well to dynamic scenes. To address this limitation, we propose a novel representation called HexPlane, which represents the scene as six planes of learned feature vectors, each of which is a 4D volume. This representation can be efficiently computed by fusing features extracted from different planes of the grid. We show that our method can generalize better than the grid-based methods and outperform them on novel view synthesis tasks.

 is a concurrent work to our work, which also uses a grid representation to represent scenes. Our method differs from theirs in several ways. First, we represent the scene by six planes instead of a grid, which makes our method more efficient and general. Second, our method does not require any additional post-processing, such as MLP evaluation, and it can be trained in an end-to-end fashion. Third, our approach does not suffer from the limitations of grid representations, which are addressed in Sec. 4.1.

 represents scenes as sets of points, while we represent them as a grid. In contrast, they represent scenes as continuous functions.

 uses a continuous function to represent 3D scenes. This function is defined as a 3D grid of points. Our representation is"," **Neural Scene Representations.** Using neural networks to implicitly represent 3D scenes [1][2][3][4][5] has achieved exciting progress recently. NeRF  and its variants [6][7][8][9][10][11] show impressive results on novel view synthesis [12][5][13][14] and many other applications including 3D reconstruction [15][11][16][17][18], semantic segmentation [19][20][18], generative model [21][22][23][24][25], and 3D content creation [26][27][28][29][30][31][32].

Implicit neural representations exhibit remarkable rendering quality, but they suffer from slow rendering speeds due to the numerous costly MLP evaluations required for each pixel. To address this challenge, many recent papers propose _hybrid_ representations that combine a fast explicit scene representation with learnable neural network components, providing significant speedups over purely implicit methods. Various explicit representations have been investigated, including sparse voxels [33][34][35][36], low-rank components [21][37][38], point clouds [39][39][40][35][41] and others [42][43][44][45]. However, these approaches assume static 3D scenes, leaving explicit representations for dynamic scenes unexplored. This paper provides an explicit model for dynamic scenes, substantially accelerating prior methods that rely on fully implicit methods.

**Neural Rendering for Dynamic Scenes.** Representing dynamic scenes by neural radiance fields is an essential extension of NeRF, enabling numerous real-world applications [46][47][48][49][25]. One line of research represents dynamic scenes by extending NeRF with an additional time dimension (T-NeRF) or additional latent code [50][51][52][50]. Despite the ability to represent general typology changes, they suffer from a severely under-constrained problem, requiring additional supervision like depths, optical flows or dense observations for decent results. Another line of research employs individual MLPs to represent a deformation field and a canonical field [53][54][55][56][57], where the canonical field depicts a static scene, and the deformation field learns coordinate maps to the canonical space over time. We propose a simple yet elegant solution for dynamic scene representation using six feature planes, making minimal assumptions about the underlying scene.

Recently, _MAV3D_[58] adopted our design for text-to-4D dynamic scene generation, demonstrating an exciting direction for dynamic scenes beyond reconstruction.

**Accelerating NeRFs.** Many works have been proposed to accelerate NeRF at diverse stages. Some methods improve _inference_ speeds of trained NeRFs by optimizing the computation [59][60][61][62]. Others reduce the _training_ times by learning a generalizable model [63][5][12][64]. Recently, rendering speeds during _both stages_ are substantially reduced by using explicit-implicit representations [21][37][36][65][43][66]. In line with this idea, we propose an explicit representation for dynamic fields to accelerate dynamic NeRFs.

Very recently, several concurrent works have aimed to accelerate dynamic NeRFs. [67][68][69][70][71] use _time-aware_ MLPs to regress spacetime points' colors or deformations from canonical spaces. However, they remain partially implicit for dynamic fields, as they rely on MLPs with time input to obtain spacetime features. In contrast, our paper proposes a more elegant and efficient explicit representation for dynamic fields without using time-aware MLPs. Like [72], _NeRFlayer_[73] uses a highly compact 3D grid at each time step for 4D field representation, which results in substantial memory costs for lengthy videos.

_Tensor4D_[74] shares a similar idea as ours, which represents dynamic scenes with 9 planes and multiple MLPs. _D-TensoRF_[75] regards dynamic fields as 5D tensors and applies CP/MM decomposition on them for compact representation. Our paper is most closely related to _\(K\)-Planes_[76], which also employs six feature planes for representation.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76]"," **Neural Scene Representations.** Neural implicit representations have been widely used in 3D reconstruction [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66][67][68][69][70][71][72][73][74][75].

NeRF [1] is the first work to use neural implicit representations for 3D scene reconstruction. It uses MLPs to represent a scene as a continuous volumetric function, parameterized by MLPs that provide the volume density and view-dependent emitted radiance at each location. NeRF requires training MLPs for each scene, which takes days or weeks to converge. To reduce the training time, several works have focused on improving the efficiency of NeRF. For example, [36] proposed to use a sparse voxel grid representation [35] to represent the scene, and [37] proposed a tensor-based representation. [44] proposed variable bitrate NeRF (VNeRF) to reduce the number of MLP evaluations required for training. [45] proposed an efficient way to compute features for NeRF, which uses a multi-resolution feature grid. [46] proposed H-NeRF to model dynamic scenes. [47] proposed IBRNeRF, a method to learn NeRF from monocular videos. [48] proposed NeRF-BR [47], a method for learning NeRF for dynamic human body modeling. [50] proposed DeVRF to learn a spatiotemporal neural irradiance field for dynamic scenes from a monocular video. [49] proposed A-NVR to learn human shape, appearance, and pose from videos.

All these methods use MLPs as the scene representation, which makes them unsuitable for modeling dynamic scenes since MLPs need to be evaluated for each frame. [6] proposed the NeRF++ method, which improves the training speed by using MLPs in a multi"," **Semantic Segmentation.** Semantic segmentation is a computer vision problem that aims to label every pixel in an image with a class name. Existing methods can be divided into two categories: 1) CNN-based methods [1][2][3][4] and 2) Transformer-based methods [5][6][7][8][9][10][11][12][13][14][15]. CNN-based methods use U-net [2], DeepLab [1], DeepLab-V3 , and DeepLab-V3+ [4] as the backbone. Transformer-based methods [6][7][16] usually use the SegFormer [5] backbone. Although Transformers achieve good performance on small-scale datasets, they are prone to overfit. The current work uses a combination of CNN and Transformer. We are inspired by [11], which propose to design a Transformer-based segmentation model using superpixel tiles [17][18][19]. In our work, we propose a group-based superpixel (GBSP) mask classification module that further improves the group-level semantics of superpixel tiles.

**Domain Generalization.** Domain generalization (DG) aims to learn a generalized representation that transfers well to a target domain without access to training samples in that domain. The effectiveness of DG is measured on public benchmarks, including CIFAR100 [20], Office31 , KITTI , and the RGB-D pixel dataset . Some studies [21][22][23][24][25] have shown that the input domain and the target domain distributions are unbalanced, making the trained models not robust to out-of-domain data. Another line of work [26][27][28][29][30][31] shows that the introduction of domain shifts can boost robustness by using diverse models or ensemble models. More recently, data augmentation methods [22][32][33] show good results on sim-to-real DG tasks. The main difference between these approaches and our work is that the target domain of our sim-to-real task is unknown. Instead, we try to discover it and improve our model in an end-to-end way. Our GBFormer solves cross-domain generalization by using domain-level features and class-level features.

**Multi-Domain Segmentation.** Domain generalization is different from multi-domain segmentation (MDS) in which the domain of each pixel is accessible during training and evaluation. MDS is aimed to learn a shared model to segment pixels from different domains. The most related work to ours is Whitening methods [34][35][36], which transform the input of the model from RGB to BGR, and whiten the color of each pixel. In this way, pixels with",,"<>
Dynamic 3D scene representation has been an active area of research in the field of 3D vision. Prior works such as Occupancy Networks [1], Neural 3D Reconstruction in the Wild [2], and Neural Scene Representation Networks [3] have explored the use of implicit representations for 3D scenes. These approaches have focused on learning-based methods for 3D reconstruction and representation. However, a common challenge faced by these methods is the trade-off between computational efficiency and the ability to represent high-resolution geometry of arbitrary topology. While existing approaches rely on implicit representations and neural networks, they require either dense inputs or lengthy per-scene training procedures.

To address the challenges of dynamic 3D scene representation, the proposed HexPlane method provides a novel approach for explicitly representing dynamic 3D scenes [Target Paper]. HexPlane introduces the concept of six planes of learned features to represent dynamic 3D scenes. By explicitly representing spacetime using learned features, HexPlane offers an elegant and efficient solution for modeling and rendering dynamic 3D scenes. By demonstrating the fusion of vectors extracted from each plane, HexPlane provides a highly efficient means of computing features for points in spacetime.

Furthermore, HexPlane is shown to pair effectively with a tiny MLP to regress output colors and is trained via volume rendering, resulting in impressive results for novel view synthesis on dynamic scenes. This approach matches the image quality of prior work while drastically reducing training time by more than \(100\times\). Ablation studies confirm the robustness of the HexPlane design, showing its effectiveness across different feature fusion mechanisms, coordinate systems, and decoding mechanisms.

The simplicity and effectiveness of HexPlane make it a promising solution for representing 4D volumes. Given its efficiency and impressive results for novel view synthesis on dynamic scenes, HexPlane holds potential for contributing broadly to the modeling of spacetime for dynamic 3D scenes. This work aligns with the recent trends in the field, as it seeks to address the trade-off between computational efficiency and representation quality in dynamic 3D scene modeling.

<>
The proposed HexPlane method makes significant contributions to the field of dynamic 3D scene representation. While previous works such as DeepVoxels [4] have addressed the lack of 3D understanding in generative neural networks, and IBRNet [5] has focused on synthesizing novel views of complex scenes by interpolating a sparse set of nearby views, none of these specifically address the challenge of explicitly representing dynamic 3D scenes. HexPlane's explicit representation by six planes of learned features sets it apart from existing approaches, as it specifically targets the efficient representation of dynamic scenes.

Additionally, compared to existing methods such as NeRF [6] and Block-NeRF [8], which focus on high-quality novel view synthesis and large-scale scene representation, HexPlane's emphasis on explicitly representing dynamic scenes using learned features provides a unique and efficient solution. The simplicity and effectiveness of HexPlane in representing 4D volumes offer a novel perspective on addressing the challenges associated with dynamic 3D scene representation. This work provides a valuable contribution to the field by presenting an innovative and efficient approach to modeling spacetime for dynamic 3D scenes, which has potential applications across various domains in 3D vision and computer graphics.

<>
The proposed HexPlane method presents a promising solution for addressing the challenges of dynamic 3D scene representation. In contrast to traditional approaches that rely on implicit representations and require dense inputs, HexPlane's explicit representation by six planes of learned features offers a more efficient and elegant solution for modeling and re-rendering dynamic 3D scenes. By computing features for points in spacetime through the fusion of vectors extracted from each plane, HexPlane demonstrates efficiency and effectiveness in representing dynamic 3D scenes.

The combination of HexPlane with a tiny MLP for regressing output colors and training via volume rendering results in impressive performance for novel view synthesis on dynamic scenes. This approach not only matches the image quality of prior work but also significantly reduces training time, showcasing the practical relevance of the proposed HexPlane method. The robustness of HexPlane design across different feature fusion mechanisms, coordinate systems, and decoding mechanisms, as confirmed by ablation studies, further highlights its potential as a simple and effective solution for representing 4D volumes.

In summary, the HexPlane method represents a significant contribution to the field of 3D vision by offering an efficient and elegant approach to explicitly representing dynamic 3D scenes. Its potential impact spans diverse applications, ranging from computer graphics to virtual reality and beyond, where the representation of dynamic 3D scenes is crucial for achieving realistic and immersive experiences.

<>
The proposed HexPlane method presents a novel and impactful approach to dynamic 3D scene representation. Existing methods such as Neural Radiance Fields (NeRF) and its variants have shown impressive results in novel view synthesis but are limited in dynamic scene representation due to lengthy training times and computational inefficiency. HexPlane addresses these challenges by introducing an explicit representation for dynamic 3D scenes with six planes of learned features, offering an efficient and elegant solution for modeling and re-rendering dynamic 3D scenes.

By fusing vectors extracted from each plane, HexPlane efficiently computes features for points in spacetime, demonstrating its effectiveness in representing dynamic scenes. The pairing of HexPlane with a tiny MLP for output color regression and training via volume rendering significantly reduces training time while maintaining high-quality novel view synthesis on dynamic scenes. The robustness of HexPlane across various feature fusion mechanisms, coordinate systems, and decoding mechanisms, as confirmed by extensive ablation studies, further solidifies its significance in dynamic 3D scene representation.

Overall, HexPlane's simplicity and effectiveness in representing dynamic 3D scenes while reducing training time by more than \(100\times\) make it a valuable contribution to the field. Its potential to broadly impact the modeling of spacetime for dynamic 3D scenes suggests its relevance and applicability in various industries and research areas, such as computer graphics, virtual reality, and visual effects.

<>
The proposed HexPlane method represents a significant advancement in the field of dynamic 3D scene representation and novel view synthesis. The introduction of six planes of learned features for explicit representation provides an efficient and elegant solution for representing dynamic 3D scenes compared to traditional implicit representations. By fusing vectors extracted from each plane, HexPlane computationally efficiently computes features for points in spacetime, demonstrating its effectiveness in representing dynamic scenes.

Additionally, the pairing of HexPlane with a tiny MLP for output color regression and training via volume rendering results in a drastic reduction in training time while maintaining impressive results for novel view synthesis on dynamic scenes. The robustness of the HexPlane design to various feature fusion mechanisms, coordinate systems, and decoding mechanisms, as demonstrated by extensive ablation studies, further strengthens its contribution to the field of dynamic 3D scene representation and novel view synthesis.

In conclusion, the HexPlane method offers a simple and effective solution to the representation of 4D volumes, showcasing its potential to significantly impact the modeling of spacetime for dynamic 3D scenes. Its efficient representation of dynamic scenes with reduced training time holds promise for broad applications in computer graphics, virtual reality, and other fields requiring high-fidelity representation and synthesis of dynamic 3D scenes.

<>"
410,410," **Animal Behavior Datasets.** There is a large body of work on human action datasets [1][2][3][4][5][6][7][8][9][10][11]. However, these datasets are typically small in size and focus on a single species. In contrast, our dataset is collected from a variety of biology experiments and focuses on multi-species interactions.

**Behavior datasets in biology.** A number of large-scale datasets have been proposed for studying social behavior in animals. For example, [12][13][14][15][16] have been introduced to study social behaviors in mice, flies, and flies. However, none of these datasets has been used to evaluate the performance of learned behavior representation learning methods. In this work, we introduce a multi-species multi-task benchmark to evaluate learned behavior representations across a wide range of animal species. We also introduce a suite of downstream tasks to assess the quality of learned behavioral representations. Our dataset is also the largest to date to date in terms of both size and diversity of the animal species and the amount of data collected. We provide a detailed comparison of our dataset with existing datasets in Table 1.
**Self-Supervised Learning.** Self-supervised learning has recently become a popular approach for learning video and trajectory representations [17][18][19][20][21][22][23]. In particular, [17] proposed a contrastive learning framework to learn representations by maximizing the mutual information between different views of the same video. [19] proposed an encoder-decoder architecture to learn video representations by minimizing the distance between the input and output features of the encoder and the decoder. [20] proposed to learn visual representations by predicting the next frame given the current frame and the previous frame. [21] introduced an unsupervised approach to learn spatio-temporal embeddings by minimizing a Euclidean distance between two consecutive frames. [22] introduced a rotation-invariant representation learning method to learn spatial and temporal representations of videos. Our work differs from these prior works in several important ways. First, we focus on the multi-agent nature of our experiments, which allows us to collect a much larger amount of diverse and diverse data. Second, we provide a more comprehensive evaluation of learned representations, including pose tracking, social behavior analysis, and other downstream tasks. Third, we include a suiteof real-life downstream tasks, such as visual odometry, behavioral cloning, and behavioral cloning.

 introduced a dataset for learning visual representations of human and animal behavior. However they focus on human actions and do not provide a benchmark for learning behavior representations for other species. Our proposed dataset is much larger in scale and diversity than their dataset, and provides a more diverse set of tasks.

 collected a large number of videos of social interactions in a number of different species and environments. Their dataset consists of video sequences of people interacting with different objects. Their goal is to study human-human interactions. In comparison, we collect videos of multiple species of interacting insects and flies interacting with other insects.

 is a dataset of"," **Related Animal Datasets.** The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments. There are several existing datasets for studying animal social behavior, including CRIM13 ([1]), Fly vs. Fly (), and CalMS21 (). These datasets contain video or pose data from interacting animals, as well as human-annotated behavior labels (Table 1); they all focus on a single species and setting. AnimalKingdom ([2]) is another recent animal behavior dataset that includes social and nonsocial behavior from multiple species, but is focused on human annotation-based action recognition only. Our dataset is unique in that it defines a range of downstream tasks for each organism; these tasks are motivated by scientific experiments, with the goal of to driving scientific discovery in biology.

**Related Human Datasets.** While animal video datasets remain comparatively rate, there are many video datasets designed for work in human action recognition. Human datasets typically have very different visual characteristics from animal datasets. Most notably, many human datasets that are used to benchmark self-supervised video representation learning, such as Kinetics ([5]), UCF101 ([3]) and HMDB51 ([4]), contain'spatially heavy' visual information that informs downstream action classification- that is, different actions have different backgrounds. Because of these differences in the visual appearance, agents' actions can be partly distinguished by these visual features alone, without models having to learn any temporal features of the agents' behavior. In contrast, our animal videos are all acquired against a stationary, neutral background, forcing models to use the temporal structure of the data to distinguish between actions.

**Related Problems in Multi-Agent Behavior.** While our dataset is composed of multi-agent data from biology, there are also multi-agent behavior datasets from other domains, such as from autonomous driving ([7]; [8]), sports analytics ([11]; [6]), and video games ([10]; [9]). These datasets often focus on forecasting, motion planning, and reinforcement learning, whereas our dataset is used for tasks from scientific applications, such as distinguishing animal strains via observed behaviors.

**Work in Animal Behavior Analysis.** In biology and neuroscience, computational models of behavior have the potential to significantly reduce human data annotation efforts, and to provide more detailed descriptions of the behavior in question (; ). Automated characterizations of animal behavior have been used to study the relationship between neural activity and behavior (), to characterize behavioral differences between species and between different strains within a species ([12]), and to quantify the effect of functional or pharmacological perturbations (; ). The input to these models may be video ([13]) or trajectory data (; [14]).

Supervised behavior models have been trained to identify human-defined behaviors-of-interest ([16]; [14]; ; ), often using frame-by-frame behavior annotations from domain experts. Another body of work discovers behaviors without human annotations, using unsupervised and self-supervised methods ([15]; ; ; ; ) that learn the latent structure of behavioral data. The learned representation may be continuous (), or discrete, such as when discovering behavior motifs ([15]; ; ). There currently does not exist a unified behavioral representation learning dataset that can compare these models across a broad range of behavior analysis settings. Here, we propose  for evaluating the performance of these representation learning methods.

**Work in Representation Learning.** Representation learning for visual ([21]; ; [17]; [18]; [20]) and trajectory data (; [19]) has been applied to a variety of tasks, such as for image classification (), speech recognition ([17]), and behavior classification (). In these works, many different unsupervised / self-supervised methods have been developed, employing various pretext tasks to pre-train a model, such as classifying image rotations ([21]), predicting future observations ([17]), contrastive learning with image augmentations (), and decoding programmatic attributes (). The quality of learned representations is often evaluated on downstream tasks.

_Behavioral Representation Learning._ For behavior analysis, applications of representation learning include discovering behavior motifs ([15]; ; ; ), identifying internal states (), and improving sample-efficiency of supervised classifiers (). These works use methods such as variational autoencoders (), autoregressive hidden Markov models (), and Uniform Manifold Approximation and Projection (UMAP) ([22]) to characterize the latent structure of behavior.

Notably, many groups have proposed methods for unsupervised behavior discovery ([15]; [23]; ; ; Hsu & ; ). These works use different methods to model the temporal structure of behavior, including wavelet transforms ([15]), autoregressive hidden Markov models (), and recurrent NNs (), as well as different methods for segmenting behavior, such as Gaussian mixture models (Hsu & ), k-means clustering (), and watershed transforms ([15]). Our goal is to develop a standardized dataset for evaluating these methods on a common set of behavior analysis tasks.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"," **Animal Behavior Datasets.** A variety of animal behavior datasets have been proposed in recent years, ranging from single-species [1][2][3][4][5][6] to multi-species datasets [7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23]. These datasets are typically collected in controlled environments, such as indoor environments [4][3] and outdoor environments [5][8], and are collected from human-human interactions [3][5], soccer matches [6], and natural language processing (NLP) tasks [18][20]. In contrast, our dataset is collected from a variety of biology experiments, and includes multiple species of interacting mice, insects, and flies.

**Multi-Agent Behavior Benchmarks.** Multi-agent behavior datasets are often collected from real-life scenarios, where multiple agents are interacting with each other in the same environment. For example, the UCF101 dataset [3] is a multi-agent dataset of 101 action classes, which was collected from grid-world grid environments. The Kinetics dataset [5] contains 400 video clips from YouTube videos, and the Kinetics Human Action Video Dataset (KVID) [4] contains video clips of human action classes from YouTube. The Animal Kingdom dataset [2] is collected in the wild, and contains 50 hours of annotated videos for the video grounding task, 30K video sequences for the fine-grained multi-label action recognition task, and 33KFV pose estimation task. The StarCraft Multi-Agent Challenge [10] is an open-world multi-agents dataset, where each unit is controlled by an independent agent that must act based on local observations. The Argoverse dataset [7] contains 360 degree images from 7 cameras with overlapping fields of view, forward-facing stereo imagery, 3D point clouds from long range LiDAR, and 6-DoF pose data. The Waymo Open Datastudio [8] dataset is a large-scale, high-quality, diverse dataset of urban and suburban scenes captured by a fleet of autonomous vehicles in the US. The dataset contains 15x50 scenes, 15x20 scenes, and 15x40 scenes. The MineRL dataset [9] consists of over 60 million automatically annotated state-action pairs across multiple related tasks in a dynamic 3D, open world environment. The Minecraft Multi-Task"," In [1], the normalized noise scale, \(\eta^{*}\), is identified as a desirable hyperparameter for training deep networks with width \(\ge\) 64. We find that \(\eta^{*}\) is well predicted by the scaling \(\eta^{*} \propto w_{1}\), provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. This result helps explain the long-standing and popular interpretation of wide neural networks as training with an overly-large learning rate [1][2]. However, we also find that using very deep networks or large input layer widths may not be beneficial in the sense of \(\eta^{*}\) - Figure 2C. For example, on the CIFAR-10 dataset, training with \(\eta=2.0\) and \(d=64\) does not improve \(\eta^{*}\) over training with \(\eta=0.5\) and \(d=64\) (Figure 4).

The approach of our theoretical analysis is to carefully disentangle the different effects of different factors that affect \(\eta^{*}\) - the scaling \(\eta^{*} \propto w_{1}\), the power of \(\eta\), and the ratio of width to depth. In this way, we are able to get more leverage on any one factor to find its dependence on the others. Moreover, we focus on the important case of wide networks, which are natural to use in practice. Unlike previous theoretical analyses which study more restricted problem settings ([3]; [5]; [4]), we show that the scaling \(\eta^{*} \propto w_{1}\) is generic, and also explain why this scaling is useful. Moreover, our experimental results on CIFAR-10 show that it is indeed beneficial to train with the scaling \(\eta^{*} \propto w_{1}\) (Figure 2A). We note that while we have found that \(\eta^{*} \propto w_{1}\) is useful, we have not proven that \(\eta^{*} \propto w_{1}\) is a necessary condition for learning a successful model. In fact, while most of our experiments use the scaling \(\eta^{*} \propto w_{1}\), there are still exceptions, e.g., training with \(\eta=2.0\) and \(d=64\) is useful for image classification on CIFAR-10 (Figure 4).

While our theoretical analysis shows the scaling \(\eta^{*} \propto w_{1}\) to be helpful, it is not the only helpful scaling. There is evidence in ([1]) that \(\eta^{*} \propto w_{1}\) is not the only helpful scaling - reducing \(\eta\) to the critical level of 0.14 in a linear sequence of layers led to better performance than training at the optimal (or empirically close) setting \(\eta=2.0\). While we find a scaling \(\eta",,"<The study presents the MABe22 benchmark, a comprehensive dataset that collects video and trajectory data from different biology experiments involving interacting animals. This includes mice, symbiotic beetle-ant interactions, and groups of flies. The dataset is accompanied by downstream analysis tasks to evaluate the quality of learned behavior representations. The authors demonstrate the use of state-of-the-art self-supervised video and trajectory representation learning methods, highlighting the importance of incorporating animal datasets in the evaluation of behavior representation learning [1]. In a similar vein, the Animal Kingdom dataset also aims to provide a large and diverse dataset for animal behavior understanding, addressing limitations in existing datasets. This includes annotated tasks to enable a more comprehensive understanding of natural animal behaviors [2]. Both MABe22 and Animal Kingdom datasets emphasize the need for datasets that cover a broad range of animal species and behaviors, promoting a more thorough exploration of behavior representation learning methods across various settings.

Moreover, the study highlights the limitations of applying methods developed using human action datasets to animal datasets [1]. The UCF101 and HMDB datasets, which are focused on human action recognition, emphasize the need for large-scale datasets with a diverse range of action categories and the evaluation of action recognition methods [3] [4]. The Kinetics Human Action Video Dataset presents a large dataset with 400 human action classes and provides baseline performance figures for neural network architectures trained and tested for human action classification [5]. The relevance of these human action datasets underscores the importance of benchmarks tailored to different species and behavior contexts, as exemplified by the MABe22 benchmark. This also invites future work to consider the implications of dataset biases and imbalances in behavior representation learning.

On a related note, the study's emphasis on trajectory representation learning aligns with research in sports analytics, where automatic tactics detection from event-stream data collected from soccer matches is explored. This field also encounters challenges in analyzing complex spatio-temporal data and emphasizes the importance of data-driven approaches for identifying patterns of movement representing potential tactics [6]. Additionally, the Argoverse dataset, focused on autonomous vehicle perception tasks, demonstrates the value of rich semantic maps and 3D tracking annotations in improving the accuracy of 3D object tracking, hinting at the potential for similar approaches in animal behavior understanding [7].

Finally, the introduction of MineRL, a large-scale dataset of Minecraft demonstrations, illustrates the relevance of large-scale simulator-paired datasets in enabling the development and evaluation of methods focused on using human examples [9]. The dataset's scale, structure, and quality facilitate research in reinforcement learning, reflecting the significance of comprehensive datasets in driving breakthroughs in understanding behavioral interactions.]

References:
[1] Social behavior recognition in continuous video
[2] Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding
[3] UCF101: A Dataset of 101 Human Actions Classes from Videos in The Wild
[4] HMDB: A large video database for human motion recognition
[5] The Kinetics Human Action Video Dataset
[6] Automatic Discovery of Tactics in Spatio-Temporal Soccer Match Data
[7] Argoverse: 3D Tracking and Forecasting With Rich Maps
[9] MineRL: A Large-Scale Dataset of Minecraft Demonstrations"
4851,4851," **Retinex-based LLIE.** Retinex model [1][2][3][4][5][6] is an effective tool for low-light image enhancement, which decomposes the illuminant map into the reflectance map and illumination map, and then transforms the illumination map to the original one. However, the decomposition of reflectance and illumination maps is time-consuming, which limits its application in real-world scenarios.

Recently, deep learning based LLIE methods [7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] have been proposed to overcome the limitations of traditional methods. Dong _et al_. [8] proposed a deep RetineX-Net to learn the mapping from low-lit images to their corresponding high-light images, which achieved state-of-the-art performance on several benchmark datasets. Zhang _et.al_. [9] proposed an end-to-end deep LLIE network with self-supervised learning, which can be trained with only a few pairs of high-quality and low-quality training images. Wang _et_. [10] proposed to use a two-branch network to progressively enhance the low-frequency details and suppress the noise in the high-frequency components, and achieved promising results on several benchmarks. Zhang [19] proposed EnlightenGAN to learn LLIE without paired training data, which is able to perform LLIE in a completely unsupervised manner without any human-annotated training data. Wang [21] proposed SNR-Net, a light-weight LLIE model, which could be trained in a single forward pass with only one forward pass. Wang  proposed a novel multi-scale network structure, which was able to achieve comparable performance with previous LLIE models. Wang and Zhang [22] proposed STAR-A [23], a novel network structure with a knowledge-aware assignment strategy and a novel Transformer block, which significantly improved the performance of LLIE networks. Wang et.al. [24] proposed Smart-Assign Transformer (SAT), a novel architecture with a novel attention mechanism and a new loss function, to achieve LLIE performance on benchmark datasets, which outperformed previous methods by a large margin.

 proposed a gamma correction network to address the problem of gamma correction in LLIE, but the correction factor gamma needs to be learned in a coarse to elaborate manner via adaptively perceiving the deviated illumination. In contrast, we propose to use Taylor Series to approximate gamma correction, accelerating the training and inference speed. Moreover, our method can be applied to various LLIE tasks, including LLIE and image restoration, without any additional training or inference cost.

 first proposed a neural network structure for LLIE based on CNNs, which learns the mapping between low- and high-resolution images via a multi-layer perceptron (MLP). However, CNNs are insensitive to recovering dark areas, and the local modelling structures are insufficient to recover accurate illumination across whole low-luminant images."," In this section, we comprehensively review existing works on low-light image enhancement and analyze their limitations in producing satisfactory results.

Conventional Methods.Many conventional LLIE methods are based on Retinex theory, which formulates an image as a multiplication of reflectance and illumination (Eq. (1)). Some methods estimate the reflectance and illumination via a variational framework, the estimated illumination is further adjusted to restore the original low-light image [1][2]. Other Retinex-based methods usually optimize an energy function derived from the Maximum-a-Posteriori (MAP) framework to enhance low-light images, in which some heuristic priors are designed to constrain the properties of reflectance or illumination [3][4][5][6]. For example, Gaussian total variation in [4], structure-aware regularization in [3] and the additional noise term in [5]. However, these hand-craft priors are usually inaccurate and limited when applied to different cases, leading to apparent color deviation.

CNN-based Learning Methods.CNN has pushed forward the solution of LLIE by large margins [7]. Some methods integrate Retinex theory with CNNs [8][9][10][11][8]. For example, Wei _et al._ propose an end-to-end Retinex-Net, in which a decomposition module and an illumination adjustment module are used to learn reflectance and illumination [8]. Wu _et al._ propose URetinex-Net which unfolds an optimization problem into several learnable network modules [11]. Some methods design effective networks to predict normal-light images directly from low-light ones via a supervised learning strategy [12][13][14][15][16][17]. For instance, [14] learns a 3D lookup table, and [16] uses color consistency to constrain network training. There are still some unsupervised methods to solve LLIE [18][19][20]. One famous work is [20], in which a self-calibrated illumination learning framework is proposed for fast, flexible, and robust low-light image enhancement. Because CNNs are usually limited by inductive bias, the illumination still cannot be well recovered for some images.

Transformer-based Methods.Transformer-based methods usually combine self-attention and convolution to extract long- and short-range dependencies, so that better performance can be obtained [21][22][23]. However, the long-range context dependencies are usually obtained by building patch-level relationships, and these methods downsample images before calculating self-attention to reduce computation. Hence, they may lose some useful information, which is adverse to recover high-quality images.

In this paper, we propose a local-to-global hierarchical self-attention to completely express images. We would like to point out that the work [24] aiming at high-level tasks proposes a TNT structure to explore local attention to assist global attention. Our work is different from it in the following aspects: 1) TNT still focuses on patch-level attention, pixel-level dependencies are still not explored, i.e., it does not completely model images. While ours completely express images via pixel-level attention. 2) Local attention in TNT just provides auxiliary information for the patch embedding of global attention. While our local attention models pixel dependencies locally and the global attention extends pixel dependencies to the whole image. 3) TNT adopts a bypass structure to realize auxiliary information enhancement. While our local attentions are directly embedded as the input for global attention.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]"," **Low-light image enhancement.** Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] mainly rely on hand-crafted priors and optimization-driven solutions to solve the low-light enhancement problem. For example, Retinex-based methods [3][8] decompose an image into illumination and reflectance layers, and then use the RetineX model [25; 26] to estimate the illumination and the reflectance layer separately. However, these methods are not robust to the deviated illumination, and the illumination correction factor gamma is not considered in these methods.

**Deep learning based methods.** Recently, deep learning-based low light image enhancement methods [9][9] have been proposed to solve this problem by learning the illumination-reflectance decomposition and illumination adjustment in an end-to-end trainable manner. For instance, KinD [10] and URetinexNet [11] propose to decompose the illumination layer into two sub-spaces, and learn the illumination adjustment and the reflection layer separately via a fully convolutional network. Retinext [8] proposes to learn the decomposition of reflectance and illumination in a deep network, and uses a decoupled decomposition module to estimate illumination adjustment. Uretinexnet [9] proposes a maximum entropy based Retinexpression of the illumination decomposition, which can be trained with low light images only. KinD  proposes to use a two-branch network to learn illumination and reflection decomposition jointly, and it is trained with a self-supervised loss function. URetinx-net [11], on the other hand, proposes to decomposition the illumination into two implicit sub-space, and use an unfolding module to adaptively fit the illumination estimation and reflectation decomposition.


"," **Image Synthesis with GANs.** GANs [1][2] have shown to be effective at generating images. Recent work on image synthesis focuses on applying these models to images of 2D faces. Large-scale training has been reported to improve GAN performance [3][4][5][6][7][8][9][10][11]. These methods have also been extended to other domains like object categories [12] and aerial scenes [13][14]. Recent advances in rendering GAN-generated images have focused on synthesizing multi-view consistent images. These methods [15][16][17][18][19][20][21] solve for the 3D geometry of an image using 2D image statistics. However, when faced with images of large poses, these methods typically fall short. To solve for this issue, previous work has attempted to use generators that learn how to edit the face geometry and appearance independently [10][11][22].

**3D-aware Image Synthesis.** The recent work on _3D-aware generative models_[23][24][25][26][27][28][29][30][22] has demonstrated the effectiveness of 3D information in image synthesis. These models have primarily been used for 2D images of faces. However, recent work [31][29] suggests that most state-of-the-art generative models are _dis-inclined_[31][29] towards the small pose range due to their normalization strategies [30]. PoF3 [30] proposes a pre-training strategy to address the dis-inclination problem, while SASA [31] proposes a pose-aware attention network. F-MX [30] is the first to generate images in all 90\(\cdot\)90 poses, but relies on 2D image priors. The synthesis in these methods is still restricted by the implicit conditions they must learn in the datasets. In contrast, our face generator in this work is free from such implicit conditions, and we present a dataset of large poses to train it.

**Facial Pose Datasets.** Existing face datasets that provide ground-truth poses typically provide the poses in small to medium yaw angles [32][33][34][35][36], which pose-inclined GANs (PI-GANs) [23][31][29] show to be inadequate for. F-MX [30] and SASA [31] are the only datasets that provide images of all yaw angles. They provide images of a very limited number of views for each pose. The diversity of images for different yaw angles are limited in these datasets. This is",,"<Related work>
The task of low-light image enhancement has garnered significant attention in the research community due to its implications for various computer vision and multimedia applications. Prior work has explored a range of approaches to address this challenge, leveraging techniques such as weighted variational models for simultaneous reflectance and illumination estimation [1], total variation models for Retinex theory [2], and deep learning-based methods such as LIME for illumination map estimation [3]. Additionally, novel approaches like low-light image enhancement with semi-decoupled decomposition [4] and robust Retinex models for handling noise in low-light images [5] have been proposed. Furthermore, the enhancement of low-light images through deep learning-based solutions has been systematically surveyed [7], shedding light on the influence of various learning strategies, network structures, and training data on the effectiveness of low-light image enhancement methods.

Recent work has also introduced innovative techniques such as the use of Taylor Series for approximating gamma correction to accelerate training and inference speed in low-light image enhancement [9], and the development of a deep Retinex decomposition network for effective low-light enhancement [8]. Furthermore, self-supervised approaches for low-light image enhancement have been explored, including self-supervised image enhancement networks that incorporate maximum entropy-based Retinex models [9], and unsupervised generative adversarial networks such as EnlightenGAN that generalize well without paired training data [19]. Moreover, research has delved into the development of specialized techniques, such as SNR-aware transformers for spatial-varying pixel enhancement in low-light images [21], smart knowledge assignment strategies for connecting different weather conditions as seen in deraining and desnowing tasks [23], and the use of structure-aware lightweight transformers for real-time image enhancement [22].

In addition to these techniques, recent advancements in the field of low-light image enhancement have explored novel architectures like Transformer in Transformer (TNT) for visual transformers with enhanced performance in encoding image features [24]. These techniques collectively demonstrate the diverse range of approaches and methodologies that have been developed to improve the quality and visibility of low-light images for various real-world applications."
2463,2463," **Tracking by Matching.** Siamese networks [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19] have been widely used in the field of visual tracking. The basic idea is to learn a similarity function between the query frame and the target frame, which is then used to estimate the confidence score for the current frame. However, these methods are usually computationally expensive due to the large number of candidate frames. Recently, transformer-based methods [20][21][22][23][24] have attracted increasing attention due to their high efficiency and superior performance. These methods mainly focus on designing effective transformer architectures to capture long-range dependencies in videos.

**Video Transformer.** Inspired by the success of transformers in natural language processing (NLP) tasks [25][26][27][28][29][30], several recent works [31][32][33][34][35][36][37][38][39][40][41][42] have explored the application of transformer to video tasks. For example, ViT [34] and Swin Transformer [28] adopt a two-stream transformer architecture to extract spatial and temporal information from video sequences, respectively. VideoTrack [33] proposes a spatio-temporal attention module to model the long-term dependencies in video sequences. In contrast to these methods, we propose a novel video transformer architecture that can effectively model the spatiotemporal information in video frames. Moreover, we introduce a novel dual-template mechanism to better disentangle the static and dynamic appearance clues in video data.

 propose a video attention module that learns long-distance dependencies in a video sequence. They use a single-stream Transformer architecture to learn the global and short-range attention. However they do not explicitly model the temporal relations in videos, which are crucial for visual tracking tasks. In this work, we design a sequence-to-sequence attention module for video transformer, which can effectively capture the temporal relationships in video clips.

 also proposes a video transformer model for video object detection task. They propose a non-local self-attention module to capture the long range dependencies in the video data, but their model is computationally inefficient. In comparison, our model is much more efficient and achieves state-of-the-art results.

 proposes a novel multi-scale transformer architecture for object detection. Their model consists of a sequence of transformer blocks with different receptive fields. Their architecture is also computationally efficient.

 introduces a new multi-resolution attention module. Their method is designed to capture global and long range interactions in the image data. Our work is different from theirs in two aspects: (1) We use a standard transformer architecture. (2) We propose a new video transformer backbone.

 firstly demonstrates the effectiveness of the video transformer in object detection tasks. We also demonstrate its effectiveness in video object tracking."," **Visual tracking paradigm.** The Siamese network [1][2][3][4][5] based tracking paradigms have drawn great attention recently, in which they formulate the tracking as per-frame target matching. Under the pair-wise matching framework, Siamese trackers are improved with the help of following techniques: powerful backbones [3][5], elaborated prediction networks [2][4][6], attention mechanism [7][8] and model fine-tuning [9][10]. Recent pure transformer-based trackers [11][12][13][14] leverage the vision transformer to unify the feature extraction and fusion, while still not consider how to effectively model the temporal dependency. Discriminative Correlation Filter (DCF) [15][16][17][18][19][20][21] is another popular tracking paradigm, which can optimize the target model by solving least-squares based regression. Though DCF can easily utilize the temporal information by updating the model online, it suffers from the complex handcrafted optimization.

**Temporal modelling in Siamese tracking.** Two representative paradigms are introduced to enhance the temporal modelling in Siamese trackers: the first one is to update templates using online mechanism  or deep-learning based networks [7][22]; the second one [23] is to propagate the target information from templates to search frame. Despite the improvements, both of them require extra hyper-parameters and redundant network modules to equip the original Siamese pipeline. In contrast to the sophisticated methods and tedious hyper-parameters mentioned above,VideoTrack is the first to encode the temporal information via a simple feedforward video transformer backbone, which is novel and conceptually neat.

**Video vision transformer.** Video transformers have recently been introduced as powerful video recognition models, motivated by the impressive performance of transformers in language and vision [24][25][11][26][12][27][28][29][30][31][32][33]. ViViT [34], Timesformer [35], VTN [36] and VideoSwin [37] are the pioneering works, which apply the pure-transformer based models for video recognition. The underlying reasons for their success lie in the characteristics of video: videos are sequential data while transformer attention can capture the global dependency among all the video segments. Considering visual tracking is highly sensitive to spatial/appearance information, rather than semantic/category, we modify the standard video transformer structure to exploit more static/dynamic appearance clues for tracking.

**Temporal modelling in video understanding.** Temporal context modelling is the key issue in video understanding task. 3D convolutional block [38][39][40] is the widely adopted technique, which expands 2D CNN into temporal domain. Then, non-local network [41] applies self-attention to capture long-range spatiotemporal dependencies on top of 2D CNN. Recently, video transformers [34][35] use self-attention as the exclusive building block to capture spatiotemporal context. In instance-level video understanding, temporal shift [42][43] and message token [44] mechanisms are equipped into video transformer to enhance the temporal modelling as well as reducing the computation cost. In this work, we empirically evaluate different temporal modelling methods and develop a disentangled dual-template scheme for VideoTrack model.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]"," **Siamese Tracking.** Siamese trackers [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45]. The pioneering work of SiamFC [4] proposes to use a two-stage Siamesen network to extract features and perform target-aware feature matching between the template and the search region. SiamCAR [2] and SiamRPN++ [3] propose to use fully convolutional networks for feature extraction and regression, respectively, and further improve the tracking performance. STMTrack [7] and RASNet [8] introduce a memory mechanism to make full use of historical information related to the target for better adapting to appearance variations.

Recently, some works [9][1][10] propose a meta-learning framework to learn the target-specific appearance model online, which can be easily adapted to the tracking task. MAM [11] proposes a mixed attention module to integrate target information into the target estimation module, and achieves state-of-the-art performance. DTT [21] proposes an end-to-end single-object discriminative tracker, based on an encoder-transformer-decoder architecture, which uses self-attention mechanism to propagate information from previous frames to the current frame. TransformerTrack [24] introduces a transformer-assisted tracking framework, where the transformer encoder and transformer decoder are separated and used to propagate the tracking cues from previous templates to current frame, which facilitates the object searching process. However, these methods heavily rely on additional sophisticated mechanism to exploit temporal information among successive video frames, hindering them from efficiency and industrial deployments. In this work, we resort to the standard video transformer architecture to learn spatiotemporal feature learning directly from frame-level patch sequences, which is more efficient and effective for visual tracking.

"," Image aesthetic assessment.Image aesthetics assessment methods have been broadly divided into _rating-based_[1][2][3] and _regression-based_[4][5][6] methods. The former requires users to provide a multi-choice rating on the target image aesthetics (e.g. ""good,"" ""bad,"" ""beautiful""), while the latter predicts the probability of whether an image is aesthetically pleasing. For example, KonIQ-10k [2] has collected 10K images, where each image has 96 randomly sampled user-created tags that can be used to rate the image aesthetics. AVA [6] contains 12K images and 500 manually-written semantic categories (e.g., _abstract_, _colorful_). IQA-Net  introduces a large-scale IAA benchmark consisting of 100K images in total and containing 41 fine-grained attributes, such as ""pattern"" and ""colorfulness."" For a more comprehensive review of IAA methods, see .

**Rating-based methods** [7][8][9] predict user ratings on a 5-point scale using learned image feature representations. In order to learn more generalized features, most methods involve task-specific architectures or design choices, such as using multi-layer perceptrons [8][10] and deep convolutional neural networks [7][11].  introduces regression models that predict a 5-dimensional rating based on different features. Feng [12] proposes a probabilistic formulation and Zhang [13] improves the decoder's architecture for more accurate predictions. Various studies also explore new features to improve the performance, including short-term spatial information [14], geometric features [15], and multiple inputs such as image labels, aspect ratios, and camera intrinsics [16]. To improve the aesthetic diversity, some works investigate applying pooling on various features or crops [17][18][19][20][21]. However, these methods require labeled rating scores from the users.

**Regression-based methods** (i.e., regression models with end-to-end training) make predictions based on the features extracted by CNNs [10], R-CNNs [10], or large-scale VLP models [22][23][24][25][26][27][28]. For instance, [6] uses a VLP model and a classification branch for the aesthetics modeling task. Pan  introduces a similar pipeline that learns from 8.8M images collected from the web and additional adversarial training. Li  improves the performance by introducing a feature recalibration mechanism and provides a more user-friendly interface for rating. Luo [29] introdu",,"<>
In recent years, the application of transformer-based architectures has gained significant attention in the field of visual object tracking. Various works have focused on leveraging the capabilities of transformers for improved spatiotemporal feature learning and object localization. VideoTrack introduces a sequence-level target matching approach, encoding temporal context into spatial features through a neat feedforward video model. This is achieved by adapting the standard video transformer architecture to enable spatiotemporal feature learning directly from frame-level patch sequences. Furthermore, the integration of sequential multi-branch triplet blocks forms the video transformer backbone, enhancing the capability to blend spatiotemporal information in video clips [1].

The proposed VideoTrack method is designed to address the limitations of existing Siamese tracking methods, which heavily rely on pair-wise matching between individual frames and additional sophisticated mechanisms to exploit temporal information. By utilizing sequence-level target matching to encode temporal contexts and employing sequential multi-branch triplet blocks, VideoTrack demonstrates significant improvements in spatiotemporal feature learning and object tracking.

In the domain of siamese-based trackers, the work by Siamese Fully Convolutional Classification and Regression for Visual Tracking (SiamCAR) introduces a novel end-to-end architecture for visual tracking, targeting the classification of pixel category and regression for object bounding box at a per-pixel level [2]. This approach departs from traditional region proposal-based trackers and enjoys the benefits of not requiring hyper-parameter tuning, reduced human intervention, and a simple yet effective framework. Additionally, SiamRPN++ and SiamFC++ have demonstrated the effectiveness of introducing residual attention and incorporating practical guidelines of target state estimation for high-performance generic object tracker design [4] [5].

The proposal from STMTrack focuses on template-free visual tracking, leveraging space-time memory networks to effectively store and utilize historical information related to the target for better adaptation to appearance variations during tracking. This novel tracking framework built on top of a space-time memory network has shown promising results in addressing appearance changes experienced during tracking, without imposing onerous computational demands or requiring complex hand-designed strategies for tracking adaptation [7].

Moreover, the Residual Attentional Siamese Network (RASNet) for high-performance object tracking introduced in [8] reformulates the correlation filter within a siamese tracking framework. This architecture, along with its general attention and target-adapted attention mechanisms, demonstrates the capacity to mitigate overfitting in deep networks and enhance discriminative ability, crucial for real-time and efficient object tracking. This development builds upon the success of previous siamese-based trackers and aims to achieve high-performance object tracking while offering improved adaptability and discriminative capacity [8].

In light of the advancements in visual tracking, it is evident that the application of transformers for spatiotemporal feature learning and improved object tracking has become a predominant trend. The future directions of research in this field could focus on further enhancing the capabilities of transformer-based architectures for real-time video instance segmentation and exploration of efficient methods for space-time feature learning [43] [44]."
455,455," **Compositional Zero-Shot Learning (CZSL).** Early works on CZSL [1][2][3][4][5][6][7][8][9][10][11][12][13] focus on learning a common embedding space for attribute-object combinations. For example, Chen _et al._[1] proposed to learn a shared attribute space for both seen and unseen attributes. In [2], a hierarchical decomposition model was proposed to decompose attributes into a set of sub-concepts and learn a classifier for each of them. In, a contrastive loss was introduced to learn the embeddings of attributes and objects in a shared space. Recently, a generative adversarial network (GAN) was proposed in  to learn an attribute embedding for each attribute.

**Attribute Learning.** Attributes have been widely used in image retrieval [14][15][16] and multi-task learning [17][18][19]. In [18], a conditional GAN was proposed for attribute learning. In  and [19], conditional attributes were used to improve the performance of object recognition. In contrast to these works, we propose to learn attributes conditioned on the recognized object and the input image.

 proposed a conditional attribute learning framework for image classification. However, they did not consider the C-GQA task. In this work, we focus on the CzSL task and propose a novel framework to learn conditional attributes.

 introduced a conditional embedding model for the CvZSL task. Their model is based on a two-stage model, where an attribute hyper learner and an attribute base learner are used to learn attribute representations. In the first stage, the learned attribute representations are used for the attribute classification task. The second stage is used to train the attribute base learners, which are responsible for generating the attributes for the attributes in the second stage. Our model differs from theirs in two aspects. First, our model learns the conditional attributes in an end-to-end manner. Second, their model is designed for the image classification task, while our model can be applied to other tasks.

 also proposed an attribute learning model for CvVSL. Their method learns the attributes by minimizing the KL divergence between the learned attributes and the ground truth attributes. Different from their method, we learn the attributes from the image itself, which is more general and can be used for other tasks, such as image retrieval and image captioning.

 first proposed the CZL task. They proposed a two stage model, which consists of an attribute classifier and an object classifier. The classifier is trained to predict the attributes of the objects in the image, while the object classifiers are trained to classify the objects of the attribute class. In their model, attributes are learned by maximizing the mutual information between the object and attribute classifiers. In our model, we do not use the classifier to generate the attributes. Instead, we directly learn the attribute representations from the images.

 is the first work to propose a CZS task. It aims to recognize novel compositional concepts based on"," **Compositional Zero-Shot Learning.** Given descriptions only, we can recognize objects that are never seen before. In conventional Zero-Shot Learning (ZSL), models have access both to images of seen classes and descriptions of seen and unseen classes [1]. In contrast, CZSL presents no description of seen and unseen attribute-object compositions while all attributes and objects as concepts are seen during training. Recently, works in CZSL are divided into two main streams. One extracts attribute and object words or visual features independently from a composition during training, including learning attributes as linear transformations of objects , learning to hierarchically decompose compositions and recompose the concepts with learned visual concepts [2], learning independent prototypes of attributes and objects and compositing prototypes via graph network [3], and learning decomposed prototypes of visual concept features [4] via siamese contrastive embedding network [5]. The other is to learn a compositional space [6], a graph network [7][8], an episode-based cross-attention module [9], and a contrastive space [10] for contextuality modeling. Also, Yang _et al._[11] rethink the CZSL task in a decomposable causal way and learn three spaces for attribute, object, and composition classifications. Additionally, with pre-trained large vision language models like CLIP, Nayak _et al._[12] propose to tune soft prompts as concept embeddings.

Recent work in [13] addresses the problem of attribute diversity. They propose to learn translational attribute features conditionally dependent on the object prototypes. Specifically, they add generic object embedding as the object prototype to the concatenated attribute and object embedding. However, this approach makes the model concentrate more on the composition instead of the attribute, causing the attribute learning degrade to learning the contextuality between attribute and object. On the contrary, we explicitly focus on learning conditional attribute embeddings. The learned conditional attribute embeddings can be changed along with the objects and input images.

**Attribute Learning.** Learning features of attributes is explored by a large community including image search [14][15], sentence generation [16], and zero-shot classification [17][18]. Conventional attribute learning approaches map the attributes into high-dimensional space and train a discriminative classification head without considering the diverse nature of attributes [19]. Our work also learns high-dimensional embeddings to represent attributes. The main difference is that our learned attribute embeddings are conditioned on different objects and input images.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"," **Compositional Zero-Shot Learning.** Compositional zero-shot learning (CZSL) aims to recognize novel compositions of seen and unseen concepts. Existing approaches can be divided into two categories: (1) methods based on attribute transfer [1][2][3][4][5][6][7][8][9][10][11][12][13] and (2) methods that use attribute embeddings [14][15][16][17][18][19].

**Attribute Transfer.** The goal of attribute transfer is to learn compositional concepts from seen to unseen compositions of objects and attributes. In [1], the authors propose to learn attribute transfer between different classes by transferring attributes from seen classes to unseen classes. In, the authors introduce a novel CZSL benchmark, C-GQA, and propose a novel attribute transfer method, CZLT, which learns attribute embedding from the seen to the unseen compositions. The authors of [2] propose to use a hierarchical decomposition-and-composition method to learn unseen concepts in a hierarchical manner. In  and [4], the author of [4] propose a method to disentangle attribute and object features in the visual space and use pre-trained word embedding to better separate and compose attribute-object pairs for recognition. In addition, the authors of  propose a model to learn a joint compatibility between attributes and objects in a shared embedding space. The author of  use an attribute-attribute embedding network to learn the compositional generalization between seen and unknown compositions. In contrast, the author in [5] propose the Siamese-contrastive embedding method, which embeds the state and object into a Siamesen contrastive space to capture prototypes of them separately. In this paper, we propose a new CZSCL method that uses attribute learning to learn conditional attributes embedding.

Our work is also related to the work of [8], which proposes to learn graph embedding of states, objects and their compositions within a graph structure to enforce the relevant knowledge transfer from seen compositions to unseen ones. In our work, we focus on the CZCL setting, where we learn attribute representations conditioned on the recognized object and the input image.

 is a recent work that proposes to use attribute learning for compositional zero shot learning. In their work, they propose a multi-task learning framework, which is similar to ours, but they use the attribute learning method to"," **3D-Aware Generative Models.** Since GANs has shown powerful image synthesis capability, a variety of 3D-aware GANs [1][2][3][4] have been proposed to tackle different aspects such as image and geometry. In the early attempts, most 3D-aware GANs rely on 3D-posed images to train a 3D-aware generator with novel view synthesis performance based on an image-to-3D style and 3D-pose encoder. For example, GRAF [9] introduces radiance field constraints for sampling 3D-pose latent codes. In [4], a coarse 3D shape model is learned and a 3D latent encoding network is employed to progressively improve the 3D pose accuracy. A 3D-style prior is also introduced into the generator in [8][7][10]. However, these methods suffer from mode collapse and reconstruct a non-photo-realistic 3D-pose latent space. To overcome these issues, EpiGRAF [11] modulates the latent vectors by color-code pose offsets to control the 3D pose variations. Although a step closer to the reality, it is still not controllable enough. GRAF-D [12] aims to recover a finer 3D shape latent vector by introducing a 3D deformation field prior. Meanwhile, GRAF-D introduces 3D-aware semantic features in the encoder to improve the representation power. Another class of 3D-aware GANs learns 3D surface geometry for novel view synthesis. For example, GET3D [4] learns a real-valued 3D surface representation from the radiance fields of novel images. GIRAFFE [7] and GET3D-C  generalize GET3D to handle challenging cases such as outdoor images and complex 3D shapes. There are also several methods for multiplane image synthesis. MLIMPI [16] proposes to combine the view synthesis performance of 2D-GANs and 3D-GANs. In [17][18], 3D surface representation is first learned and then decoded with volume rendering [19]. In [15], a hybrid multiplane 3D-Aware GAN is proposed by lifting the generator of [1] to the latent space of a 3D-Aware GAN. These 3D-Aware GANs are powerful but often suffer from the lack of real 3D data. In contrast, GRAF-D  is not reliant on 3D data. Instead, it trains from a large-scale monocular portrait dataset to ensure the diversity of the latent code.

**GAN inversion.** As the GANs-based synthesis approaches can",,"\<To address the challenge of Compositional Zero-Shot Learning (CZSL), several prior works have focused on different aspects of attribute-object recognition and composition. Some works have delved into attribute-based classification [1], leveraging human-specified high-level descriptions of target objects, while others have explored hierarchical decomposition and composition for learning unseen concepts [2]. Additionally, independent prototype propagation has been proposed to deal with underspecified datasets and leverage contextual clues during classification [3]. Moreover, efforts have been made to disentangle visual embeddings for attributes and objects in the context of compositional zero-shot learning [4]. A novel Siamese Contrastive Embedding Network has been introduced for recognizing unseen composition, capturing prototypes of states and objects in a contrastive space to alleviate their entanglement [5]. Open World Compositional Zero-Shot Learning has been addressed with a method proposing feasibility scores that boost performance in this challenging scenario [6]. On the other hand, leveraging variational graph embeddings for open world compositional zero-shot learning has been explored to address the issue of infeasible compositions in the search space [7]. Additionally, a graph-based model was proposed to recognize both single- and multi-attribute-object compositions, addressing the challenge of compositional recognition [17].>"
1795,1795," **Bandits with Knapsacks.** Bandits with knapsacks (BwK) ([3]; [5]; [4]) is a general bandit problem where the goal is to find an optimal packing of items into a limited-size knapsack. In BwK, the reward function is a linear function of the number of arms, and the bandit policy is a closed-form function of these arms. In this paper, we consider the setting where the reward and the consumption vector associated with each action is a class-dependent linear function. The problem is also related to the online revenue management problem ([1]; [2]), which is a special case of Bwk. However, in the online setting, the budget grows linearly with the time horizon.

**Linear Bandits.** The problem of linear bandits with linear constraints has been studied in ([6]; [7]; [8]; [9]). In ([6]), the authors consider a linear MAB problem with safety constraints that depend (linearly) on an unknown parameter vector. The authors propose an algorithm for solving this problem with a pessimistic optimistic pessimistic bandit algorithm. In ([8]), they extend the pessimistic pessimistic algorithm proposed in ([9]) to the linear setting, and propose a new algorithm for the problem with non-convex constraints. In (), the authors propose a linear stochastic MAB algorithm with a non-asymptotic regret bound. In (; ), the authors extend the problem to a multi-armed bandit setting, where the rewards and constraints are linear functions of the arms and the budget, respectively. In addition, they propose a primal-dual based algorithm that achieves a problem-dependent logarithmic regret bound with respect to the size of the budget. In the adversarial setting, ([5]) considers a general model of the problem, and proposes an algorithm with an upper-confidence bound of \(\mathcal{O}(n^{2})\) on the regret of the algorithm.

 consider the linear contextual multi-class multi-period packing problem (LMMP) and extend the results of ([3]) to LMMP. They consider the case where the contexts are non-degenerate, and extend their results to the setting with \(T=T\) classes. They propose an iterative algorithm with \(O(n)\) iterations and a regret of \(\Omega(\sqrt{T})\). In contrast, we extend their result to a more general setting with a \(T\)-dimensional context dimension, and provide a faster convergence rate and a lower regret.

 study the contextual setting of the linear bandit problems and propose an optimal policy for solving it. They assume that the context is a Gaussian distribution and the reward is linear. They also propose a \(O\)-log-sum-exp(-\frac{1}{2}\log(n-1)}{2}\) algorithm to solve the problem. In their algorithm, the regret is linear in the context dimension and \(T\) when the budget is at least as large as \(\frac{T}{"," There are two streams of work that are relevant for LMMP. In online revenue management literature, Gallego and Van  introduced the dynamic pricing problem where the demand is a known function of price (action).  and  extended the problem under unknown demands with multiple resource constraints. [1] proposed a Thompson sampling-based algorithm and extended it to contextual bandits with knapsacks. When the expected demand is a linear function of the price vector, the dynamic pricing problem is a special case of linear contextual bandits with knapsack (LinCBwK) proposed by .

The LinCBwk is a common generalization of bandits with knapsacks ([3]; [5]; [4]) and online stochastic packing problems (; ; [2]). Recently,  proved a logarithmic regret bound for LinCBwK when there exists a problem-dependent gap between the reward of the optimal action and the other actions. Instead of the gap assumption, we require non-degeneracy of the stochastic contexts (see Assumption 3 for a precise definition) to obtain a regret bound sublinear in \(d\) and extends to the case when the contexts are generated from \(J\) different class.

[6] proposed a variant of LinCBwK where the selected action must satisfy a single constraint with high probability in all rounds, i.e., LinCBwK with anytime constraints. [8] and [7] proposed a Thompson sampling-based algorithm and an upper confidence bound-based algorithm, respectively, for LinCBwK with a single anytime constraint. [9] highlighted the difference between global and anytime constraints and proposed a pessimistic-optimistic algorithm for the anytime constraints. We focus on the global constraints; however, we note that the extension to the anytime constraints is straightforward with minor modifications.

Let \(\mathbb{R}_{+}\) denote the set of positive real numbers. For two real numbers \(a,b\in\mathbb{R}\), we write \(a\wedge b:=\min\{a,b\}\) and \(a\lor b:=\max\{a,b\}\). For a natural number \(N\in\mathbb{N}\), let \([N]:=\{1,\ldots,N\}\).

","[1, 2, 3, 4, 5, 6, 7, 8, 9]"," **Bandits with knapsacks.** The bandit problem has been studied in the context of online revenue management ([1]; [2]; [3]; [4]; [5]; [6]; [7]; [1]). [3] and [4] consider the setting where the reward and the consumption vector associated with each action is a class-dependent linear function of the context, and the decision-maker receives bandit feedback. [1] propose a dynamic pricing algorithm which incorporates the inventory constraints of the problem. [5] propose an adversarial bandit algorithm which can be trained adversarially. [7] propose the optimistic pessimistic linear bandit (OPLB) algorithm which achieves a problem-dependent logarithmic regret bound for solving the general BwK problem. They also propose a primal-dual algorithm which uses multiplicative updates to improve the regret. [6] propose Safe-LUCB, which includes necessary modifications to respect safety constraints. [8] propose safe linear Thompson sampling (SLS) which uses TS to select actions that are safe at each round and achieves a cumulative regret of order \(O(d^{3/2}d^{1/2}\log t\log t)\). [9] consider stochastic linear bandits with general nonlinear constraints and propose a pessimistic-optimistic algorithm which is efficient in terms of the number of constraints and the number time horizons.




"," Off-policy evaluation.Our problem setting is closely related to the problem of off-policy evaluation (OPE) ([2]; ), where the key idea is to estimate a function's value function, using experiences generated under a behavior policy. The OPE problem has found wide applicability in reinforcement learning. A key open question in OPE is to identify conditions under which an estimator will converge to the true value of the target policy, or alternatively when the target policy has no value at all ([1]).

Slotta &  provide lower bounds on the mean-squared-error of policy evaluation in a stochastic setting where the agent is executing stochastic policies. They identify two types of estimation error: one is inherent in the estimator, the other depends on the exploration policy. They also identify a sufficient condition on the reward structure under which it is possible to efficiently estimate the value of the target policy in an offline setting.

Compared to the RCTs we consider, OPE settings are much simpler. We can work in an ideal stochastic setting, and the assignment mechanism has no effect on the outcomes. In contrast, in RCTs the allocation rule of resources may affect outcomes even when we are only estimating their _marginal effects_, and our goal is to design an allocation rule that reduces these effects in order to obtain clean estimates of the impacts. Slotta &  discuss in their Appendix B.1 the specific case of randomized trials (single treatment RCTs). There they show that a sophisticated baseline adjustment, or zero-intercept specification, can reduce the estimation bias of the OPE estimator in RCTs.

Estimation based on cross-fitting.Our estimator can be seen as a cross-fitting estimator of the average treatment effect, like those considered in (; ) (see also [3]). Our main result on cross-fitting is in the context of assignment mechanisms such as randomized trials, while they considered the context of multi-armed bandits with random assignment.

The two main contributions of this work are the estimation method itself, and the policy design to improve it. Hence, while we also discuss in some detail our experience with other cross-fitting estimators, the comparison is limited, as we focus on our estimator.

Estimators based on propensity scores.Estimators based on propensity scores are related to those based on cross-fitting. In the context of RCTs, propensity scores are the probability that an individual gets assigned to a given treatment, conditional on their observed characteristics. There are numerous approaches in the literature on estimating the average treatment effect with propensity scores.

Perhaps the most direct way to estimate the average treatment effect is by simply matching the propensities of each treated and control individual using optimal matching methods. This provides a strong guarantee on the bias of the estimate of the average treatment effect, and has been extensively used in",,"<>
The linear contextual multi-class multi-period packing problem (LMMP), as discussed in the target paper, is a complex optimization problem that has applications in various domains, such as online revenue management [1], resource allocation [2], and bandits with knapsacks [3]. Online revenue management using Thompson sampling [1] utilizes a Bayesian machine learning approach, primarily applied to online ad display, and handles inventory constraints by leveraging a linear program formulation. This aligns with the problem addressed in the target paper, as it involves optimizing allocation under budget constraints. Additionally, the work on resource allocation problems in the online setting with stochastic input [2] is relevant to the target paper as it presents algorithms for a class of resource allocation problems and introduces a new distributional model, which can be potentially applied to LMMP.

The concept of ""bandits with knapsacks"" in the literature [3] provides a framework that combines stochastic integer programming with online learning and encompasses problems with supply or budget limits. This encompasses the problem setting discussed in the target paper, where the decision-maker receives bandit feedback and aims to optimize item packing given budget constraints. Moreover, the work on the symmetry between arms and knapsacks in the bandits with knapsacks problem [4] introduces a primal-dual algorithm with a logarithmic regret bound, addressing the challenge of regret minimization in bandits with knapsacks, which is also pertinent to the challenges discussed in the target paper.

Furthermore, the study on adversarial bandits with knapsacks [5] contributes a competitive ratio algorithm for the BwK problem, where the outcomes can be chosen adversarially, providing a different perspective on regret minimization. Additionally, the work on linear stochastic bandits under safety constraints [6] considers bandit algorithms in safety-critical systems, where the algorithm needs to respect safety constraints that rely on the bandit's unknown parameters at every round, similar to the constraints discussed in the LMMP problem. The proposed UCB-based algorithm ensures safe actions and controls regret, showcasing relevance to the safety constraints involved in the LMMP problem.

Finally, the research on Stochastic Bandits with Linear Constraints [7] and the development of a new safe algorithm based on linear Thompson Sampling (TS) [8] are relevant as they address linear stochastic bandit problems under additional unknown linear safety constraints, demonstrating similarity to the LMMP problem where safety constraints need to be satisfied. Additionally, an efficient pessimistic-optimistic algorithm for stochastic linear bandits with general constraints [9] addresses a similar problem of maximizing expected cumulative reward subject to constraints, highlighting applicability to the LMMP problem. These works collectively form the related work for the target paper, providing insights and solutions relevant to the LMMP problem."
4212,4212," **Video Instance Segmentation (VIS).** In recent years, video instance segmentation [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16] has been widely studied in the computer vision community due to its wide applications in video surveillance, video object tracking, and video object detection. In this section, we briefly review the VIS methods with per-clip input.

**Per-clip Video Instance segmentation.** Most existing VIS methods [8][1][3] are based on the one-stage framework, which first generates a set of object proposals and then segments the instances in each proposal frame. For instance, YOLACT [10] and Mask R-CNN [12] first generate object proposals in each frame and then classify and segment each proposal in the video. QueryInst [1] and MinVIS [2] adopt the query-based segmentation framework to segment instances in videos. However, these methods often fail on challenging videos with occluded objects and crowded scenes. This is mainly because the instance queries in these methods cannot encode well the discriminative embeddings of instances, making the segmenter difficult to distinguish those 'hard' instances. To address these issues, we propose MDQE, the first VIS method with perclip input, which aims to mine discriminatively query embedding representations for instance queries and achieve state-of-based mask segmentation on challenging challenging videos. In addition, we also propose an inter-instance mask repulsion loss to address the occlusion problem in the VIS task, which is orthogonal to the existing methods. **Occlusion-aware VIS.** Recently, there are several works [17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32] proposed to address occlusions in VIS. For example, DINO [29] proposes a deformable convolutional network (DCN) [19] to deform the convolution kernels to capture the spatial-temporal information of objects in a video. IFCN [21] proposes an end-to-end VIS framework with a transformer-based encoder-decoder architecture to capture long-range dependencies in videos and achieve competitive performance on simple videos. Inter-frame communication transformer (ICT) [22] uses a transformer to model the temporal dependencies between consecutive frames to improve the performance of VIS. In particular, ICT uses a bidirectional transformer decoder to learn the inter-frame dependencies between the query and the query features, which can be regarded as a special case of our work. In our work, we further improve ICT by introducing a novel query repulsion module to learn object-aware discriminable query embeds.

 propose a VIS method based on a two-stage pipeline. They first generate bounding box proposals in the first stage and then perform mask propagation in the second stage to obtain instance masks in the final stage. In contrast, our method is based on"," Our work is related to the many per-frame VIS methods, the recently proposed per-clip VIS methods, as well as the methods for learning query embeddings.

**Per-frame input based VIS methods.** A popular VIS pipeline [1][2][3][4][5][6][7][8][9] is to extend the representative image instance segmentation methods [10][11][12] by adapting a frame-to-frame instance tracker. For example, in [1][2][8], the clues such as category score, box/mask IoU and instance embedding similarity are integrated into the tracker. However, these trackers may struggle in distinguishing instances with similar appearance. Inspired by contrastive learning [13][14][15][16], IDOL [7] learns discriminative instance embeddings for multiple object tracking frame by frame, achieving state-of-the-art results on OVIS [17]. Besides, clip-to-clip trackers [18][5][17] propagates the predicted instance masks from a key frame to other frames using deformable convolution [18][19], non-local block [5], correlation [4][17], graph neural network [20], _etc_. By exploiting the temporal redundancy among overlapped frames, clip-to-clip trackers improve much the performance of per-frame methods.

**Per-clip input based VIS methods.** A clip-in clip-out VIS pipeline was firstly proposed in  to model a video clip as a single 3D spatio-temporal pixel embedding. In recent years, transformer based per-clip methods [21][22][23] have achieved impressive progress on the YouTube-VIS datasets [8]. VisTR [22] views the VIS task as a direct end-to-end parallel sequence prediction problem, but it needs a large memory to store spatio-temporal features. To solve the issue, IFC [21] transfers inter-frame information via efficient memory tokens, and SeqFormer  locates an instance in each frame and aggregates temporal information to predict video-level instances. To keep object temporal consistency, EfficientVIS [23] transfers inter-clip query embeddings via temporal dynamic convolution.

However, per-clip VIS methods do not perform well on the challenging OVIS videos [17] with occluded objects in crowded scenes. Actually, occlusion-aware models have been developed for related tasks [24][25][26]. For instance, a bilayer convolutional network is developed in [27] to infer the occluder and occluded instances in image segmentation. A repulsion detection loss is designed in [26] to distance the bounding box of an object from the surrounding non-target objects for detecting individual pedestrian in a crowd. Inspired by these works, we propose an inter-instance mask repulsion loss to distinguish the pixels of each instance from its nearby non-target instances.

**Query initialization.** Existing query-based VIS methods adopt zero-initialized (_e.g_., DETR ) or randomly-initialized (_e.g_., Deformable DETR [28]) inputs as initial queries. The initial queries cannot encode well the spatiotemporal priors of objects, making the query-based segmenter difficult to distinguish occluded instances with similar appearance. Actually, query initialization with contextual and positional information has been used in many computer vision tasks [29][30][31][32] for higher performance or faster convergence. However, it has not been well explored in VIS. In this paper, we thus propose a query initialization method to obtain temporally-aligned frame-level queries.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]"," **Video instance segmentation (VIS)** aims to segment and associate all instances of predefined classes for each frame in videos. Existing VIS methods can be roughly divided into two categories: two-stage methods and one-stage ones. Two-stage VIS methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32] first detect and segment each instance of interest in each frame and then associate the instance masks across frames by tracking or matching. One-stage approaches [4][4] first predict bounding boxes around the ground-truth bounding box, and then segment the instances in the corresponding ROIs. Recently, MaskTrack [8] introduces a new tracking branch to Mask R-CNN [12] to jointly perform the detection, segmentation and tracking tasks simultaneously. In this paper, we propose to mine discriminative query embeddings to segment occluded instances on challenging videos.


**Contrastive learning (CL)**[13][13] has been widely used in many computer vision tasks, such as image classification [14][13], object detection [13], and object tracking [15]. Recently, CL has also been applied to video understanding tasks, e.g., video instance detection [8], video object detection, and video object segmentation [9]. In this work, we focus on CL for VIS, which is a more challenging task due to the large number of occlusions and crowded scenes in videos, and the lack of temporal information in the instance mask embeddungs.

"," **High-speed light steering**. Numerous methods have been proposed for high-speed light steering over the last few decades, including ultrasound waveguides [1], multiple-photon fluorescence excitation [2][3], and selective plane illumination microscopy [4]. Using a patterned illumination, Liu et al. recently demonstrated centimeter-range scanning in \(>\)100 ms with sub-pixel spatial resolution [5]. Other recent approaches include structured light 3D scanning [6], which requires pattern projectors with millions of elements [7], optical phase coding [8], which has the issue of real-time power and bandwidth limitations, and Slope Disparity Gating [9][10][11][12][13][14][15][16][17][18][19], which requires ultrahigh-resolution cameras and, in some cases, specialized hardware, such as polarization-sensitive detectors. In contrast, our light steering technology is passive and requires no moving parts.

**Non-moving light steering**. While most light steering technologies involve moving mirrors [20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36], many alternatives do not require moving parts. Reflective light steering, in which light reflected from static objects is used to steer a viewing direction [37], is passive but inefficient and suffers from high occlusion due to multipath effects. Electro-optical steering in the infrared has also been proposed, but it has narrow field of view. Selectable light-sheet uniformity is achieved by switching laser beam delivery channels with an AOTF in the visible or near-infrared [38], but this approach requires multiple visible lasers. Moreover, selective light-sheet uniformity suffers from occlusion due to reflections off objects behind the sample. OCT has been used to steer the focus of an ultrasonic transducer in a tunable AOTF for OCT-guided imaging [39], or in a fixed AOTF for rapid focus steering [40], but neither approach offers good bandwidth or temporal resolution. These approaches are still non-moving and, as a result, they have low effective bandwidth and cannot steer at the terahertz frequencies (or higher) we desire. Finally, the literature describes approaches for light steering by controlling the focal position of an ultrasound transducer, but the focus steering range and speed of these approaches are limited [41][42].

The only other non-moving light steering technology we are aware of is Slope Disparity G",,"<>Video instance segmentation (VIS) has seen significant advancements, and recent methods have primarily focused on improving instance query embeddings to address challenges such as occluded objects and crowded scenes. The approach proposed in [1] leverages query-based object detectors to achieve strong instance segmentation performance, demonstrating the effectiveness of treating instances as learnable queries. Additionally, [2] introduces MinVIS, a minimal video instance segmentation framework that achieves state-of-the-art performance without requiring video-based architectures or extensive training procedures. Furthermore, [3] proposes Prototypical Cross-Attention Networks for multiple object tracking and segmentation, which leverages spatio-temporal information to achieve superior performance on various datasets. These methods demonstrate distinct approaches to addressing the challenges in video instance segmentation and provide valuable insights into the current state of the field.

Another perspective on video instance segmentation is presented in [6], where SG-Net, a one-stage spatial granularity network, is proposed. This method explores the effectiveness of a compact architecture that shares features among the detection, segmentation, and tracking tasks, showcasing improved performance on the YouTube-VIS dataset. Moreover, [7] defends the use of online models for video instance segmentation and demonstrates their potential to achieve superior performance compared to their offline counterparts. The study conducted in [10] introduces YOLACT, a real-time instance segmentation model that delivers impressive results in terms of accuracy and speed, demonstrating the effectiveness of breaking instance segmentation into parallel subtasks and leveraging prototype masks for efficient mask generation.

In the context of temporal modeling for video instance segmentation, [4] proposes STMask, a one-stage framework that addresses the limitations of conventional one-stage video instance segmentation networks by incorporating spatial feature calibration and temporal fusion. This approach aims to enhance mask sensitivity to spatial location and exploit temporal correlation between adjacent frames. Additionally, [9] presents CrossVIS, a fast on-line VIS model that leverages a crossover learning scheme for efficient cross-frame instance-to-pixel relation learning, demonstrating a notable trade-off between latency and accuracy. These advancements in temporal modeling and online learning approaches provide significant contributions to the field of video instance segmentation.

Overall, the related work in video instance segmentation encompasses a wide range of approaches, including query-based frameworks, temporal modeling techniques, and online learning methods, each aiming to address the unique challenges associated with the task. These diverse methodologies showcase the current state-of-the-art advancements in video instance segmentation and lay the foundation for future developments and improvements in the field. <>

[1] Instances as Queries  
[2] MinVIS  
[3] Prototypical Cross-Attention Networks  
[4] Spatial Feature Calibration and Temporal Fusion  
[6] SG-Net  
[7] In Defense of Online Models  
[9] CrossVIS  
[10] YOLACT"
1052,1052," **Simulators for Autonomous driving.** Most existing autonomous driving simulators [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49]. These simulators can be broadly categorized into physics-based and neural-based. Physics-based simulators are based on the physics of the physical world, such as gravity, acceleration, and inertia. They are able to simulate complex physical systems, but they are not able to model complex real-world interactions between the SDV and other actors in the scene. In addition, they do not support closed-loop simulation, which is the main focus of our work.

**Neural-based sensor simulators.** Recently, researchers have started to use neural networks to synthesize realistic sensor data for autonomous driving. OpenRAVE [1] and OpenAI Gym [8] use a neural network to generate sensor data from a single recorded driving log. However, it is limited to a single sensor modality, e.g., LiDAR, and does not support open-loop evaluation. CARLA [14] is an open-source sensor simulator that takes a recorded driving logs as input and generates sensor data, but it does not model the interactions between actors and SDV in the way we do. In contrast, UniSim leverages a novel neural feature grid to model the interaction between SDV, actors, and other sensor modalities, and is able to generate high-quality sensor data beyond what can be collected safely in the real world. VISTA [47] and VISTA 2.0 [48] also use a differentiable renderer to generate data for SDV policy learning, but their focus is on policy learning rather than sensor data generation. Our work is complementary to these works, as we can also use them to improve the quality of the generated sensor data.

 and [46] are the closest works to ours in the sense that they both use a driving log as input, but both of them focus on generating sensor data in a fully-differentiable manner. However in contrast to UniSim, VISTA does not allow the agent to modify the sensor data or the actor's trajectories in the loop, while UniSim allows the agent's actions to be modified based on previously recorded data. Moreover, both of these works do not provide a way to generate new sensor data outside of the recorded data, which limits their applicability to SDV applications.

 is the closest work to ours. It also uses a single driving log for training, but unlike us, it uses a static scene representation and a static physics engine, which makes it difficult to model interactions between SDVs and actors.

 uses a neural scene graph to represent the scene, but the scene is static"," Simulation Environments for Robotics:The robotics community has a long history of building simulators for safer and faster robot development [1][2][3][4][5]. Early works focused on modeling robot dynamics and physical forces for parameter identification and controller modelling [2][6]. Several works then developed accurate physics engines for improving robot design and motion planning [1][7][8], and for specific domains such as grasping , soft robotics [9], and SDVs . But to enable end-to-end testing of full autonomy systems, we must also simulate realistic sensor observations of the 3D environment for the robot to perceive, interact with its surroundings, and plan accordingly . Most prior sensor simulation systems use 3D-scanned or manually built synthetic environments for small indoor environments [7][3][10], and perform rasterization or ray-tracing [11] to simulate various sensor data [12][13]. For high-speed robots such as SDVs, simulators such as CARLA and AirSim [14] applied a similar approach. But due to the costly manual effort in creating scenes, these simulators have difficulty scaling to all the areas we may want to test in, have limited asset diversity (e.g., roads, vehicles, vegetation) compared to the real world, and generate unrealistic sensor data that require substantial domain adaptation for autonomy [15][16].

Novel View Synthesis:Recent novel view synthesis (NVS) work has achieved success in automatically generating highly photorealistic sensor observations [17][18][19][20]. Such methods aim to learn a scene representation from a set of densely collected observed images and render the scene from nearby unseen viewpoints. Some works perform geometry reconstruction and then warp and aggregate pixel-features from the input images into new camera views, which are then processed by learning-based modules [21][22]. Others represent the scene implicitly as a neural radiance field (NeRF) and perform volume rendering with a neural network [23][24][25][23]. These methods can represent complex geometry and appearance and have achieved photorealistic rendering, but focus on small static scenes. Several representations [26][27][28][29][30][31][32] partition the space and model the volume more efficiently to handle large-scale unbounded outdoor scenes. However, these works focus primarily on the NVS task where a dense collection of images are available and most test viewpoints are close to the training views, and focus on the static scene without rendering dynamic objects such as moving vehicles. In contrast, our work extends NVS techniques to build a sensor simulator from a single recorded log captured by a high-speed mobile platform. We aim to render image and LiDAR observations of dynamic traffic scenarios from new viewpoints and modified scene configurations to enable closed-loop autonomy evaluation.

Data-driven Sensor Simulation for Self Driving:Several past works have leveraged computer vision techniques and real world data to build sensor simulators for self-driving. Some works perform 3D reconstruction by aggregating LiDAR and building textured geometry primitives for physics-based rendering [33][34][35][36], but primarily simulate LiDAR or cannot model high-resolution images. Another line of work perform object reconstruction and insertion into existing images [37][38][25][39] or point clouds [40][41][42][43], but these methods are unable to render sensor data from new views for closed-loop interaction. DriveGAN [44] represents the scene as disentangled latent codes and generates video from control inputs with a neural network for differentiable closed-loop simulation, but is limited in its realism and is not temporally consistent. AADS [45] and VISTA 2.0 [46][47][48], perform multi-sensor simulation via image-based warping or ray-casting on previously collected sensor data to render new views of the static scene, and then insert and blend CAD assets into the sensor data to create new scenarios. These approaches, while promising, have visual artifacts for the inserted actors and rendered novel views, resulting in a large domain gap. Neural Scene Graphs (NSG) [19] and Panoptic Neural Fields (PNF) [17] represent the static scene and agents as multi-layer perceptrons (MLPs) and volume render photo-realistic images of the scene. However, the single MLP has difficulties modelling large scale scenes. These prior works also focus on scene editing and perception tasks where the SDV does not deviate significantly from the original recording. Instead, we focus on multi-sensor simulation for closed loop evaluation of autonomy systems, and specifically design our system to better handle extrapolation.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]"," **Robotics Simulation.** Robotic simulation has a long history in computer vision and robotics research [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48].

**Sensor Simulator.** Sensor simulation has been a long-standing research area in computer graphics. Early work in this area focused on building 3D models of static scenes [13][4] or rigid bodies [2][9]. More recent work has focused on modeling dynamic scenes, such as autonomous driving [14][10] and embodied AI [10]. However, these methods are limited to static scenes, and do not consider the interactions between actors in the scene.

Recently, several works have focused on simulating dynamic scenes. OpenRAVE [1] is a planning engine for autonomous robotics, which provides a unified framework for planning, visualization, planning, planning and control. OpenAI Gym [8] and Gazebo [7] are dynamic multi-robot simulators, which are designed for multi-sensor simulation. iGibson 2.0 [3] and Webots [4] are mobile robot simulators that support object-centric simulation of everyday household tasks. These methods are not designed for closed-loop sensor simulation, and are not optimized for safety-critical scenarios. In contrast, UniSim is designed for real-world sensor simulation.

 is the closest work to ours, as it is also a sensor simulator. However, it is not designed to simulate the interactions of actors and the SDV in a closed loop scenario.

"," Unpaired Image-to-Image Translation.**Generative methods.** The pioneering work [1] introduces a generative adversarial network to address image-to-image translation. Later, Pix2pix [2] replaces the L2 loss in GANs with an adversarial loss, and introduces a cycle-consistency loss for generating content-preserving images. By alternating two different discriminators for content loss and cycle loss, DualGAN [3] is proposed to address the mode collapse problem. CUT [4] uses a cycle-consistency loss with additional spatial consistency constraint. DualUnet [5] introduces a dual network that takes the latent code from another network as input to facilitate the translation of high-resolution images. CycleGAN  employs a latent code as input to bypass the cycle-consistency. AttentionGAN [6] and Vector-SymbaticGAN [7] utilize attention mechanism and vector-symmetric attention, respectively, to allow generating images along the path and reducing the domain discrepancy. UGAT-IT [8] introduces an attention-based module to further generate more content-preserving images. [9] introduces an instance-wise normalization layer to generate images that are more aligned with a single object. Contrastive [10][11][12][13][14][15][16] and disentanglement [17][18][19][20][21][22] methods focus on preserving content through the use of contrastive or disentanglement loss. Other methods include the multi-source-domain translation [23][24], the instance-level translation [25][26], and the multi-modal translation [27].

**Reconstruction methods.** Recovering a source image from a given target image is essentially a problem of mapping between the two domains and has been studied in multi-marginal GANs [28]. Diverse images are synthesized by using different weights assigned to different domains in StarGAN [29][30]. Diverse images are generated by introducing a memory bank in Pix2pixHB . Low-quality samples are first rejected to obtain an augmented dataset. The image generation process is extended in a pair of GANs where the samples in the augmented dataset are assigned higher weights and content-preserving images are generated by an adversarial discriminator [31].

**Continuous methods.** Continuous methods employ a continuous latent space and generate images along a continuous path. UNIT [32] and Stargan-AD [33] employ distance learning between two latent spaces. HLIS [34] is proposed to improve diversity in image translation by adding a flow-based module to the adversarial network. DLOW [35] improves",,"<>
Neural simulation techniques, such as UniSim, are becoming essential for testing autonomy systems, particularly for self-driving vehicles (SDV). The need to generate safety-critical scenarios beyond real-world data drives the development of synthetic closed-loop simulations [1]. Open-source software architectures like OpenRAVE [1] provide a seamless platform integrating 3-D simulation, visualization, and control, beneficial for autonomous robot applications. Additionally, Webots™ [4] offers a prototyping environment for simulating mobile robots, enabling the transfer of control programs to real robots. Furthermore, iGibson 2.0 [3] emphasizes object-centric simulation for robot learning and development, especially for household tasks, demonstrating a potential aspect for sensor-equipped vehicle simulations. Gazebo [7] serves as a 3D dynamic multi-robot environment for creating complex worlds, aligning with the need to simulate interactions among actors in closed-loop scenarios like the one mentioned in UniSim.

Complementary to simulations for robot tasks, software tools like OpenAI Gym [8] provide a benchmark toolkit for reinforcement learning research, acting as a potential platform for evaluating autonomous navigation algorithms. Furthermore, Habitat [10] offers a platform for training embodied agents in photorealistic 3D simulations, valuable for testing agent interaction and navigation scenarios. Moreover, the technical advancements in differentiable simulators, like ChainQueen [9], demonstrate the potential for incorporating gradient-based optimization algorithms for solving inverse problems, a valuable feature for optimization tasks in sensor simulation. Lastly, LiDAR-based simulator frameworks, such as LiDARsim [34], address the challenges involved in creating realistic LiDAR simulations, potentially providing a favorable solution for sensor-equipped vehicles in realistic environments.

In addition, novel rendering techniques, such as the neural simulation engine DriveGAN [44], aim to create high-quality, controllable simulations that are particularly relevant for autonomous vehicle development. The use of data-driven algorithms, as demonstrated in AADS [45], represents a scalable way to build simulators and enable training and testing of autonomous driving systems. Furthermore, learning robust control policies using data-driven simulation, as presented in [46], addresses the need for training autonomous vehicle control policies using sparse rewards, which are critical for the safe development of autonomous driving systems.

Overall, the related work demonstrates a diverse range of simulation and control techniques that can be instrumental for the development and testing of autonomy systems, including those specifically geared towards sensor-equipped vehicle simulations. These approaches reflect the ongoing efforts to create realistic, controllable, and scalable simulators for autonomous vehicles, addressing the challenges mentioned in the UniSim paper."
3505,3505," **Equivariance in CNNs.** The equivariance property of CNNs has been extensively studied in the literature. A number of methods have been proposed to obtain rotation equivariant CNNs [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40]. In this paper, we focus on the problem of depth and normal predictors.

**Supervised Depth and Normal Predictors.** In the supervised setting, CNNs have achieved state-of-the-art performance on a wide range of tasks. However, the performance of these models is limited by the fact that they are trained on synthetic data generated by cropping and resizing images. To address this issue,  proposed a regularization technique that enforces the output of the network to be invariant to cropping, resizing, and translation transformations. This regularization is based on the assumption that the input and output images are symmetric under certain transformations. In practice, however, this assumption does not always hold in practice. In particular, when cropping the input images during training, the network tends to ignore the symmetric property of the input image. In this work, we propose to address this problem by introducing an averaging procedure to the loss function. Our method does not incur extra cost during training and can be applied to both supervised and semi-supervised learning.

 proposed to improve the robustness of CNN-based depth and normals predictors by introducing a self-consistency loss. This loss is inspired by the idea of contrastive learning [29] and has been shown to be effective in self-supervision [30][30]. However, it is unclear how to extend this loss function to the case of dense prediction. In contrast, our loss function does not require any data augmentation and is applicable to both CNN and Transformer architectures. Moreover, our approach does not introduce any additional cost.

 recently proposed to use a contrastive loss to learn invariant feature representations. Their loss function is designed to minimize the distance between the embeddings of different views of the same image while maximizing the distances between images of different images from different views. While this loss can be viewed as a form of self-training, we show that it does not enforce the invariance of the learned feature space. Instead, we use it to regularize the network by enforcing the output to be symmetric in the feature space of the original input images. Our approach is complementary to their approach and can potentially be combined with it to further improve its performance.

 propose to use the self-contrastive loss in a supervised setting. They propose to learn feature representations that are invariant under transformations such as rotation, scaling, and rotation-invariant transformations. They show that the learned features are robust to these transformations. Their approach requires a"," Equivariance in MLEquivariance is tied closely to geometry and symmetry. The entire subject of physics is founded on concepts surrounding symmetry. A wide range of natural phenomena admits equivariance inherently since the underlying mechanism is oftentimes geometrical. As a consequence, a lot of data that machine learning deals with has the equivariance property. For example, camera photography follows simple 3D geometry rules, thus a shift in camera position leads to a shift in the photograph; the molecules and point clouds have translation and rotation symmetry in 3D, thus an SE(3) transform should not change any property. Therefore, it is natural to consider equivariance in developing machine learning models.

Equivariance in 2D computer visionConvolutional neural networks (CNN) for 2D images are shown to have the approximate translation equivariance property due to the nature of convolution . There is a line of work developing rotation equivariant 2D CNNs [1][2][3][4][5]. The transformation group for 2D rotation is the Special Orthogonal group SO(2), and the Special Euclidean group SE(2) if the translation is allowed. The derivation of group equivariance constraint typically results in steerable filters constructed from 2D harmonic bases. The convolution filter weights are parameterized as a linear combination of the harmonic bases.

Equivariance can also be achieved by parameter sharing of the neural net weights [6]. However, this approach is only possible for limited kinds of groups, such as 90-degree rotations. 2D scale equivariant CNN has been studied [7][8][9]. This is typically done by applying the same convolution kernel on several scales or constructing steerable filters from the bases. Equivariant network design method can be generalized to other groups [10][11][12][13] and has rich theory in math and physics [14][15][16]. Equivariance can also be achieved by transforming the data to canonical coordinate systems [17][18][19]. In particular, [18]transforms the data to key canonical frames of the group and averages over those frames, while we average over a random sample of the cropping transform. Transformers are the current state-of-the-art neural net architecture [20]. People have sought to combine Transformer and equivariance, resulting in Lie-Transformer [21].

In terms of applications, there is good evidence that equivariance benefits image semantic segmentation [22][23], object detection to shifting  and rotation of images [24]. Equivariance is also useful for generative modeling, for example, for normalizing flow-based generative models [25][26], and variational autoencoders [27]. Equivariance to rotation is beneficial in digital pathology . Extension to time-equivariance for video is also possible [28].

**Equivariance in self-supervised learning.** Equivariance and invariance are useful in self-supervised learning. The popular contrastive learning algorithm relies on the invariance of representations between augmented views of the same image [29][30]. More recently, people are exploring ways to use equivariance in contrastive learning [31]. Leveraging equivariance to cropping transform results in dense contrastive learning at pixel-level: PixelPro [32], DenseCL [33], and at region-level: RegionCL , Det-Con [34]. Equivariance to 4-way rotation can be jointly used with the image-level contrastive objective to improve performance [35]. Self-supervised learning from equivariance between flow transformations of the input image is also effective [36] and between matching points for landmark representation learning [37].

These works are especially successful for downstream segmentation and detection tasks. However, the advancements in these work have yet to be thoroughly explored in the state-of-the-art depth or normal predictors to the best of our knowledge [38][39][40], where the dominant paradigm is still supervised training. Inspired by prior work in SSL and segmentation, our work brings in the powerful idea of equivariance to improve state-of-the-art supervised depth and normal predictors.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]"," Equivariant Neural Networks.Equivariant neural networks have been extensively studied in the literature. In the context of CNNs, there is a rich literature on group equivariant CNNs [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40]. In this work, we focus on depth and normal predictors.

**Group Equivariance in CNNs.** Group equivariance has been studied extensively in recent years. For example, [4][4] propose to use group convolutional layers to achieve equivariances to rotations and translations. [5] propose group convolutions, which are equivariants to permutations of the group \(\mathbb{G}\). [6] propose a parameter sharing scheme to induce group equivariances. [10] propose generalizing Lie groups to convolutions. [11] prove that convolution can be equivariant to the action of any compact group. [14] provide a general theory of equivariantly convolution on homogeneous spaces and the sphere. [13] propose attentive group convolls, which learn meaningful relationships among symmetry combinations and suppress false symmetry combinations. [7] introduce scale-equivariant cross-correlations, which inject meaningful relationships between different scales. [9] propose steerable networks with steerable filters. [3] introduce steerable filter networks, which achieve equivarities to both translations and rotations. [8] propose scale-spaces, a generalization of convolutions to scale-space and semigroup groups. [15] propose the gauge equivariable transformer, which is equivarient to both position-based and content-based information. [18] propose frame averaging, which replaces the group averaging operator with an averaging operator over a small subset of group elements. [17] propose an alternative to the frame averaging operator by choosing one element from a (possibly continuous) orbit based on a fixed criterion. [24] propose ReDet, a method for object detection in aerial images, which explicitly encodes rotation equivariancy and rotation invariance. [28] propose Time-Equivant Contrastive Video Representation Learning (TEC"," Adversarial Robustness and Generalization.Many recent works show that adversarial robustness and standard (clean) accuracy have a trade-off ([4]; [3]; [5]; [6]). For instance, [3] consider a parameter \(\alpha\in[0,1]\) that determines the maximum perturbation for adversarial training. They assume that the most robust model to _standard_ accuracy is \(\alpha=0.5\), and that the worst-case robustness error (CWOA) (equivalent to our concept of AT) is \(\alpha\geq1\). They show that CWOA also increases when \(\alpha\) increases. Similarly, [2] use robustness error (robust CWOA) to train a robust classifier and show that robust CWOA increases with \(\alpha\). A different approach to trade-off between robustness and standard accuracy is used in  and [1]. They find that by increasing \(\alpha\), they are able to simultaneously improve robustness and standard accuracy. Similarly, , [5], [8], [10], [9] and [7] show that using a larger robust dataset can improve model robustness and accuracy simultaneously.

Exploring Training Details.Various works have explored the training details of AT, e.g., gradient-based ([11]), knowledge distillation ([6]), self-supervision () and ensemble-based training () methods. For instance, [11] show that AT models trained using knowledge distillation often exhibit improved robust generalization. Similarly, [6] shows that joint training the robust and standard accuracy with data augmentation leads to an improvement in robust accuracy. In addition, ,  and [1] show that the performance on clean inputs is improved by augmentation-based training in addition to robust training.

Previous works have also studied the role of individual modules in DNNs. For instance, [12] uses Bert, ResNet ([14]), VGG ([13]) and DenseNet ([14]) architectures to explore the robustness/accuracy trade-off and show that ResNet has the highest robustness and the lowest standard accuracy. Similarly, [13] propose architectures with heterogeneous capacity that outperform simple architectures with a single hidden layer and [15] propose a random weight initialization strategy which outperforms both the regular weights and the heterogeneous architecture.  and [16] study the relationship between layers and show that the classifier and the adder layers in a ResNet perform similarly. [17] measures the intrinsic robustness of individual modules by computing the gradient signal as a function of perturbation magnitude and identify two types of modules, _stable_ and _unstable_ that show drastic changes in performance under perturbations.

Fine-tuning Large Pretrained Models.Fine-tuning pre-trained models is the de facto standard method for improving a model's performance on the target task. Typ",,"<The field of computer vision has seen a surge in research efforts focused on developing equivariant neural networks, with a specific emphasis on improving depth and normal prediction models. A study by [1] introduced Rotation Equivariant Vector Field Networks (RotEqNet), a CNN architecture encoding rotation equivariance, invariance, and covariance. The study demonstrated its performance across various tasks such as image classification, biomedical image segmentation, and orientation estimation. Furthermore, [3] presented Steerable Filter CNNs (SFCNNs) designed to achieve joint equivariance under translations and rotations by design, ensuring an equivariant mapping through the use of group convolutions. Another notable development in this area was introduced by [4], where Harmonic Networks (H-Nets) demonstrated equivariance to patch-wise translation and 360-rotation using circular harmonics as a replacement for regular CNN filters, showcasing substantial advancements in rotational invariants. Additionally, the work by [19] proposed Equivariant Transformers (ETs), a family of differentiable image-to-image mappings that improve the robustness of models towards predefined continuous transformation groups, resulting in relative improvements in error rate and increasing model performance on limited data, while increasing model parameter count by less than 1%.>

<The application of self-supervision in visual representation learning has been pivotal in recent research. [29] introduced Momentum Contrast (MoCo) for unsupervised visual representation learning, offering competitive results under the linear protocol on ImageNet classification, with a significant impact on downstream tasks. Similarly, [32] proposed Pixel-level feature Clustering using Invariance and Equivariance (PiCIE), a novel framework capable of segmenting both things and stuff categories without any hyperparameter tuning or task-specific pre-processing. Furthermore, [35] introduced Equivariant Self-Supervised Learning (E-SSL), extending popular self-supervised learning methods to a more general framework that encourages non-trivial equivariance to some transformations while maintaining invariance to others, which has shown significant improvements in performance on computer vision benchmarks. These advancements in self-supervised learning have laid the foundation for impactful developments in visual representation learning and have the potential to further revolutionize the field.>

<An area of interest in computer vision research is the development of models for dense prediction tasks. [40] proposed dense prediction transformers, an architecture leveraging vision transformers for tasks such as monocular depth estimation and semantic segmentation, resulting in substantial improvements in performance compared to conventional fully-convolutional networks. Another significant advancement in this domain is the introduction of uncertainty-guided sampling by [38], where the network estimates per-pixel surface normal probability distributions, enabling the network to predict the expected value of the angular error as a measure of aleatoric uncertainty and presenting a novel decoder framework that prevents bias in training towards large planar surfaces and enhances prediction quality, especially near object boundaries and on small objects. The development of efficient and robust models for dense prediction tasks holds promise for addressing complex computer vision challenges and has the potential to impact a wide range of applications.>

<>In the context of unsupervised learning, the concept of equivariance has been instrumental in driving advancements in representation learning. [28] presented Time-Equivariant Contrastive Video Representation Learning, introducing a novel self-supervised contrastive learning method that achieves state-of-the-art results in video retrieval and action recognition benchmarks and demonstrates the effectiveness of time-equivariant representations. Moreover, [33] developed Dense Contrastive Learning (DenseCL) as a self-supervised learning method optimizing a pairwise contrastive loss at the pixel level between two views of input images, which has consistently demonstrated superior performance when transferred to various downstream dense prediction tasks, ultimately surpassing state-of-the-art methods. These developments have significantly advanced the capabilities of unsupervised learning and have the potential to impact a wide range of application domains.>"
1156,1156," Generative Adversarial Networks (GANs)  and Variational Auto-Encoders (VAEs) [1][2][3][4][5][6][7][8][9][10][11] have been widely used for image and video generation. However, these methods are limited in their ability to synthesize high-fidelity images due to the lack of controllability in the latent space. To overcome this limitation, several approaches have been proposed to improve the quality of the generated images. One approach is to use a differentiable renderer [12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42]. While these methods can generate high-quality images, they are limited by the fact that they require a large amount of training data. In contrast, NeuralField-LDM does not require any training data and can be trained in an end-to-end fashion.

**Neural Radiance Fields.** Neural radiance fields (NeRF)  is a generative model that uses a neural network to model the appearance of a 3D scene as a function of a set of voxel occupancy grids. NeRF has been successfully applied to a variety of image generation tasks such as view synthesis [26][31] and image inpainting [38; however, it is limited in its ability to model complex 3D scenes. To address this issue, Neural Radiance fields have been extended to a hierarchical latent space [42] and have been shown to be able to model more complex scenes. In this work, we extend NeRF to the scene generation task and propose a hierarchical diffusion model that can be used to model high-level 3D geometry.

 propose a scene auto-encoder that can synthesize novel views of a scene conditioned on an input image and a latent code. The latent code is represented as a neural field and the decoder is trained to map the latent code to the output image. Our approach is similar to NeRF in the sense that we also use a decoder that maps the image to a latent space, however, we differ in several important ways. First, instead of using a single latent code, we use multiple latent codes to represent a scene. Second, we train a scene autoencoder that is capable of modeling a hierarchical structure of the scene. Third, we propose a novel scene generation pipeline that combines the advantages of NeRF and diffusion models. Finally, we show how our model can be applied to various 3D content creation tasks.

 introduce a scene generation model based on a neural radiance field. Their model is able to generate realistic 3D point clouds, but requires a large number of training samples.

 present a scene representation based on an autoregressive neural network. They use an encoder-decoder architecture to learn a latent representation of the 3D space. Their approach is limited to simple scenes."," 2D Generative ModelsIn past years, generative adversarial networks (GANs) [1][2][3] and likelihood-based approaches [4][5][6][7] enabled high-resolution photorealistic image synthesis. Due to their quality, GANs are used in a multitude of downstream applications ranging from steerable content creation [8][9][10][11][12] to data driven simulation [13][14][15][9]. Recently, autoregressive models and score-based models, e.g. diffusion models, demonstrate better distribution coverage while preserving high sample quality [16][17][18][19][20][21][22][23][24][25]. Since evaluation and optimization of these approaches in pixel space is computationally expensive, [23][25] apply them to latent space, achieving state-of-the-art image synthesis at megapixel resolution. As our approach operates on 3D scenes, computational efficiency is crucial. Hence, we build upon [23] and train our model in latent space.

Novel View SynthesisIn their seminal work , Mildenhall et al. introduce Neural Radiance Fields (NeRF) as a powerful 3D representation. PixelNeRF [26] and IBRNet [27] propose to condition NeRF on aggregated features from multiple views to enable novel view synthesis from a sparse set of views. Another line of works scale NeRF to large-scale indoor and outdoor scenes [28][29][30][31]. Recently, Nerfusion [31] predicts local radiance fields and fuses them into a scene representation using a recurrent neural network. Similarly, we construct a latent scene representation by aggregating features across multiple views. Different from the aforementioned methods, our approach is a generative model capable of synthesizing novel scenes.

3D Diffusion ModelsA few recent works propose to apply denoising diffusion models (DDM) [19][20][32] on point clouds for 3D shape generation [33][34][35]. While PVD [35] trains on point clouds directly, DPM [33] and LION [34] use a shape latent variable. Similar to LION, we design a hierarchical model by training separate conditional DDMs. However, our approach generates both texture and geometry of a scene without needing 3D ground truth as supervision.

3D-Aware Generative Models3D-aware generative models synthesize images while providing explicit control over the camera pose and potentially other scene properties, like object shape and appearance. SGAM  generates a 3D scene by autoregressively generating sensor data and building a 3D map. Several previous approaches generate NeRFs of single objects with conditional coordinate-based MLPs [36][37][38]. GSN [39] conditions a coordinate-based MLP on a ""floor plan"", i.e. a 2D feature map, to model more complex indoor scenes. EG3D [40] and Vox-GRAF [41] use convolutional backbones to generate 3D representations. All of these approaches rely on adversarial training. Instead, we train a DDM on voxels in latent space. The work closest to ours is GAUDI [42], which first trains an auto-decoder and subsequently trains a DDM on the learned latent codes. Instead of using a global latent code, we encode scenes onto voxel grids and train a hierarchical DDM to optimally combine global and local features.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]"," **Generative Adversarial networks.** Generative adversarial networks (GANs) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42]. These methods have been shown to be very successful in generating high-quality 2D images. However, they have not yet been applied to 3D content creation.

**Latent Diffusion Models.** Latent diffusion models (LDFs) are a class of generative models inspired by the thermodynamic process in non-equilibrium thermodynamics [33][33]. They have been successfully applied to 2D image generation [33], and 3D shape generation [34][33], where they have been used to synthesize novel views from a single image [33]. However, these models are limited to a limited number of voxel grids, and are not suitable for large-scale 3D scenes due to the high computational cost of votel grids.


"," Image Generation.GANs [1][2][3] are the state-of-the-art image generation models. They can be trained on different datasets including raw images [4][5][6][7], simulated images [8][9][10], or partially annotated images [11][12]. A variety of methods have been proposed to alleviate the effect of out-of-distribution data by incorporating a simulator that complements GAN training. For example, [13][14][15] trained a 2D GANs on 2D synthetic scenes generated by a 3D simulator, thus preventing the model from over-fitting to un-modeled 3D effects. More recently, diffusion models [16][17][18][19][20][21][22][23] have been shown to outperform GANs in image generation. Additionally,  introduced a flow-based latent space that can be used to provide context-dependent generative flow for GANs.

3D Scene Generation.Novel views of real world scenes can be synthesized by learning to generate a feature representation for a scene [24][25]. Voxel and Neural Radiance Fields are a class of latent representations that are highly competitive for generating high-quality novel views of the scene [26][27][28][29][30][31]. These methods use MLPs to generate voxel grids that are passed to neural renderers to produce images. In this paper, we introduce the NeuralField-LDM, a voxel-based generation framework that leverages latent diffusion models for generating high-quality novel view images. We compare this model to VoxNeRF [26], pixelNeRF [25], and UrbanNeRF [29].

Diffusion Models.Diffusion models have recently been applied to perform denoising and image generation in the latent space. Denoising diffusion models [32][19][33][34][35] learns a generative process by iteratively adding noise to the latent variable and perform updates in the feature space. Other works on diffusion models include probabilistic point cloud generation [33], object generation [17], and noise control of latent variables [23]. As the original diffusion process is too slow for high-fidelity image generation, some works utilized adversarial training to accelerate the generation process. In the context of image generation, point-to-image diffusion [36] and Neural Diffusion [17] learn a linear latent vector to deterministically translate the noise to an image.

Differentiable Rendering.The differentiable rendering (DR) framework is an emerging way to combine learning based methods and data-driven representations. The most relevant works leverage latent representations of scenes to generate novel views",,"<NeuralField-LDM (NFD) presents a novel approach to scene generation by leveraging Latent Diffusion Models. This method sets itself apart by utilizing a scene auto-encoder to express image and pose pairs as a neural field, which is then compressed using a latent-autoencoder. The hierarchical diffusion model is then used to complete the scene generation pipeline [1]. While existing state-of-the-art models are successful, NFD presents a substantial improvement, especially in the areas of conditional scene generation, scene inpainting, and scene style manipulation. This marks a significant advancement in the field of 3D content creation and generation [2]>

<NFD's utilization of Latent Diffusion Models is a key contribution to the field of computer graphics and generative modeling. The ability to synthesize complex 3D environments, as demonstrated by NFD, represents a significant leap in the realm of virtual reality and robotics simulation. Previous works have demonstrated the effectiveness of diffusion models, but NFD's application in 3D scene generation presents a unique and valuable contribution [3]. The hierarchical nature of the model, coupled with its ability to efficiently compress and generate novel scenes, showcases the potential for widespread adoption and impact in the field of generative modeling [4]>

<Works such as Large Scale GAN Training for High Fidelity Natural Image Synthesis, StyleGAN-XL, and Generating Diverse High-Fidelity Images with VQ-VAE-2 have contributed significantly to advancing the state of the art in generative image modeling [5][6][7]. These papers have laid the groundwork for exploring new techniques and models for synthesizing high-quality images. Additionally, other works such as Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis and Stochastic Backpropagation and Approximate Inference in Deep Generative Models have made strides in unsupervised learning and generative modeling through deep neural networks [8][9]. By building upon the concepts and techniques proposed in these works, NeuralField-LDM can further advance the capabilities of 3D scene generation and expand its potential applications [10]>

<In summary, NeuralField-LDM introduces a novel approach to scene generation by leveraging Latent Diffusion Models and achieves substantial improvements over existing state-of-the-art models. The hierarchical nature of the model and its ability to efficiently compress and generate 3D scenes showcase the potential for widespread adoption and impact in the field of generative modeling. These advancements build upon existing works in generative image modeling and unsupervised learning, laying the groundwork for further exploration and advancement in the realm of 3D content creation and generation."">
"
1409,1409," **Representation Learning.** Representation learning has been widely studied in computer vision tasks such as 3D object detection [1][2][3][4][5][6][7], semantic segmentation [8][9][10][11][12], image classification [13][14][15][16][17][18], and road network extraction [19][20][21][22]. In this work, we focus on the representation learning of road network.

**Non-Autoregressive Neural Machine Translation.** Recently, non-autoregressive neural machine translation (NMT) [23][24][25][26][27][28][29][30][31][32][33][34][35][36] has drawn increasing attention due to its ability to model long-range dependencies in natural language processing (NLP) tasks. In this paper, we adopt the NMT framework to generate road network from images.

 proposed a non-auto-regressive sequence-to-sequence (Seq2Seq) model to model the non-Euclidean structure of road networks. However, their method is limited by the fact that they only consider the Euclidean data domain. In contrast, we propose to model both road landmarks and road topological structures in a unified representation by projecting the road network into an integer series. This representation allows us to leverage the advantages of both types of data domains while fixing the gap between the auto- and non-regression dependencies.

 first proposed to model road network as an integer sequence. Their method is based on an encoder-decoder architecture, where the encoder encodes road network and the decoder decodes images into a fixed-length vector. The decoder then decodes the vector into a latent space, which is then decoded back to the original input image. Our work differs from theirs in two aspects. First, we model both the road landmarks location and the road topology in an unsupervised manner. Second, we use the Insertion Transformer [28] to decouple the dependency of the RoadNet Sequence from the original image sequence.

 introduced the idea of decoupling the dependency between the input image and the output sequence, which has been successfully applied in many NLP tasks. Their work is based upon the Seq2seq framework, which can be viewed as a special case of our method. However their method does not consider the road networks in the latent space. In addition, their approach does not explicitly model the relationship between the road structure and the image sequence, while our method explicitly models the relation between the two data domains.

 is the first work to introduce non-localization to road network generation. They proposed to learn a mapping function that maps the image to a set of road nodes and their corresponding road network paths. Their model is trained on a large-scale dataset of road proposals generated by LiDAR sensors. Their results show that the mapping function is effective in capturing road network structures. However the performance is limited due to the limited dataset size.

 also proposed a similar approach. Their approach uses a similar mapping function"," **Vision-based ego-car (BEV) feature learning** A rising tendency is conducting self-driving downstream tasks [1][2][3][4][5][6][7][8][9] under ego-car coordinate frame. To learn ego-car feature learning from onboard cameras, [10][11] project image features to ego-car coordinate based on depth estimation. OFT [1], LSS  and FIERY [6] predict depth distribution to generate the intermediate 3D representations. [5][12][2][3] resort neural network like Transformer to learn ego-car feature without depth. In order to learn the topology of the road network under the ego-car coordinate frame, we use the straightforward method of applying LSS [4] to extract ego-car features from the multiple onboard cameras.

**Road network extraction** With the emergence of deep learning [13][14][15][16][17][18], researchers in the field have explored the utilization of DNNs to decode and recover maps from aerial images and GPS trajectories [19][20]. Moreover, STSU [21] first detect centerline from front-view image with Transformer and then predict the association between centerline with MLP layer, followed with a final merge to estimate road network. Based on STSU, TPLR [22] introduces minimal cycle to eliminate ambiguity in its connectivity representation. Existing methods spend great effort to deal with problem in non-Euclidean domain but ignore the cooperation between Euclidean and non-Euclidean. However, we create a Euclidean-nonEuclidean unified representation.

**Non-Autoregressive generation** Non-Autoregressive gen eration for Neural Machine Translation (NMT) [23] has been proposed as a solution to speed up the one-by-one sequential generation process of autoregressive models. [24][25][26] utilized knowledge distillation to help NAT model capture target sequence dependency. [27][28][29][30] proposed iterative models that refine the output from the previous iteration or a noised target in a step-wise manner. [31] kept the auto-regressive approach for global modeling, but introduced the parallel output of a few successive words at each time step. [32][33][34][35] used latent variables as intermediates to reduce the dependency on the target sequence. The current state of non-autoregressive NMT models is far from perfect, with a significant gap in performance compared to their auto-regressive counterparts [36]. But in the case of our RoadNet Sequence, the auto-regressive dependency can be decoupled, leading to both acceleration and improved performance.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]"," **Image-to-BEV translation.** The translation of images to BEV has been extensively studied in recent years. For example, BEVDet [1][2][3][4][5][6][7][8][9][10][11][12] projects images to bird's-eye-view (BEV) coordinates and leverages 3D object detection and semantic segmentation to generate BEV maps. BEVFormer [2] and BEVTiny [4] propose to learn BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. However, these methods rely on the assumption that the BEV coordinates are known, which is not always true in real-world scenarios. To address this issue, we propose a novel non-autoregressive approach to generate road network directly from images.

**Image-based mapping.** Image-based map generation has been widely studied in the past few years [13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36]. However, most of these methods are based on the conventional autoregressive paradigm, which generates each target word in parallel. In contrast, our RoadNet Sequence is an integer series, which can be viewed as a special case of the non-auto-regressive model.


 where the target words are generated in parallel by projecting both Euclidean and non-Euclidean data into an integer sequence.

.

"," **Image Compression:** Modern video codecs (e.g., HEVC [2] and VVC [1]) have not only introduced powerful tools to enhance the compression performance, but also designed the comprehensive architecture for data compression (e.g., DCU and CU [2]). However, the hand-crafted codecs are designed for humans' viewing, while some researchers ([7]; ; [3]; ; [4]; ; ; [5]; [6]; [10]) proposed end-to-end trainable methods for learning image compression. Among them, [8] proposed a Transformer-based image compression framework (i.e., TIC) that builds on top of the variational auto-encoders (VAEs) with non-local [6] and hyperprior modules (i.e., hypercoders). However, it is difficult for the image compression method to achieve significant performance improvement without the underlying VAE framework, which is independent from the task-specific hypercoders. Besides, the encoder and decoder of the current Transformer-based image compression model cannot be optimized to different tasks without fine-tuning. We aim to optimize the Transformer-based image compression model to different tasks without fine-tuning.

**Image Compression for Machines:** Learning-based image compression methods can achieve more advanced compression performance than traditional image compression methods. Despite the superiority of learning-based image compression methods, it is still time-consuming to decode the image due to the pixel-wise decoding, thus the efficient machine vision methods require decoding at inference time. Some researchers ([16]; [14]; [15]) proposed a variety of Transformer-based image compression codecs for machine vision tasks, and some of them focus on the parameter sharing between the human and machine codecs, and some of them focus on compression at the pixel level. These researchers mainly aim to improve the compression performance by fine-tuning the image compression model, which is time-consuming for multiple machine tasks. For better compression performance and time-saving, we transfer the Transformer-based image compression codec to the machine vision tasks in an end-to-end learning manner without fine-tuning the codec.

**Image Compression with Prefixes:** Prefix compression is an efficient compression method for reducing the transmission overhead in wireless and IoT communications. In the image compression field, some researchers ([17]; ; [18]; ; [19]) proposed a prefix compression method to reduce the image's bitrate via transmitting compressed prefixes instead of compressed images. To adapt prefix compression to the image compression tasks, we aim to transfer a pre-trained image compression model for the human task into different prefix compression tasks for the machine tasks.

**Prompt Tuning:** Prompt tuning ([23]) is an efficient and effective alternative to full fine-tuning for large language models (e.g., GPT-3 [21",,"<Related work>

The task of translating images to a road network has gained notable attention due to its significance in generating high-definition maps for precise localization and navigation. Existing methods have faced challenges in effectively merging conflicting data structures of Euclidean (e.g., road landmarks location) and non-Euclidean (e.g., road topological connectivity) features. This has led to the development of novel approaches, such as Orthographic Feature Transform for Monocular 3D Object Detection [1], which focuses on reasoning about the world in 3D for improved object detection in challenging scenarios. Another relevant work is BEVFormer [2], which introduces a new framework for learning unified Bird's-Eye-View (BEV) representations, leveraging spatiotemporal transformers for multiple autonomous driving perception tasks. Additionally, the research presented in Learning Ego 3D Representation as Ray Tracing [3] offers a novel architecture for ego 3D representation learning from unconstrained camera views, providing a compelling approach for 3D semantic representation extraction from multiple cameras collectively into the BEV coordinate frame.

Furthermore, the BEVDet paradigm introduced in BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View [4] is noteworthy for its focus on 3D object detection in the Bird's-Eye-View, demonstrating significant improvements in accuracy and time-efficiency. Additionally, the work on Translating Images into Maps [5] proposes a novel transformer network to map images to an overhead map or bird's-eye-view of the world, achieving state-of-the-art results for instantaneous mapping. Moreover, FIERY: Future Instance Prediction in Bird’s-Eye View from Surround Monocular Cameras [6] presents a probabilistic future prediction model from monocular cameras, catering to the crucial task of predicting future behavior of road agents for navigating safely.

Continuing with relevant research, SUIT: Learning Significance-guided Information for 3D Temporal Detection [7] emphasizes the importance of effective and efficient utilization of sparse format for temporal fusion in 3D object detection from LiDAR point clouds. Besides, VectorNet [8] introduces a hierarchical graph neural network for behavior prediction in dynamic, multi-agent systems, operating on the vectorized high definition maps and agent trajectories to avoid computationally intensive encoding steps. Also, MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction [9] presents a structured end-to-end Transformer for efficient online vectorized HD map construction, achieving the best performance and efficiency with only camera input among existing vectorized map construction approaches.

In addition to these works, Pseudo-LiDAR From Visual Depth Estimation [10] addresses the problem of 3D object detection by proposing a novel approach to convert image-based depth maps to pseudo-LiDAR representations, significantly improving the accuracy of objects within specific ranges. The research on Learning Ego 3D Representation as Ray Tracing [3] offers a promising solution for ego 3D representation learning from unconstrained camera views, demonstrating superior performance in camera-based 3D object detection and BEV segmentation.

<>"
5390,5390," **Semantic localization.** Semantic information has been shown to be a powerful tool for image-based localization [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33]. In this work, we focus on semantic localization, which aims to localize a camera in a given query image given a set of semantic labels. In this section, we briefly review the most related work in this direction.

**Semantic segmentation.** The most relevant work to ours is from [4], where the authors propose a self-supervised approach for semantic localization. In their approach, a semantic segmentation network is first trained to predict a semantic label for each pixel in the query image, and then a matching network is trained to find the most similar image in the database by minimizing the label inconsistency between the predicted labels and the ground truth image. In contrast, we propose a new localization framework, SegLoc, that learns a robust image segmentation-based representation for localization in the context of privacy-preserving visual localization. We show that our localization framework can achieve state-of-the-art localization results while using a compact 3D map representation, while preserving the privacy of the original images.

 propose a localization framework that uses semantic segmentations to create a compact and robust 3D scene representation. However, their method requires a large amount of training data, which is not available in many real-world applications. In addition, their localization pipeline is not end-to-end trainable and requires a pre-trained localization network, which makes it difficult to scale up to large-scale applications. Our approach, on the other hand, does not require any training data and can be trained in a fully trainable manner. Moreover, our localization pipeline does not need any pre-training and thus is more robust.

 is a recent work that proposes a localization method based on the idea of learning a dense 3D point cloud representation. Their method is based on a 3D convolutional neural network (CNN) and is trained in an adversarial manner to reconstruct the 3D points of the scene from the 2D image features. In particular, they use the reconstructed point cloud as a proxy for the original image and train the localization network to match the points in the point cloud with the image features extracted from the CNN features. This approach is limited by the fact that the point clouds are reconstructed from 2D CNN features, which are not suitable for privacy preserving localization tasks. In our work, instead, we use the segmentation results of the semantic labels to reconstruct a compact, accurate, and privacy preserving 3D representation of the entire scene.

 proposes a privacy preserving method for image retrieval. Their approach uses a generative adversarial network (GAN) to generate images that are indistinguishable from the original ones while preserving their semantic information. In the process, the generated images can be used for localization. However"," **Semantic-based Visual Localization.** Semantic segmentation is used in structure-based localization methods as a way to facilitate feature selection or matching [1][2][3], to filter 2D-3D matches by maximizing the semantic consistency between 2D images and 3D models [4][5] or to improve keypoint tracking . In these works, the pre-trained segmentation model is used mainly to filter matches or to improve SfM/VO, hence they still rely on keypoints descriptors. Similar to FGSN [4], our model learns robust image segmentation in a self-supervised manner exploiting label consistency between matched keypoints. Contrary to FGSN, our model provides both global and local representations, resulting in a full localization pipeline.

**Semantics-based retrieval and pose approximation.** To cope with extreme environmental, seasonal, and illumination changes in place recognition and image retrieval, several methods leverage image-to-image translation to handle the domain shift between the database and query images [6][7][8][9][10][11]. Other methods directly aim at obtaining image representation by leveraging weather-invariant semantic [12][13][14][15] or geometric information [16]. In particular, [14] describes images through histograms of semantic classes from pre-trained semantic segmentation, while LoST [13] performs a semantic-based pooling of convolutional features. Closest to our work, [7] and  train global representations within a deep metric learning framework and utilize semantic segmentation as an auxiliary task to infuse semantic information. Instead, we learn a finite set of (not necessarily semantic) classes to perform image segmentation from which we build local and global representations.

**Pose refinement.** Pose refinement approaches obtain an accurate camera pose estimate from an initial approximate pose via image alignment. In contrast to early methods based on handcrafted features [17] or pixel intensities [18], more recent methods learn deep features through direct feature alignment suitable for such pose refinement [19][20] or cast the camera pose localization as a metric learning problem. [14] proposed a semantic-based pose refinement relying on a pre-trained model, handcrafted global descriptors, and a geometric prior. Our pose refinement is inspired by PixLoc [19], except that instead of using multi-scale deep features, we align 1D features (labels) by minimizing a reprojection error as a function of label inconsistency.

**Privacy-preserving localization.** Storing the 3D maps on the cloud or sending images or descriptors from mobile devices to a server raise the question of privacy. As shown in [21], detailed and recognizable images of the scene can be obtained from sparse 3D point clouds with local descriptors. Geometry-based matching methods [22] do not rely on visual descriptors to localize, hence they are less subject to privacy issues. Still, [21] shows that depth and color are sometimes sufficient to recover details in a scene. Learning-based pose regression or scene point regression models [23][24] do not explicitly store the 3D map and, thus, partially avoid the privacy issues. Yet, according to [25], since these models memorize the scenes quite well, model inversion is often possible. Given a set of pre-selected scene landmarks, [26] learns to detect them and to regress the associated bearing vectors used by geometric camera pose estimation. However, this method still faces the same scaling issues as regression methods.

To address privacy, [27] propose to transform 3D point clouds into 3D line clouds, thus obfuscating the scene geometry. However, according to [28], a significant amount of information about the scene geometry is preserved in these line clouds, allowing to (approximately) recover image content.  propose a cloud-based mapping solution to preserve the privacy of users by hiding critical content of the input images. As the recovered pose may also be considered as sensitive, [29] perform a partial estimation of a 3DoF pose on a single dimension against a partial map. These partials maps are distributed on distinct servers so that the 6DoF pose can only be recovered on the user side. However, they do not tackle the privacy of the partial maps directly. In addition, these approaches do not consider private information contained in the query images, which could,, allow an attacker to track individuals and to study their behavior. Concerning privacy preservation of the query, [30][31] show that it is possible to reconstruct the original image from local image features. To address this, [32] propose to obfuscate the appearance of the original image by lifting the descriptors to affine subspaces. [33] propose to replace 2D points in the query image with randomly oriented 2D lines passing through the given point. This allows them to address privacy of both the query and the map. By relying on class labels, where only labels are kept in the map and hence making it impossible to recover fine details that could reveal private information, our SegLoc representations jointly tackles query and map privacy.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]"," **Visual localization.** Visual localization has been extensively studied in computer vision and robotics for a long time [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33]. In this section, we focus on methods that are most relevant to our work.

**Image retrieval-based localization** methods [4][4] have been widely used for visual localization in the past few years. These methods retrieve similar images from a database of geo-tagged images and use them to approximate the pose of the query image by minimizing the pose difference between the query and retrieved images. However, these methods are sensitive to lighting and weather changes and are not robust to occlusion and viewpoint changes. To address this issue, recent methods have proposed to use image retrieval to learn domain-invariant features [8][7], to learn a domain-independent feature extractor [9][8], to use the extracted features to perform domain adaptation [9], or to learn feature disentanglement [10]. In addition to the above methods, recent works have also explored the use of generative adversarial networks (GANs) [7][11] for image translation [6][6] and domain adaptation. In this paper, we propose a new localization framework that leverages image segmentation for privacy-preserving visual localization. We build upon the correspondence-supervised, fine-grained segmentation approach from [4] and make it more robust by introducing discriminative clustering and additional consistency regularization terms.

 is a recent work that proposes to use semantic segmentation to localize cameras in cross-weather and cross-season environments. They propose to use an adversarial network to disentangle domain-unrelated and domain-related features across extremely changing appearances. In contrast, our approach leverages segmentation-based representation learning to learn compact, compact, and privacy preserving representations for localization.

"," **Vision-Language Pre-training.** With the great success of natural language processing [11], researchers are increasingly interested in applying powerful language-based models to solve computer vision tasks, leading to the surge of interest in vision-language pre-training (VLP). To this end, various studies [5]; ; [6]; [3]; [2] apply NLP pre-training methods on the VLP task to align the visual and textual features. Furthermore, they usually learn cross-modal alignment by optimizing the joint feature space of the two modalities through contrastive loss [6]; [3]; [4] or a combination of contrastive and classifier loss . In addition, the cross-modal representation learning in VLP can also be considered as learning to encode the information in other modalities, which has attracted many works. In these works, a special type of cross-modal alignment is achieved by fine-tuning or freezing the pre-trained weights of a uni-modal model or generating the other modalities by a separate decoder or a generator ; ; ; [8]; ; ; [10]; [7]. However, these methods lack a direct alignment of visual and language features for better representation learning in the cross-modal modality. Differently, our SCL task aims to help the model learn the corresponding features of the masked visual tokens by performing cross-modal alignment in a uni-modal model. In other words, our SCL task seeks to learn an explicit multi-modal alignment between image and text features through semantic completion learning.

**Masked Modeling Tasks.** Masked modeling tasks have been widely used in natural language processing to learn word representations. These tasks are based on Masked Language Modeling (MLM) and generally consist of randomly removing words from a text and training the model to predict them [11]; [9]; . Besides MLM, MLM variants have also been introduced in the VLP area to obtain effective cross-modal representations. Previously, masked visual feature learning has been investigated as a special MML task . Some recent works introduce different variants of MLM tasks into VLP to learn global-to-local representations. For example, Masked Object Prediction (MoP) is introduced as a masked MML task to align the visual features with textual features . Meanwhile, Masked Language Prediction (MLP) is proposed to perform MML task on the language encoder [9]; [12]; [13]. However, these methods rarely consider the influence of the semantics of masked features on modeling. Different from the previous MML works, we introduce semantic completion learning into MML task to align the missing semantics of masked samples. The proposed SCL task is a special case of MoP and MLP, where the MLM and MLR tasks with an extra prediction sub-task are proposed, respectively.

",,"<In recent years, there has been a significant focus on privacy-preserving visual localization techniques, driven by the growing concerns about personal data security. Various research efforts have explored novel approaches to address these concerns, including leveraging image segmentation, semantic understanding, and geometric information. For instance, [1] introduces methods to improve the speed and quality of image feature matching by employing semantic scene segmentation. The study proposes capturing semantic scene context and learning correct match ability of descriptors from these semantic contexts, demonstrating significant improvements in accuracy and speed benefits. Additionally, [4] presents the Fine-Grained Segmentation Network (FGSN), a neural network that provides image segmentations with a larger number of labels, training in a self-supervised fashion, and outputting consistent labels. These advancements contribute to the development of more discriminative and robust scene representations for privacy-preserving visual localization.

Another research area focuses on the use of semantics-aware localization under challenging perceptual conditions. [3] introduces a discriminative holistic image representation approach, enabling precise segmentation of stable image regions over varied datasets, a valuable capability for long-term visual navigation. Furthermore, [5] introduces a visual localization algorithm that combines structure-based and image-based methods with semantic information, demonstrating significant improvements in challenging viewing condition variations, such as weather, illumination changes, and day-night variations.

There is also a substantial body of work on domain adaptation for semantic-aware image-based localization. [7] presents a novel multi-task architecture to fuse geometric and semantic information into a multi-scale latent embedding representation for visual place recognition. The study effectively employs a feature discriminator for adversarial training to achieve domain adaptation, outperforming state-of-the-art baselines for retrieval-based localization. Additionally, [8] proposes a domain-invariant feature learning method for retrieval-based localization, introducing a feature consistency loss to train encoders to generate domain-invariant features, thereby overcoming challenges posed by varying environmental conditions.

Furthermore, recent research has emphasized the essential role of privacy-preserving solutions for cloud-based localization. [27] proposes a lifting approach to transform the map representation from a 3D point cloud to a 3D line cloud, effectively obfuscating underlying scene geometry while enabling robust and accurate pose estimation. Additionally, [32] introduces privacy-preserving image features via adversarial affine subspace embeddings, dropping constraints from each feature descriptor by embedding it within an affine subspace, significantly enhancing privacy for visual localization and mapping applications.

In summary, the research landscape for privacy-preserving visual localization is dynamic and multidisciplinary, spanning areas such as semantic understanding, domain adaptation, and privacy-preserving feature representations. These advancements showcase the ongoing efforts to develop robust and secure localization techniques amidst privacy challenges.</>"
3184,3184," **Self-supervised learning for skeleton-based action recognition.** Recently, many works [1][2][3][4][5][6][7][8][9][10] have been proposed for unsupervised representation learning for 3D human action recognition, which can be roughly divided into two categories: contrastive learning and contrastive loss. Contrastive learning [11][12][13] aims to learn representations by contrasting positive samples against negative samples, which is widely used in many computer vision tasks, such as image classification and object detection. In this paper, we adopt the contrastive framework for skeleton action representation learning.

**Tempo prediction for video understanding.** Tempo prediction has been widely studied in the field of video understanding [14][15][16][17][18], which aims to predict the visual tempo of each frame in a video sequence. For example, Temporal Pyramid Network (TPN) [15] models the temporal scale of different actions by calculating the relative visual tempo between adjacent frames. In [16], a motion-driven video representation learning method is proposed to learn the motion representation by predicting the relative video speed. However, these methods directly predict the video speed without considering the temporal evolution of video frames, which may suffer from insufficient feature representation issue. Different from these methods, we propose a novel RVTL task to model the relative temporal evolution in skeleton sequence data, and propose a new RVTCLR framework to learn more effective feature representation.

 propose to learn motion representation in skeleton data by predicting visual tempo. They design a two-stream network to predict visual tempo for skeleton data, which consists of two branches: one branch for motion prediction and the other branch for appearance prediction. In contrast, RVTL aims to explore the motion information in intra-video clips, and an appearance-consistency task to learn appearance information simultaneously.

 proposes to learn spatio-temporal features by predicting relative visual speed of each pixel in a sequence of frames. They propose to use optical flow to estimate the visual speed, and then use the optical flow as the supervision signal for skeleton feature learning. In comparison, our RVTL is more in line with human intuition, and thus provides more effective supervision signals. Moreover, we further design a new Distribution-Consistency (DC) branch to learn high-order semantics, and a new Skeleton-specific Data Augmentation (S-DA) branch for fine-grained skeleton encoding.

 is the most related work to ours. They also propose a RVTL framework, but they focus on skeleton data augmentation, while RVTL focuses on learning appearance information. In addition, they design a novel DC branch for RVTL, while we design a S-DA branch for skeleton encoding.

 also proposes to use visual tempo prediction for skeleton representation learning, but their method is based on optical flow prediction, while our method is more related to RVTL.

 designs a new visual tempo learning method for action recognition task. They focus on action recognition by predicting optical flow. In summary, our work is different from theirs in two aspects: (i) we propose RVTL"," **Self-supervised skeleton-based action recognition.** To meet the requirements of real-time recognition, it's necessary for the models to be able to directly extract dis criminative action representations from the label-free online videos. Recent progress in self-supervised skeleton-based action recognition can be summarized into two categories: pretext-task based learning [1][2][3][4][5] and contrastive learning [6][7][8][9][10]. For example, Zheng et al. [4] design a skeleton inpainting architecture to learn the long-term dynamics. Lin et al. [1] integrate multiple tasks such as jigsaw puzzle recognition to learn more general skeleton features. Xu et al. [2] propose reverse sequential predictions based on encoder-decoder structure to extract motion pattern. However, the representations learned by these methods may not be good enough, in the sense that they could be exclusively particular to the pre-designed tasks. Inspired by the success of contrastive learning in image classification (e.g., instance discrimination [11], SimCLR [12], and MoCo [13]), Rao et al.  first propose to perform contrastive learning among different augmentation-s of unlabeled skeleton data, to learn inherent action patterns. Thoker et al. [8] propose to generate contrastive pairs based on different input-representations of the skeleton sequences, i.e., graph, sequence, and image representation. By leveraging multiple views of the skeleton data, i.e., joint, bone, and motion, Li et al. [6] introduce SkeletonCLR and CrosSCLR to perform the single-view and cross-view contrastive learning. Guo et al. [7] further propose AimCLR to learn from extremely augmented skeleton sequences.

Nevertheless, none of the aforementioned methods concentrate on visual tempo, which is crucial for characterizing human action dynamics. To our best knowledge, Su et al. [10] propose motion consistency and continuity learning, which has overlap with our framework. However, the differences are obvious, which mainly lie in three aspects: (1) contrastive pairs are generated differently. In [10], speed-changed clips are considered as positive pairs, while we focus on relative visual tempo and regard these clips as negative pairs. (2) appearance information is modeled differently. In [10], appearance information is implicitly modeled in the contrastive learning process, while we design another Appearance-Consistency (AC) task specifically for spatial modeling. (3) learned features are enhanced differently. In [10], learned features are enhanced by designing a self-reconstruction based motion continuity module, while we introduce a novel Distribution-Consistency (DC) branch to guide the models focus on high-order semantics.

**Visual tempo modeling.** Visual tempo describes the speed of human movements, which has already been applied into various action recognition methods [14][15][16]. For example, Feichtenhofer et al. [14] first propose SlowFast network to explore the potential of different visual tempos. It consists of two pathways, operating at different frame rates, to capture both spatial semantics and motion dynamics. When it comes to video self-supervised learning [17][18], the potential of visual tempo in motion modeling is further verified. Yao et al. [17] utilize video playback rates as self-supervision signals and propose playback rate perception to learn spatiotemporal features in a collaborative discrimination-generation manner. The above methods usually require assigning visual tempos to each video clip according to different sampling rates, and then elaborate learning paradigms through reconstruction or prediction, but this is suboptimal in learning discriminative representations. Chen et al. [18] argue that relative speed is more in line with motion pattern. They propose a new video self-supervised learning framework (called RSPNet) to leverage the relative speed between two video clips to supervise the representation learning. Inspired by RSPNet, the proposed RVTCLR focuses on relative visual tempo learning with skeleton-specific modifications.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"," **Self-supervised Skeleton-based Action Recognition.** Recently, self-supervision has been widely used in skeleton-based action recognition [1][2][3][4][5][6][7][8][9][10]. Most of these methods are based on the contrastive learning [11][12][13], which aims to learn representations from unlabeled data by contrasting positive and negative samples. For example, MS2L [1] integrates motion prediction, jigsaw puzzle recognition, and contrastive loss to learn more general representations. MS2LR [1], MS2CLR [1]), and MS2C [1]) use the jigsaw puzzles to learn skeleton features from different aspects, and the skeleton-specific data augmentation to learn high-order semantics. PCRP [2] proposes to use the encoder-decoder structure to extract more discriminative temporal patterns, and derives action prototypes by clustering to explore the inherent action similarity within the action encoding. CPC [4] explores an unsupervised representation learning approach for learning a fixed-dimensional representation, guided by additional adversarial training strategies. PREDICT & CLUSTER [5] and CVC-KM [6] propose to learn the representations from different views of the skeleton data, and use the cross-view consistency to improve the performance of the model. However, these methods only consider the motion information in skeleton sequences, which may suffer from insufficient feature representation issue. To address this issue, we propose RVTCLR, which directly predicts the relative visual tempo for skeleton action recognition.




"," Recognizing chemical structures is a common task in cheminformatics, enabling automated data mining and interpretation of millions of papers. Extant work on this topic is extensive, and many contributions in the literature address it from different angles. For instance, the research community has developed various classes of tools that support a variety of visualization and data formatting standards, such as 3D rendered SMILES ([1]), InChI ([3]), pictographs ([4]), and graphics ([2]). These tools help researchers visualize and analyze various molecular structures and integrate them with proprietary systems to discover more insights. In this work, we mainly focus on developing a tool for the challenging task of **automated recognition of molecular structures**.

In the past decade, researchers have made significant progress on this task ([8]; [10]; [3]; [12]; [7]; [6]; [9]). Most of these systems rely on a handcrafted approach to recognize the chemical structure based on the node position and size, among other features. Most of these systems utilize convolution neural networks to detect atoms and bonds. ([5]) also introduced an InChI conversion system to explicitly convert the image to its SMILES representation. However, these models only consider one single image as input ([6]; [9]) and all assume that each molecular structure is laid out in the same 2D coordinate system. Recently, [7] propose a graph-based representation of the molecular image and establish a data augmentation pipeline to train deep learning models on synthetic data. Although the graph allows for learning and recognizing 3D structures, most previous approaches are only trained on 2D images and fail to consider diverse 2D representations and lighting conditions.

The R2R system (; [11]) of OSRA () utilizes a hybrid rule-based and machine learning framework to identify nodes and bonds in 2D and 3D structures. Each 2D structure is represented by a vector as the input. However, its real-time performance is very poor and is difficult to scale up. The latest version of OSRA (), along with MolGrapher, utilizes pre-trained neural networks for speed and scale. The method [13] adopts a rule-based methodology to recognize the graph from an image. [14] improve their methodology by applying statistics, learning and various graph transformations. Similarly, [17] adopts a similar approach by training a multi-task convolution neural network for atomic point detection and SMILES prediction.  develop an offline open-source tool to perform image analysis, with various database and pre-processing functionalities, but no system for real-time visualization.

Another line of work aims to recognize molecular structure as image caption using SMILES, SELFIES and DeepSMILES ([4]; [16]; [5]; [2]; [9]). Although these systems have proven promising for image caption",,"<The field of self-supervised skeleton-based action recognition has gained significant attention in recent years. Various approaches have been proposed to alleviate the limitations of insufficient feature representation and overfitting issues in learning skeleton-based action representations. Lin et al. [1] introduced a Multi-Task Self-Supervised Learning framework for skeleton-based action recognition, integrating motion prediction, jigsaw puzzle recognition, and contrastive learning to learn more general representations in a self-supervised manner. Similarly, Zhang et al. [2] proposed the Prototypical Contrast and Reverse Prediction framework to perform reverse motion prediction for extracting more discriminative temporal patterns and action prototypes for unsupervised skeleton-based action recognition.

Addressing the challenges of unsupervised representation learning and expensive annotated data, Mo et al. [3] presented a novel skeleton cloud colorization technique to learn spatial-temporal features from unlabeled skeleton sequences through unsupervised representation learning. Furthermore, Wang et al. [4] explored an unsupervised representation learning approach to capture the long-term global motion dynamics in skeleton sequences using conditional skeleton inpainting architecture guided by additional adversarial training strategies. 

On the contrary, Yu et al. [8] introduced the Skeleton-Contrastive 3D Action Representation Learning method, which emphasized learning invariances to input skeleton representations and various skeleton augmentations through inter-skeleton contrastive learning. Moreover, Chen et al. [18] proposed the Relative Speed Perception for Unsupervised Video Representation Learning approach, which capitalized on exploiting the relative speed between two video clips as labels, effectively perceiving speed and learning better motion features to improve the performance on action recognition and video retrieval tasks. Collectively, these studies have made significant advances in self-supervised skeleton-based action recognition, paving the way for robust and effective representation learning approaches for this domain.>"
2780,2780," **Adversarial Examples.** Szegedy _et al_. [1] first discovered the existence of adversarial examples and proposed the Fast Gradient Sign Method (FGSM) to generate them. FGSM iteratively updates the adversarial example by adding a small perturbation to the original image. After that, many variants of FGSM have been proposed [2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19].

**Style Transfer.** Style transfer [20][21][22][23][24][25][26][27] aims to transfer the style of an input image to another image. Style transfer can be achieved by perturbing the instance normalization (AdaIN) [28][29][30][31][32][33][34][35] of the input image. For example, StyleDrop [26] perturbs the mean and covariance of the feature maps of each layer of a pre-trained DNN. AdaIN [23] and AdaIN++ [31] perturb the mean, covariance, and mean of the batch normalization layers of the DNNs.

In this paper, we focus on improving the transferability of the attack. Specifically, we propose a novel style-less attack method called StyleLess, which does not require any pre-training and can be applied to any DNN architecture. We show that StyleLess can significantly improve the attack transferability.



\[\begin{tabular}{l c l l c c c l c l \hline \l l \l c c \multirow{2}{c l}{c}{l} \multicolumn{3}{l}{2}{1}{2} \hcline{4}{3}{3} \langle{3-5}{4}{5}{6}{7}{8}{9}{10}{11}{12}{13}{14}{15}{16}{17}{18}{19}{20}{21}{22} \cline {3-10}{3-2}{5-6}{8-9}{15-9} \rangle{2-5} \multicline}{2-4}{6-8}{10-11}{14-12}{15+13}{16} \mathcal{L}_{2}^{2}-\(3}{6}+\(4}{7}-10^{9}-15}{8+11}{13-11} \\ \langled{1}{4}-1}{6,5}{10,6}-3}{11,9}{12}-13}{15,10}{14,11}{15}-2-6} \\ _hline{3}-5-7}{4-8}-4-10-15-10}-12-15,15-12,15,8}{16_-10,15_-11,19}{12,10,11}"," **Adversarial Attacks.** Adversarial attacks reveal the vulnerability of current DNNs [1]. The classic adversarial attack methods are gradient-based, such as FGSM [2] and I-FGSM [3]. C&W [4] considers optimizing the distance between adversarial examples and benign samples, and proposed optimization-based attacks. Adversarial attacks can also be performed in the physical world [5][6]. As for defending against adversarial examples, adversarial training is a popular defense method that uses adversarial examples as extra training data to improve robustness [7].

**Increasing Attack Transferability.** An intriguing property of adversarial attacks is the transferability. Ensemble-based attack  uses multiple surrogate networks instead of one network. Ghost networks [8] generates different surrogate networks by perturbing skip connection and dropout layers. Optimization methods, such as MI [9], uses a momentum-based optimization, while VT [10] introduces gradient variance to control the stability of the localized gradients. RAP [11] generates adversarial examples located in a flat loss region. Data augmentation methods, such as DI [12], uses image transformation like resizing and padding, while TI [13] considers translating image pixels. SI [14] calculates gradients with the help of several scaled benign samples. Admix [15] calculates iterative gradients by mixing the benign images with randomly sampled images.

Various network architectures and features exhibit different relationships with adversarial attacks. DNNs' linearity is believed to cause adversarial vulnerability [2], and LinBP [16] skips the nonlinear activation during the backpropagation. SGM [17] uses more gradients through skip connections in residual networks. To better leverage the intermediate layers, one can train auxiliary classifiers based on feature spaces [18][19], maximize the distance between natural images and their adversarial examples in feature spaces , or fine-tune the existing adversarial examples in intermediate layer level by ILA [20].

**Style Transfer and Instance Normalization.** Style transfer can change the style of an image to match the style of another one [21][22]. Fast feedforward networks can perform stylization with arbitrary styles in a single forward pass [23][24]. Interestingly, style transfer has a wide range of applications. AdvCam [25] uses natural styles to hide non-\(L_{p}\) restricted perturbations. FSA [26] generates natural-looking adversarial examples by using optimized style changes. Style transfer has also been used to improve network robustness by exploring additional feature information [27]. Latent style transformations can detect adversarial attacks [28]. AMT-GAN [29] proposes an adversarial makeup transfer to protect facial privacy by preserving stronger black-box transferability.

The family of instance normalization (IN) including batch normalization [30], layer normalization [31], instance normalization [32], and group normalization . Normalizations are mainly used to reduce the covariate shift, and speed model training. Recently, normalizations have been found to be related to robustness. It has been shown that batch normalization makes DNNs use more non-robust but useful features [33][34]. AdvBN proposed adding an extra batch normalization into network training to increase training loss adversarially, which enables the network to resist various domain shifts [35]. Adjusting batch normalization statistics such as the running mean and variance in the inference phase, which are estimated during training, improves robustness and defense common corruption [36][37].

Among existing style-based attacks, FSA [26] differentiates style features and content features, which is similar to our method. However, there are three significant differences between FSA and our approach: 1) FSA proposes to hide adversarial perturbations in the optimized style, while we avoid relying on any style. 2) FSA aims at enhancing the natural looking of non-\(\ell_{p}\) restricted attacks, while we focus on the transferability of \(\ell_{p}\) restricted adversarial examples. 3) Both FSA and our work are inspired by AdaIN [23], but we use IN layer differently. FSA perturbs the IN layer to search malicious styles and requires a decoder. But we use randomized IN layers to augment attacks and don't need to train a decoder.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]"," **Adversarial attacks.** Adversarial examples are generated by adding imperceptible perturbations to benign examples [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29]. The transferability of adversarial examples is a critical issue in the black-box setting, where the attacker does not know the architecture or parameters of the target model. Existing transferable attacks can be divided into two categories: white-box transfer attacks and black box transfer attacks.

**White-box Transfer Attacks.** Goodfellow _et al_. [1] first proposed the first attack method called Projected Gradient Descent (PGD) [1], which generates adversarial perturbation by maximizing the loss of the network with respect to the input image. The adversarial example is generated by applying the gradient of the original image to the adversarial image, and the loss function is defined aswhere \(\mathcal{L}(\mathbf{x})\) is the gradient on the original input image, \(\mathbf{\mathbf {x}\) is the loss term, and \(\mathbb{E}_{\mathbf{{\mathbb}_{x}^{\prime}\) denotes the loss on the input, and \(L_{\theta}\) norm is the norm of the gradient. The attack is formulated aswhere \(l_{\phi}\) and \(l\) are the loss terms. The first-order and second-order methods [1, [2] are the most widely used in the literature. The second- and third-order attacks [3][8] are more efficient and more effective than PGD [1]. For example, [3] proposed the Fast Gradient Sign Method (FGSM) [3], which is a general attack method that can generate adversarial attacks with high success rate. [4] proposed a method called Defensive Network Distillation (DN) [4], which uses distillation to improve the transferability. [8] proposed Ghost Networks [8], which use an ensemble of networks to generate adversarially perturbed examples. [10] proposed Variance Tuning (VT) [10], which improves the attack transferability by using the gradient variance of the previous iteration. [15] proposed Ad"," **Novel View Synthesis.** A variety of Neural View Synthesis (NVS) methods have been proposed in recent years. These typically rely on N-D networks to capture 3D geometric properties, with point-based methods (NeRF [1]; NeRF++ [1], MipNeRF [2], and BlockNeRF [3]) and voxel-based methods (MegaNeRF [4], TorusNeRF , and SpaceNet [5]) serving as the most popular ones. These methods, however, require fine-tuning on pixel-aligned training data [6] or 3D geometry , which is often inadequate for typical city scenes. This motivates us to directly model a wide range of scenes using a general-purpose template that does not require expensive supervision.

**Dynamic Scene Modeling.** Dynamic Scene Modeling methods [7][8][9][10][11][12][13][14][15][16][17][18] provide a general method to model dynamic scenes. Wang et al. [13] propose a Neural Groundplan (NG) network that can distinguish objects from background, and performs dynamic scene reconstruction using a novel groundplane. Wang et al. [16] propose Neural Scene Graph (NSG), which uses MLPs to encode each pair of nodes of the scene graph in a scene. Song et al. [17] propose a general approach to infer an object-centric scene representation from a single image, allowing objects to freely move in the scene. Tang et al. [15] develop Neural Groundplans (NG), a method for identifying and detecting motion in scenes by learning a dynamic scene representation from a single image. Instead of inferring motion and distinguishing the moving objects, Jain et al. [14] propose D$^{2}$NeRF, a method that distinguishes between objects in a scene and decouples the static and dynamic objects in the scene.

To further extend dynamic scene modeling to more realistic environments, a number of recent works [19][20][21][22][23][24][25][26] focus on how to distinguish between different moving objects in an environment. In particular, Song et al. [19] propose Giraffe, a method for editing and rendering the scene composed of multiple articulated objects. Liu et al. [20] propose Object NeRF, a method for editing a scene from a sequence of dynamic images, without any a-priori object knowledge. Chu et al. [21] propose Panoptic NeRF, which aims to synthesize panoptic segmentation labels from a RGB-D or RGB-I sequence. Wang et al. [22] propose Panoptic NeRF, which aims to synthesize panoptic",,"<Adversarial attacks on deep neural networks have become a significant concern due to their potential to mislead models with imperceptible perturbations. The attack transferability enables adversarial examples to attack black-box DNNs with unknown architectures or parameters [1,2]. To address this issue, various methods have been proposed to improve the robustness of DNNs against adversarial attacks. For instance, defensive distillation has been proposed to increase the robustness of neural networks, reducing the success rate of current attacks' ability to find adversarial examples [4]. Other approaches, such as ensemble-based methods and momentum-based iterative algorithms, have been introduced to boost the transferability and success rates of adversarial attacks [8,9,10].

Another set of related works has focused on generating adversarial examples with improved transferability by leveraging techniques such as input diversity, translation-invariant attacks, and linear backpropagation [12,13,16,17]. These methods aim to enhance the transferability of adversarial examples across different model architectures and datasets. Moreover, research has explored the impact of batch normalization on adversarial vulnerability and transferability. It has been shown that batch normalization can increase adversarial vulnerability and decrease transferability, thus highlighting the importance of considering non-robust features and the inherent geometry of the data [33,34,35].

Furthermore, the concept of style transfer and feature normalization has been utilized to improve the robustness of DNNs against adversarial attacks and distributional shifts. Instances of feature transforms, instance normalization, and adaptive instance normalization have been employed to achieve fast stylization and improve adversarial robustness [23,26,30,32,33]. Additionally, works have delved into the adaptation of batch normalization and covariate shift to enhance model robustness against common corruptions and domain shifts, thus improving the overall corruption robustness of DNNs [36,37].

In summary, the related work encompasses a broad spectrum of approaches aimed at addressing the vulnerability of DNNs to adversarial attacks, improving the transferability of adversarial examples, and enhancing model robustness against various types of corruptions and distributional shifts [1,2,4,8,9,10,12,13,16,17,33,34,35,36,37]. These works contribute to the ongoing efforts to develop more resilient and robust deep learning models in the face of emerging security concerns and real-world challenges.>"
1635,1635," **Pre-trained Language Models** Pre-trained language models (PLMs) ([4]) have achieved great success in many natural language generation tasks, such as question answering (QA), machine translation, summarization, and summarization ([2]). In this paper, we focus on the attribute-based controlled text generation (ATTG) task. Attribute-based CTG aims to generate text that satisfies a pre-specified attribute (_e.g._, emotions and topics). Existing methods can be roughly divided into two categories, _i.e._, fine-tuning-based and attribute classifier-based methods.

**Fine-Tuning-Based CTG.** Fine-tuned-based approaches fine-tune a PLM to satisfy a specified attribute. [1] propose a two-stage approach, which first pre-trains a language model on a large-scale dataset and then finetunes it to satisfy the desired attribute.  propose a multi-task pre-training method, which uses a task-specific pre-trained LM as the attribute discriminator. [3] propose FUDGE, which learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. [5] propose Contrastive Prefix, which utilizes a contrastive loss to guide the generation of a fixed language model to satisfy different attributes.  introduce a new attribute-specific language model, which can be trained with a small number of attribute classifiers. However, these methods require a large amount of training data, which is expensive to obtain. In contrastive prefix, [5, we propose a parameter-efficient method to address these concerns.



\[\begin{tabular}{l c c c l l l c l c c \hline \multirow{2}{c}{2}{3}{4}{5}{6}{7}{8}{9}{10}{11}{12}{13}{14}{15}{16}{17}{18}{19}{20}{21}{22][23][24][25][26][27][28][29][30][31][32]. \[cline{3}{c,c,d,c}{6,8,9,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27] \cline {2,4,5,6,7,16]}{13,8]}{14,9-12,15-16,18]}{19,10,19]}{20,11]}{21,12]}{22,19][23,25].\[l c,c_{\text{CTG}=\mathbb{E}_{\theta}(\mathbf{x}_{i},\textbf{y}_{j})\sim\mathcal{L}_{ij}\) (\(x_{i}\) is the target attribute, \(y_{i}\in\{0,"," **Attribute-Based CTG** focuses on generating sentences containing pre-specified attributes, such as sentiment and topic. As a vital demand for intelligent writing ([2]), existing efforts include fine-tuning PLMs and utilizing extra attribute classifiers. The first type usually fine-tunes separately and stores a full copy of PLM for each desirable attribute ([1]). To alleviate the storage problem, CTRL () provides 55 kinds of control codes (_i.e._, special keywords) to fine-tune one PLM for generating sentences of various styles. StylePTB () also proposes several style transfer tokens (_i.e._, a sequence of numbers) to guide a GPT-2 ([4]) to multiple styles transfer. GSum () introduces four guidance signals (_e.g._, keywords and relations) to enhance the controllability of PLMs in text summarization. Although they make successful attempts in attribute-based CTG, re-training whole PLMs could be expensive ([3]). To improve the flexibility and extensibility of the CTG model, the second type makes efforts in the inference stage. In short, utilizing extra attribute classifiers to guide PLMs in each generating step. PPLM () iteratively modifies latent representations of a GPT-2 referring to the gradient of attribute classifiers, yet notably increasing the inference time. To solve this problem, Fudge ([3]) uses an attribute predictor to adjust the output probabilities of a PLM. Similarly, GeDi () uses smaller PLMs as generative discriminators to hint a larger PLM generating sentences that satisfy desirableattributes. Despite their progress, the fluency of generating sentences tends to decrease compared with the original PLM (see SS 4.2) and extra inference time costs still existed. In comparison, utilizing Tailor, PLMs can benefit from the manner of controllability on single-attribute prompt combinations, with a negligible decrease on text quality.

**Prompt Learning** is a new paradigm in NLP summarised as ""Pre-train, Prompt and Predict"" (). In short, it guides a single PLM to solve various downstream tasks by reformulating these tasks into a text-to-text manner. Recently, the continuous prompt has attracted attention (; ; ), which usually forms as a set of continuous task-specific vectors to the input. Despite their encouraging progress, the prompt composition is rarely explored but undoubtedly important in prompt learning. In that case, a composable task could be accomplished by composing various subtasks with multiple sub-prompts (). To achieve it, PTR () introduces manual sub-prompts for entity recognition and relation classification, respectively. Then, these two kinds of prompts are composed by logic rules as a complete prompt for the relation extraction task. Unfortunately, the composition of continuous prompts is rarely explored yet has demonstrated great potential ([5]). The main difference between contrastive prefix [5] and Tailor is that the former needs attribute data to be occurred contrastively (e.g, positive and negative attribute data must be available at the same time), which might be limited for the single attribute. For multi-attribute, contrastive prefix trains a new prompt (twice the size of their single prompt) for each combination. Instead of it, Tailor only trains an extra prompt connector to enhance the combinations of single prompts. It can act as an efficient plug-and-play manner with extremely low training parameters to attribute-based CTG.

","[1, 2, 3, 4, 5]"," **Fine-tuning-based CTG.** Fine-tuned language models (LMs) have been widely used in controlled text generation ([4]; [1]; [2]). [4] propose to fine-tune GPT-2 ([4]) to generate sentences that satisfy a pre-specified attribute. [2] provide a comprehensive survey on fine-tunable text generation. [3] propose FUDGE, a flexible and modular method for controlled generation, which uses a preprocessing step to learn an attribute predictor and a sequence-to-sequence model. [5] propose Contrastive Prefix, which utilizes a set of small attribute-specific vectors, called prefixes, to steer the generation of GPT2 generation.

**Attribute-based Controlled Text Generation.** Existing work on attribute-based text generation mainly focuses on generating sentences that meet a preselected set of attributes (_e.g._, emotions and topics).  propose to use an extra attribute classifier to predict the probability of a sentence to satisfy a specified attribute.  propose a multi-attribute prompt-based method, which represents each attribute as a single-attribute vector, which is then concatenated as a whole for generating sentences satisfying a prespecified attribute in a fixed-length vector.  utilize a pretrained language model to generate a sentence that satisfies a specified set of attribute labels, which are then used to train an extra classifier for the attribute prediction task.  introduce a new attribute-conditioned language model, which generates a sentence with the attribute label as the input.  use a language model as the attribute predictor to generate the sentence with a fixed length vector.

"," **Knowledge Distillation.** Knowledge distillation is a popular approach for knowledge transfer from large pre-trained teacher models to compact student models. Early works (; ; [1]) showed that the dark knowledge of the teacher could be transferred to the student by finding a distance metric and adopting the teacher's outputs as ground-truth label. Since then, there have been many attempts to extend the knowledge distillation methods to solve a variety of problems. In recent years, there have been research efforts on fixing the student's capacity (; [5]) and expanding the pre-training context in distillation (; [4]). Specifically, it has been observed that many variants of KD suffer from the _capacity gap_ problem, in which the student model underperforms when receiving knowledge from the teacher model that is significantly larger in capacity ([3]; [2]). In contrast, [6] distills from a meta-learner that learns to guide the student model's optimization by its performance on the distilled data. Additionally, [7] uses the training data generated by a small teacher model to guide the student model's training. [8] uses the teacher model to re-weight data samples for the student model.

Instead of adapting the teacher's knowledge according to the student model's pre-trained data, we take the student model's pre-training context as the guideline for learning the knowledge that is appropriate for the student model. Our model is flexible in handling any pre-trained context of the student, whether fixed or adaptive, and can be easily combined with existing methods. As a result, our model achieves a promising performance without adding any parameters.

**Meta Pseudo Labels.** MPL ([9]) and its variants, MPL+KD () and MetaDistil ([6]), have shown significant performance improvements in neural machine translation and language modeling on several tasks, by incorporating teacher model feedback. This feedback is usually in the form of the training loss of the student model in the distillation task, suggesting that the teacher model is suitable for the student. In MPL+KD and MetaDistil, the student model's loss is calculated from the training data that have been adversarially corrupted by the teacher model, emphasizing the correlation between the teacher's and student's outputs. In this paper, we use the teacher model's output without adversarial noise, because the student model has an identical predictive distribution when the teacher model has no noise.

In previous work, distillation influence is measured by the difference between the teacher and student's outputs as the confusion matrix ([3]), or as the loss term of the teacher's output ([6]). Instead, we measure the distillation influence in terms of _training efficiency_, which is more useful in a teacher training context. Furthermore, we focus on measuring the _proportion_ of training samples that would increase the student model's generalization ability, rather than measuring",,
609,609," There is a large body of work on the use of symmetries to improve the optimization landscape of deep neural networks. [4] and [2] show that, for certain networks with ReLU activation functions, the loss landscape can be approximated as a linear combination of a set of orthogonal subspaces. [1] propose a path-normalized optimization method to make the SGD optimization landscape more smooth. [3] propose an algorithm to solve the problem of vanishing gradients by re-normalizing the activation function. [5] propose symmetry teleportation, a method that allows the network to travel a large distance on the loss level set. [6] study the effect of symmetry on the dynamics of neural networks and show that it can be used to estimate the functional dimension of the network. [7] propose to use symmetry to reconstruct the hidden state of a neural network.

In this work, we focus on the case where the input dimension \(d\) is smaller than \(\mathcal{O}(d^{d})\) and the network architecture \(\mathbb{E}(x)\) is ReLU. We show that for this case, there exist parameter settings with no hidden symmetry. We also show that the probability that these parameter settings exist decreases as the network depth increases.

 prove that for any ReLU function \(\theta=1\), for any \(d=0\), there exists a subset of \(\mathbf{f}(\theta)\) with \(f=1\) that is symmetrized by \(f\), where \(f\) is the ReLU functional. They show that this subset corresponds to the set of ReLU functions with \(\mathrm_{\theta}=\frac{1}{2\times 1}\), where \(\bm{\theta}\in[0,1]^{d\times d}\) and \(\bm{x}\) is a ReLU weight matrix. They also prove that the functional dimensions of the subset \(\mathfrac{f}{2}\times d}\) of the set \(\mathsf{f}\) is the product of the activation functions of a subset \(\Theta\) of \(f(\Theta,\bm{\Theta})\).

 show that \(\mathscr{\mathrm{f}}(\thet)=\frac{\frac{2}{2}^{d}\sum_{i=1}^{n}d^{n}\frac{i}{n}f(x,y)\),\), and they prove that \(\frac{d}{d}\frac{\sum_{j=1}{n}}f(y,x)\).

"," Several important lines of work have considered the symmetries of the parametric representations of deep ReLU networks and their implications for learning. One focus area has been in designing optimization methods for neural networks that are invariant to scaling symmetries at individual neurons. Approaches for achieving this goal have include path normalization ([1]), manifold optimization (), proceeding in a different vector space ([2]), and projection onto a normalized manifold ().

Another fruitful direction of work has been in understanding how permutation symmetries in parameter space affect connectivity of the loss landscape. [4] consider when different parameter permutations of a trained network are connected via piecewise linear paths in parameter space with low loss.  show that linearly interpolating between different permutations of a network leads to flat regions of the loss landscape. Several recent works have shown that, if permutation symmetries are taken into account, then it is possible to interpolate between networks trained from different initializations, while maintaining a low loss barrier (; ; [3]).

In Armenta & , the authors define and study the _moduli space_ of neural network functions using quiver representation theory. This theory provides a framework for extracting global symmetries of parameter space of a network architecture from symmetries of the computational graph and the activation functions involved (see also Ganev & ), ; [5] build on these ideas to define _neural teleportation_ algorithms aimed at using symmetries in the loss landscape to improve the efficiency of gradient descent in finding a minimal-loss solution. In [6] the authors argue that symmetries in the loss landscape have associated conserved quantities that impact training dynamics.

A number of works have explicitly considered which symmetries are admitted by different ReLU networks. A number of authors consider the relationship between the parameters of a ReLU network and the geometry of its _bent hyperplane arrangement_ (aka _fold set_); [7], Rolnick & , and  use these properties to reverse-engineer the parameters of certain networks up to permutation and scaling symmetries, and Bui Thi Mai &  proves that for certain architectures there exist parameter settings without hidden symmetries. In particular, in Rolnick & , the authors provide a geometric condition on the bent hyperplane arrangement of a parameter ensuring that the parameters can be reverse-engineered up to permutation and scaling, hence has no hidden symmetries. It follows nearly immediately that all depth 2 networks and a positive measure subset of any depth 3 network have no hidden symmetries. In Bui Thi Mai & , the authors prove that a positive measure subset of parameters in every non-widening (\(n_{0}\geq n_{1}\geq\ldots n_{d}\)) architecture has no hidden symmetries. In the present work, we prove the complementary result that a positive measure subset of parameters in every architecture whose hidden layers are at least as wide as the input layer (that is, \(n_{0}\leq n_{\ell}\) for all \(\ell<d\)) has no hidden symmetries. An example of a family of architectures for which the question of the existence of parameters without hidden symmetries remains unresolved after the present work is an architecture of the form (\(n_{0},n_{1},n_{2},n_{3},n_{4}\)) with \(n_{0}<n_{1}\) and \(n_{2}<n_{0}\).

[8] study the _functional dimension_ of a network parameter setting - the dimension of the space of functions that can be achieved by infinitesimally perturbing the parameters - proving an upper bound on functional dimension that we conjecture is achieved for almost all parameter settings without hidden symmetries (cf. Lemma F.6).

","[1, 2, 3, 4, 5, 6, 7, 8]"," **Path-Normalized Optimization.** Path-SGD ([1]) is a method for optimizing ReLU networks in its positive scale-invariant space, which is invariant to rescaling of weights. The authors show that the value of a path in G-space is sufficient to represent a ReLU network under mild conditions, and prove that the linear span of the value vector of all the basis paths is sufficient for optimal optimization. The same authors also prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. [2] also prove the existence of path-normalized optimization for ReLU neural networks, but they do not provide a solution to the problem of variance collapse.

**Symmetry in Neural Networks.** Symmetry has been shown to be an important factor in the optimization of neural networks ([4]; [3]; [6]; [5]). [6] show that any symmetric architecture of a neural network imposes geometric constraints on gradients and Hessians, which leads to a conservation law in the continuous-time limit of stochastic gradient descent (SGD) ([6]). [5] prove that symmetry teleportation can improve the convergence speed of SGD for optimization problems including test functions, multi-layer regressions, and MNIST classification. [3] show the existence and the degree of permutation invariance of neural network activations, and [4] show how this invariance can be used to establish low-barrier linear connectivity between SGD solutions. [5], on the other hand, prove that for any fixed architecture of feedforward neural networks with permutation and positive scaling of parameters, there exists a loss-free linear interpolation between the solution of the same class of functions. [4], on top of this work, show that for a fixed architecture, the mode of activation of a neuron can be chosen to be piece-wise linear, with as few as two segments, and that this permutation is sufficient in the loss landscape of optimization problems. [8] prove the functional dimension of feed-forward neural network ([8]), and [7] show a model reconstruction algorithm that can recover the gradient of a two-layer ReLU model from model explanations.

"," There are various ways to modify optimization to leverage symmetries. Methods include precomputed initialization (; ), minimizing _consistency loss_ between optimization trajectories of parameter settings in an invariance group (; [5]), optimizing in the (positively scale-invariant) G-space (; [2]; [1]), and normalizing neuron activations (; ; [3]).

Since hidden symmetries allow parameter points in an invariance group to map to many values in the underlying function space, networks trained with SGD tend to quickly converge to invariance groups of interest. However, when training is uninterrupted, networks can explore large parameter regions (; ; [4]; ; ), and symmetry breaking can prevent convergence (; ; ). There is also a body of work related to optimizing model architectures to minimize symmetry-breaking factors (; ; [6]; ). Our work differs from these, as it shows that even without biasing towards symmetry-breaking, networks that allow for hidden symmetries can have unconstrained function spaces.

(and ) prove that, for fixed architecture and set of initial points in the parameter space, the path-space integral curve of the training trajectory provides a linear upper bound on the loss. If \(\text{Lip}(f)\geq\epsilon\) (the number of neurons can change the bound), then the total path integral must be within \(\epsilon\) of the loss. In the context of this paper, \(\text{Lip}(f)\) is a quantity of interest, but not a well-defined notion for functions with hidden symmetries. Moreover, using the path integral as an upper bound on the loss might encourage the neural network to become less symmetric. It would be interesting to investigate how hidden symmetries affect the training dynamics of neural networks.

(; ) also show that, given the path integral of the SGD trajectory, a lower bound on the Lipschitz constant of the function can be estimated.

(; ) find methods to estimate the Lipschitz constant of a neural network's activation function. However, their methods require a minimum depth of \(k\) neurons, which is much higher than the Lipschitz constant of fully-connected neural networks.

For these works that make statements about a model's Lipschitz constant, our result in Theorem 1.1 gives a lower bound on the number of parameters to take an upper bound on the Lipschitz constant to within \(O(1)\epsilon\).

A different body of work makes statements about the probability that an initialization lies on a specified segment of the parameter space, under a given initialization distribution. () consider fully-connected networks, while (; ) focus on convolutional networks. Our results in Section 5.1 build on these works, but rely on different techniques. Our results also give upper bounds on the probability that an initialization lies on a segment, that is not a segment of any invariance group.

More generally,",,"<<Related work>>

The study of hidden symmetries in neural networks has garnered significant attention due to its implications on the representation and optimization of neural network architectures. Prior research has delved into the exploration of symmetries in neural networks, such as permutation of neurons and positive scaling of parameters [1], as well as the impact of invariance to positive scaling on the optimization process [2]. Furthermore, recent work has focused on addressing the challenge of establishing low-barrier linear connectivity between Stochastic Gradient Descent (SGD) solutions in neural networks through methods like REPAIR (REnormalizing Permuted Activations for Interpolation Repair) [3]. Additionally, efforts have been made to understand mode connectivity in the loss landscape of deep nets and the implications for the optimization process [4]. Notably, studies have also explored the use of symmetry teleportation for improved convergence speed in optimization problems, including multi-layer neural networks [5]. Moreover, insights have been gained into the dynamics of neural network parameters during training, emphasizing the impact of symmetries present in the network architecture and their relationship to conservation laws [6].

Furthermore, a key focus has been on the reconstruction of neural network models from gradient-based explanations, shedding light on the tension between maintaining model secrecy and offering model explanations [7]. Additionally, there has been a comprehensive study on the functional dimension of feedforward ReLU neural networks, highlighting the inhomogeneous nature of symmetries across the parameter space and the examination of when the functional dimension achieves its theoretical maximum [8]. These collective efforts contribute to a deeper understanding of the symmetries and hidden structures within neural networks, addressing critical questions concerning their representation, optimization, and dynamics during training.

Overall, the body of related work provides valuable insights into the hidden symmetries of neural networks, shedding light on their impact on network representation, optimization, and training dynamics. The studies offer diverse perspectives on the role of symmetries in neural network architectures, providing a foundation for further exploration and understanding of these fundamental properties in the context of deep learning."
4483,4483," **Visual Emotion Recognition.** Visual emotion recognition has been studied for a long time [1][2][3][4][5][6][7][8][9]. Most of the existing methods are based on hand-crafted features [10][11][12]. Recently, deep learning based methods have been proposed to learn discriminative features from large-scale datasets. For example, Wang _et al_. [13] proposed a deep metric learning method to learn a shared embedding space for both emotional and non-emotional images. However, most of these methods only focus on recognizing emotions from images. In contrast, our work aims to generate emotional images from text.

**Style Transfer.** Style transfer aims to transfer the style of a specific domain to another domain [14][15][16][17][18][19][20][21][22][23][24]. In this work, we focus on the style transfer between images and texts. Previous style transfer methods mainly focus on transferring the style from the text to the image. For instance, StyleFormer [20] learns a style embedding for each image and transfers the style by concatenating the text embedding and the image embedding. Different from StyleFormer, our model learns the embedding from the image and the text separately. Besides, we use a transformer architecture [25][26] to learn the embeddings from the images and the texts separately. Differently, we transfer the emotions from the texts to the images in an end-to-end manner. We also propose a new loss function to encourage the model to generate images with appropriate emotional content. Our work is also related to text-image generation [27][28][29]. Different from these works, we aim to transfer emotions from text to images in a fine-grained way. We propose a novel loss function and a new dataset for training and evaluating our model. We show that our model can generate visually appealing images with emotion-aware content.
**Vision Transformer.** Vision Transformer (ViT) [30] has achieved great success in many computer vision tasks, such as image classification [31], object detection [32], semantic segmentation [33][34] and image restoration [35][36][37][38]. In particular, ViT has been widely used in image restoration tasks [39][40][41]. However, it has not been explored in the domain of emotion recognition.

\(\rightarrow\)"," To achieve the desired properties of a well-qualified AIF solution, related work from three aspects need to be discussed: _(i)_ **visual emotion analysis**, which plays an important role in producing corresponding concrete visualization with extracted emotional features and measuring whether the synthesized results could evoke specific emotional responses from human observers; _(ii)_ **style transfer**, which aims to create aesthetically pleasing images based on a reference of colors and textures, while maintaining the visual content of user-provided content images, thereby sharing similar goals with the AIF task; and _(iii)_ **vision transformer**, which has been demonstrated effective in modeling global token-to-token relationship between multi-modal inputs, making it a promising approach for interacting with user-provided images and texts in the AIF task.

Computer vision is increasingly focused on understanding emotion in context for more than two decades [1]. Before the advent of deep learning models, researchers developed a variety of handcrafted features for analyzing affective images [2][3][4][5][6][7], which are typically vulnerable and difficult to generalize to out-of-distribution scenarios. This situation has been improved since neural networks are used to adaptively predict emotions [8][9][10], where models could extract multi-grained emotional features  or focus on local image regions [11]. On the basis of previous works [12][13], we could design novel constraints to learning the inherent semantics of emotions and makes synthesized results favorable by human observers.

Early works adopt the optimization-based method [14] or design end-to-end models  to achieve style transfer for one specific style. To improve the efficiency of style transfer, researchers explore the approach to train multiple styles in one model [15][16][17], and further propose the first arbitrary style transfer model [18]. Using self-attention mechanisms to build long-range dependencies between regions [19][20] has received considerable attention in numerous studies devoted to improving the performance of arbitrary style transfer models [21][22][23][24]. Following them, StyTr2[25] further adopts the transformer architecture to extract and maintain the global information of in put images. Recently, researchers have attempted to replace the reference image with semantic textures of text to provide more flexible and user-friendly style guidance [26]. Compared to AIF algorithm, which is required to accurately understand _visually-abstract emotions_ from the text, image style transfer methods aim to create images based on a reference of _visually-concrete colors and textures_.

Transformer [27] has gained significant attention and popularity in recent years, leading to the development of unique feature fusion mechanisms for cross-modality tasks, _e.g_., text-to-image generation [28][29], visual grounding [30][31], and referring segmentation [32][33]. Meanwhile, researchers have explored the use of pure vision transformer models for a wide range of vision applications to achieve better performance, _e.g_. image classification [34], object detection [35], and semantic segmentation [36][37]. Great efforts have also been made to adapt vision transformer models to low-level vision problems, _e.g_., inpainting [38][39], super-resolution [40][41], and colorization [42]. For better concrete visualization of visually-abstract emotions and an in-depth understanding of the inherent properties of emotional words, we adopt the transformer architecture to interact with cross-modal inputs.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]"," **Visual Emotion Recognition.** Visual emotion recognition has been studied for a long time. The International Affective Picture System (IAPS) [1] provides a set of normative emotional stimuli for experimental investigations of emotion and attention, which is one of the most widely used datasets for emotion recognition [2][3][4][5][6][7][8][9][10][11][12][13]. In recent years, deep learning based methods [14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42] have been proposed for this task. For example, Gatys _et al_. [14] propose a neural algorithm of style transfer, which can separate and recombine the image content and style of natural images. StyleBank [15] proposes a style bank to represent the style of each filter bank, and the auto-encoder is used to combine the style information from the style bank with the content information from a content image. StyleGAN [10] proposes to use the style distribution alignment module to eliminate the explicit distribution gaps between various style domains and reduce the risk of mode collapse. StyleFlow [21] projects the content images backward to the style images in a reversible way to prevent the content leak phenomenon.






2  proposes a multi-task deep learning framework to learn the style transfer model from a large-scale style dataset. In this work, we propose a novel model that unifies both images and texts into tokens and uses the multi-modal transformer architecture [27] to represent images and text into tokens, which unifies the visual-linguistic context into a unified representation and enables the model to capture the prior knowledge of emotions from the text.

"," Our work is closely related to several related topics, including hand-object interactions, 6-DoF grasping, visual affordance, and hand pose estimation.

**Hand-Object Interaction.** The current advances in hand-object interaction has been widely promoted by the works of [1][2][3][4][5][6] on estimating hand-object interaction through Deep Neural Networks. However, these works are highly dependent on the reconstruction of hand and object shapes and can be further improved through the understanding of hand-object interaction from visual perception. Currently, only a few datasets are provided in the literature, including Roke , ReconHAND [5], Hand+Movies , GanHand [6], BigHand2.2M Benchmark [7], RHO dataset [5], and HOnnotate [8][9]. These datasets are collected for reconstructing hand and object pose in the wild. However, all these datasets are not equipped with part-level labels, which hinders the capability of estimating appropriate hand-object interactions.

For grasping, Dex-YCB dataset [10][11] consists of both hand and object images. It uses a simplified version of the RGB-D Hands dataset for the real hands and 2D synthetic images for the real objects. Dex-YCB is the first realistic grasping dataset. Feng [12] presents a robotic grasping dataset from simulations based on the Dexterous Robot Manipulation (DoMan) environment [13]. They proposed a novel multi-finger contact interaction term to facilitate the simulation-based hand-object grasping. Xu [14] extended their simulation-based grasping dataset with more object categories. Besides, Dex-YCB was also integrated into several hand-object interaction benchmarks, e.g. Dex7 [15], YCB-OAKINK [16], YCB-VizDoom [17]. Note that Dex-YCB, YCB-OAKINK, and YCB-VizDoom all do not contain hand object interactions, so AffPose is complementary to them.

A similar work to ours on hand-object interaction is H2O [17]. It is the first large-scale hand-object interaction benchmark for human actions, including handover and grasping. It contains 1062 real hand-object interactions from videos collected by user participation. However, H2O is not equipped with part-level labels, which hinders the capacity of grasping specific objects or parts. Therefore, we present a large-scale and part-level labeled AffPose dataset, which can be used in both human and robot hand-object interactions and grasping researches.

**Affordance Prediction.** For the prediction of grasping, many works [18][19][20][21][22] focus",,"\<Related work in the field of affective image understanding has witnessed significant advancements in recent years, especially in tasks related to emotion recognition, sentiment analysis, and image style transfer. Several studies have focused on understanding and reflecting emotions from text to images, which aligns closely with the objectives of the proposed Affective Image Filter (AIF) model [1]. These works have explored various approaches to model the relationship between text and visual content, including using psychological theories and web mining to construct visual sentiment ontologies [2] and extracting low-level features inspired by psychology and art theory for affective image classification [3]. Moreover, the use of deep metric learning for affective image retrieval and classification has been proposed, emphasizing the hierarchical and complex nature of emotions [5]. Additionally, the application of principles-of-art features for image emotion recognition and the development of multi-graph learning for affective image retrieval have demonstrated the importance of incorporating artistic principles and semantic concepts in emotion analysis [6, 7].

Some studies have also addressed the challenges of facial expression recognition and sentiment analysis using deep learning models, which are relevant to the task of understanding visually-abstract emotions and reflecting them to visually-concrete images as proposed in the AIF model [4]. Furthermore, the use of adaptive deep metric learning for affective image retrieval and classification has shown promise in addressing the hierarchical and complex nature of emotions, which is a key aspect of the AIF model's objectives [5]. The integration of artistic principles and psychological theories for image emotion classification aligns with the goals of the AIF model to understand and reflect emotions from text to images [6]. Lastly, the exploration of multi-graph learning for affective image retrieval has shown the significance of incorporating a diverse range of features and levels of abstraction in understanding visual emotions, which is pertinent to the multi-modal approach of the AIF model [7].>\<References: [1] International Affective Picture System (IAPS) : Technical Manual and Affective Ratings, [2] Large-scale visual sentiment ontology and detectors using adjective noun pairs, [3] Affective image classification using features inspired by psychology and art theory, [4] EASE: Robust Facial Expression Recognition via Emotion Ambiguity-SEnsitive Cooperative Networks, [5] Adaptive Deep Metric Learning for Affective Image Retrieval and Classification, [6] Exploring Principles-of-Art Features For Image Emotion Recognition, [7] Affective Image Retrieval via Multi-Graph Learning.>

This related work highlights the key contributions made in the field of affective image understanding and how they are relevant to the proposed Affective Image Filter model. It emphasizes the importance of leveraging multi-modal approaches, understanding complex emotions, and incorporating principles from psychology and art theory to achieve the objectives of the AIF model."
3701,3701," **Unsupervised Domain Adaptation.** Unsupervised domain adaptation (UDA) aims to transfer a model trained on a labeled source domain to a target domain with unlabeled target domain. UDA methods can be divided into two main categories. The first category aims to minimize the Maximum Mean Discrepancy (MMD) between the distributions of the source and the target domains. This can be achieved by minimizing the maximum mean discrepancy between the two domains ([6]; [8]; [1]; [3]). The second category of UDA aims to learn a domain-invariant feature extractor that can generalise well to the target domain ([17]; [14]; [5]).

**Domain Generalization.** Domain Generalization (DG) refers to the ability of a model to generalise to unseen distributions. DG is a technique for improving the generalization ability of machine learning models to unseen data distributions. It has been shown to be effective in a wide range of tasks, such as: (; [23]) and (; ).

"," **Domain adaptation (DA).** DA assumes a setting in which labelled data is available for a source domain, and unlabelled data for a target domain. The goal is to maximize performance on the target domain. DA methods can be roughly divided into three types ([4]): _domain-invariant training_ (also called _feature alignment_) aims to ensure that the features generated by the model for the source and target domain are indistinguishable by some metric ([1]; ; [7]; [6]; ; [2]; [8]; [3]); _self-training_ involves generating pseudo-labels for the unlabelled data ([5]); and _self-supervision_ involves training an unsupervised/self-supervised model, later fine-tuned or jointly trained with supervision ().

**Source-Free Domain Adaptation (SFDA) and Test-time Adaptation (TTA).** These methods additionally assume that the source data itself is not available, e.g., because of resource, privacy, or intellectual property concerns. The distinction between SFDA and TTA is subtle: the latter is transductive, motivated by an online setup where adaptation happens on (unlabelled) target examples as they appear and evaluation is subsequently performed on the same examples. SFDA considers an offline adaptation phase and the adapted model is then evaluated on held-out examples. In practice, though, the methods developed for either are similar enough to be applicable to both. Related problems also include black-box (), online ([9]), continual ([10]), and universal ([11]) source-free domain adaptation.

Of the three types of DA methods discussed above, self-training most easily transfers to the SFDA and TTA settings ([12]; [17]), and we focus on this category since it's also the most generalizable to new modalities. Other methods use output prediction uncertainty for adaptation ([9]; [15]; [14]) or generative training to transform target domain examples or synthesize new ones (; ; [16]; [13]; [18]). Interestingly,  show that previous methods suffer from large hyperparameter sensitivity, and may _degrade_ the performance of the source model if not tuned in a scenario-specific manner; this violates the assumption that labelled target data is unavailable.

**Test-time Training (TTT).** TTT ([19]) is a related problem where, like in TTA, a pre-trained model is adapted on the target test examples using a self-supervised loss, before making a prediction on those examples. Unlike SFDA and TTA, though, TTT modifies the source training phase to incorporate a similar self-supervised loss there too.

**Domain generalization (DG).** In DG ([24]), like in SFDA, the target domain is unknown. However, unlike SFDA, no adaptation set is available. Instead the aim is to train a robust source model which works directly on new target distributions. Another important distinction is that DG assumes that information about the source domain is available during deployment on the target domain. A popular strategy for DG is to increase the source model's generalizability by exposing it to diverse ""conditions"" at training time via domain randomization ([20]) or adversarialdata augmentation ([23]; [21]), or to learn domain-_invariant_ representations by training to match all available training ""environments"" (; ), minimizing the worst-case loss over a set of such environments ([22]), or decomposing the latent space or model weights into domain-specific and domain-general components (; ).

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]"," **Source-free domain adaptation.** The goal of unsupervised domain adaptation (UDA) is to adapt a pre-trained model to a new domain with only unlabeled target data. Existing UDA methods can be broadly classified into two categories: supervised methods and source-free methods. Supervised methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] assume access to labeled source data during the training phase, and use the source model as a teacher model to guide the target model during the adaptation phase. For example, [1] use the second-order statistics of the source and target distributions to minimize the domain shift. [2] use adversarial learning to learn disentangled and transferable representations. [3] use a generative adversarial network (GAN) to generate target samples and use them to train a domain discriminator. [8] use an adversarial discriminative domain adaptation method [8]. [6] propose a deep adaptation network (DAN) which embeds all task-specific layers in a reproducing kernel Hilbert space where the mean embeddings of different domains can be explicitly matched. [7] use Maximum Mean Discrepancy (MMD) [7][6] to learn a representation that is both semantically meaningful and domain invariant. [14] use Laplace approximation to identify target data points that do not lie in the source manifold and use it to guide target adaptation. [9] propose casting a second classifier to find misclassified features. [12] use information maximization and self-supervised pseudo-labeling to align representations from the target domain to the source domain. [13] use cGANs to generate pseudo-labels for target samples to filter domain shift noise. [17] use self-training to select reliable source samples with the self-entropy criterion and define them as class prototypes. [5] use noise injection to improve the performance of the teacher model. [10] propose continual test-time adaptation (CoTTA) which uses weight-averaged and augmentation augmentation predictions to reduce the error accumulation and catastrophic forgetting. [16] use generative models to generate source samples to train the target classifier. [18] propose an unsupervision method for adapting a source model to target domain along natural axes"," The related work for training strategies can be roughly divided into two categories. The first category is existing study techniques which are inspired by human learning techniques and aim at improving the model learning performance. The second category is existing meta-learning methods which aim at imitating human learning and exploring efficient meta-learning algorithms. In what follows, we introduce each category and explain the differences between these methods and our methods.

Study techniques.In the above section, we explained that many effective study techniques, e.g., the Feynman technique, peer questioning, have been developed to improve human learning. In this work, we propose to utilize some basic elements of these study techniques, including learners, interaction functions, learning stages, etc., to create new training strategies, which we refer to as _study strategies_. The learners and interaction functions of our methods are simple yet powerful. Specifically, our methods define learners as the model that learns from a curriculum of training data, and we design interaction functions for deciding when the learner should learn, revise, etc. In this way, our learners, interaction functions, and stages are very different from conventional learners, interaction functions, and stages in the study techniques, as illustrated in Table 1. However, our methods can improve the model learning performance in various tasks, and we believe that the powerful elements of these study techniques can inspire the design of methods for studying modern machine learning problems.

Meta-learning methods.Meta-learning ([14]; ; [19]; [6]) has become popular in many domains of deep learning, such as meta-learning for hyperparameter optimization ([8]; [1]), meta-learning for multi-task learning ([17]; [10]; [11]; [15]), meta-learning for model-based Bayesian optimization ([4]), meta-learning for architectural search (; [12]), meta-learning for neural architecture search ([7]), meta-learning for meta-learning ([6]; [16]; ; ; [13]), meta-learning for weakly-labeled image classification ([18]), etc. Some of the recent works for meta-learning study the application of meta-learning to better distill the knowledge from neural networks into sub-networks ([20]; [21]). However, most of the existing methods are designed to improve training speed, while our methods focus on improving the model performance. In addition, we provide a high-level framework for constructing new training strategies for specific applications, e.g., neural architecture search, which will help with future investigation of meta-learning techniques and lead to efficient improvement of model learning.

",,"<In recent years, the field of domain adaptation has witnessed significant advancements, with a focus on adapting models to new domains using unlabelled data. This trend aligns with the research presented in the target paper ""In Search for a Generalizable Method for Source Free Domain Adaptation,"" which discusses the application of existing SFDA techniques to bioacoustics, a domain distinct from the commonly studied computer vision domain[1]. Existing methods in the field have shown promise, such as the CORAL method, which aligns second-order statistics of source and target distributions for unsupervised domain adaptation [2]. Another notable approach, Conditional Adversarial Domain Adaptation, introduces conditioning strategies to enhance discriminative information and manage uncertainty in classifier predictions [3]. The proposed method leverages a generative adversarial network to align source and target distributions and achieve state-of-the-art performance across different datasets [12]. The concept of domain adaptation benchmarks has been extended with the Wilds 2.0 update, addressing the need for datasets that reflect real-world scenarios [4].

The relevance of the work extends to online and offline source-free domain adaptation techniques, particularly in addressing the practical challenge of utilizing only the source model during adaptation to the target domain [9]. Additionally, the concept of continual test-time domain adaptation [10] and universal source-free domain adaptation [11] present further insights into adapting models without access to source data. Notably, the Source Hypothesis Transfer (SHOT) approach provides a generic framework for unsupervised domain adaptation without access to the source data, achieving state-of-the-art results across multiple benchmarks [12]. Generative Pseudo-label Refinement addresses the noise in pseudo-labels for target samples, showcasing resilience in the context of conditional Generative Adversarial Networks and unsupervised domain adaptation [13].

Furthermore, uncertainty-guided SFDA presents a novel probabilistic treatment for source-free domain adaptation that outperforms traditional SFDA in both closed-set and open-set settings [14]. The tent approach introduces test-time training to improve model performance under distribution shifts, demonstrating its efficacy in addressing diverse image classification benchmarks [15]. Meanwhile, Domain Impression introduces a generative framework that enables domain adaptation without the actual source data through energy-based modeling and joint distribution learning [16]. The research point towards the evolution of domain adaptation methods that minimize the dependance on source data while improving model robustness to distribution shifts.
>"
940,940," **LiDAR Semantic Segmentation.** LiDAR point cloud semantic segmentation is a challenging task due to the irregularity and sparsity of the 3D point cloud data. To address this problem, point-based methods [1][2][3][4][5][6][7][8][9][10][11][12] are proposed to directly process the unordered point clouds. However, these methods usually suffer from high computation and memory consumption. To reduce the computational cost, the proposal-free methods [13][14][15][16][17][18][19][20][21][22][23][24] have been proposed to address this issue. Panoptic-PolarNet [11] and SMAC-Seg [12] firstly propose to solve the problem by proposing a two-stage framework, i.e., the proposal stage and the center stage, and then perform the panoptic segmentation in the two stages. LiS3Net [23] proposes a graph-based clustering module to model the relationship between the original point cloud and the virtual instance center. LiDASet [20] proposes to perform the feature shifting [21] to align the original points with the virtual center points. LiPHNet [22] introduces a graph neural network (GNN) to learn the feature embedding of the original and virtual points, and performs the clustering in the graph space to obtain the final instance centers.

Although these methods have achieved promising results, their effectiveness and efficiency are still limited owing to the difficulty of modeling the non-existent instance centers and the costly center clustering modules. In this paper, we propose a novel center focusing feature encoding and a fast center deduplication module to improve the efficiency and accuracy of the LiDSPANet [11].

**Proposal-free Methods.** PanopanopticNet [13] is the first attempt to address the problem of proposal-based instance segmentation. It firstly proposes a baseline network Mask R-CNN [14] to predict the instance center for each object and then performs the instance-wise panopti

\[\begin{tabular}{l c l l c l c c l r l l l} \hline\] (1)where \(\mathcal{O}(n^{2})\) and \(n^{3}\) are the number of object instances and their corresponding object centers, respectively. \(\mathbb{E}_{n}\) is the number and type of the object instances, \(n_{2}\) denotes the size of the training set, and \(\mathrm{L}_{\text{Panoptic}(N)\) is the total number of training instances. The goal of the objective function is to predict \(N\) instances for each instance and \(N_{1}\) for the whole training set. For each training instance, \(N^{2}\times N\) points are assigned to the corresponding object center. The objective function can be divided into two parts: the center-based and the"," The LiDAR panoptic segmentation methods usually adopt the LiDAR semantic segmentation networks as the backbone and jointly optimize the semantic segmentation and instance segmentation tasks. The results of the two tasks are fused to generate the final panoptic segmentation results. The LiDAR semantic segmentation backbone is first introduced. Then, the panoptic segmentation methods, achieving instance segmentation upon the semantic segmentation backbone, are grouped into the proposal-based and proposal-free methods, and are further discussed.

LiDAR semantic segmentation backbone.The backbone network for LiDAR semantic segmentation can be divided into three groups: point-based, voxel-based, and 2D projection-based methods. The point-based methods [1][2][3][4] directly operate on the raw point clouds but are extremely time-consuming due to the expensive local neighbor searching. The voxel-based methods [5][6] discretize the point clouds into structured voxels, where sparse 3D convolutions are applied. These methods are still difficult to meet the real-time applications, although they achieve the highest accuracy. The 2D projection-based methods are more efficient since most of their computation is done in the 2D space, such as range view (RV) , bird's-eye view (BEV) [7], polar view [8], and multi-view fusion [9][10]. For fast LiDAR panoptic segmentation, the Panoptic-PolarNet [11], SMAC-Seg [12], and LPSAD [13] all adopt the 2D projection-based backbone. A recent real-time semantic segmentation method, namely CPGNet [10], is a 2D projection-based one that explores an end-to-end multi-view fusion framework by fusing the features from the point, BEV, and RV.

**Proposal-based methods.** The proposal-based methods conduct instance segmentation through a two-stage complicated process. They first detect the foreground instances and then refine the instance segmentation results independently within each detected bounding box. Based on the Mask R-CNN [14] for instance segmentation, the MOPT [15] and EfficientLPS [16] insert a semantic branch to achieve panoptic segmentation on the range view (RV). Recently, LidarMultiNet [17] unifies LiDAR-based 3D object detection, semantic segmentation, and panoptic segmentation in a single framework to reduce the computation cost by sharing a strong voxel-based backbone.

**Proposal-free methods.** The proposal-free methods usually apply the class-agnostic clustering to the _things_ points to conduct instance segmentation. The LPSAD [13] clusters points into instances by regressing the center offsets. The Panoster [18] directly predicts the instance IDs from a learning-based clustering module, where the time-consuming DBSCAN [19] is used to refine the instance segmentation results. The DS-Net [20] proposes a learnable dynamic shifting module that adjusts the kernel functions of the MeanShift [21] to handle instances with various sizes. The GP-S3Net [22] constructs a graph convolutional network (GCN) on the over-segmentation clusters to identify instances. The SMAC-Seg [12] proposes a sparse multi-directional attention clustering and center-aware repel loss for instance segmentation. These methods adopt the time-consuming clustering methods. To further accelerate the algorithm, recently, Panoptic-PolarNet [11], SCAN [23], and Panoptic-PHNet [24] adopt the BEV center heatmap and center offsets for instance segmentation. However, the BEV center heatmap is also costly and confuses on \(z\)-axis.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]"," **Point-based methods.** Point-based LiDAR semantic segmentation methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] directly process the point cloud as raw point clouds and apply 2D or 3D convolutional neural networks to extract features. PointNet [1] is the first to directly consume point clouds as input for 3D classification and segmentation. PointNets [1], PointNet++ [2], KPConv [3], and RandLA-Net [4] are proposed to improve the efficiency and robustness of point-based segmentation algorithms. However, these methods suffer from high computational complexity due to the use of 2D voxel grids. To reduce the computational cost, some methods [5][8] utilize the cylindrical partition and asymmetrical 3D CNNs [5] to explore the 3D geometric pattern while maintaining the inherent properties of the outdoor LiDar point cloud. CPGNet [10] proposes a point-grid fusion block to extract semantic features mainly on the 2D projected grid for efficiency, and summarizes both 2D and 3D features on 3D point for minimal information loss. PolarNet [8] proposes an improved grid representation to balance the points per grid and redistributes the network's attention over the long-tailed points distribution over the radial axis in polar coordination. CPDNet [9] exploits multiple projections of the point clouds to mitigate the loss of information inherent in single projection. CPMNet  and MPFuse [9], which use the bird's-eye-view projection and the spherical projection respectively, respectively, to achieve real-time inference speed.

**Proposal-free panoptic segmentation approaches.** The proposal-free methods [11][13] directly use the instance centers of the points as the instance proposals and perform clustering on the proposal-based clustering module [19][24]. Panoptic-PolarNet [11] proposes the bird-eye view representation and proposes a novel instance augmentation technique to circumvent the issue of occlusion among instances in urban street scenes. SMAC-Seg [12] proposes to use a learnable sparse multi-directional attention clustering method to segment multi-scale foreground instances. PanoptICNet [13] uses the geometric information of"," Image-to-image translation (I2I) [1][2][3][4][5][6][7] seeks to transfer content information from one domain to another. Zhu _et al_. [8] proposed a conditional GAN model for style transfer. Johnson _et al_. [9] introduced the AdaIN instance normalization (IN) technique to improve the style transfer speed. Wang _et al_. [10] proposed a similarity-matching mechanism to determine the alignment of image pairs in different domains. It enables the images in different domains to be aligned by matching their internal representations. Similarly, we can use the structure to determine the alignment between source and target domains in font generation.

Few-shot image-to-image translation (F-I2I) [11][12] has recently been proposed. This work aims to develop a method that can learn from unpaired datasets using the few available image pairs. It can be used to learn the mapping between the source image domain and the target domain. They exploit the semantic relationships between source and target domains. However, these methods generally require multiple steps to find the alignment and match the semantic structures between source and target domains.

Our work is closely related to few-shot font generation (FFG) [13][14][15][16][17][18][19][20][21][22][23][24][25][26]. Disentangling content from style is a common process for FFG. The style feature can be defined by the reference style images. The cross-domain feature alignment and style-feature fusion methods are the main categories of FFG. Existing methods generally use either style-content disentanglement or concatenation methods to determine the input feature.

Style-content disentanglement methods extract style features independently from content features. HC-GAN [13] transferred the strokes of source fonts to the destination fonts by using a two-stage framework. SCF-GAN  synthesizes Chinese characters using a network of three encoders and two decoders. The concatenation methods take the content and style features and process them through the same network. HGAN [13] first converted stroke information to style vectors and then concatenated them with content vectors. PEGAN [14] obtained the style and content vectors through different encoding networks and concatenated them.

Style-vectors can be efficiently refined through a fine-grained self-supervised discriminator. However, the conventional encoder-decoder architecture does not enable a finer feature distribution reconstruction. A lightweight iterative style-vector refinement (ISR) strategy has been proposed in some FFG works. Factor-based style refinement (FBSR)  allows the GAN model to refine the content feature to a matrix by accumulating the weight of all reference style images. The main objective of FBSR is to obtain a",,"<LiDAR panoptic segmentation has gained significant attention in recent years due to its importance in enabling autonomous vehicles to understand their surrounding environment. Several methods have been proposed to improve the accuracy and efficiency of LiDAR panoptic segmentation. PointNet [1] and PointNet++ [2] introduced neural networks that directly consume point clouds, providing a unified architecture for object classification, part segmentation, and scene semantic parsing, thus proving to be efficient and effective. KPConv [3] presented a new design for point convolution, extending to deformable convolutions for adapting kernel points to local geometry. RandLA-Net [4] focused on efficient semantic segmentation for large-scale 3D point clouds by efficiently inferring per-point semantics. Cylindrical and Asymmetrical 3D Convolution Networks [5] proposed a framework that utilized 3D voxelization and 3D convolution networks to explore 3D geometric patterns in outdoor point clouds. (AF)2-S3Net [6] proposed an end-to-end encoder-decoder CNN network for 3D LiDAR semantic segmentation, achieving state-of-the-art performance. PointPillars [7] introduced a fast encoder for object detection from point clouds, outperforming previous encoders in terms of speed and accuracy. PolarNet [8] focused on online LiDAR point clouds semantic segmentation by providing a new grid representation to balance accuracy and speed. Multi Projection Fusion [9] proposed a novel approach for 3D point cloud semantic segmentation that exploits multiple projections of the point cloud to mitigate information loss.

Recent advancements have also been made in developing specialized architectures for LiDAR panoptic segmentation. CPGNet [10] proposed a Cascade Point-Grid Fusion Network that effectively processed the potentially large 3D scene, achieving a speed-accuracy trade-off. Panoptic-PolarNet [11] presented a fast and robust LiDAR point cloud panoptic segmentation framework, outperforming baseline methods on SemanticKITTI and nuScenes datasets with an almost real-time inference speed. SMAC-Seg [12] introduced a novel LiDAR-based panoptic system, called SMAC-Seg, for real-time clustering-based panoptic segmentation, achieving state-of-the-art performance. LiDAR Multi-Net [17] presented a unified multi-task network that combined LiDAR-based 3D object detection, semantic segmentation, and panoptic segmentation, achieving state-of-the-art performance. EfficientLPS [16] introduced a novel top-down LiDAR panoptic segmentation architecture that addresses challenges in segmenting LiDAR point clouds, achieving state-of-the-art results on nuScenes and SemanticKITTI datasets.

The LiDAR panoptic segmentation methods proposed in recent years have paved the way for efficient and accurate segmentation of LiDAR point clouds, enabling advanced applications in autonomous driving and environmental understanding. These methods have leveraged techniques such as convolutional networks, point cloud encoders, and novel grid representations to achieve state-of-the-art results on benchmarks like SemanticKITTI and nuScenes. By addressing challenges such as speed-accuracy trade-offs, large-scale segmentation, and real-time inference, these methods have significantly advanced the field of LiDAR panoptic segmentation. <19>


Overall, these advancements in LiDAR panoptic segmentation are crucial for the development of autonomous vehicles and robotics systems, allowing them to comprehensively understand and interpret their surroundings in real-time. These methods have demonstrated the capability to efficiently process large-scale 3D point clouds, address sparsity and varying densities, and achieve accurate segmentation and clustering of objects and scenes. With the emergence of panoptic segmentation frameworks such as GP-S3Net [22], Sparse Cross-scale Attention Network [23], and Panoptic-PHNet [24], the field of LiDAR panoptic segmentation continues to evolve, leading to faster and more accurate solutions for real-world applications. This progress is vital for ensuring the safety and effectiveness of autonomous systems in diverse environments. <16>


In addition to the significant progress in LiDAR panoptic segmentation, there is growing interest in integrating other cutting-edge techniques such as mean shift clustering [21] and graph-based convolutional networks [22] to improve the robustness and accuracy of panoptic segmentation in the context of LiDAR point clouds. This integration of diverse methodologies indicates the multidisciplinary nature of LiDAR panoptic segmentation research and its potential to advance the capabilities of autonomous systems and robotics in real-world scenarios. The proposed methods have demonstrated superior performance, with some achieving 1st place on competitive public leaderboards and outperforming state-of-the-art approaches by notable margins. This indicates the continuous evolution and refinement of techniques for accurate and efficient panoptic segmentation of LiDAR point clouds, signaling promising advancements in autonomous driving and environmental perception. <21>"
2819,2819," **Pose transfer for 3D characters.** Deformation transfer methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25] aim to transfer the pose of a reference 3D character to a stylized character of the same shape. These methods either require the stylized characters to be rigged [2][25], or require ground truth pose of the reference character to be available at test time [21][24]. In contrast, our method does not require any of the above requirements.

**Implicit representation of 3D shapes.** Implicit representations have been widely used in 3D shape modeling [26][27][28][29][30][31][32][33][34][35][36][37][38]. In particular, DeepSDF [29] and OccupancyNet [30] use a neural network to represent a 3D object as a function of its signed distance to the surface, and an occupancy function to represent the distance between the point and the surface. These representations have also been used for pose transfer. However, these methods do not explicitly model the deformation between the reference and stylized 3D models, and thus cannot be directly applied to the case where the target pose does not correspond to the reference pose. In this work, we propose a novel implicit pose deformation module that can be applied to both the source and target pose.



\begin{tabular}{l c l c l l l c c l} \hline c cl cl l c r l cl r c l r r r c c c r r \l c c \l l r c r c}{l r r l r} \l r l \r l c}{c}{l} \r r r}{r r}{c} \cl c c}{r}{r c}{2} & c c & c & d c & e c & f c & g & e f g & fc & e g & g f c  & f g f f f g g f h f f h g f g h f h  & h f gf h f  & gfh f hf hg f h & hgfhf hfhgf h. \hr{h} & hr{r}{hg fhg hg hf f hg gf g h hg  & efh. \lh{h}\rho_{h}& hh{r} & fh_{h}\hr_{hg}& gf_{hfh}hf_{g hhf gf fh fh hgh fg fg h fhf  & c g fh. & hh_{g} & gh_{r}& fh_fh_gf_{fh] & gg_{h.h.** "," **Deformation Transfer.** Deformation transfer is a long-standing problem in the computer graphics community [1][2][3][4][5]. Sumner _et al_. [1] apply an affine transformation to each triangle of the mesh to solve an optimization problem that matches the deformation of the source mesh while maintaining the shape of the target mesh. BenChen _et al_. [4] enclose the source and target shapes with two cages and transfer the Jacobians of the source deformation to the target shape. However, these methods need tedious human efforts to annotate the correspondence between the source and target shapes. More recently, several deep learning methods are developed to solve the deformation transfer task. However, they either require manually providing the correspondence [6] or cannot generalize [7][8] to stylized characters with different shapes. Gao _et al_. [8] propose a VAE-GAN based method to leverage the cycle consistency between the source and target shapes. Nonetheless, it can only work on shapes used in training. Wang _et al_. [9] introduce conditional normalization used in style transfer for 3D deformation transfer. But the method is limited to clothed-humans and cannot handle the large shape variations of stylized characters.

We argue that these learning-based methods cannot generalize to stylized characters because they rely on encoding their global information (_e.g._, body or parts), which is different from traditional works that focus on local deformation, _e.g._, the affine transformation applied to each triangle in [1]. Using a neural network to encode the global information easily leads to overfitting. For example, modelstrained on human meshes cannot generalize to a stylized humanoid character. At the same time, early works only focus on local information and cannot model global information such as correspondence between the source and target shapes, which is why they all need human effort to annotate the correspondence. Our method tries to learn the correspondence and deform locally at the same time.

**Skeleton-based Pose Transfer.** Besides mesh deformation transfer, an alternative way to transfer pose is to utilize skeletons. Motion retargeting is also a common name used for transferring poses from one motion sequence to another. Gleicher _et al_. [10] propose a space-time constrained solver aiming to satisfy the kinematics-level constraints and to preserve the characters' original identity. Following works [11][12] try to solve inverse-kinematics or inverse rate control to achieve pose transfer. There are also dynamics-based methods [13][14] that consider physics during the retargeting process. Recently, learning-based methods [15][16][17][18][19] train deep neural networks to predict the transformation of the skeleton. Aberman _et al_. [20] propose a pooling-based method to transfer poses between meshes with different skeletons.

All these works highly rely on the skeleton for pose transfer. Other works try to estimate the rigging of the template shape [21][22][23][24] when a skeleton is not available. But if the prediction of the skinning weights fails, the retargeting fails as well. Liao _et al_. [25] propose a model that learns to predict the skinning weights and pose transfer jointly using ground truth skinning weights and paired motion data as supervision, which limits the generalization of this method to categories where annotations are more scarce compared to humans (_e.g_., quadrupeds). Instead, our method uses posed human or animal meshes for training and deforms stylized characters of different shapes at inference.

**Implicit 3D shape representation.** Implicit 3D shape representations have shown great success in reconstructing static shapes [26][27][28][29][30][31] and deformable ones [32][33][34][35][36][37][33][29]. DeepSDF [29] proposes to use an MLP to predict the signed distance field (SDF) value of a query point in 3D space, where a shape code is jointly optimized in an auto-decoding manner. Occupancy flow [34] generalizes the Occupancy Networks [30] to learn a temporally and spatially continuous vector field with a NeuralODE [38]. Inspired by parameteric models, NPMs [36] disentangles and represents the shape and pose of dynamic humans by learning an implicit shape and pose function, respectively. Different from these implicit shape representation works that focus on reconstructing static or deformable meshes, we further exploit the inherent continuity and locality of implicit functions to deform stylized characters to match a target pose in a zero-shot manner.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]"," **Deformation Transfer.** Deformation transfer is a fundamental problem in computer graphics, and has been studied for decades. Classical methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39]. In this work, we focus on the problem of pose transfer between stylized 3D characters.

**Pose Transfer for 3D Models.** Pose transfer has been extensively studied in the computer graphics community. Early methods [2][4] require point-wise correspondences between the source and target shapes, which are usually obtained by manually defining a set of constraints on the source shape. For example, [4] requires the source mesh to have the same number of vertices as the target mesh, and [1] requires that the target meshes share the same connectivity with the source meshes. [2] uses a set-of-line markers to compute the geometric correspondence between the two meshes, and then uses the resulting energy minimization to transfer the source pose to the target pose. [3][5] use the correspondence as a basis for a deformation transfer problem. [6] use a coarse control mesh to embed the source skeleton and skinning weights into the target shape, and use the resulting deformation to warp the source to the surface. [8] use two convolutional variational autoencoders to map the source shapes to target shapes. [9] use spatially adaptive instance normalization to map source shapes into the same pose as the source. [7] use GANs to map deformed source meshes to target meshes.  use a variational auto-encoder to learn the deformation of the source character mesh to that of the target. [18] use cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. [17] propose a pose-movement network that explicitly learns poses and movements of a character. [16] combine the U-Net  and the DC-IGN [16], and use a Variational Autoencoder to combine the deep convolution and inverse graphics networks. [20] use skeleton-aware operators to transform the source motion into a collection of deep temporal features associated with"," **Zero-Shot Learning:** Zero-shot learning aims to classify novel instances according to an arbitrary set of seen classes, by learning a feature representation from data from the seen classes [1]. Early approaches explore to utilize local and global representations of a visual feature using class-agnostic and class-specific features [2][3]. Later, deep learning methods learn a cross-modal representation by jointly learning from unseen and seen classes using the pre-trained VAE [4][5] and contrastive learning [6]. Recently, CLIP [7] pre-trained on a large-scale dataset has demonstrated to capture general visual-linguistic representations, which has led to zero-shot models in a number of tasks including captioning [8], action recognition [9], image retrieval [10][11], mask proposal retrieval [12], and object detection [13]. However, most zero-shot learning methods explore task-specific supervised pre-training (e.g. image classification, image retrieval) for zero-shot tasks and the effectiveness of the general pre-trained visual representation is yet to be fully understood. In this paper, we explore to leverage the strong general visual representation to capture both object-centric and instance-level semantic information of a target object.

**Open Vocabulary Object Detection:** Recent object detection methods are mostly restricted to few pre-defined classes or object categories for training and testing [14][15][16][17][18][19]. However, we seek to detect objects according to an arbitrary set of seen classes as well as detect novel classes, which is referred as _open-vocabulary object detection_. The crucial difference between these two tasks lies in the training of an inference module [16][14] with the pre-trained object detection module [17][18][19]. In the former setting, the pre-trained object detection module is extended by a learned classifier in order to learn a classifier to detect novel classes [16], while in the latter, the pre-trained object detection module is utilized as a part of the inference module to detect the novel classes [14]. To detect novel classes, a pre-trained object detection module is also extended in terms of classification capabilities [17] and multi-task learning [18]. However, these approaches fail to capture complex instance-level relationships among objects in the image. In this paper, we propose to leverage general image features learned from the general zero-shot model in order to capture both object-centric and instance-level relationships in the image.

**Open Vocabulary Semantic Segmentation:** Open-vocabulary semantic segmentation approaches directly learn to segment an image with an arbitrary set of classes or object categories, or query words [20][21][22][23]. Since the strong visual representations are pre-trained on a",,"<In computer graphics, the task of transferring poses of reference avatars to stylized 3D characters is fundamental for various applications. Existing methods either require the stylized characters to be rigged or use the stylized character in the desired pose as ground truth during training [1][2]. However, a recent approach called zero-shot pose transfer aims to eliminate these requirements by leveraging widely available deformed non-stylized avatars in training and deforming stylized characters of significantly different shapes at inference. Classical methods achieve strong generalization by deforming the mesh at the triangle level, which requires labeled correspondences [1]. While some methods address the need for explicit correspondences at test time, they may still require labeled correspondences during training [3][4]. To bypass the need for explicit correspondences at test time, a semi-supervised shape-understanding module has been introduced alongside an implicit pose deformation module to deform individual surface points to match the target pose, addressing the need for labeled correspondences [5]. As a result, this model can generalize to categories with scarce annotation, such as stylized quadrupeds [6].

Semantic deformation transfer is another important area of research that seeks to preserve the semantic characteristics of the motion when transferring mesh deformation from one character to another, allowing for automatic transfer of new poses and animations [3]. Additionally, existing work has explored methods for generating natural and intuitive deformations via direct manipulation for a wide range of shape representations and editing scenarios by using a collection of affine transformations organized in a graph structure [5]. Furthermore, there are approaches that propose novel learnable representations for detail-preserving shape deformation and focus on preserving surface details regardless of shape intricacy and topology [6]. These related works provide valuable insights into methods for shape representation, manipulation, and deformation. Moreover, advances in deep learning have led to the introduction of neural networks for unsupervised pose transfer and motion retargeting, leveraging techniques such as spatially adaptive instance normalization to address the limitations of existing approaches [9][18]. These methods have shown potential for advancing the state-of-the-art in pose transfer and motion retargeting for 3D characters.>"
262,262," Denoising Diffusion Probabilistic Models (DDPMs) are one of the most popular generative models. DDPMs consist of a forward diffusion model and a reverse diffusion model. The forward diffusion process maps the data to a noise distribution, and the reverse diffusion process learns how to map the data back to the original distribution. However, diffusion models require hundreds to thousands of iterations to generate a single sample.

To address this issue, DDPM samplers have been proposed to reduce the number of iterations required for generating a sample. Truncated DDPM ([7]) truncates the backward diffusion process to a truncated version, which can be trained in an end-to-end manner. Pseudo-Numerical DDPM (PNNDPM) ([8]) proposes a pseudo-numerical method for DDPM sampling on manifolds. Diffusion-GAN ([6]) proposes to inject instance noise into the discriminator input to improve the sample quality of GANs. D-DDGAN ([5]) improves the sampling efficiency of diffusion models by adding a noise layer to the forward diffusion chain.
 Recently, RL has been applied to the training of Gans. RL-GAN-Net ([3]) uses RL to control the generator of a GAN network for real-time point cloud shape completion. SeqGAN ([1]) uses policy gradient ([9]) to train a sequence GAN for sequence data. RL has also been applied in other applications, such as recommender systems ([2]) and information retrieval ([4]).

In the context of DDPM finetuning, [12] proposes to finetune a DDPM with the maximum mean discrepancy (MMD) loss. [11] proposes a method to fineto a DDM with the MMD loss, and [10] studies the training and performance of MMD-based generative adversarial networks.

 proposes to learn a sampling shortcut by minimizing the IPM. In contrast to these works, we propose to learn an alternative sampling shortcut through the direct minimization of IPM, instead of learning a shortcut from scratch. Our method can be applied to any DDPM model, and can further improve the sampling quality of a pretrained DDM sampler.

 also proposes to use RL to train diffusion models. In their work, the RL agent is trained to maximize the objective function of the diffusion model, which is different from our method. In addition, their method does not consider the sampling shortcut, and their method is not applicable to DDPM fine-tuning.

 introduces a method for fast sampling of DDM. Their method is based on the maximum likelihood estimation (MLE) framework, and they propose to use the MLE objective to fine-tune DDM samples. In this work, our method differs from theirs in two aspects. First, our approach does not use MLE or MLE objectives. Second, our algorithm is applicable to both DDM and diffusion models, while their method only applies to DDM-based models.

 is the first study to apply RL to training diffusion models for image generation. They propose a method"," GAN and RL.There are works using ideas from RL to train GANs ([1]; [4]; [3]; [2]). The most relevant work is SeqGAN ([1]), which uses policy gradient to train the generator network. There are several main differences between their settings and ours. First, different GAN objectives are used: SeqGAN uses the JS divergence while we use IPM. In SeqGAN, the next token is dependent on tokens generated from all previous steps, while in diffusion models the next image is only dependent on the model output from one previous step; Also, the critic takes the whole generated sequence as input in SeqGAN, while we only care about the final output. Besides, in our work, rewards are mathematically derived from performing gradient descent w.r.t. IPM, while in SeqGAN, rewards are designed manually. In conclusion, different from SeqGAN, we propose a new policy gradient algorithm to optimize the IPM objective, with a novel analysis of monotonic improvement conditions and a new regularization method for the critic.

Diffusion and GAN.There are other works combining diffusion and GAN training: [5] consider multi-modal noise distributions generated by GAN to enable fast sampling; [7] considers a truncated forward process by replacing the last steps in the forward process with an autoencoder to generate noise, and start with the learned autoencoder as the first step of denoising and then continue to generate data from the diffusion model; Diffusion GAN ([6]) perturbs the data with an adjustable number of steps, and minimizes JS divergence for all intermediate steps by training a multi-step generator with a time-dependent discriminator. To our best knowledge, there is no existing work using GAN-style training to fine-tune a pretrained DDPM sampler.

Fast samplers of DDIM and more.There is another line of work on fast sampling of DDIM (), for example, knowledge distillation (; ) and solving ordinary differential equations (ODEs) with fewer steps ([8]; ). Samples generated by DDIM are generally less diverse than DDPM (). Also, fast sampling is generally easier for DDIM samplers (with deterministic Markov chains) than DDPM samplers, since it is possible to combine multiple deterministic steps into one step without loss of fidelity, but not for combining multiple Gaussian steps as one (). Fine-tuning DDIM samplers with deterministic policy gradient for fast sampling also seems possible, but deterministic policies may suffer from suboptimality, especially in high-dimensional action space ([9]), though it might require fewer samples. Also, it becomes less necessary since distillation is already possible for DDIM.

Moreover, there is also some recent work that uses sample quality metrics to enable fast sampling. Instead of fine-tuning pretrained models,  propose to optimize the hyperparameters of the sampling schedule for a family of non-Markovian samplers by differentiating through KID ([10]), which is calculated by pretrained inception features. It is followed by a contemporary work that fine-tunes pretrained DDIM models using MMD calculated by pretrained features ([12]), which is similar to the method discussed in Section 3.3 but with a fixed critic and a deterministic sampling chain. Generally speaking, adversarially trained critics can provide stronger signals than fixed ones and are more helpful for training ([11]). As a result, besides the potential issues discussed in Section 3.3, such training may also suffer from sub-optimal results when \(p_{0}^{\theta}\) is not close enough to \(q_{0}\) at initialization, and is highly dependent on the choice of the pretrained feature.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"," Reinforcement learning (RL) has been applied to various applications, such as online recommendation ([2]; [4]), image generation ([1]), and generative adversarial networks (GANs) ([3]; [3]). In particular, RL-GAN-Net ([3]) is the first attempt to use RL to control the training of GANs. The authors propose to train a GAN by injecting noise into the discriminator input, and then use the noise to guide the optimization of the generator. However, the noise is injected into the input of the GAN discriminator as a form of reward signal, which may introduce bias in the training process. In contrast, our method injects the noise directly into the model parameters, and directly optimizes the model with IPM.

DDPMs have been used to accelerate the sampling of diffusion probabilistic models ([8]; [7]; [6]; [5]). However, DDPMs require hundreds to thousands of iterations to generate high-quality samples, which limits their applicability in many real-world applications. To address this challenge, [6] propose to use a forward diffusion chain to generate Gaussian-mixture distributed instance noise, and [5] propose a denoising diffusion GAN to reduce the sampling cost of DDPM sampling. In this work, we propose a novel method to optimize DDPM samplers through the direct minimization of IPM, instead of learning the backward diffusion process, which enables sampls to discover an alternative and more efficient sampling shortcut.
 more efficiently and effectively. In addition, we show that our method is equivalent to policy gradient ([9]) for DDPMG sampling.


"," The most popular method for HPO is Bayesian optimization (BO), a model-based optimization technique which fits a surrogate model to the target function and queries the best configurations at small regular time intervals [1][2][3]. One of the most popular surrogate models is Gaussian process, which is an accurate, albeit expensive, model for functions that are also easy to fit with recent advances in kernel-based BO methods . At present, there is a number of Bayesian BO methods to consider including asynchronous approaches such as ACOBO [4], BOHB [5], HyB-kTune [6], DEHB [7], and FT-BO [8], hybrid approaches such as HYPERBAND [9], H-Tune [10], and Paramsweep [11], and bandit-based methods such as BERT . All of these methods can be categorized as model-based HPO algorithms that approximate the optimal HPO objective by either a proxy objective (BO), a probabilistic model (HPO-AS) or a surrogate function (GPR-HPO).

Most existing methods make strong assumptions about the model for the target function or observations noise, but recent work showed that a method based on the HyperBand algorithm can improve the results of HPO with randomized early stopping without any modeling assumptions on the observations noise [12][13]. In a similar vein, we propose to leverage quantile regression for more accurate modeling of the target function and, in doing so, improve the results of HPO algorithms that leverage surrogate models.

Quantile regression is a common technique for quantifying uncertainty in the target function in the HPO literature and has been used for decision-making and experiment design. For instance, [14] leveraged a quantile-regression-based model for the hyperparameter-objective mapping to perform HPO on CNN models. Other methods were proposed for HPO in the computer vision domain such as quantile regression for HPO in segmentation [15] and classification [16], for instance. Similarly,  proposed to use quantile regression for design of experiments for objectives that are modeled by linear regression.

In the same vein, [17] proposed to use quantile regression for experimental design with posterior exchangeability assumptions. These assumptions are quite stringent and leave the underlying mechanisms behind the apparent empirical successes of quantile regression somewhat unknown. These works [17][14][15][16] treat quantile regression as a model of the underlying objective, while in this work we propose to leverage it to learn the target function itself.

In the HPO literature, it is common to use Gaussian processes (GPs) as the surrogate model of the target function. While GPs can be used for HPO without making assumptions about the objective function (HPO-AS), they cannot learn complex models without an explicit prior for their parameters. This is in contrast with",,"<>
Shortcut Fine-Tuning (SFT) proposed in this study leverages reinforcement learning (RL) methods for training diffusion models [1]. While traditional methods in generative modeling, such as Generative Adversarial Networks (GAN) [2], have shown success, they often face limitations in fast sampling of pretrained models. The proposed SFT advocates for the fine-tuning of Denoising Diffusion Probabilistic Models (DDPMs) through the direct minimization of Integral Probability Metrics (IPM) [1]. This innovative approach enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process, resulting in sample quality comparable to or even surpassing that of the full-step model across various datasets [1]. These findings are in line with developments in reinforcement learning applied to other domains, such as the control of the Generative Adversarial Network in real-time point cloud shape completion [3], and the use of RL methods for offline policy learning in recommendation systems [2].

Moreover, the proposed SFT-PG algorithm aligns with the objectives of leveraging reinforcement learning methods to optimize policies for recommender systems and to control the training of GANs [4]. The use of reinforcement learning methods in diffusion models sets a new precedent as it is the first attempt to utilize RL methods for training diffusion models [1]. Additionally, the study addresses the challenges posed by the slow sampling of denoising diffusion models, aligning with recent efforts to improve the efficiency and performance of diffusion models using adversarial training and reduced reverse diffusion steps [5] [7] [8].

The innovations proposed in the study align with the broader landscape of research in generative modeling, addressing the trade-offs between high sample quality, mode coverage, and fast sampling, known as the generative learning trilemma [5]. Additionally, the proposed SFT-PG algorithm shares similarities with other reinforcement learning algorithms, such as deterministic policy gradient algorithms for continuous actions in reinforcement learning [9]. The integration of reinforcement learning methods for the training of diffusion models demonstrates potential for addressing the challenges and limitations faced by traditional generative modeling approaches.

The proposed Shortcut Fine-Tuning method also complements recent efforts to accelerate the inference process and maintain sample quality in diffusion models, such as the use of pseudo numerical methods for diffusion models [8]. Additionally, the proposed MMD-DDM method for fast inference in denoising diffusion models resonates with the need to improve the speed-quality trade-off, aligning with the strategies proposed in the target paper for enhancing fast DDPM samplers [12]. Overall, the proposed SFT method contributes to the advancement of reinforcement learning techniques for training generative models, addressing the challenges of fast sampling and sample quality in diffusion models."
3439,3439," **Visual Prompting.** Visual prompts have been used to improve the performance of language models [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45]. In this paper, we focus on the task of visual prompt engineering, where the goal is to learn a visual representation that can be used to guide a language model to focus on a specific region of an image.

**Intermediate Representations.** In this work, we use the intermediate representations of a pre-trained language model as intermediate representations to guide the model to attend to a specific image region. In addition, we also use these intermediate representations as the input of a Visual Question Answering (VQA) model. We use the VQA model as a visual encoder to generate a natural language prompt.



"," **Emergent Behaviour from Large Scale Pretraining** has mainly been observed in Large Language Models (LLMs). Most notably, GPT-2 [1], GPT-3 [2], and Chat-GPT  have been shown to be capable of tasks such as zero-shot translation, question answering, arithmetic, as well as planning actions for embodied agents [3]. Fine-tuning LLMs can also lead to models that can generate code from docstrings [4] or solve math problems [5][6]. Only a few emergent zero-shot behaviours have been reported for VLMs like CLIP, mainly for classification [7] and OCR [8]. Generative VLMs like FLAMINGO [9] and BLIP [10] excel in captioning and visual question-answering tasks, but also have no way of solving pixel-level computer vision tasks.

**Prompting VLMs** is most commonly performed by prepending a set of learnable tokens to the text input [11][12], vision input [13][14][15], or both text and vision inputs [16][17], in order to easily steer a frozen CLIP model to solve a desired task. [18] learn augmentations in pixel space, such as padding around the image, or changing a patch of the image, which are optimized with gradient descent on a downstream task. [19] cast image inpainting as a visual prompting task, using a generative model trained on figures from academic papers. Coloring regions of an image has been used for the VCR task [20], where a model is fine-tuned on annotated images [21]. Colorful Prompt Tuning (CPT) [22] color regions of an image and use a captioning model to predict which object in an image an expression refers to by predicting its color. Similarly to CPT, we augment the input image in pixel space and perform zero-shot inference. However, we _annotate_ the image in a human-like manner and show that our method is more powerful and more flexible than CPT.

**Referring Expression Comprehension** (REC) aims to localize a target object in an image that corresponds to a textual description. Most approaches to REC start with object proposals, for example, generated with Faster-RCNN [23], and learn to score them [24][25][26][27][28]. REC is sometimes considered together with referring expression generation -- the task of generating a description of a given region. [26] use a comprehension model to guide a generator,whereas [29] jointly train a detector with a caption generator. Some works model the scene as a graph [25][27][28] or use language parsers and grammar-based methods [30][31], leading to a more interpretable result. More recently, transformer architectures have been used [32][33][34][35][36]. [32][33][34][36] perform text-modulated object detection, where a transformer decoder takes the referring expression as an input and predicts a bounding box. [35] train with a text-to-pixel contrastive loss, which allows for a text-driven segmentation or detection at test time.

**Unsupervised Referring Expression Comprehension** is a less explored area, only made possible with the introduction of large pre-trained models such as CLIP [7]. ReCLIP [37] crops object proposals and ranks them using CLIP before an ad-hoc postprocessing step to take into account relations such as left/right, smaller/bigger, etc. CPT [22] colors object proposal boxes and use a pre-trained captioning model [38] to auto-regressively predict which colored proposal corresponds to the query description. Pseudo-Q [39] generates descriptions for multiple objects in an image, which is used to train a REC network. However, this model is not fully unsupervised as the pseudo descriptions it uses are generated using a captioning model trained on COCO.

**Visual Reasoning Using Large Pretrained Models** has been an area of significant interest in the last few years. In addition to referring expression detection [37], CLIP [7] has been used for semantic segmentation [40][41]. [41] use CLIP to assign text labels to object parts after doing part co-segmentation in the latent space of a GAN. [40] utilize CLIP for open-vocabulary segmentation by using a general-purpose mask proposal network and CLIP as a classifier. CLIP has also been used for unsupervised object proposal generation [42] and open-set detection [43]. Semantic segmentation also emerges from image only [44][45] or image-text [46] self-supervision.

**Bias of VLMs** is an increasingly popular area of research, as downstream applications come with the risk of perpetuating biases and stereotypes existing in the training data. However, methods for assessing the bias of a VLM are still not well established. [47] measure the misclassification rate of CLIP of faces of people of different races with non-human and criminal categories, whereas [48][49][50] measure fairness in retrieval results. Here, we show a different kind of bias, where the addition of a red circle over a person can trigger a negative connotation.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]"," Vision-Language Models.Vision-language models (VLMs) have been shown to learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35][36][37][38][39][40][41][42][43][44][45][46][47][48][49][50]. In this work, we focus on CLIP-based VL models, which have shown promising results in zero- and few-shot learning [2][1][4], image captioning [7][20], referring expression comprehension [26][29], and referring image segmentation [35].

**Visual Prompt Tuning.** Visual prompt tuning (VPT) [13] is a parameter- and data-efficient method for adapting large-scale vision-language (VL) models to a new task by adding a small amount of trainable parameters in the input space. VQT [14] proposes to aggregate intermediate features of the VL model's backbone, and VPT [13], VPT-VQT, VQPT [14], and VVQPT-M [13][17] propose to learn intermediate representations of the model by adding query tokens to each layer of the backbone, while keeping the model backbone frozen. PromptCAL [15] proposes a two-stage contrastive affinity learning method to learn a single transferable prompt for each target task. MVLPT [16] extends VPT by learning a single prompt vector from multiple source tasks to initialize the prompt for the target task, and then fine-tuning the model for the task of interest. CPT [22] presents a colorful prompt tuning method for VLMs, where the model is trained to produce a color-coordinated version of the image caption.


\begin{tabular}{c}\[22]\[\]\] \[\[c[16]=\[p_{i}_{i=1}^{n}\)_{i+1}_{j}_{k}=1\]_{j=0}^{k}"," **Pose Transfer with Paired Data.** Early methods require 3D meshes with paired poses as supervision, which are difficult to obtain [1]. Recently, researchers attempt to transfer a pose from a given 3D mesh to another given target mesh with no paired data. Gatys _et al._[2] solved the pose transfer task as an image style transfer [3][4][5][6] by using a dense correspondence between the two meshes. However, Gatys _et al._[2] did not consider the shape and topology of the source and target meshes, which will cause the target to change its shape in the process of deformation.

The previous pose transfer methods have shown promising results, but their performance are limited when the pose transfer is required between characters with dissimilar topology or shape. To tackle this problem, Wang _et al._[7] proposed a skeleton-free pose transfer method to handle the skeleton-free meshes with diverse shapes, topologies and mesh connectivities. Li _et al._[8] devised a simple neural network that takes as input two 3D shapes and produces a deformation field and the point-to-point correspondences.

The methods above solve the 3D pose transfer problem in an end-to-end manner, which typically requires large amounts of supervision. The other direction, unsupervised methods, have been proposed recently and shown their effectiveness [9][10]. However, most of these methods require additional loss, such as shape consistency  and pose consistency [10], which affects the performance of the model.

**Shape Transfer.** The key problem of shape transfer is to deform a mesh to another given mesh while maintaining its identity. Traditional methods [11][12][13][14][15] usually rely on the spatial deformation such as biharmonic coordinates [16] for shape deformation. Recently, deep learning has been adopted for mesh deformation, achieving the best performance. Wang _et al._[13] proposed a cage-based neural network that predicts the deformations by controlling the cage. Peng _et al._[14] learned a set of deformation meta-handles and each of them represents an intuitive deformation.

",,"<Large-scale Vision-Language Models, such as CLIP, have gained significant attention in the field of computer vision and natural language processing [1]. These models have been shown to learn powerful image-text representations, enabling applications ranging from zero-shot classification to text-to-image generation. However, their capabilities for solving novel discriminative tasks through prompting have been observed to lag behind those of large language models [2]. In the context of visual prompt engineering, this study explores an innovative approach to directing the attention of CLIP by simply drawing a red circle around an object, enabling the model to focus on that region while retaining global information [target paper]. The study demonstrates the effectiveness of this simple approach by achieving state-of-the-art performance in zero-shot referring expressions comprehension and strong results in keypoint localization tasks.>

<Some potential ethical concerns related to large language-vision models are also discussed in the target paper. These concerns revolve around the potential biases and societal stereotypes encoded into the models as well as the implications for broader applications and downstream tasks [48]. The study looks at the challenges of characterizing the broader capabilities of CLIP and the downstream implications of its use [47]. It presents an analysis of the biases and societal stereotypes encoded within vision-language models, highlighting the potential ethical risks posed by such models.

The related work in this domain also addresses the issue of bias in vision-language models [49]. The study proposes a method for debiasing vision-language models by projecting out biased directions in the text embedding. By focusing on methods to mitigate biases and address ethical concerns, the research contributes to the broader discourse on fair and unbiased AI models in vision-language tasks.]

[1] Radford, A., et al. (2018). Language Models are Unsupervised Multitask Learners.
[2] Brown, T. B., et al. (2020). Language Models are Few-Shot Learners.
[47] Author. (Year). Title.
[48] Author. (Year). Title.
[49] Author. (Year). Title."
1751,1751," **Anchor-based Lane Detection.** Anchor based methods [1][2][3][4][5][6][7][8][9][10][11][12] have been widely used in lane detection due to their simplicity and effectiveness. These methods first detect lane lines in an image and then group them into different lanes. LaneASD [1] and RESA [2] adopt spatial convolutional networks to extract features from the whole image and apply spatial pooling to aggregate the features. LaneAF [3] and LaneATT [5] adopt a two-stage pipeline, where the first stage performs pixel-wise segmentation and the second stage predicts the binary segmentation masks and per-pixel affinity fields for lane lines. Line-CNN [4] proposes a line proposal unit (LPU) to generate lane proposals, which is followed by a segment clustering step to generate the final lane line predictions. LaneATT  introduces a lane center regression module to refine the segmentation results of anchors. These anchor-based methods mainly focus on improving the quality of anchors, while ignoring the limitations of anchors that stem from the edges of the image. To overcome this limitation, we decompose the lane detection task into learning the heat map of starting points and their associated directions, making our algorithm adaptable to different lane types in various datasets.

**Lane Shape Prediction.** In recent years, several methods have been proposed to predict the lane shape directly from the image, which can be roughly divided into two categories. The first category is based on the anchor-free methods, which directly predict the shape of lane lines from the input image. For example, CornerNet  and CenterNet  predict the curve of the lane lines by predicting the center point of each lane line. However, these methods are sensitive to the initial position of the anchors, which limits their performance in complex scenarios. To address this issue, some methods [7][10] propose to use the predicted heat maps as the intermediate representation for lane shape prediction. CondLaneNet [7] and CLRNet [8] adopt the conditional convolution to model the topology of lanes. The second category predicts the lane shapes directly from input images. For instance, KeyPointNet [9] and FocalNet [10] first predict the key points of each pixel and then associate these key points with the corresponding lane lines to form the final lanes. Global Association Network (GANet) [11] and Deformable Convolutional Networks (DCN) [12] deform the convolution kernels to learn the deformation of lane shapes. These two types of methods have achieved promising results on various datasets, but they are still limited by the fixed anchor shape. In this paper, we propose a novel anchor decomposition method to overcome the limitations on the starting point of anchors and improve the performance of these methods.

 propose a method to predict lane shapes from the heat maps of anchors by learning the direction of the starting points. This method uses the learned directions to guide the feature extraction of the feature pyramid network (FPN). However, this method does not"," In segmentation-based methods, the task of identifying lane lines has been converted to a per-pixel prediction task. [1] first introduces a spatial mechanism passing messages between pixels row-wise and column-wise that fails to perform in real time. [2] further proposes a recurrent aggregator fully utilised lane shape priors to obtain better performance. On [3], additional affinity fields are predicted simultaneously with the binary segmentation map, which is used in the decoder to cluster lane pixels. Segmentation-based method can achieve high accuracy when lane lines are visible, but it's unstable in complex traffic scenarios and inefficient.

Anchor-based & detection-based methods define lane lines in a similar way. They divide an image into slices or cells, and then convert the lane detection task into either offsets' regression on each slice or a row-wise classification task.  first predicts lane lines via a simple linear layer using row-wise classification. [4] improves the representation of lane lines by converting cell representation into anchor representation, and identifying lane shape through regression of the offsets on every slice between anchors and ground truth. [5] further enhances this formulation by adding anchor-based pooling and a lane attention mechanism to it. [6] proposes a hybrid anchor system to improve the performance of UFLD. [7] proposes a conditional convolution and RIM migration to solve the instance-level discrimination problem on lane detection. [8] develops ROIGather to fuse lane context from different layers and, for the first time, changes the anchor-based formulation into an anchor-free manner, achieving state-of-the-art performance on multiple benchmarks.

Anchor-based and detection-based methods heavily rely on the position of anchors. On one hand, this can bring higher accuracy since anchors contain prior information on lane lines. On the other hand, these inherent properties lead to some shortcomings, such as the starting point of the anchor may not always be located on the three edges of the image, limiting its application.

Keypoint-based methods treat lane lines' prediction as a key point estimation task. Usually, the algorithm will first predict all the possible key points that most likely belong to lane lines, and follow up with a post-process of assigning different points to different lanes. [9] predicts key points on lane lines and distinguishes each instance by embedding features of predicted points. [10] predicts local key points in a bottom-up manner and refiners key points' location via its offsets between adjacent points. [11] clusters points via offsets between key points and start points, and a modified deformable convolution network [12] to extract holistic lane features. Lane instances are predicted by keypoint-based methods via low-efficient post-processing of key points from the heat map, moreover, the accuracy of the algorithm highly relies on the resolution of the input image, together with time-consuming post-processing, making keypoint-based methods hard to strike a balance between latency and accuracy.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"," **Anchor-based Lane Detection.** Anchor-Based Lane Detection (ABLD) methods have been widely used in recent years due to their simplicity and effectiveness. They have predominantly focused on anchor-based methods, which are based on the anchor-stacked hourglass architecture. These methods first detect the anchor points and then group them into lanes. For example, SCNN [1] and RESA [2] use the spatial convolution to pass messages between pixels across rows and columns of feature maps, which enables message passing between lanes and poles. Liao _et al._[3] propose a lane detection method based on a binary affinity field to cluster lanes. Line-CNN [4] utilizes line proposals as references to locate accurate traffic curves, which forces the system to learn the global feature representation of the entire traffic lines. LaneATT [5] proposes an anchor-driven representation to aggregate global information to improve the localization accuracy. LaneID [6] and CondLaneNet [7] use a top-to-down network to first detect lane lines and then predict each lane line as a whole. CLRNet [8] first detects lanes with high-level semantic features and then performs refinement based on low-level features.


\({}^{2}\)Lane [9] and FOLOLane [10] propose to detect lanes from the bottom-up manner. PINet [9], which is a keypoint-based method, first predicts the existence of key points of the same lane line and then groups them into different lanes. FOLolane [11] proposes to regress the offsets to the corresponding starting points of lanes globally without dependence on each other, which could be done in parallel to greatly improve the efficiency. LFA [12] introduces two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling, which can readily replace their plain counterparts in existing CNNs and can be easily trained end-to -end by standard back-propagation.

"," With the tremendous success of CNNs in several fields [1][2], there has been an influx of lane detection methods in recent years. One of the most common approaches is to rely on CNNs to generate a compact high-level representation for all lanes, and then to use global information to guide the process of localization and clustering. These methods include [3][4][5][6], where extra modules are employed to generate global feature map for lane detection. Although these methods are quite successful in complex scenarios, they may lack flexibility and efficiency to accommodate different lane types.

Another approach focuses on fine-grained local information by detecting points on lanes [7][8][9][10][11] and then clustering the detected points into lanes. They also rely on global information in the post-processing to optimize their result. The key difference is that instead of first detecting the lanes, they directly predict the position and number of key points on lanes with pre-defined shapes.

Inspired by the success of the deformable convolutional networks (DCNs) [12] in the scene parsing problem, Zhu _et al._ proposed the IDEAL network to utilize the global features to guide the lane detection. However, IDEAL has a bottleneck that localizes lanes through anchors, which limits the diversity of detection. Zhou _et al._ proposed SLaneDet with row-wise anchors as the detectors to further improve the localization precision. By contrast, our system not only relies on anchors to localize lanes but also decomposes the anchors into anchors and starting points, which makes our system more flexible than the existing methods.

",,"<In the field of lane detection, several recent works have focused on improving the performance of anchor-based methods. For example, Spatial As Deep (SCNN) [1] proposed a Spatial CNN that uses slice-by-slice convolutions to capture spatial relationships between pixels, enhancing the detection of long continuous shape structures like traffic lanes. Similarly, RESA [2] introduced the Recurrent Feature-Shift Aggregator (RESA) to enrich lane features by capturing spatial relationships of pixels across rows and columns. LaneAF [3] presented an approach involving per-pixel affinity fields to cluster lane pixels into corresponding lane instances. These methods have demonstrated improved performance in detecting and segmenting lanes in challenging scenarios.

Other works have explored the use of anchor-driven approaches to achieve superior lane detection results. For example, Ultra Fast Deep Lane Detection (UFLD) [6] proposed an anchor-driven ordinal classification formulation for lane detection based on global features, achieving top-tier performance in terms of both speed and accuracy. Similarly, Keep your Eyes on the Lane (LaneATT) [5] proposed an anchor-based deep lane detection model with an attention mechanism that aggregates global information, outperforming the current state-of-the-art methods in efficacy and efficiency.

Moreover, recent works have also investigated novel network designs and strategies for lane detection. CondLaneNet [7] introduced a top-to-down lane detection framework that dynamically predicts line shapes for each instance using conditional convolution and Recurrent Instance Module (RIM). CLRNet [8] presented the Cross Layer Refinement Network, utilizing both high-level semantic and low-level features to detect and refine lanes, showcasing substantial improvements over existing methods. These approaches reflect the increasing trend towards developing innovative network architectures and strategies to enhance lane detection performance.

In addition, some works have proposed alternative approaches to lane detection. Key Points Estimation and Point Instance Segmentation Approach [9] focused on adaptive perception techniques and proposed the Point Instance Network (PINet) for traffic line detection based on key points estimation and instance segmentation, achieving competitive accuracy and false positives on popular datasets. Focus on Local (FOLOLane) [10] addressed the limitations of complex lane marker detection methods by focusing on modeling local patterns and predicting global structures in a bottom-up manner, demonstrating superior performance and real-time processing capability.

Furthermore, recent advancements in the field of lane detection have also benefited from innovations in the capabilities of convolutional neural networks. For instance, Deformable Convolutional Networks [12] introduced deformable convolution and deformable RoI pooling to enhance the transformation modeling capability of CNNs, showing promising results in tasks such as object detection and semantic segmentation. These developments highlight the potential of leveraging advanced neural network modules to enhance the performance of lane detection systems.>"
3646,3646," **Deep Learning on Point Clouds.** PointNet [1] is the first deep learning framework for point cloud processing, which directly takes raw unordered point clouds as input and applies a symmetric symmetric function to extract features. PointNet++ [2] extends PointNet by introducing a hierarchical structure to extract multi-scale features. Wang _et al_. [3] propose a relation network to learn inner-group relations between points. Recently, many works [4][5][6][7][8] have been proposed to improve the performance of deep learning on unordered and unstructured point clouds. Point Transformer [5] is a transformer-based network that operates directly on point sets. However, these methods are designed for supervised learning and cannot be directly applied to unsupervised point cloud shape correspondence.

**Unsupervised Point Cloud Shape Correspondence.** Traditional methods [9][10][11][12][13][14][15][16] mainly focus on designing hand-crafted descriptors and matching functions to establish dense point-to-point correspondence. For example, Liu _et.al._[13] propose to represent the deformation and combination of learnable elementary 3D structures, which are primitives that are invariant to the shape deformation, to establish the correspondence. Li _et_. [15] design a linear-invariant embedding function to learn the correspondence between two point clouds by minimizing the Euclidean distance between the two point cloud pairs, which can be regarded as anisometry. Li [16] introduce a deep neural network to estimate the dense correspondence between point clouds, which is able to handle the non-rigid deformations. However these methods require a large number of point pairs to train the network, which may be laborious and time-consuming. In this paper, we propose a self-ensembling framework to solve the problem of the mispredictions of symmetrical parts and noise in point cloud, which significantly improves the accuracy of correspondence learning without using any point pairs. Besides, we design an orientation estimation module with a domain adaptive discriminator to align the orientations of point clouds in the feature space, which alleviates the misclassification of symmetric parts and improves the robustness of the correspondence learning. In addition, we introduce an auxiliary loss to constrain the consistency of the predictions of the student and teacher networks, which improves the generalization ability of the network and achieves state-of-the-art performance for un-supervised shape correspondence in both supervised and un-unsupervised learning settings. **Semi-Supervised Learning.** Our work is also related to the semi supervised learning (SSL) [17][18][19], which aims to learn from unlabeled data by using a teacher network and a student network simultaneously. In [17], the teacher network is used as a regularizer to enforce the consistency between the outputs of the two networks, while in [18], a temporal ensembling loss is proposed to enforce consistency between two networks. In contrast, our method is designed to enforce consistent representations between the teacher and student networks"," In this Section, we give a brief overview of related works on point cloud shape correspondence, including learning on point clouds, shape correspondence, and self-ensembling approaches.

**Learning on Point Clouds.** PointNet [1] learns from global information through multi-layer perceptrons and max-pooling operation. PointNet++ [2] devises a hierarchical architecture that recursively partitions the point cloud to extract local features more effectively. Recent works explore local aggregators via relations [3][4][5], and graphs [6][7]. PointCNN [8] transforms neighboring points to the canonical order to apply traditional convolution on point clouds. DGCNN [6] creates a graph in the feature space and designs EdgeConv [6] to learn the edge features of the graph in each layer. However, the methods are commonly based on some assumptions of implicit local geometry, which may result in sensitivity to point cloud disturbances.

**Shape Correspondence.** As an active research area in computer vision and graphics, point cloud shape correspondence methods roughly include spectral-based methods [9][10][11][12] and point-based methods [13][14][15][16]. Spectral-based methods require connectivity information to compute the LBO eigenvectors as basis functions and infer a linear transformation for shape correspondence. However, with regard to point cloud data, connectivity information is difficult to obtain directly while point-based methods directly process point clouds without connectivity information to find the dense point-to-point mapping between two point clouds with deformable 3D shapes. Deprelle et al. [13] propose representing shapes as the deformation and combination of learnable elementary 3D structures. Groueix et al.  employ an encoder-decoder architecture to obtain and constrain the similarity matrix with manually annotated labels. The deep learning methods train their neural networks in a data-driven manner and improve performance to a large extent. However, manually labeling the point-to-point cor respondence between two point clouds in 3D space takes much time and effort. Therefore, some unsupervised point cloud shape correspondence methods [14][16] are proposed to reduce the overhead of labeling. CorrNet3D [16] is the first unsupervised deep learning framework. DPC [14] designs several reconstruction losses to smooth point cloud representations. Due to the lack of annotation, the mismatching issue of symmetrical parts in point clouds with different orientations has become an undeniable problem in unsupervised shape correspondence area.

**Self-ensembling Approaches.** Self-ensembling approaches improve the model generalization by encouraging consensus among ensemble predictions of unknown samples with small perturbations. \(\Gamma\) model [17] consists of two identical parallel branches that respectively take raw images and corrupted images as inputs. In contrast to \(\Gamma\) model, \(\Pi\) model [18] integrates two parallel branches into a single branch. As an extension of the \(\Pi\) model, the temporal model [18] forces the consistency between the outputs and the aggregated outputs over previous training epochs. Mean Teacher [19] replaces network prediction average with network parameter average via the exponential moving average (EMA) strategy. We design a framework similar to Mean Teacher and adapt it for the unsupervised point cloud shape correspondence task. The proposed framework facilitates the network to yield consistent and accurate predictions under noise perturbations and orientation rotations.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"," **Point cloud shape correspondence.** PointNet [1] is the first attempt to directly consume point clouds for 3D classification and segmentation tasks. PointNet++ [2] extends PointNet by introducing a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point cloud. Point Transformer [3][4][5][6][7][8] is proposed to extract local and global features and relate both representations by introducing the local-global attention mechanism, which aims to capture spatial point relations and shape information. However, PointNet does not capture the local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes.

**Shape correspondence with anisotropic deformations.** Traditional methods [9][10][11][12][13][14][15][16] are based on hand-crafted geometric features. For example, the Laplacian-based methods [11][10] use the Laplace-Beltrami operator [9] to compute the correspondence between two point clouds. The spectral-based method [11] uses the spectral decomposition of the Laplanacian operator associated with the shape to obtain the correspondences. The point-to-point correspondence method [12] uses a random sampling method to find the matched points and then use the matching information to select the most similar points. The unsupervised method [13] learns the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. The deep learning-based approach [14] uses latent similarity and the input coordinates themselves to construct the point cloud and determine the correspondence, replacing the coordinate regression done by the decoder. The recent deep learning based approach [16] uses deformation-like reconstruction to drive the learning of dense correspondence between 3D shapes by means of deformation like reconstruction to overcome the need for annotated data. However these methods are not suitable for the unstructured point cloud correspondence task, which is more challenging than the shape correspondence task.

"," Neural Radiance Fields.Most NeRF-based methods are designed to synthesize novel views of static scenes. The majority of them are based on volume rendering (or volumetric rendering). Volume rendering means that a volume is scanned and rendered in each viewing direction, similar to rasterization, but without the need to actually store and render 3D meshes. [1][2] review current volume rendering techniques.

Prior works on NeRFs mostly focus on improving view synthesis quality. As noted by [1], the image quality is directly correlated to the number of captured images. To address this, some works have proposed to collect more input images, with view synthesis methods improved by adding multi-view constraints [3][4][5], synthesizing low dynamic range images with multi-exposures [6], or using burst denoising [7]. A new sampling method has also been introduced [8], rendering is boosted with artifact-reducing image post-processing [9], and a more controllable and parametric NeRF generation is studied [10]. As for novel view synthesis quality, [11] introduced a view-dependent structured appearance model. Finally, as briefly discussed in Section 1, in its NeRF-SyN model [12] the water medium is added as a volumetric scattering medium with a phase function, but it does not support dynamic scene reconstruction.

However, these approaches assume that the medium is absent. NeRF still remains unable to properly model media like water, even in still scenes. The reason is that the medium may be local, and it is impossible to separate the local medium effects from the scene structure in volume rendering. The majority of related works have used feature-based methods [13] or particle-based Monte Carlo methods [14][15][16] to simulate media scattering effects. These methods are too expensive for practical use. The task of recovering a single scattering model [17][18][19] has also been explored, but there is no well-accepted analytical solution to the problem yet.

A different strategy has been proposed in a series of works [20][21][22], which attempts to acquire the medium information (phase function, scattering coefficient, etc.) from multiple images. This has the potential to be a promising way of estimating the scattering properties of the medium.

For all the aforementioned works, the scene and the medium are treated as separate entities. That is, the media effects are incorporated into the model for scene synthesis only via a medium model. To our knowledge, no work has attempted to model both the medium and scene simultaneously using NeRF, so we are the first to propose a complete model for media.

Underwater Scene Synthesis.Compared to terrestrial environments, underwater scenes present different and more challenging problems. Significant lighting conditions,",,"<The problem of unsupervised point cloud shape correspondence has been the focus of recent research, with various methods proposed to address the challenges associated with orientation, noise, and symmetry. PointNet and PointNet++ [1][2] introduced neural network architectures designed to directly process point cloud data, respecting the irregular format of point sets and demonstrating strong performance in 3D classification and segmentation tasks. RPNet [3] explored the use of local relation operators for efficient classification and segmentation on point clouds, while PointASNL [4] presented a robust network for point cloud processing in the presence of noise. These works form a foundation for addressing challenges related to noisy and irregular point cloud data.

Several methods have specifically focused on addressing the challenges of orientation-aware point cloud processing, bearing direct relevance to the target paper. For instance, Point Transformer [5] introduced a network that operated directly on unordered and unstructured point sets, extracting local and global features with the aim of capturing spatial point relations and shape information. Additionally, AdaptConv [7] proposed an adaptive graph convolution approach to improvise the flexibility of point cloud convolutions and capture diverse relations between points from different semantic parts. These orientation-aware methods provide insights into handling orientation-related challenges in point cloud analysis.

Furthermore, recent efforts have also focused on self-ensembling and unsupervised learning techniques, aligning with the proposed approach in the target paper. PointCNN [8] presented a framework for feature learning from point clouds, addressing the challenges posed by irregular and unordered data, and demonstrating competitive performance on benchmark datasets. Additionally, semi-supervised learning techniques, including Ladder Networks [17] and Temporal Ensembling [18], have been explored in the context of deep neural network training with limited labeled data. These methods are highly relevant to the proposed self-ensembling framework for unsupervised point cloud shape correspondence in the target paper.>

Overall, the related work encompasses methods that address challenges related to point cloud noise, orientation-aware processing, and unsupervised learning, forming a strong foundation for addressing the key issues targeted in the SE-ORNet model. The combination of orientation-aware processing, self-ensembling techniques, and the utilization of deep neural networks for point cloud analysis aligns well with the objectives outlined in the target paper. The proposed SE-ORNet model can benefit from insights and methodologies presented in these related works to further advance the state-of-the-art in unsupervised point cloud shape correspondence."
5544,5544," **Latent Space Interpretability of GANs.** There has been significant recent interest in discovering interpretable directions in the latent space of pretrained generative models. [2]; [4]; [3]; [1]; [5]; [6] use unsupervised methods to discover interpretable latent directions that can be used to control the generation process of images. [4] and [3] use orthogonal Jacobian regularization (ORJ) ([5]) to regularize the GAN latent space to encourage disentanglement. [1] use a contrastive learning approach ([7]) to find directions that are semantically meaningful for human-interpretable image transformations.  use a similar approach to find interpretable semantic directions for face image generation. However, these methods rely on human-annotated labels for supervision, which is not always available.


**GAN Auditing.** GAN Auditing is an emerging area of research that aims to provide transparency and accountability to the training and testing process of generative model developers. Existing GAN auditing methods can be broadly categorized into two groups: (1) model-based and (2) attribute-based. Model-based methods ([11]; [10]; [9]; [8]; ) compare the outputs of a pre-trained GAN against a reference model to identify model-independent properties that are relevant to the generated images. These methods are typically based on FID, recall, or recall-based metrics. In contrast, our approach identifies attribute-level similarities and differences between the reference and a newly-developed GAN. Our approach is complementary to these methods as we can jointly identify attributes that are common across both reference and new GAN models, as well-defined as well as novel to the reference GAN and are missing from the client GAN, which allows us to provide a more fine-grained assessment of similarity and difference between the two models. Our work is also related to recent work on attribute alignment ([12]; [13]; [14]; [15]). However, our work differs from these prior works in that we focus on attribute level similarity and differ from them in the way we identify them.

 propose a model-agnostic method for attribute level comparison between two GAN architectures. They use contrastive loss functions to align the latent representations of the reference model and a new client model. Our method differs from theirs in that our method identifies attribute levels that are both common and novel across the two architectures and that are not represented by the contrastive losses.

 introduce a GAN-based method that compares the output of a pretrained GAN model against a randomly sampled reference model. Their approach is similar to ours in the sense that they compare the output distributions of the two generators, however, their approach is based on the idea that the reference generator can be seen as a proxy for the target generator and the target GAN can be viewed as a ""client"" model. We differ from this approach as we do not use a proxy model and instead use a reference-based approach to identify attribute levels.

 use a self-supervised"," **Attribute Discovery** Several approaches have been successful in extracting attribute directions in StyleGAN's latent space in the past few years. InterfaceGAN [1] used an external classifier and human annotations to label sampled images in order to build a simple linear model that captures the attribute direction in a GAN's latent space. GANSpace [2] applies PCA to these intermediate representations to find the large factors of variation and then reprojects these directions onto a GAN's latent space. Similarly, SeFa [3] directly captures these directions via matrix factorization of the affine mapping weights in styleGAN, which identify directions of large changes without the need to sample the latent space. An alternative strategy is to directly learn the interpretable directions through a jointly-trained predictive model by assuming that the more predictive variations are more likely to be semantically meaningful [4]- or that using a Hessian penalty , or Jacobian [5], in the image space enables learning of directions. LatentCLR [6] used a similar optimization framework, but instead of training a separate predictive model, it leveragedthe GAN's internal representation and adopted a contrastive loss [7] for attribute discovery.

**Model Auditing** With increased awareness of the societal impact of machine learning models, there is an increased interest in characterizing and criticizing model behavior under the broad umbrella of auditing [8][9]. There has been relatively less work in auditing generative models. For example, [10] introduce a new performance metric for generative models that measures fidelity, diversity, and generalization. Another related work is from Bau et al., [11] who investigate what a GAN cannot generate, whereas our interest is in distinguishing a client GAN from a reference GAN.

**Interpretation of Domain Shift** Some of the most related work comes from methods that aim for characterizing domain shift [12][13], but these methods are limited to specific settings: either relying on human intervention [12] or needing a disentangled generator in the input [13]. An indirect way to obtain aligned attributes is via _aligned GANs_- GANs where one is fine-tuned from the other [14][15]. Here, the attribute direction will be inherent to the children models, eliminating the need to do joint discovery to identify similar attributes. However, obtaining an _aligned GAN_ through a separate fine-tuning process for attribute discovery across distributions is neither practical nor feasible.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"," **Latent space discovery.** The discovery of interpretable directions in the latent space of pretrained GANs has been extensively studied in the literature [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]. These directions enable controllable image editing operations, such as zoom or rotation, and support a wide range of semantic editing operations. However, these methods are typically performed in a supervised manner and require manual annotations of the target attribute. In contrast, our approach is unsupervised and does not require any form of supervision.

**Attribute level GAN auditing.** Prior work has studied the identification of attribute-level similarity and difference between GAN models. For example, [1] identify the disentangled face representation learned by the state-of-the-art GAN model and study the properties of the facial semantics encoded in its latent space. [2] identify interpretable GAN controls by applying Principal Component Analysis (PCA) on the first layer of the pretrained model. [3] propose a closed-form factorization approach to decompose the semantic representations learned by pretrained models into semantically meaningful factors. [4] find directions corresponding to sensible semantic manipulations without human labels by a model-agnostic procedure. [5] introduce a simple orthogonal Jacobian regularization method to encourage the variation of output caused by perturbations on different latent dimensions to be orthogonally orthographic. [6] propose LatentCLR, a contrastive learning-based approach to discover interpretable latent directions. Latentangled in a self-supervised manner. [7] propose SimCLR that uses a memory bank to store a large amount of unlabeled data to learn discriminative representations. [14] propose StyleAlign, an approach to perform cross-domain image translation by aligning two aligned StyleGAN models. [15] propose Resolution Dependent GAN Interpolation (RDI) to perform image-to-image translation between domains. However these approaches are limited to coarse-grained model-data comparisons based on summary statistics such as FID or recall. In this work, we propose a novel approach to identify attribute-based attribute level similarities and differences between GANS.


"," **Image Generation with StyleGAN2:** A comprehensive overview of the pioneering StyleGAN2 and its predecessors can be found in [1]. In recent years, several approaches have been proposed to generate better images with enhanced fidelity using large datasets such as BigGAN [2] and multi-scale generation [3]. State-of-the-art image editing methods [4][5] have used StyleGAN2 to enable local-editing based on the semantic encodings of the source image. Another recent line of work has focused on the discovery of meaningful dimensions in the latent space of StyleGAN2 and their use for editing [6][7][8][9][10][11][12]. StyleCLIP [4], which we build on in this work, is the pioneering approach in this direction. To improve the efficiency and speed of such editing, the authors proposed StyleGAN2 and CLIP2StyleGAN [4] which leverages CLIP to accelerate image editing. Our work builds on these approaches and, in the process, addresses several challenges of StyleCLIP and provides a fast co-optimized method to achieve high-quality edits.

**Text-to-Image Generation:** A recent line of work focuses on the use of generative models such as StyleGAN2 [4] to manipulate source images with text-based natural language supervision [13]. This approach alleviates the issues faced with semantic encodings and manual exploration in editing [14][15][16][17][18]. More recently, these models have been further improved with the use of Transformers [19]. Various efforts have also been made to speed up editing and achieve higher fidelity [20][21]. Different from this work, FEAT [22] and StyleFusion [23] propose to only focus on fusing (modifying) the spatial dimension of StyleGAN2 by concatenating multiple latent codes. Our method focuses on how to do the spatial fusion and blending (integrating) of multiple layer representations of StyleGAN2 with different semantic attributes.

**Spatial Aggregation in Image Editing:** A few prior works focus on spatial aggregation in image editing [15][24]. These works are often slower, less reliable, or both. While most of these methods follow a two-stage process in which the primary image is edited and the result is then processed to yield a more plausible end result [15][24], our method allows for both plausibility and efficiency by integrating temporal sequences in the spatial fusion process.

",,"<Generative Adversarial Networks (GANs) have proven to be effective in generating realistic images, but auditing these models has become increasingly important, especially in identifying biases and ensuring fairness. Several recent works have focused on interpreting the representations learned by GANs, allowing for the manipulation of various facial attributes such as gender, age, and expression [1]. These studies have shed light on the disentangled face representation and the properties of facial semantics encoded in the latent space. Additionally, techniques for discovering interpretable controls for image synthesis have been proposed, outlining the analysis of GANs and the creation of interpretable controls for various transformations [2]. Furthermore, methods for identifying latent semantics for image editing in a more unsupervised manner have also been explored, leading to the discovery of semantically meaningful dimensions in the latent space of GANs [3].

Another line of related work focuses on the unsupervised discovery of interpretable directions in the latent space of GANs, enabling controllable image generation and supporting semantic editing operations [4]. These approaches have addressed the issue of identifying interpretable directions in the latent space without relying on supervised or semi-supervised methods. Moreover, other techniques have been developed to encourage deep generative models to learn disentangled representations, effectively enabling disentangled and controllable image generation [5]. These works have proposed novel frameworks and algorithms to address the challenges associated with unsupervised disentanglement learning. The advancement of unsupervised contrastive learning-based approaches to discover semantic directions in the latent space of pre-trained GANs has also been explored, offering self-supervised methods for finding semantically meaningful dimensions [6].

In the context of contrastive learning, recent efforts have introduced frameworks for contrastive learning of visual representations. These frameworks simplify the process of contrastive self-supervised learning and have shown improved performance in self-supervised and semi-supervised learning tasks [7]. Additionally, mechanisms for active fairness auditing and end-to-end frameworks for internal algorithmic auditing have been proposed, aiming to address the challenges of auditing machine learning models and ensuring fairness in AI systems [8, 9]. These works provide insights into the development of audit frameworks to evaluate and monitor AI systems to ensure accountability and fairness.

Furthermore, the development of evaluation metrics for generative models and the identification of distribution shift have been important areas of research. Several studies have introduced evaluation metrics that characterize the fidelity, diversity, and generalization performance of generative models in a domain-agnostic fashion [10]. Moreover, efforts have been made to detect and address distribution shift and identify covariate shift in large image datasets, emphasizing the importance of human-interpretable techniques in characterizing the extent of covariate shift [13]. These works have provided valuable insights into the challenges of detecting distribution shift and addressing covariate shift in machine learning.>"
5553,5553," **Dual-pixel photography.** Dual-pixel cameras [1][2][3][4][5] are capable of capturing a wide range of depth of field, resulting in a severely blurred image, degrading the depth estimation performance of conventional RGB-D cameras. However, there are still several challenges to fully utilizing the dual-pixel camera. First, the disparity of the dual pixel image is bidirectional, which includes both positive and negative values, depending on the focus plane depth in an image. Second, the blurriness of the captured image is proportional to the depth of the disparity. In contrast to the conventional stereo image, the DPP image contains a blur kernel that is spatially-symmetric, which means that the disparity can be estimated from a single pixel image.

**Supervised stereo learning.** Supervised learning methods [6][7][8][9][10][11][12][13][14][15][16][17][18] have been proposed to solve the stereo matching problem with ground-truth disparity labels. These methods are usually trained on synthetic datasets [11][10] or synthetic images [13][16] generated by rendering the stereo images with different viewpoints and camera poses. These synthetic datasets are limited in terms of diversity and resolution, which limits their generalization ability to real-world scenarios. To overcome these limitations, self-supervised methods are proposed to learn the disparity from unlabeled real images. For example, Wang _et al_. [6] proposed a pyramid stereo matching network (PSMNet) that learns the disparity directly from the left and right stereo images. The PSMNet consists of three sub-networks: disparity estimation, disparity refinement, and disparity refinement. The disparity refinement sub-network first estimates the disparity and then refines the disparity with the refinement subnet. The refinement subnetwork consists of two subnetworks, one for disparity refinement and the other for cost aggregation. The first subnetwork estimates the left-right disparity and the second subnetwork computes the right-to-left disparity by computing the cost of each pixel in the left/right image. The cost aggregation subnetwork then aggregates the estimated disparity to obtain the final disparity. The main drawback of these methods is that they require a large number of training images with ground truth disparity labels, which are difficult to obtain in practice. In this work, we propose a self-learning method that can learn the depth from the single-pixel image.

 proposed to use the anisotropic blur kernels to estimate the disparity, which is a special case of the bidirectionality of the DP disparity. They proposed a method that estimates the depth using the blur kernels of a single DP image. Their method is based on the assumption that the DP image has an anisotropy function, which assumes that the blur kernel is a linear function of the focal length of the camera. In practice, however, this assumption does not always hold. In addition, their method does not work well when the camera poses are different from the ground truth.

 and  proposed to estimate disparity from the dual image. They use"," Dual-pixel depth estimation.Since dual-pixel cameras have been released, many works have been proposed to estimate depth from a dual-pixel image. Garg _et al_. [1] present a learning-based dual-pixel depth estimation using the affine invariant objective to estimate inverse depth. Zhang _et al_.  introduce a supervised learning method similar to [1] to estimate disparity by using two dual-pixel cameras. Recently, Pan _et al_. [2] propose a dual-pixel simulator and also presented the learning-based inverse depth estimation method, which is trained with their simulated data. Xin _et al_. [3] present an unsupervised optimization method based on the estimated defocus map, which can also be shown as an inverse depth map in dual-pixel data. These methods estimate unidirectional information of inverse depth maps [1][2][3] only, often assuming that the focus plane is fixed to the nearest or the farthest location of the scene [3].

Punnappurath _et al_. [4] show that the sign of a bidirectional dual-pixel disparity changes depending on the focus-plane depth in an image. Also, Wadhwa _et al_. [5] apply traditional stereo matching on the separated stereo images from a dual-pixel image to estimate bidirectional disparity. These two traditional methods [4][5] discover the bidirectional nature of dual-pixel disparity and anisotropic blur kernels. However, their performance often degrades due to severe defocus blur of a shallow depth of field and the affine ambiguity of bidirectional disparity w.r.t. focus-plane depth. A learning-based approach would be impactful in mitigating these challenges as the problem is severely ill-posed. However, as mentioned earlier, there is no dual-pixel dataset, including pairs of dual-pixel photographs and bidirectional disparities for every focus-plane depth.

In contrast, our self-supervised method estimates bidirectional disparities while implicitly imposing the reflective-symmetry constraint in anisotropic kernels.

Learning-based stereo.Learning-based stereo methods can be divided into two groups. First, _supervised_ learning-based approaches [6][7][8][9] have been proposed that train neural networks with traditional stereo image datasets [10][11][12]. These network architectures resemble many aspects of the traditional stereo algorithms, for instance, searching correspondences between two rectified images in a coarse-to-fine manner using an image pyramid. However, it is challenging to create a supervision dataset with ground-truth depth labels in the real world. Available datasets are still insufficient to cover the variety of daily stereo-imaging scenarios.

The other group of approaches is _self-supervised_-based to address the limitation of acquiring the true dense depth labels in stereo depth estimation [13][14][15][16][17]. The key advantages of the self-supervised approach are that it does not require any ground-truth depth labels for learning and that once a self-supervised network is pretrained with a large number of observation samples in advance, the network can infer depth at a faster speed than the traditional binocular stereo methods [18].

As mentioned earlier, there is no publicly available dual-pixel dataset with the ground-truth bidirectional disparity labels with a wide range of variation of focus-plane depths. We therefore adopt this self-supervised learning scheme for the dual-pixel disparity problem. Note that the traditional learning-based stereo scheme is not directly applicable to dual-pixel photography since the sign of a bidirectional disparity should change according to an arbitrary focus-plane depth, and the blur kernels should be anisotropic for left/right photodiodes, respectively. To the best of our knowledge, this is the first learning-based solution that can estimate a bidirectional disparity w.r.t. an arbitrary focus-plane depth in dual-pixel photography.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"," **Depth estimation from dual-pixel images.** The dual pixel image formation process is different from the conventional stereo image formation, which is based on the assumption that the left and right image have the same disparity. However, this assumption is not always true, as the dual pixel has a bidirectional disparity that includes both positive and negative values, depending on the focus plane depth in an image. To address this issue, several methods have been proposed to estimate the inverse depth or blurriness map from a dual-pixels image. For example, [1][2][3][4][5] estimate the depth map from the DP image by using the defocus blur. [4] propose a parametric point spread function to model the deffocus blur in DP images. [3] propose an end-to-end DP-based depth and blur network to jointly estimate depth and restore the image. [2] propose to use the symmetry property of the DP blur kernels to improve the depth estimation performance. [5] use the depth information from a single-camera mobile phone to synthesize shallow depth-of-field images with a single mobile camera.


**Deep learning for stereo matching.** Recently, deep learning has been widely used in stereo matching [6][7][8][9][10][11][12][13][14][15][16][17][18][19]. [6] propose the first deep learning based stereo matching network, PSMNet, which uses a 3D convolutional neural network to regularize the cost volume. [7] propose GC-Net to incorporate geometry and context information for finding correspondence in ill-posed regions. [8] introduce a two-layer guided aggregation network to capture the local and global context information. [9] propose RAFT-Stereo, a multi-level Convolutional GRU-based architecture for rectified stereo. [14] introduce the H-Net, a Siamese autoencoder architecture for unsupervised stereo depth estimation that leverages epipolar geometry to refine stereo matching, which allows mutual information between rectified images to be extracted. [17] propose left-right consistency loss to train a monocular depth estimation network. [15] leverage an image domain learning combined with stereo epipolar constraints to learn a CNNs architecture for matching cost computation. [16] present a framework for learning stereo matching costs without human supervision. [13] propose CoT-stereo to solve the occlusion"," Explanation by Gradient[1][2][3] relies on the gradients of the output generated by the network. The drawback of this method is that it requires full access to the model, which is a limitation for black-box models, and the prediction confidence on the gradients is weak.

Saliency Map[4][5][6][7] is based on the perturbations of image, where it relies on the gradients of the predictions generated by the network to obtain a saliency map to show where the network is likely to focus on. A drawback of this method is that the optimization of perturbations is hard and many assumptions and constraints are required to obtain an acceptable result [8][5][6].

Activation Map is to view the features of each node as a concatenation of local decision boundaries of all the samples at that node. The drawing method for the activation map is easy to realize but hard to obtain an activated area for each decision boundary due to the similarity between samples [9].

Concept Map[10][11] relies on the model's concept representations to explain, which can provide the basis for verifying and reasoning the model's decision. The drawback of this method is that its computational cost is high, and the explanation results of different samples are difficult to compare and analyze.

Visual ConceptGraph[12] aims to explain by visual concept graphs, which has the advantages of reducing the influence of the appearance difference between the test set and the training set, and can be used to analyze the relationship between samples. The drawback of this method is that it requires high-performance CNNs and obtaining high-quality visual concepts may require additional time-consuming data pre-processing and feature selection.

Weighted Gradient [13][14][15] is to weight the gradients of the classifier based on the importance of the pixels, and then calculate the weighted gradient to get the visual attention map. The main disadvantage of this method is that the gradient is a vector-valued quantity that is hard to visualize, and the calculation process is long and costly.

The perturbation-based explanation methods [16][17] use local perturbations to find a region or pattern that can generate a large change in the model's output. This method has advantages in simplifying the explanation process, but the changes in the results of different samples are difficult to analyze and compare.

In this work, we combine a variety of methods and propose the STCE framework, and obtain the feature explanation of each feature. Furthermore, we propose a novel method of score-weighted attribution maps which can be used to explain the influence of each feature on the overall network prediction.

",,"<<In the field of depth estimation and disparity estimation, significant research has been conducted on utilizing various techniques to improve the accuracy and performance of computational photography. One key area of focus has been on leveraging dual-pixel technology to estimate depth from a single camera [1]. Previous research has shown that traditional stereo algorithms and learning-based depth estimation techniques underperform when applied to dual-pixel data, which led to the development of novel approaches that address the inherent ambiguity in depth estimated from dual-pixel cues [2]. These studies have demonstrated a significant improvement in depth estimation accuracy when utilizing dual-pixel imagery, leading to the development of new data-driven approaches and models to mitigate the challenges of dual-pixel photography [3]. 

Another important direction of related work is the exploration of dual-pixel sensors' capabilities and their impact on depth estimation. Studies have proposed mathematical models and data-driven networks that exploit the formation process of dual-pixel images to enhance depth estimation [4]. Additionally, the use of dual-pixel technology to computationally synthesize shallow depth-of-field images with a single mobile camera has been explored, demonstrating the potential for practical applications of dual-pixel photography in real-world scenarios [5].

Furthermore, the application of deep learning techniques and convolutional neural networks (CNNs) has significantly advanced the field of depth and disparity estimation [6]. Several works have proposed novel architectures for stereo matching networks that leverage global context information and 3D convolutional neural networks to improve disparity estimation accuracy [6]. Additionally, the development of unsupervised learning methods for stereo matching using correspondence consistency and left-right consistency has shown promising results in addressing the challenges associated with obtaining large quantities of ground truth depth data for training [17][15].

The use of advanced techniques such as semi-global matching (SGM) for stereo processing, mutual information-based matching costs, and post-processing steps for improving the accuracy and robustness of depth and disparity estimation has also been investigated [18]. These approaches have demonstrated notable performance in stereo processing and disparity estimation, providing valuable insights for advancing the field of computational photography. >>"
2398,2398," **3D Object Detection.** 3D object detection aims to localize and classify 3D objects in the 3D space. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15] are mainly based on the two-stage pipeline, which first generates a set of 2D proposals and then refines them in the second stage. Recently, 3D-CNN [2] and PointRCNN [11] are proposed to directly predict 3D bounding boxes from 3D point clouds. However, these methods require a large amount of labeled data, which is expensive and time-consuming to obtain.

**Semi-Supervised Learning.** Semi-supervised learning (SSL) [16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33] aims to improve the generalization ability of the model with limited labeled data. In this paper, we mainly focus on SSL for 3D detection. Pseudo-Labeling (PL) [21] is the most widely used method in SSL, which generates pseudo-labels for the unlabeled data to supervise the training of the student model. It has been widely used in both 2D and 3D detectors [32][31]. For 2D detectors, SESS [31] and U-SSOD [26] adopt the teacher-student paradigm, where the teacher model is trained with pseudo-labeled data and the student network is trained using unlabelled data. The teacher model learns to predict the pseudo labels of unlablated data, and then the student models is trained to predict pseudo labels for unlabeled data. Different from these methods, we propose a novel noise-resistant instance supervision module for SSL in 3D detector, which can be easily integrated into existing SSL frameworks.

 and [33] are the two most related works to our work. Specifically, [32] proposes to use the teacher network to generate pseudo-labels and then use them to train a student network. In contrast, we design a novel instance-level noise-resilient regularization module to handle the noisy data. Moreover, we introduce dense pixel-wise feature consistency constraints to eliminate the negative impact of noisy labels. In addition, instead of directly applying vanilla pseudo labels as in [32], we propose to use them as the instance supervision for better generalization.

 is a representative work of SSL for 2D detection, which uses a teacher model and a student model to jointly train the two models. Differently, our method is a teacher-free method, which does not require any extra supervision from the auxiliary network.

 uses a multi-stage training strategy, which consists of two stages: 1) training the teacher and student networks simultaneously, and 2) fine-tuning the two networks. The main difference between our method and theirs is that our method does not need any additional supervision from auxiliary networks.

 firstly introduces a new loss function for the"," 3D object detection aims to detect instances in the 3D space, which is of great importance in various applications, such as autonomous driving, navigation robot, and augmented reality. Current LiDAR detectors [1][2][3] mainly voxelize the point cloud into a BEV or voxel representations and adopt traditional 2D convolutions to predict bounding boxes. To improve the inference speed, some works [4][5] attempt to detect objects directly through the range view. [6] combines both BEV and range-view features together to achieve better performance. VoxelNet [2] adopts PointNet [7] to extract local features through raw point clouds and fill them into the predefined voxel space. PointPillar [8] directly processes points inside a pillar to naturally formulate BEV feature representations, establishing an ultra-fast baseline detector. Inspired by CenterNet [9], CenterPoint [1] introduces a center-based label assignment strategy in 3D object detection, achieving competitive detection accuracy among various approaches. In addition to dense one-stage detectors, many works [10][11] apply an R-CNN-style two-stage detection paradigm. Point R-CNN [11] propose 3D RoIAlign to aggregate regional features based on the proposals and then refines the detections. PV-RCNN [12] and Voxel R-CNN [13] construct two-parallel branches to extract both point-level and voxellevel features to enjoy the best of each. However, independently localizing moving instances can introduce misalignment noise, 3D-MAN [14] and MPPNet [15] leverage multi-frame information to further enhance predictions with temporal knowledge.

Semi-supervised learning is an important task in leveraging easy-to-access unlabeled data to improve the supervised model. Most current SSL methods [16][17][18][19] involve adding additional supervision on unlabeled data to regularize the learning of the model. Among them, pseudo-labeling is a popular pipeline [20][21], where unlabeled data is firstly labeled with a supervised model, and then acts in a common training paradigm. In order to guarantee the quality of the generated labels, [21] often filter them with a hard threshold based on the classification score. In addition to hard pseudo-labels, NoisyStudent [22] explores soft supervision to avoid ambiguity problems. Besides, it injects different augmentations to the student and teacher models, to encourage consistency regularization. In light of this, plenty of approaches [23][24][25] enforce the model to predict similar results when applying various input permutations. Such strategies are also verified on 2D object detection, where [26] borrows the idea from FixMatch to achieve promising performance. MUM [27] introduces Mix/UnMix augmentation, enforcing students to reconstruct unmixed features for the mixed input images.

Semi-supervised learning (SSL) has been rapidly developed in both classification and object detection domains and obtains promising results in recent years. There are two main streams in SSOD: consistency learning and pseudo labeling. Consistency-based works [28][29] apply data augmentations/perturbations to the input, which forms natural regularization for the network predictions. Such a consistency-learning target enforces the model to acquire valid information from the unlabeled data, therefore improving the performance. SESS [30] is the first work to attempt such a paradigm on 3D object detection, where the classification and regression predictions are matched through L2 distance and supervised with the similarity loss on the teacher and student. Inspired by Mean Teacher [28], it also adopts the exponential moving average (EMA) technique to further boost the performance. Apart from the consistency-based approaches, pseudo-labeling is another solution [31][32]. Most PL works pay attention to the quality enhancement of pseudo labels. 3DIoUMatch [32] proposes to learn the IoU of the network predictions and utilize it to adaptively filter the low-quality pseudo labels. In order to remove the duplicate score threshold search, Proficient-Teacher [33] introduces a novel clustering-based box voting module, replacing the hand-crafted NMS process. Different from the previous approaches, our method views pseudo-labeling as a noisy learning problem, therefore delivering more generalization against noisy pseudo-data.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]"," **3D Object Detection.** 3D object detection has been extensively studied in recent years. Most of the existing methods are based on the point-based paradigm [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15], where the input is a point cloud. For example, CenterPoint [1] first detects the keypoints of objects using a keypoint detector and regresses to other attributes, such as 3D size, 3D orientation, and velocity, and then refines these estimates using additional point features on the object. 3DSSD [3] proposes a lightweight anchor-free 3D single-stage detector, which uses a single anchor head and an anchor assignment head to achieve comparable detection accuracy with voxel-based methods. CenterNet [9] detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. RangeDet [4] introduces a range-view-based detector to address the scale variation between nearby and far away objects. RangeRCNN [5] utilizes the dilated residual block to better adapt different object scales and obtain a more flexible receptive field on range image.

**Semi-Supervised Learning.** Semi-supervised learning (SSL) has been widely studied in computer vision [16][17][18][19][20][21][22][23][24][25][26][27][28][29][30][31][32][33][34][35]. Pseudo-labeling [21][20] is a popular method for SSL, which generates pseudo-labels for unlabeled data and uses them to supervise the training of the student model. For instance, FixMatch [21] uses a teacher model to generate pseudo labels for the student network, which is then used to train the teacher model. To avoid confirmation bias, FlexMatch [16] proposes Curriculum PseudoMatch [20] to flexibly adjust thresholds for different classes at each time step to let pass informative pseudo labels to the student. Recently, some works [24][29] propose to use MixUp [24] to generate the pseudo labels. However, MixUp is not suitable for 3D detection due to the large number of points in 3D point cloud, and it may introduce misclassified labels or loose localized predictions. To address this, we propose NoiseDet, a simple yet effective framework for semi-"," 3D object detection aims to localize and classify 3D objects in a 3D scene. Existing approaches could be roughly divided into two categories, representation based methods and proposal based methods.

Representation-based methods aim to improve the representation capability of backbone network on unlabeled data. CenterNet [1] proposed to detect objects by fusing the RoI features with the CenterNet features. Followed by this, VoxelNet [2] and SSD 3D  achieved better performance by performing the 3D detection in a voxelized representation. Based on the success of voxelization, point-based methods [3][4][5][6][7] were proposed to estimate object locations in 3D space. Wang [8] introduced PointPillar [8] and Fast Point RCNN [10] to further improve the detection accuracy. Based on this framework, several methods [9][11][12] were proposed to improve the detection quality with keypoint feature and augmented features. Recently, DAC  improved the point based detectors with a relative dense anchor generation strategy. Multi-frame based methods [13][14][15] achieved the state-of-the-art detection performance with more temporal information on multiple frames.

Auxiliary tasks-based methods [16][17] achieved the best performance by extending 2D pre-trained networks on unlabeled data to conduct effective learning on auxiliary tasks. Many semi-supervised learning methods [18][19][20][21] also leverage the large-scale data to train deep networks by constraining the output feature similarity. However, the previous methods ignore the inherent characteristics of 3D object detection such as occlusion and severe object localization errors. Therefore, they often face severe instability and poor performance.

Semi-supervised learning (SSL) aims to reduce the annotation cost for large-scale datasets. A comprehensive survey of self-supervised learning [18] provided a rich source of research for SSL. Noise-resistant methods [22][23] improved the detection performance by training on large-scale noisy datasets. Recent SSL methods [24][25][26] improved the accuracy of detectors by providing pseudo-labeled samples as a form of supervision on unlabeled datasets.

The earliest semi-supervised SSL methods [27][28] in 3D object detection fields improved detection performance with the noise-resistant networks. Recent methods [29][30][31][32][33] took point cloud-based 3D object detection as the core. Li [31] proposed PseCo to learn different scales of label features, and VoteNet  leveraged IoU predictions for more robust labels. A renewable adaptive pseudo labeling algorithm was introduced in 3DIoUMatch [32]. Numerous",,"<>
In the field of 3D object detection, recent advancements have led to the development of various semi-supervised learning approaches to mitigate the dependence on a large number of labeled samples. The utilization of pseudo-labels has proven to be a critical strategy in semi-supervised 3D object detection (SSOD) [1]. Pseudo-Labeling (PL) involves generating pseudo-labels using a teacher model and leveraging them for semi-supervised detection. However, the use of vanilla pseudo-labels has its limitations, such as the introduction of misclassified labels and loose localized box predictions, which can sub-optimally impact detection performance [2]. To address these issues, recent work has taken a noisy learning perspective, focusing on incorporating a noise-resistant instance supervision module for improved generalization [3]. This involves softening the classification targets by considering the quality of pseudo labels, converting the regression task into a probabilistic modeling problem, and integrating dense pixel-wise feature consistency constraints to mitigate the impact of noisy labels.

In addition to incorporating noisy learning perspectives, recent work has also focused on leveraging advanced network architectures and learning methods specifically tailored for 3D object detection. For example, the development of CenterPoint, a framework for 3D object tracking that effectively detects object centers using a keypoint detector and refines estimates using additional point features, has showcased state-of-the-art performance on the nuScenes benchmark [4]. Similarly, approaches like VoxelNet, which unifies feature extraction and bounding box prediction into a single stage end-to-end trainable deep network, have outperformed existing LiDAR-based 3D detection methods on benchmarks like KITTI [5]. These advancements underscore the importance of tailored architectures and learning methods for achieving superior performance in 3D object detection tasks.

To address the challenges of point cloud sparsity and variable point density, the proposed end-to-end multi-view fusion (MVF) algorithm offers an innovative solution by synergizing bird's-eye and perspective views, effectively learning to utilize complementary information from both viewpoints for optimized feature encoding and context fusion [6]. Similarly, the introduction of PointNet has demonstrated the effectiveness of directly consuming point clouds in a manner that respects the permutation invariance of points while achieving strong performance for object classification, part segmentation, and scene parsing [7]. These point-cloud-centric advancements highlight the significance of novel approaches tailored specifically for the unique characteristics of 3D data, showcasing the potential for further improvements in the field of 3D object detection.

The emergence of novel semi-supervised learning methods tailored for overcoming noisy label challenges and enhancing generalization has also made significant strides. For instance, recent work has introduced FlexMatch, a semi-supervised learning algorithm that leverages noisy labels and features curriculum pseudo-labeling to achieve state-of-the-art performance on SSL benchmarks, especially in scenarios with limited labeled data or challenging tasks [16]. These advancements underscore the potential of combining semi-supervised learning with noise-resistant training paradigms to achieve robust and optimized performance in the context of 3D object detection. These approaches reflect the ongoing exploration of innovative semi-supervised learning methods uniquely designed for the challenges posed by 3D object detection tasks [17]. The development of robust semi-supervised methodologies tailored for handling specific domain challenges can significantly contribute to advancing the effectiveness of 3D object detection in semi-supervised settings."
5551,5551," In recent years, scene text detection has witnessed rapid progress due to the recent development of convolutional neural networks (CNNs). Most of the existing methods can be roughly divided into two categories, _i.e._, segmentation-based and detection-based methods.

Segmentation-based approaches [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] first detect text segments and then group them into words or text lines. For example, MSR [6] and MSR++ [3] first generate a coarse text bounding box and then refine it with a multi-scale shape regressor. TextField [9] and TextFuseNet [10] use a deep direction field to model the irregular shape text. TextRay [12] and ContourNet [13] use contour-based geometrical modeling to detect arbitrary-shaped scene text. However, these methods require a large amount of annotated training data, which limits their deployment in real-world scenarios. Recently, pretraining approaches based on vision language models have made effective progresses in the field of text detection. For instance, CLIP [24] uses a large-scale contrastive language-image pre-training (CLIP) model to directly predict word-level bounding boxes without pretraining process. In this paper, we propose a novel method, termed TCM, to turn a CLIP model into existing scene text detectors.

 proposed a novel framework to improve the few-shot training capability of existing text detection methods. The key idea of this method is to use a pre-trained model to guide the training process of a new text detector. In contrast to these methods, our method does not require any pretraining and can be applied to improve existing text detectors without any additional training process.

 first proposed an end-to-end framework for text detection based on CLIP. The model consists of two sub-networks, namely, a language model and a vision model. The language model is used to learn a mapping from the input image to the output text representation. The vision model is then used to extract visual features from the text representation, and the visual features are used to train the language model to predict the text boxes. The main difference between our method and these methods is that our model does not need to be pretrained. In fact, our model can be directly applied to any existing text detector without any training process, which significantly improves the performance of the baseline method on 4 benchmarks.

 also proposed a new framework, termed TextNet, which is based on the framework of TextNet. TextNet uses a two-stream network to extract text features and a layout model to generate text proposals. The layout model first generates text proposals and then predicts text boxes for each proposal. The text boxes are then grouped into words and text lines by a text-segmentation network. The proposed method is different from TextNet in two aspects. First, TextNet does not"," Unimodal Scene Text Detection.Unimodal scene text detection represents the method directly adopts the bounding boxes annotation only . It can be roughly divided into two categories: Segmentation-based methods and regression-based methods. The segmentation-based methods usually conduct pixel-level [1][2][3][4][5][6], segment-level [7][8][9][10][11], or contour-level [12][13] segmentation, then grouping segments into text instances via post-processing. The regression-based methods [14][15][16][17][18][19][20][21] regards text as a whole object and regress the bounding boxes of the text instances directly.

Cross-modal Assisted Scene Text Detection.Unlike unimodal based scene text detection, cross-modal assisted scene text detection aims to make full use of cross-modal information including visual, semantic, and text knowledge to boost the performance. Wan _et al._[22] utilized an image-level text recognition pretraining tasks to enhance backbone via the proposed self-attention based text knowledge mining mechanism. Song _et al._[23], inspired by CLIP, designed three pretraining fine-grained cross-modality interaction tasks to align unimodal embeddings for learning better representations of backbone. Xue _et al._ jointly learned and aligned visual and partial text instances information for learning effective visual text representations via the proposed weakly supervised pretraining method. Long _et al._[24] proposed an end-to-end model to perform unified scene text detection and visual layout analysis simultaneously. The above methods explicitly leverage text or visual information to assist text detection. Instead, our method focuses on improving the performance results by turning a CLIP model into a scene text detector via leveraging pretrained text knowledge.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]"," **Scene text detection.** Scene text detection has been extensively studied in recent years. Traditional methods [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24] mainly focus on designing novel post-processing techniques to improve the performance of the detector. For example, [9] proposes to learn a deep direction field to separate adjacent text instances. [12] proposes a top-down contour-based geometric modeling and geometric parameter learning within a single-shot anchor-free framework. [13] proposes the local texture information of proposal features in two orthogonal directions and represents text region with a set of contour points. [21] proposes one novel Fourier Contour Embedding method to represent arbitrary shaped text instances in the Fourier domain. [14] proposes an iterative refinement module to refine the text proposals. [20] proposes EAST to detect text instances of extreme aspect ratios and multi-orientation. [19] proposes FCN to predict the centroid of each character and then combine the character components with a fully convolutional network.


**Pre-training approaches.** Recently, pre-trained models have been widely used in the field of text detection [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, etc. For instance, [22] proposes STKM to pre-train the model on SynthText dataset and then fine-tune it on the target dataset. [23] uses the pre-training approach to improve text detection performance. [24] introduces the task of unified scene text detection and layout analysis. [25] introduces a hierarchical scene text dataset and proposes a unified model that is able to simultaneously detect scene text and form text clusters in a unified way. [15] introduces an attention mechanism to suppress the background interference in the convolution feature maps. [27] proposes single shot text detector with an attentional map and a hierarchical inception module. [10] proposes TextFuseNet to perceive texts from character-, word- and global-level representations, and then introduces a novel text representation fusion technique to help achieve robust arbitrary text detection, which can effectively describe texts into individual characters while still maintaining their general semantics.

"," **Human Mesh from Image.** Modern methods [1][2][3] for recovering the human body shape and pose from an RGB image are mostly based on a learned parametric body model, which represents the human body shape and pose by a set of parameters, _e.g_., SMPL [1]. To reduce the complex and uncategorical manifold of SMPL parameters, an active shape model (ASM) [4] and its variants are often adopted to represent the dynamic human shape by a set of automatically selected 100-200 parameters. However, these methods cannot recover the detailed human shape under occlusion [5], thus unable to recover the 3D human mesh.

Recently, a few works [6][7][8] have tried to obtain the detailed human mesh from an RGB image. SPIN [7] is the pioneering work, which recovers the 3D human body mesh from a single RGB image by training an image reconstruction network to solve an energy minimization problem, using the body parameters estimated from the 3D human mesh as an additional supervision. SMPLoss [8] proposes a joint optimization framework, which can obtain the 3D human mesh under complex backgrounds and other challenging scenarios via learning to predict both the human body pose and the mesh from a single RGB image. HMR [6] presents an end-to-end framework for recovering the 3D human mesh with a deep-learning based joint optimization method. GANimation  uses a generative adversarial network (GAN)  to learn a 3D-GAN that can generate human meshes from an image. SHREC  learns a loss function for the human mesh recovery by iteratively predicting the human mesh, 3D pose and body-shape in one forward pass, where the refinement-training method reduces the mis-alignment of the predicted human body mesh. MDN  proposes to learn a discriminative representation from a pre-trained SMPL model, where the key idea is to leverage a small dataset of single-image-captured meshes to facilitate training.

**Human Mesh from Video.** Learning to predict 3D human pose, shape and camera parameters from video has been a hot research topic [9][10][11][12][13][10].

Liao _et al_.  propose to model the temporal dependency of human body and motion, where the sequence is treated as a frame-by-frame prediction problem. The proposed method then progressively estimates the shape, pose and camera parameters in each temporal window by a joint optimization method. Zhou _et al_. [13] and Ma _et al_. [10] propose to focus on the temporal consistency issue of human motion, where a self-attention network [11] and motion continuity attention module [13] are designed to capture the temporal information. For a",,"<The field of scene text detection has seen significant advancements with the proliferation of deep learning models and pretraining approaches. For instance, Liao et al. [3] proposed the Differentiable Binarization (DB) method, which integrates the binarization process into a segmentation network, leading to enhanced text detection performance. Similarly, Wang et al. [7] introduced a character region awareness method that effectively detects text areas by exploring each character and the affinity between characters. These segmentation-based methods have demonstrated superior performance in detecting arbitrary-shaped texts.

Moreover, recent works have addressed the challenges of speed and accuracy in text detection. For instance, Wang et al. [2] presented a method with differentiable binarization and adaptive scale fusion, which achieved state-of-the-art results in terms of both detection accuracy and speed. Additionally, TextFuseNet proposed by Liu et al. [10] utilized richer fused features for more robust arbitrary text detection, showcasing competitive performance on various datasets.

Furthermore, several works have focused on addressing specific challenges in text detection, such as the detection of multi-oriented or curved texts. Wang et al. [18] presented the LOMO method, which progressively localizes text multiple times and addresses complex text layouts. Wu et al. [9] introduced TextField, a novel text detector for irregular text detection, which achieved a large margin of improvement on datasets containing curved texts. Additionally, Zou et al. [21] proposed Fourier Contour Embedding (FCE) for arbitrary-shaped text detection with the Fourier domain representation, demonstrating superior performance on challenging highly-curved text subsets.>"
1463,1463," **Vision Transformer.** Transformer [1][2] has achieved great success in natural language processing (NLP) tasks [1]. Inspired by the success of Transformer in NLP, a number of works [3][4][5][6][7][8][9][10][11][12][13][14][15][16] have applied it to vision tasks. ViT [3] is the first work to apply Transformer to the computer vision domain. It firstly applies the self-attention mechanism to the image domain and achieves the state-of-the-art performance on image classification and object detection tasks. However, ViT suffers from high computational complexity due to the quadratic complexity of the global attention. To reduce the computation complexity, some works [7][14] propose to use window attention to reduce the receptive field size and achieve better performance. Swin [11] and CSWin [12] use shifted windows to reduce computation complexity and achieve comparable performance with CNNs. Recently, Wang _et al._[13] propose a stand-alone self attention to replace the window attention and achieve competitive performance with convolution.

**Local Self-Attention.** Local attention is a new type of attention mechanism in Vision Transformer, which can be viewed as an extension of convolution to the local region of the input image. It has been widely used in recent works [8][15] to improve the performance of local feature extraction. Wang [15] propose Neighborhood Attention (NA-SA) and Neighborhood-SA-Transformer [16] to learn the local feature representation by calculating the similarity between the query and its neighboring pixels. Wang  propose a local convolution module to capture long-range dependencies in the feature maps. Liu [8] propose Deformable Attention (DA) to learn local feature representations with a deformable attention module. Liu  design a new local attention module, which is based on the convolution operation and achieves better performance than NA-SA and NA-NA-NA. In this paper, we propose a novel sliding window attention module to achieve high efficiency, flexibility and generalizability.



Our work is also related to some recent works on the design of local attention modules. For example, Liu [15], Wang [14] and Wang [16], propose to learn a local feature map by computing the weighted sum of the features of each pixel and the neighboring pixels in a local region. However these local attention methods either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In contrast, our proposed slide attention module can leverage common convolution operations to achieve better efficiency and flexibility.

 propose a new attention mechanism based on relative position encoding (RPE) [5] to capture sequence ordering of input tokens. The relative position encodes the relative positions of the tokens in the input sequence and is used as a feature map to guide the feature learning. In our work, we design a deformed shifting module to adaptively learn the position-aware local features.

 design a"," Transformer and the self-attention mechanism have shown great progress in the field of Natural Language Processing [1][2] and successfully applied to vision tasks thanks to the pioneering work of Vision Transformer [3]. Following its path, researchers have extended Vision Transformer models along various directions, including data efficiency [4], position encoding [5], and optimization [6]. To better adapt Vision Transformers to downstream tasks, several works focused on investigating pyramid model structures, and show advanced performances over convolution-based approaches. PVT [7] considers sampling sparse locations in the feature map as key and value pairs. DAT [8] takes a further step and shifts fixed locations toward different directions in a data-dependent way. MViT [9][10] considers the pooling function on the input to obtain key and value pairs, which can be seen as a lower resolution of the feature map. Other approaches adopt an alternative strategy and restrict the attention to carefully designed patterns. Swin Transformer [11] designs non-overlapped windows and shifts windows between consecutive blocks. On this basis, CSwin Transformer [12] adopts a cross-shape window to further improve model capacity.

By constraining the attention receptive field of each query in its own neighboring pixels, local attention inherits the advantages from traditional convolution including local inductive bias and translation-equivariance [13]. Researchers follow this path and target improving the efficiency of local attention. HaloNet [14] combines window attention with local attention by first dividing the input into blocks and considering neighborhood windows instead of pixels. Another direction is to design CUDA kernels with high inference speed. SAN [15] designs a novel patchwise attention pattern and achieves better performances based on convolution architectures. NAT [16] adopts neighborhood attention and specifically considers situations for corner pixels. Nevertheless, current local attention models either use inefficient Im2Col function and endure huge increase in inference time, or rely on carefully written CUDA kernels that restrict applicability on CUDA-free devices.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"," **Vision Transformer.** Transformer [1][2] is firstly proposed in [2] for natural language processing (NLP) tasks and has been widely used in computer vision [3][4][5][6][7][8][9][10][11][12][13][14][15][16]. The self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. In this paper, we focus on the local self-Attention module, which is a key component of ViT.

**Vision Transformation.** ViT [3] is the first ViT-based model to achieve state-of-the-art performance on ImageNet classification. It is based on the Transformer architecture [2], which is composed of two self-contained blocks: a self-extractor and a self attention module. The self attention mechanism is first proposed in  and has achieved great success in NLP tasks. The Transformer is also widely used for vision tasks [3]. The pioneering work [3], ViT, firstly applies the self-transformer to image classification and achieves state-level performance. The ViT model is trained on Imagenet dataset [4] and achieves excellent performance on COCO dataset [7]. However, ViT suffers from poor optimization stability [6]. To address this problem, the authors [4][7] propose to use convolutional stem [6] to replace the stride-p p*p convolution [6 in ViT and achieve better optimization stability. However, the convolution operation is not suitable for image recognition tasks. To further improve the efficiency, the recent work [5] proposes to use the relative position encoding (RPE) [5], which can reduce the computation complexity of the self attention. However the RPE is not efficient and may limit the receptive field of the attention mechanism. To solve this issue, the Deformable attention [8] proposes a deformable attention module to focus on relevant re-gions and capture more informative features. The cross-shaped window [12] and Swin [11] are proposed to reduce the computational complexity by splitting the input feature into horizontal and vertical stripes, respectively. The Swin Transformer uses the shifted window attention [11], which reduces the computational cost of self-supervised learning and achieves better performance. However these methods are designed for language tasks and are not"," **De-raining and desnowing.** Rain removal methods are widely divided into two categories [1], namely single-image de-raining and video-based de-raining, of which former is the most studied task. To improve weather classification performance, Zhang _et al_. [2] and Guo _et al_. [3] utilize sparsity to reconstruct clean patches by computing the similarity to dictionary atoms. Further, low-rank model is proposed to remove rain streaks from low-rank rain images [4][5]. Rain streaks are often seen as independent layers and thus decomposed [6]. Although these methods generate promising results on rain removal, their performance is severely limited to rainy weather type. Yang _et al_. [7] combine rain/snow detection and a guided image decomposition framework to deal with rain or snow by hierarchy based method. Zhao _et al_. [8] design a dual attribute-aware ranking network to better capture the relationship among weather types. Recently, a spectrum of deraining methods are developed to take full advantage of multi-task learning. Bao _et al_. [9] propose an end-to-end SA-RNN to learn a better rain mask from a carefully constructed real rainy dataset, which achieves state-of-the-art performance. Wang _et al_. [10] explore how to train a multi-stream CNN with a unified architecture that can handle multiple datasets with incremental data, showing promising performance. By adopting deep networks for deraining, Huang _et al_. [11] introduce two-stage knowledge learning based method and multi-contrastive regularization. However, these methods can not handle different weather types with a unified network, thus bring some disadvantages.

**Snow removal.** For the sake of more accurate removal, snow removal methods usually adopt different kinds of metrics. For example, Liu _et al_. [12] propose a sparse representation based method to obtain the correspondences between the clean and snowy images. Xue _et al_. [13] utilize a trainable real-time image restoration network to remove the adverse weather with a unified architecture. Recently, since transformers [14][15][16][17][18][19][20][21][22][23] are released, they are adopted to restore images affected by snow [24][25]. However, snow removal methods focus on removing snow streaks, so the ability to handle snow and rain mixture is limited.

**Multi-task learning.** Multi-task learning (MTL) has been widely applied to solve problems in different areas of computer vision [26][27][28]. For MTL, the most important learning principle is sharing information among the related tasks, and is applicable in many cases [29]. To maximize the task-relevant information, many CNN structures are designed for multi-task learning",,"<In recent years, attention mechanisms have become a key component in the field of computer vision, particularly in the development of Vision Transformers (ViT). A seminal work by Dosovitskiy et al. introduced the ViT model, demonstrating that transformers can surpass convolutional neural networks (CNNs) in image recognition tasks [3]. These breakthroughs in ViT have led to significant advancements, as evidenced by subsequent studies such as Swin Transformer by Liu et al. [11] and CSWin Transformer by Wang et al. [12]. These papers proposed novel transformer architectures specifically tailored for vision tasks, addressing concerns such as global self-attention, computational efficiency, and the modeling of long-range dependencies.

The study by Li et al. delved into the efficacy of self-attention as a standalone primitive for vision models and showcased the viability and advantages of using pure self-attention in ResNet models for image classification and object detection [13]. Furthermore, a paper by Shan et al. examined the scalability of local self-attention in parameter-efficient visual backbones, showcasing improvements in speed, memory usage, and accuracy of self-attention models compared to traditional convolutional models [14]. This work aligns with the increasing interest in exploring the potential of self-attention for image recognition, as demonstrated by a study conducted by Wang et al., which explored variations of self-attention and its effectiveness for image recognition, showing that self-attention networks may have significant benefits in terms of robustness and generalization [15].

Additionally, a notable contribution by Touvron et al. introduced a new attention mechanism called Neighborhood Attention (NA) and the corresponding Neighborhood Attention Transformer (NAT), demonstrating the linear time and space complexity benefits of NA compared to traditional self-attention [16]. This aligns with the goal of achieving more efficient and scalable sliding window attention mechanisms for vision tasks.

In summary, the evolution of attention mechanisms, especially in the context of Vision Transformers, has led to a rich landscape of research focusing on novel transformer architectures, the potential of self-attention as a standalone primitive, and the development of efficient and scalable attention mechanisms for vision tasks. These advancements collectively demonstrate the growing importance and potential of attention mechanisms in revolutionizing computer vision techniques.>"
271,271," **Out-of-distribution (OOD) detection.** The problem of OOD detection has been extensively studied in computer vision and natural language processing (NLP). [1] proposed a closed-set setting for OOD classification. [7] proposed an outlier detection method based on the maximum mean discrepancy (MMD) between the distribution of known and unknown examples. [8] proposed the Gaussian mixture model (GMM) to model the unknown distribution. [4] proposed to use the softmax probability distribution of unknown examples as the OOD score. [11] used the margin loss to train the model.

**Open set learning.** Open set learning has also been studied in the field of relation extraction. [13] used a Bayesian neural network to learn a distribution over possible relations. [12] proposed Graph Convolutional Neural Networks (GCN) to learn the distribution over relations.

 proposed an open-set learning method for the task of text classification.  proposed an unknown-aware training method for open set classification.

 used a generative adversarial network (GAN) to generate OOD examples.

 introduced a new generative model for the open set learning task of dialogue systems.

 presented a new open-domain learning method.

 studied the problem of outlier recognition in the context of natural language understanding.

 developed a new model for outlier classification in the open-space.

 designed a new OOD recognition method for image recognition.

 applied a new unknown detection method for dialogue system.

 defined an unknown detection task.

 investigated the problem in the domain of open-world learning.

"," **Open-set Classification**: The open-set setting considers knowledge acquired during training phase to be incomplete, thereby new unknown classes can be encountered during testing. The pioneering explorations in ([1]) formalize the open-set classification task, and have inspired a number of subsequent works, which roughly fall into one of the following two groups.

The first group explores model regularization using unknown data. [3] manually collect unknown data to train a \((n+1)\)-way classifier with one additional class, where \((n+1)^{th}\) class represents the unknown class. Instead of manually collecting unknown data, [2] generate feature vectors of unknown data using a generative adversarial network (). [4] use MixUp technique () to synthesize known data into unknown data.

The second group approaches this problem by discriminative representation learning, which facilitates open-set classification by widening the margin between known and unknown classes. MSP ([7]) is a maximum posterior probability-based baseline and ODIN ([5]) enlarges the difference between known and unknown classes by adding temperature scaling and perturbations to MSP. More recently, different optimization objectives such as large margin loss ([11]) and gaussian mixture loss ([8]) are adopted to learn more discriminative representations. [6]; [10]; [9] also impose gaussian assumption to data distribution to facilitate distinct unknown data.

**Open-set Relation Extraction**: Open-set RE is a pressing but underexplored task. Most of the existing RE methods manually collect NOTA data and adopt a \((n+1)\) way classifier to deal with NOTA relations ([12]; [14]; [13]). However, the collected NOTA data with manual bias cannot cover all NOTA relations and thus these methods cannot effectively deal with open-set RE ().

Our method avoids the bias and the expensive cost of manually collecting NOTA data by automatically synthesizing negative data. Compared with general open-set classification methods, our method takes relational linguistic rules into consideration and outperforms them by a large margin.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"," **Open-Set Recognition.** Open-set recognition aims to recognize unknown classes in the test set, which are not present in the training set. The open-set problem has been studied in computer vision [1] and natural language processing [2][3][4][5][6][7][8][9][10][11][12][13][14]. In this paper, we focus on the open- set relation extraction problem, which is more challenging than the open set recognition problem in vision.

**Outlier Detection.** Outlier detection aims to identify outlier input utterances that are different from the training data. The methods in this field can be categorized into two groups: (1) methods that rely on hand-crafted features [5][7] and (2) methods based on deep neural networks [1][10]. The methods based in the first group rely on the assumption that outlier utterances are misclassified or outlier examples. For example, ODIN [5] and ODIN++ [7] use the maximum softmax probability to detect unknown outlier samples. ODIN+ [5], which is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, has been shown to be more effective than ODIN-based methods [7]. ODIN uses the temperature scaling technique [7], which has been widely used in image classification [1]. ODin+ [7][5] is based upon the observation of that incorrectly classified examples tend to have higher maximum soft-max probabilities than erroneously classified examples. In addition of the class-conditional distribution on feature spaces of DNNs via Gaussian discriminant analysis (GDA) to avoid over-confidence problems, ODin++ [10] uses the Mahalanobis distance to measure the confidence score of whether a test sample belongs to OOD or not, and shows that it can achieve SOTA performance on OOD detection. In this work, we propose a novel unknown-aware training method, which synthesizes ""difficult"" negative instances to regularize the model.

 is a recent work on unknown intent detection [4], which uses self-supervised learning to generate synthetic OOD utterances for training. However, their method requires the OOD samples to be available during the training phase, which limits its applicability in real-world scenarios. In contrast, our method"," **Open-Set Detection:** Open-set detection has been studied in computer vision and natural language processing. We only focus on open-set detection that has a similar setting with ours. Open-set relation extraction, as a specific variant of open-set detection, is an important task in natural language processing. The goal of open-set relation extraction is to identify and distinguish the unknown relations from known relations.

[1] propose a 1-vs-set method to solve open-set relation extraction. Their method obtains state-of-the-art results using a binary SVM with a linear kernel. But it also suffers from the severe drawback that a linear kernel cannot well model the non-linear mapping in natural language.  propose a neural method using a matching layer to reduce the gap between the textual distribution of unknown relations and that of known relations. They use the cosine similarity between feature vectors to estimate the confidence score of relations. This approach is much easier to implement than 1-vs-set but may fail when dealing with text-based open-set relation extraction.

Open-set detection has also been studied in computer vision. These methods rely on deep generative models to generate pseudo outliers, such as GANs . Some approaches that are more related to ours do not need any prior knowledge of the unknown classes, as [4] generates pseudo outliers for in-scope intent detection by applying an auto-encoder to map input utterances into a latent code and then generating outliers from the code using a GAN. Another approach that is more related to ours is [2]. They modify an existing GAN architecture to introduce an identity mapping layer, then use the output of this layer as the initial condition of the generator and the discriminator. The modified GAN could be used to generate pseudo outliers in vision. But using the GANs as in [4] and [2] may have a negative effect on our objective to increase the ability of the model to identify known relations, especially when training data is limited. In contrast, our method directly adds small perturbations to original training instances and thus having a much smaller impact on known relation detection.

Unlike previous methods that rely on the adversarial loss to model the confidence score, our approach is more like the maximum likelihood estimation. Our method naturally avoids a possible mode collapse problem of adversarial learning.  does not provide any theoretical guarantee that the predicted probabilities and ground truth probability are equal.

**Out-of-Distribution Detection:** Out-of-distribution detection is a more general problem than open-set relation extraction. Existing methods for OOD detection can be roughly divided into three categories. [5] detects out-of-distribution examples by simulating the test scenario in training. To ensure that an image belongs to an outlier class, it needs to satisfy the following two conditions: 1) The maximum of model predictions for known classes is higher than the maximum of model predictions for unknown classes; 2",,
4709,4709," **Unsupervised Domain Adaptation (UDA).** UDA aims to learn a model that performs well on a target domain with labeled source data and unlabeled target data. Early UDA methods [1][2][3][4][5][6][7] focus on learning domain-invariant representations by minimizing the Maximum Mean Discrepancy (MMD) between the source and target distributions. Recently, adversarial learning based methods [8][9][10][11][12][13][14][15][16][17] have been proposed for UDA. In these methods, a domain discriminator is trained to distinguish the source domain from the target domain, while the feature extractor is optimized to confuse the discriminator. For example, [20][21][22][23][24][25][26][27][28][29] employ a two-player min-max game between a domain classifier and a domain-adversarial generator. In contrast, our method does not employ a domain adversarial generator and instead directly aligns the feature distributions.

 proposed a test-time adaptation (TTA) method for semi-supervised learning. TTA is a special case of UDA, where a model is trained on a labeled source domain and adapted to an unlabeling target domain. However, in TTA, a model does not have access to the labeled target domain during training. Therefore, it is necessary to adapt the model to the test domain without modifying its training procedures. In this work, we propose a novel feature alignment loss for TTA.

 introduced a semi-Supervised Learning (SSL) framework for semi supervised learning (SSL), where a pre-trained model is fine-tuned on a small amount of labeled data. SSL aims to improve the generalization ability of a model trained on labeled data by leveraging a small number of labeled labeled data during the pre-training phase. In the SSL framework, the labeled data are used as pseudo-labels to train the model on a limited number of unlabelable data. The main difference between SSL and TTA lies in that SSL is designed for SSL, while TTA aims to adapt a model to a new domain at test time without modifying the model's training procedure.

**Test-Time Normalization (TNT).** TNT aims to align the test distribution to the source distribution by normalizing the batch statistics [10][18]. Specifically, TNT [10] normalizes the mean and variance of the test data to match those of the training data. TTT++ [15] proposes a self-training strategy that alternates between training the model and applying TNT on the pseudo-labeled test data. In TTT, the model is encouraged to learn test-adaptive representations by performing TTT on both the labeled test data and the unlabelled test data at the same time.

 first introduced TNT for semi/supervised machine translation. TNT is a generalization of SSL. It aims to train a model on semi/weakly supervised machine translation tasks. Specifically, it aims to minimize the distance"," Existing UDA approaches [1][2][3][4][5][6][7] have addressed distribution shifts effectively by adapting to target domains at training time. UDA approaches generally assume that 1) source data is available during adaptation, and 2) we already know which target domain the models are adapted to. However, these assumptions sometimes do not hold in real-world scenarios. To address such concerns, approaches that adapt a model at test time have been proposed, not requiring access to the source data during adaptation [8][9][10][11][12][13][14][15][16]. Several methods [14][15] perform adaptation in an offline manner, predicting test samples after iterating multiple epochs over the entire set of the test samples (_i.e.,_ test-time training). These approaches modify the training procedure to have self-supervised losses (_e.g.,_ rotation prediction or contrastiveloss) and utilize them as proxy losses for adaptation. However, as also pointed out in Wang _et al._[8], it is not guaranteed that optimizing the proxy losses helps in improving the main task since they are not directly related to classifying images into categories. Addressing such concerns, test-time adaptation (TTA) methods [8][9][10][11][12][17] have been proposed. These approaches do not require any modification of the training procedures, allowing the algorithms to be applicable to a given pretrained deep learning network. TENT [8], a recent seminal work in TTA, proposed to update the modulation parameters in batch normalization [18] layers while minimizing the entropy loss, effectively mitigating distribution shifts.

Feature alignment is widely adopted in UDA studies to mitigate distribution shifts [19][20][21][22]. However, most of these approaches do not consider categorical information but rather match the source and target distributions globally. This may harm class discrimination performance since it does not guarantee class-to-class matching between two distributions [23]. Tacking the problem, various studies have proposed to align distributions in a class-discriminative manner [23][24][25][26][27][28][29]. This point of view is also relevant to test-time adaptation, and we design an effective loss that simultaneously mitigates the distribution gap while improving class discriminability.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]"," **Unsupervised Domain Adaptation.** Unsupervised domain adaptation (UDA) aims to learn a model that performs well on a target domain with unlabeled data, while minimizing the domain discrepancy between the source and target domains [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23][24][25][26][27][28][29]. Most UDA methods are based on adversarial training, which trains a domain discriminator to distinguish the source domain from the target domain and align the feature distributions of the source data and the target data. For example, [4][4] use maximum mean discrepancy (MMD) to align the joint distributions of multiple domain-specific layers across domains. [5][8] use similarity learning to learn domain-invariant features. [6] use the task-specific decision boundaries of the target samples to align feature distributions. [7] use a generative adversarial network (GAN) to learn feature representations that are invariant to the domain shift.

**Test-Time Adaptation (TTT).** Test-time adaptation (TTA) is an emerging paradigm that aims to adapt a model to unlabundant test data at test time, without access to any labeled source data during training. [14] propose Test-Time Training (TT), which uses pseudo-labeling [13] to train a model on a single unlabelled test sample and then updates the model parameters before making a prediction on the test sample. [15] show that TTT can deteriorate, instead of improve, the test-time performance in the presence of severe distribution shift. [17] propose AdaContrast, which uses contrastive learning to train an encoder-decoder model on the source dataset and then uses the pseudo-labels to train the target model. [16] propose T3A, which adapts a linear classifier on the last layer of a pre-trained network and then classifies each test sample based on its distance to the nearest-neighbor pseudo-prototype representation. [11] propose MEMO, which performs test-test adaptation via adaptation and data augmentation. [9] propose MixNorm, which estimates the batch-norm statistics of test samples and uses them to adapt the model during test time. [8] propose TentNet,"," **Crowd Counting.** State-of-the-art methods can be divided into two categories. One line of methods learns a density map by modeling Gaussian likelihood and optimize density map with iterative refinement in [1][2]. The other line of methods focus on tackling occlusions of the pedestrians by filtering out crowd from the background using human head/bounding box as a proxy [3][4][5]. Recent methods focus on refining the sparsely annotated dot map [6][7]. But all of these methods focus on the absolute accuracy of counting, there are very few of methods focus on the detection accuracy [8][9][10][11][12][13][14]. One pioneer work utilizes the detector to do the counting but without taking into account the uncertainty estimation.

**Semi-supervised Learning.** Since unlabeled data are available, several methods have been proposed to exploit unlabeled data for crowd counting. Early approaches merely utilize the unlabeled data to train a classifier on partial annotations [15]. Recent approaches employ the unlabeled data for feature learning with additional loss [16]. However, all of these methods do not estimate the uncertainty in model prediction.

**Uncertainty Estimation**. Several methods have been proposed to estimate uncertainty in semi-supervised learning. The early work for uncertainty estimation is proposed for domain generalization, where the goal is to learn a model to perform well across different datasets [17]. In deep learning, uncertainty is often estimated via a two-step approach, _i.e_. the first step produces a model that estimates the uncertainty, and the second step utilizes this model to approximate uncertainty [18][19][20]. Such methods are not suitable for semi-supervised learning. Recent works on the domain of uncertainty estimation propose two-step methods. Wang _et al._ estimated uncertainty in the semi-supervised setting by using bounding box transformation and localization variance [21]. The methods in [22] are designed for monocular 3D object detection. Recently, Lee _et al._ proposed a confidence estimation method for semi-supervised setting [20]. Our approach belongs to the second type of methods which estimates uncertainty by first training a probabilistic network and then performing the inference on both training data and test data. The surrogate task for semi-supervised crowd counting is to generate pseudo-labels, and is a novel strategy.

**Crowd Counting with Pseudo-Labels.** Our approach follows the pipeline of supervised learning for semi-supervised learning, _i.e_. a surrogate function is trained and used to estimate uncertainty. Earlier pseudo-label approaches utilize the supervised pseudo-labels for better model optimization [14]. Later, some methods propose to use unlabeled data as the pseudo-label. These methods utilize the unlabeled data to learn the unlabeled",,"<In recent years, deep learning has shown great promise in various domains, but the challenge of adapting deep neural networks to new data distributions remains a crucial issue. Test-time adaptation (TTA) has emerged as a potential solution to address this challenge by adapting a pre-trained model to unlabeled test data [1]. One approach is feature alignment, which aims to align the representation space of test samples to the source distribution. However, performing feature alignment in TTA is challenging due to the restricted access to labeled source data during adaptation [2]. The need to learn target representations in a class-discriminative manner, while mitigating distribution shifts at test, has spurred the development of novel approaches such as Class-Aware Feature Alignment (CAFA) [target paper].

Prior work on domain adaptation has explored techniques like domain separation networks [2], conditional transferable components [3], and joint adaptation networks [4]. These approaches focus on aligning representations, learning invariant features, or transferring domain-specific knowledge. Methods like unsupervised domain adaptation with similarity learning [5] and maximum classifier discrepancy [6] have demonstrated the use of classifiers and adversarial training to improve domain adaptation performance. Additionally, techniques such as Cycle-Consistent Adversarial Domain Adaptation [7] and fully test-time adaptation by entropy minimization [8] have contributed to enhancing model robustness in test-time adaptation scenarios. These studies show the breadth of strategies applied to address domain shift.

Other significant contributions include methods focused on test-time adaptation, such as MixNorm [9], Test-time Batch Statistics Calibration for Covariate Shift [10], and MEMO [11], which highlight the importance of test-time robustness and adaptation methods. These approaches offer insights into improving model performance when dealing with distribution shifts during testing. Moreover, work on unsupervised domain adaptation [12], semi-supervised learning [13], and test-time training with self-supervision [14] has expanded the understanding of leveraging unlabeled data and adapting models during testing.

Recent advancements have also witnessed the exploration of novel tasks like contrastive test-time adaptation [17] and associative domain adaptation [26], which integrate self-supervised learning, contrastive learning, and association losses to facilitate domain adaptation. Furthermore, the proposed Progressive Feature Alignment Network [23] explicitly addresses the need for progressive and effective feature alignment for unsupervised domain adaptation, emphasizing the importance of preserving cross-domain category consistency. These studies collectively contribute to the evolving landscape of domain adaptation and robustness in deep learning.>"
5146,5146," NLG evaluation is a fundamental task in the field of natural language processing (NLP) and has attracted increasing attention in recent years ([1]). Most of the existing evaluation metrics are based on reference-based methods, such as BLEU ([5]), ROUGE ([8]), and METEOR ([4]), which measure the quality of generated text by comparing the generated text with a reference text. However, these metrics are trained on evaluation datasets of specific tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. To address this problem, some recent works have proposed to use unsupervised evaluation metrics for NLG tasks. BARTScore ([6]) and UNION ([9]) use a pre-trained language model as the reference to evaluate generated texts, which is similar to BLEURT ([2]) and CTRLEval ([10]). However, BARTScore requires a large amount of human-annotated data for training, which limits its applicability in real-world applications. To improve the interpretability of evaluation metrics, MoverScore ([12]) uses context-aware embeddings and earth mover distance to measure the similarity between generated text and reference text, which can be regarded as a cross-encoder-based metric. To make the evaluation process more interpretable, ELECTRA () decomposes the evaluation task into several subquestions and uses the generated texts as evidence to obtain the final evaluation score. In addition, some works have also tried to improve the evaluation performance of NLG task by using multi-reference datasets ([3]; [7]; ). However, they still require human annotations for training. In this work, we propose a simple yet effective evaluation metric based on instruction-style question answering, which does not require any human annotations.


Instruction tuning ([15]; [16]; [14]; [17]; [18]; [19]) is a new learning paradigm that aims to improve generalization ability of PLMs. It is usually implemented by fine-tuning a large-scale PLM on a set of instructions and then following the instructions to solve a new task. Instruction tuning has been widely used in zero-shot learning ([13]; ). In this paper, we adopt the instruction-tuned PLMs as our reference for evaluating generated texts.

 is a recent work that proposes to use a PLM to solve the task of text summarization. It uses a language model to generate a natural language description for each sentence in the summary, and then uses a question-answering model to answer the questions about the generated summary. The generated summary is then compared with the ground truth summary to obtain a final evaluation result. Our work differs from this work in two aspects. First, we use the PLMs to generate the questions instead of generating the natural language descriptions. Second, we decompose the generated sentences into sub-questions to obtain an evaluation result instead of using the generated summaries as evidence.

 and  are the two most related works to our work. They both propose to use PLMs for summarization and text generation, respectively.

 aims to evaluate summarization tasks."," Evaluation is a long-standing task in the field of NLG ([1]), which becomes more critical with the rapid development of PLMs. There are two main categories of automatic evaluation metrics, i.e., untrained and trained metrics ([3]). Untrained metrics without training on specific datasets of evaluation tasks or related tasks aim to measure the relationship among source texts, generated texts, and reference texts via n-gram overlap ([5]; ; [8]), semantic similarity ([12]; ), or language modeling / masked language modeling scores ([6]; [10]; ). In comparison, trained metrics are commonly trained on the evaluation datasets to fit human scores ([7]; [2]) or distinguish human-written texts from negative samples ([9]; [4]), aiming to achieve higher correlations with human judgments on specific datasets. Among these metrics, there are some similar works which re-frame NLG evaluation as QA tasks and adopt the generated answers or generation probabilities as evaluation results ([11]; [4]).

The most similar work to our method is UniEval ([4]). UniEval re-frames NLG evaluation as a Boolean QA task and trains the evaluation model on the pseudo data constructed from the evaluation dataset and other related datasets in a unified Boolean QA format. Compared with UniEval, our method is untrained since we transform NLG evaluation to an instruction-style QA task that can be solved by instruction-tuned PLMs without further training. Also, our method can provide some evidence (i.e., the answers to decomposed subquestions) to interpret how the model reaches the evaluation result, instead of only providing a final evaluation score.

Instruction learning ([16]) which trains PLMs to follow human instructions has attracted much attention recently since it shows the strong zero-shot cross-task generalization ability. To improve instruction understanding, existing works adopt instruction tuning ([14]) which trains PLMs on massive tasks describedvia instructions with multi-task learning, such as FLAN [14]; [17], T0 [18], and InstructGPT [15]. Other works systematically study instruction tuning in specific areas such as dialogue systems [19] and multi-modal learning [13].

In comparison, our work is the first to explore the potential of instruction-tuned PLMs in the evaluation of NLG without further training. We show that equipped with well-designed input prompts and suitable question decomposition, instruction-tuned PLMs can sequentially measure the quality of each sentence and finally recompose all the sub-questions with their answers to obtain surprisingly great evaluation results in an unsupervised fashion.

","[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"," Evaluating NLG.Existing NLG evaluation metrics can be divided into two categories: human-centric and machine-learned metrics [1]. Human-centric metrics [5]; [8]; [1]; [7]; [2]; [3]; [6]; [4] evaluate the quality of generated texts based on human judgments. These metrics are trained on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. In contrast, machine-learnt metrics [2] do not require any training on specific evaluation datasets and can generalize to unseen tasks. However, these metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained.

Unsupervised evaluation metrics [11]; [12]; [10]; [9]; [11] are proposed to evaluate text generation without using any evaluation datasets. [10] propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. [9] propose UNION, a learnable unreferenced metric for evaluating open-ended story generation, which measures the quality without any reference. [12] propose MoverScore, which combines contextualized embeddings and Earth Mover Distance to measure the text quality. [6] propose BARTScore, a metric that evaluates text generation as a text generation problem. BARTScore is trained to convert the generated text to the reference output or the source text, and then it is used to calculate the score. [3] propose DEBDE, a multi-reference based metric, which is pretrained on a large-scale pretrained language model and then finetuned on a Reddit dataset. However these metrics are designed for specific tasks and dimensions, and they do not consider the generalization ability of the model.


Recent works [16]; [14]; [19]; [18]; [13]; [17]; [15]; [16] have shown that instruction-tuned language models can achieve good zero-shot performance on unseen tasks by fine-tuning on a collection of datasets described as instructions. [14] propose FLAN, a model that takes a pre-trained GPT-3 model and fine-tunes it on a set of tasks described via natural language instructions. FLAN achieves strong few-shot learning ability on a variety of tasks and outperforms the state-of-the-"," Large language models for instruction learningMany studies (; ; [2]; [1]) have looked into instruction learning, but they often assume the existence of human-written instructional prompts and mainly focus on optimizing prompts to achieve better performance. For example,  proposed a prompt evaluation metric that measures the ability of prompts to cover a task's knowledge. [3] and [2] formulated a problem setup for studying cross-task generalization ability. ,  and [4] proposed to use automatically generated instruction-following data for training LLMs, and  investigated to what extent the generated data can be used for promoting LLMs' cross-lingual capabilities. Most studies assume human-written prompts are given in the form of text examples, while we instead assume the input to the LLMs is the instruction definition and model performance depends on model understanding of the definition.

Prompt optimization for instruction learningPrompt optimization has been the dominant strategy in instruction learning, and previous works mainly focus on formulating or solving the problem of prompt optimization ([7]; ; [5]; [8]; ; [6]). [8] proposed a gradient-free edit-based search approach to improve prompts. They found manually reformulating tasks into more effective instructions can improve performance by 4.3% on average, but it requires substantial effort from humans. [6] proposed an algorithm to produce new prompts from the existing ones based on a gradient-guided search. [7] studied reframing techniques for manual rewriting of prompts into more effective ones.  proposed an algorithm to automatically obtain better prompts by optimizing it on all the training examples. Compared to prompt optimization, we propose to compress a task definition, making the system able to better understand tasks.

Evaluation criteria for instruction learningEvaluating instruction learning from the perspective of prediction accuracy has not been studied before. [9] used zero-shot accuracy as an evaluation criteria for the task definition and proposed a method to calibrate a model's prediction to have the same prediction on all different outputs.  proposed to study the information flow in a training process by evaluating the pedagogical qualities of a training process. However, it can be problematic to evaluate the effectiveness of a definition by checking only the accuracy of the model. Since a definition describes the task by combining a set of various words, it is possible that a definition does not include all relevant information to complete the task. It is thus important to understand what are the contents of a definition that are most essential to a task completion. [11] studied the role of demonstrations in in-context learning and found that the most important part is the input-label pair, the format of the sequence, and the label space. However, [11] trained their models with a single model instead of multiple models as in the standard instruction learning setup. In this paper, we study the role of instruction definitions, and for this reason, we also consider human annotations.

",,
